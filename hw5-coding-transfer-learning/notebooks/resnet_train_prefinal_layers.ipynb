{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 2 prefinal layers\n",
    "\n",
    "In ResNet18, the forward pass is:\n",
    "```\n",
    "def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.maxpool(x)\n",
    "\n",
    "    x = self.layer1(x)\n",
    "    x = self.layer2(x)\n",
    "    x = self.layer3(x)\n",
    "    x = self.layer4(x)\n",
    "\n",
    "    x = self.avgpool(x)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = self.fc(x)\n",
    "```\n",
    "We can train only the 2 prefinal layers (layer3, layer4) by loading the pre-trained model, then setting the `requires_grad` attribute of the parameters outside these two layers to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(layer):\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Samples: 0/5760, Loss: 4.717750072479248\n",
      "Epoch: 1, Samples: 32/5760, Loss: 4.6913251876831055\n",
      "Epoch: 1, Samples: 64/5760, Loss: 4.779818058013916\n",
      "Epoch: 1, Samples: 96/5760, Loss: 4.927428245544434\n",
      "Epoch: 1, Samples: 128/5760, Loss: 4.704293251037598\n",
      "Epoch: 1, Samples: 160/5760, Loss: 4.946402072906494\n",
      "Epoch: 1, Samples: 192/5760, Loss: 4.80712366104126\n",
      "Epoch: 1, Samples: 224/5760, Loss: 4.796818256378174\n",
      "Epoch: 1, Samples: 256/5760, Loss: 4.815865993499756\n",
      "Epoch: 1, Samples: 288/5760, Loss: 4.6611433029174805\n",
      "Epoch: 1, Samples: 320/5760, Loss: 4.5955400466918945\n",
      "Epoch: 1, Samples: 352/5760, Loss: 4.680673599243164\n",
      "Epoch: 1, Samples: 384/5760, Loss: 4.770688533782959\n",
      "Epoch: 1, Samples: 416/5760, Loss: 4.472278594970703\n",
      "Epoch: 1, Samples: 448/5760, Loss: 4.494872093200684\n",
      "Epoch: 1, Samples: 480/5760, Loss: 4.540658950805664\n",
      "Epoch: 1, Samples: 512/5760, Loss: 4.629475116729736\n",
      "Epoch: 1, Samples: 544/5760, Loss: 4.522205829620361\n",
      "Epoch: 1, Samples: 576/5760, Loss: 4.327990531921387\n",
      "Epoch: 1, Samples: 608/5760, Loss: 4.466837406158447\n",
      "Epoch: 1, Samples: 640/5760, Loss: 4.326505661010742\n",
      "Epoch: 1, Samples: 672/5760, Loss: 4.490858554840088\n",
      "Epoch: 1, Samples: 704/5760, Loss: 4.512996673583984\n",
      "Epoch: 1, Samples: 736/5760, Loss: 4.522268295288086\n",
      "Epoch: 1, Samples: 768/5760, Loss: 4.214163303375244\n",
      "Epoch: 1, Samples: 800/5760, Loss: 4.374087810516357\n",
      "Epoch: 1, Samples: 832/5760, Loss: 4.419887065887451\n",
      "Epoch: 1, Samples: 864/5760, Loss: 4.361236572265625\n",
      "Epoch: 1, Samples: 896/5760, Loss: 4.495641231536865\n",
      "Epoch: 1, Samples: 928/5760, Loss: 4.309957504272461\n",
      "Epoch: 1, Samples: 960/5760, Loss: 4.290599346160889\n",
      "Epoch: 1, Samples: 992/5760, Loss: 4.248483657836914\n",
      "Epoch: 1, Samples: 1024/5760, Loss: 4.384559631347656\n",
      "Epoch: 1, Samples: 1056/5760, Loss: 4.186560153961182\n",
      "Epoch: 1, Samples: 1088/5760, Loss: 4.1842241287231445\n",
      "Epoch: 1, Samples: 1120/5760, Loss: 4.187821865081787\n",
      "Epoch: 1, Samples: 1152/5760, Loss: 4.092676639556885\n",
      "Epoch: 1, Samples: 1184/5760, Loss: 4.113513469696045\n",
      "Epoch: 1, Samples: 1216/5760, Loss: 4.032754421234131\n",
      "Epoch: 1, Samples: 1248/5760, Loss: 4.371953010559082\n",
      "Epoch: 1, Samples: 1280/5760, Loss: 4.244162082672119\n",
      "Epoch: 1, Samples: 1312/5760, Loss: 4.1234307289123535\n",
      "Epoch: 1, Samples: 1344/5760, Loss: 4.227885723114014\n",
      "Epoch: 1, Samples: 1376/5760, Loss: 4.115753173828125\n",
      "Epoch: 1, Samples: 1408/5760, Loss: 3.9171695709228516\n",
      "Epoch: 1, Samples: 1440/5760, Loss: 4.018011569976807\n",
      "Epoch: 1, Samples: 1472/5760, Loss: 4.01894998550415\n",
      "Epoch: 1, Samples: 1504/5760, Loss: 3.83986759185791\n",
      "Epoch: 1, Samples: 1536/5760, Loss: 3.8884615898132324\n",
      "Epoch: 1, Samples: 1568/5760, Loss: 4.298206329345703\n",
      "Epoch: 1, Samples: 1600/5760, Loss: 4.087759494781494\n",
      "Epoch: 1, Samples: 1632/5760, Loss: 4.047949314117432\n",
      "Epoch: 1, Samples: 1664/5760, Loss: 3.979689359664917\n",
      "Epoch: 1, Samples: 1696/5760, Loss: 3.8705527782440186\n",
      "Epoch: 1, Samples: 1728/5760, Loss: 4.1346917152404785\n",
      "Epoch: 1, Samples: 1760/5760, Loss: 4.035809516906738\n",
      "Epoch: 1, Samples: 1792/5760, Loss: 3.9699840545654297\n",
      "Epoch: 1, Samples: 1824/5760, Loss: 3.8592748641967773\n",
      "Epoch: 1, Samples: 1856/5760, Loss: 4.033615589141846\n",
      "Epoch: 1, Samples: 1888/5760, Loss: 4.103029251098633\n",
      "Epoch: 1, Samples: 1920/5760, Loss: 3.911536455154419\n",
      "Epoch: 1, Samples: 1952/5760, Loss: 3.977407217025757\n",
      "Epoch: 1, Samples: 1984/5760, Loss: 3.7973389625549316\n",
      "Epoch: 1, Samples: 2016/5760, Loss: 3.644031047821045\n",
      "Epoch: 1, Samples: 2048/5760, Loss: 3.8166933059692383\n",
      "Epoch: 1, Samples: 2080/5760, Loss: 3.946944236755371\n",
      "Epoch: 1, Samples: 2112/5760, Loss: 3.9173107147216797\n",
      "Epoch: 1, Samples: 2144/5760, Loss: 3.9770100116729736\n",
      "Epoch: 1, Samples: 2176/5760, Loss: 3.9848904609680176\n",
      "Epoch: 1, Samples: 2208/5760, Loss: 4.117058753967285\n",
      "Epoch: 1, Samples: 2240/5760, Loss: 4.0634942054748535\n",
      "Epoch: 1, Samples: 2272/5760, Loss: 3.811398506164551\n",
      "Epoch: 1, Samples: 2304/5760, Loss: 3.795989751815796\n",
      "Epoch: 1, Samples: 2336/5760, Loss: 3.711285352706909\n",
      "Epoch: 1, Samples: 2368/5760, Loss: 3.659827947616577\n",
      "Epoch: 1, Samples: 2400/5760, Loss: 3.6964499950408936\n",
      "Epoch: 1, Samples: 2432/5760, Loss: 3.554208278656006\n",
      "Epoch: 1, Samples: 2464/5760, Loss: 3.616405963897705\n",
      "Epoch: 1, Samples: 2496/5760, Loss: 3.859645128250122\n",
      "Epoch: 1, Samples: 2528/5760, Loss: 4.031191349029541\n",
      "Epoch: 1, Samples: 2560/5760, Loss: 3.2612576484680176\n",
      "Epoch: 1, Samples: 2592/5760, Loss: 3.7529027462005615\n",
      "Epoch: 1, Samples: 2624/5760, Loss: 3.508669376373291\n",
      "Epoch: 1, Samples: 2656/5760, Loss: 3.345480442047119\n",
      "Epoch: 1, Samples: 2688/5760, Loss: 3.3214266300201416\n",
      "Epoch: 1, Samples: 2720/5760, Loss: 3.510633945465088\n",
      "Epoch: 1, Samples: 2752/5760, Loss: 3.617964267730713\n",
      "Epoch: 1, Samples: 2784/5760, Loss: 3.494699478149414\n",
      "Epoch: 1, Samples: 2816/5760, Loss: 3.682757616043091\n",
      "Epoch: 1, Samples: 2848/5760, Loss: 3.5162041187286377\n",
      "Epoch: 1, Samples: 2880/5760, Loss: 3.2827365398406982\n",
      "Epoch: 1, Samples: 2912/5760, Loss: 3.7096705436706543\n",
      "Epoch: 1, Samples: 2944/5760, Loss: 3.628943681716919\n",
      "Epoch: 1, Samples: 2976/5760, Loss: 3.279667377471924\n",
      "Epoch: 1, Samples: 3008/5760, Loss: 3.3900887966156006\n",
      "Epoch: 1, Samples: 3040/5760, Loss: 3.544193983078003\n",
      "Epoch: 1, Samples: 3072/5760, Loss: 3.6032915115356445\n",
      "Epoch: 1, Samples: 3104/5760, Loss: 3.221195936203003\n",
      "Epoch: 1, Samples: 3136/5760, Loss: 3.2018356323242188\n",
      "Epoch: 1, Samples: 3168/5760, Loss: 3.5372865200042725\n",
      "Epoch: 1, Samples: 3200/5760, Loss: 3.369584321975708\n",
      "Epoch: 1, Samples: 3232/5760, Loss: 3.5163352489471436\n",
      "Epoch: 1, Samples: 3264/5760, Loss: 3.524275302886963\n",
      "Epoch: 1, Samples: 3296/5760, Loss: 3.055525779724121\n",
      "Epoch: 1, Samples: 3328/5760, Loss: 3.216409206390381\n",
      "Epoch: 1, Samples: 3360/5760, Loss: 3.4940073490142822\n",
      "Epoch: 1, Samples: 3392/5760, Loss: 3.411668062210083\n",
      "Epoch: 1, Samples: 3424/5760, Loss: 3.0406501293182373\n",
      "Epoch: 1, Samples: 3456/5760, Loss: 3.3893532752990723\n",
      "Epoch: 1, Samples: 3488/5760, Loss: 3.207350254058838\n",
      "Epoch: 1, Samples: 3520/5760, Loss: 3.168942451477051\n",
      "Epoch: 1, Samples: 3552/5760, Loss: 3.6809563636779785\n",
      "Epoch: 1, Samples: 3584/5760, Loss: 3.2487828731536865\n",
      "Epoch: 1, Samples: 3616/5760, Loss: 3.424473285675049\n",
      "Epoch: 1, Samples: 3648/5760, Loss: 3.3333141803741455\n",
      "Epoch: 1, Samples: 3680/5760, Loss: 3.121821165084839\n",
      "Epoch: 1, Samples: 3712/5760, Loss: 2.988316059112549\n",
      "Epoch: 1, Samples: 3744/5760, Loss: 2.9169414043426514\n",
      "Epoch: 1, Samples: 3776/5760, Loss: 3.4410934448242188\n",
      "Epoch: 1, Samples: 3808/5760, Loss: 3.467668294906616\n",
      "Epoch: 1, Samples: 3840/5760, Loss: 3.6870923042297363\n",
      "Epoch: 1, Samples: 3872/5760, Loss: 3.513376235961914\n",
      "Epoch: 1, Samples: 3904/5760, Loss: 3.177521228790283\n",
      "Epoch: 1, Samples: 3936/5760, Loss: 3.2823333740234375\n",
      "Epoch: 1, Samples: 3968/5760, Loss: 3.0154776573181152\n",
      "Epoch: 1, Samples: 4000/5760, Loss: 3.1263041496276855\n",
      "Epoch: 1, Samples: 4032/5760, Loss: 3.0890848636627197\n",
      "Epoch: 1, Samples: 4064/5760, Loss: 3.0376596450805664\n",
      "Epoch: 1, Samples: 4096/5760, Loss: 3.216848373413086\n",
      "Epoch: 1, Samples: 4128/5760, Loss: 3.194068193435669\n",
      "Epoch: 1, Samples: 4160/5760, Loss: 3.184419870376587\n",
      "Epoch: 1, Samples: 4192/5760, Loss: 3.238865375518799\n",
      "Epoch: 1, Samples: 4224/5760, Loss: 3.008453845977783\n",
      "Epoch: 1, Samples: 4256/5760, Loss: 3.066126585006714\n",
      "Epoch: 1, Samples: 4288/5760, Loss: 2.722529411315918\n",
      "Epoch: 1, Samples: 4320/5760, Loss: 3.0676772594451904\n",
      "Epoch: 1, Samples: 4352/5760, Loss: 2.8267853260040283\n",
      "Epoch: 1, Samples: 4384/5760, Loss: 3.1468281745910645\n",
      "Epoch: 1, Samples: 4416/5760, Loss: 3.1089539527893066\n",
      "Epoch: 1, Samples: 4448/5760, Loss: 2.735029935836792\n",
      "Epoch: 1, Samples: 4480/5760, Loss: 3.0768544673919678\n",
      "Epoch: 1, Samples: 4512/5760, Loss: 2.9739694595336914\n",
      "Epoch: 1, Samples: 4544/5760, Loss: 2.7394824028015137\n",
      "Epoch: 1, Samples: 4576/5760, Loss: 3.3134336471557617\n",
      "Epoch: 1, Samples: 4608/5760, Loss: 3.479445457458496\n",
      "Epoch: 1, Samples: 4640/5760, Loss: 3.0369627475738525\n",
      "Epoch: 1, Samples: 4672/5760, Loss: 2.5634491443634033\n",
      "Epoch: 1, Samples: 4704/5760, Loss: 3.2187893390655518\n",
      "Epoch: 1, Samples: 4736/5760, Loss: 3.082740068435669\n",
      "Epoch: 1, Samples: 4768/5760, Loss: 2.915720224380493\n",
      "Epoch: 1, Samples: 4800/5760, Loss: 2.957815408706665\n",
      "Epoch: 1, Samples: 4832/5760, Loss: 3.0208802223205566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Samples: 4864/5760, Loss: 3.1376218795776367\n",
      "Epoch: 1, Samples: 4896/5760, Loss: 3.321040391921997\n",
      "Epoch: 1, Samples: 4928/5760, Loss: 2.7606546878814697\n",
      "Epoch: 1, Samples: 4960/5760, Loss: 3.1814992427825928\n",
      "Epoch: 1, Samples: 4992/5760, Loss: 2.851891040802002\n",
      "Epoch: 1, Samples: 5024/5760, Loss: 2.5713911056518555\n",
      "Epoch: 1, Samples: 5056/5760, Loss: 3.266106367111206\n",
      "Epoch: 1, Samples: 5088/5760, Loss: 2.7504682540893555\n",
      "Epoch: 1, Samples: 5120/5760, Loss: 2.9286606311798096\n",
      "Epoch: 1, Samples: 5152/5760, Loss: 2.537576913833618\n",
      "Epoch: 1, Samples: 5184/5760, Loss: 2.8131489753723145\n",
      "Epoch: 1, Samples: 5216/5760, Loss: 2.4810752868652344\n",
      "Epoch: 1, Samples: 5248/5760, Loss: 2.913292407989502\n",
      "Epoch: 1, Samples: 5280/5760, Loss: 2.726879119873047\n",
      "Epoch: 1, Samples: 5312/5760, Loss: 2.8416314125061035\n",
      "Epoch: 1, Samples: 5344/5760, Loss: 2.788360834121704\n",
      "Epoch: 1, Samples: 5376/5760, Loss: 2.637686252593994\n",
      "Epoch: 1, Samples: 5408/5760, Loss: 2.7634057998657227\n",
      "Epoch: 1, Samples: 5440/5760, Loss: 2.6703267097473145\n",
      "Epoch: 1, Samples: 5472/5760, Loss: 2.5449600219726562\n",
      "Epoch: 1, Samples: 5504/5760, Loss: 2.1862518787384033\n",
      "Epoch: 1, Samples: 5536/5760, Loss: 2.642066717147827\n",
      "Epoch: 1, Samples: 5568/5760, Loss: 2.8338942527770996\n",
      "Epoch: 1, Samples: 5600/5760, Loss: 3.382760524749756\n",
      "Epoch: 1, Samples: 5632/5760, Loss: 2.7648777961730957\n",
      "Epoch: 1, Samples: 5664/5760, Loss: 2.648557424545288\n",
      "Epoch: 1, Samples: 5696/5760, Loss: 3.075309991836548\n",
      "Epoch: 1, Samples: 5728/5760, Loss: 2.7238190174102783\n",
      "\n",
      "Epoch: 1\n",
      "Training set: Average loss: 3.6121\n",
      "Validation set: Average loss: 2.5604, Accuracy: 399/818 (49%)\n",
      "Epoch: 2, Samples: 0/5760, Loss: 2.6134378910064697\n",
      "Epoch: 2, Samples: 32/5760, Loss: 2.529996633529663\n",
      "Epoch: 2, Samples: 64/5760, Loss: 2.2158219814300537\n",
      "Epoch: 2, Samples: 96/5760, Loss: 2.102444887161255\n",
      "Epoch: 2, Samples: 128/5760, Loss: 2.4223647117614746\n",
      "Epoch: 2, Samples: 160/5760, Loss: 2.252002716064453\n",
      "Epoch: 2, Samples: 192/5760, Loss: 2.1858139038085938\n",
      "Epoch: 2, Samples: 224/5760, Loss: 2.067967414855957\n",
      "Epoch: 2, Samples: 256/5760, Loss: 2.55389404296875\n",
      "Epoch: 2, Samples: 288/5760, Loss: 2.672750949859619\n",
      "Epoch: 2, Samples: 320/5760, Loss: 2.403679847717285\n",
      "Epoch: 2, Samples: 352/5760, Loss: 2.314720869064331\n",
      "Epoch: 2, Samples: 384/5760, Loss: 2.1242547035217285\n",
      "Epoch: 2, Samples: 416/5760, Loss: 2.6779239177703857\n",
      "Epoch: 2, Samples: 448/5760, Loss: 2.588984966278076\n",
      "Epoch: 2, Samples: 480/5760, Loss: 2.2625083923339844\n",
      "Epoch: 2, Samples: 512/5760, Loss: 2.4539685249328613\n",
      "Epoch: 2, Samples: 544/5760, Loss: 2.326972484588623\n",
      "Epoch: 2, Samples: 576/5760, Loss: 2.6037492752075195\n",
      "Epoch: 2, Samples: 608/5760, Loss: 2.278608798980713\n",
      "Epoch: 2, Samples: 640/5760, Loss: 2.480802059173584\n",
      "Epoch: 2, Samples: 672/5760, Loss: 1.9886071681976318\n",
      "Epoch: 2, Samples: 704/5760, Loss: 2.5493321418762207\n",
      "Epoch: 2, Samples: 736/5760, Loss: 2.6079139709472656\n",
      "Epoch: 2, Samples: 768/5760, Loss: 2.0527503490448\n",
      "Epoch: 2, Samples: 800/5760, Loss: 2.274336338043213\n",
      "Epoch: 2, Samples: 832/5760, Loss: 2.4853227138519287\n",
      "Epoch: 2, Samples: 864/5760, Loss: 2.2290823459625244\n",
      "Epoch: 2, Samples: 896/5760, Loss: 1.9924559593200684\n",
      "Epoch: 2, Samples: 928/5760, Loss: 2.391193389892578\n",
      "Epoch: 2, Samples: 960/5760, Loss: 2.4241726398468018\n",
      "Epoch: 2, Samples: 992/5760, Loss: 2.4440743923187256\n",
      "Epoch: 2, Samples: 1024/5760, Loss: 2.284834861755371\n",
      "Epoch: 2, Samples: 1056/5760, Loss: 2.6904139518737793\n",
      "Epoch: 2, Samples: 1088/5760, Loss: 2.368823528289795\n",
      "Epoch: 2, Samples: 1120/5760, Loss: 1.957177758216858\n",
      "Epoch: 2, Samples: 1152/5760, Loss: 2.415915012359619\n",
      "Epoch: 2, Samples: 1184/5760, Loss: 2.235311508178711\n",
      "Epoch: 2, Samples: 1216/5760, Loss: 2.4635350704193115\n",
      "Epoch: 2, Samples: 1248/5760, Loss: 2.3868248462677\n",
      "Epoch: 2, Samples: 1280/5760, Loss: 2.2341058254241943\n",
      "Epoch: 2, Samples: 1312/5760, Loss: 2.025088310241699\n",
      "Epoch: 2, Samples: 1344/5760, Loss: 2.2582895755767822\n",
      "Epoch: 2, Samples: 1376/5760, Loss: 2.3685929775238037\n",
      "Epoch: 2, Samples: 1408/5760, Loss: 2.027445077896118\n",
      "Epoch: 2, Samples: 1440/5760, Loss: 2.0041849613189697\n",
      "Epoch: 2, Samples: 1472/5760, Loss: 2.1310408115386963\n",
      "Epoch: 2, Samples: 1504/5760, Loss: 1.969759225845337\n",
      "Epoch: 2, Samples: 1536/5760, Loss: 2.2857091426849365\n",
      "Epoch: 2, Samples: 1568/5760, Loss: 2.4704325199127197\n",
      "Epoch: 2, Samples: 1600/5760, Loss: 2.170252799987793\n",
      "Epoch: 2, Samples: 1632/5760, Loss: 2.1943140029907227\n",
      "Epoch: 2, Samples: 1664/5760, Loss: 2.4179251194000244\n",
      "Epoch: 2, Samples: 1696/5760, Loss: 1.9388842582702637\n",
      "Epoch: 2, Samples: 1728/5760, Loss: 1.9385446310043335\n",
      "Epoch: 2, Samples: 1760/5760, Loss: 1.9407005310058594\n",
      "Epoch: 2, Samples: 1792/5760, Loss: 2.204681158065796\n",
      "Epoch: 2, Samples: 1824/5760, Loss: 1.9983327388763428\n",
      "Epoch: 2, Samples: 1856/5760, Loss: 2.105708599090576\n",
      "Epoch: 2, Samples: 1888/5760, Loss: 2.335630416870117\n",
      "Epoch: 2, Samples: 1920/5760, Loss: 2.074131488800049\n",
      "Epoch: 2, Samples: 1952/5760, Loss: 2.095057249069214\n",
      "Epoch: 2, Samples: 1984/5760, Loss: 2.1035523414611816\n",
      "Epoch: 2, Samples: 2016/5760, Loss: 2.2051355838775635\n",
      "Epoch: 2, Samples: 2048/5760, Loss: 2.0218169689178467\n",
      "Epoch: 2, Samples: 2080/5760, Loss: 2.012411594390869\n",
      "Epoch: 2, Samples: 2112/5760, Loss: 1.832546591758728\n",
      "Epoch: 2, Samples: 2144/5760, Loss: 2.248947858810425\n",
      "Epoch: 2, Samples: 2176/5760, Loss: 2.2306039333343506\n",
      "Epoch: 2, Samples: 2208/5760, Loss: 1.7562614679336548\n",
      "Epoch: 2, Samples: 2240/5760, Loss: 1.7352732419967651\n",
      "Epoch: 2, Samples: 2272/5760, Loss: 1.9080896377563477\n",
      "Epoch: 2, Samples: 2304/5760, Loss: 2.030350685119629\n",
      "Epoch: 2, Samples: 2336/5760, Loss: 2.3221771717071533\n",
      "Epoch: 2, Samples: 2368/5760, Loss: 2.162019968032837\n",
      "Epoch: 2, Samples: 2400/5760, Loss: 2.3965489864349365\n",
      "Epoch: 2, Samples: 2432/5760, Loss: 2.02622652053833\n",
      "Epoch: 2, Samples: 2464/5760, Loss: 2.231309652328491\n",
      "Epoch: 2, Samples: 2496/5760, Loss: 1.8408759832382202\n",
      "Epoch: 2, Samples: 2528/5760, Loss: 2.1439740657806396\n",
      "Epoch: 2, Samples: 2560/5760, Loss: 1.9852744340896606\n",
      "Epoch: 2, Samples: 2592/5760, Loss: 1.7366143465042114\n",
      "Epoch: 2, Samples: 2624/5760, Loss: 2.05672287940979\n",
      "Epoch: 2, Samples: 2656/5760, Loss: 1.6982195377349854\n",
      "Epoch: 2, Samples: 2688/5760, Loss: 1.761429786682129\n",
      "Epoch: 2, Samples: 2720/5760, Loss: 1.7836254835128784\n",
      "Epoch: 2, Samples: 2752/5760, Loss: 2.0443778038024902\n",
      "Epoch: 2, Samples: 2784/5760, Loss: 2.3505523204803467\n",
      "Epoch: 2, Samples: 2816/5760, Loss: 1.9764363765716553\n",
      "Epoch: 2, Samples: 2848/5760, Loss: 1.9705607891082764\n",
      "Epoch: 2, Samples: 2880/5760, Loss: 1.9806946516036987\n",
      "Epoch: 2, Samples: 2912/5760, Loss: 1.8902065753936768\n",
      "Epoch: 2, Samples: 2944/5760, Loss: 1.759421706199646\n",
      "Epoch: 2, Samples: 2976/5760, Loss: 1.8625181913375854\n",
      "Epoch: 2, Samples: 3008/5760, Loss: 1.9391756057739258\n",
      "Epoch: 2, Samples: 3040/5760, Loss: 2.202550172805786\n",
      "Epoch: 2, Samples: 3072/5760, Loss: 1.7530642747879028\n",
      "Epoch: 2, Samples: 3104/5760, Loss: 1.8672771453857422\n",
      "Epoch: 2, Samples: 3136/5760, Loss: 2.021244764328003\n",
      "Epoch: 2, Samples: 3168/5760, Loss: 1.8869242668151855\n",
      "Epoch: 2, Samples: 3200/5760, Loss: 1.6478824615478516\n",
      "Epoch: 2, Samples: 3232/5760, Loss: 1.4873223304748535\n",
      "Epoch: 2, Samples: 3264/5760, Loss: 2.2194972038269043\n",
      "Epoch: 2, Samples: 3296/5760, Loss: 1.7078638076782227\n",
      "Epoch: 2, Samples: 3328/5760, Loss: 1.4657894372940063\n",
      "Epoch: 2, Samples: 3360/5760, Loss: 1.9165666103363037\n",
      "Epoch: 2, Samples: 3392/5760, Loss: 1.6524475812911987\n",
      "Epoch: 2, Samples: 3424/5760, Loss: 1.9471185207366943\n",
      "Epoch: 2, Samples: 3456/5760, Loss: 1.901648759841919\n",
      "Epoch: 2, Samples: 3488/5760, Loss: 1.9668740034103394\n",
      "Epoch: 2, Samples: 3520/5760, Loss: 1.6410471200942993\n",
      "Epoch: 2, Samples: 3552/5760, Loss: 1.8085172176361084\n",
      "Epoch: 2, Samples: 3584/5760, Loss: 2.154491424560547\n",
      "Epoch: 2, Samples: 3616/5760, Loss: 1.8894670009613037\n",
      "Epoch: 2, Samples: 3648/5760, Loss: 1.960170030593872\n",
      "Epoch: 2, Samples: 3680/5760, Loss: 1.5830508470535278\n",
      "Epoch: 2, Samples: 3712/5760, Loss: 2.22910737991333\n",
      "Epoch: 2, Samples: 3744/5760, Loss: 1.9136466979980469\n",
      "Epoch: 2, Samples: 3776/5760, Loss: 1.8624836206436157\n",
      "Epoch: 2, Samples: 3808/5760, Loss: 1.9082144498825073\n",
      "Epoch: 2, Samples: 3840/5760, Loss: 1.5905513763427734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Samples: 3872/5760, Loss: 1.6333404779434204\n",
      "Epoch: 2, Samples: 3904/5760, Loss: 1.8073368072509766\n",
      "Epoch: 2, Samples: 3936/5760, Loss: 1.5882678031921387\n",
      "Epoch: 2, Samples: 3968/5760, Loss: 1.843001365661621\n",
      "Epoch: 2, Samples: 4000/5760, Loss: 1.834728717803955\n",
      "Epoch: 2, Samples: 4032/5760, Loss: 1.419399380683899\n",
      "Epoch: 2, Samples: 4064/5760, Loss: 1.4353458881378174\n",
      "Epoch: 2, Samples: 4096/5760, Loss: 1.4912391901016235\n",
      "Epoch: 2, Samples: 4128/5760, Loss: 1.859710693359375\n",
      "Epoch: 2, Samples: 4160/5760, Loss: 1.3978596925735474\n",
      "Epoch: 2, Samples: 4192/5760, Loss: 1.4487136602401733\n",
      "Epoch: 2, Samples: 4224/5760, Loss: 1.9073240756988525\n",
      "Epoch: 2, Samples: 4256/5760, Loss: 1.4318039417266846\n",
      "Epoch: 2, Samples: 4288/5760, Loss: 1.8428969383239746\n",
      "Epoch: 2, Samples: 4320/5760, Loss: 2.111941337585449\n",
      "Epoch: 2, Samples: 4352/5760, Loss: 1.6386313438415527\n",
      "Epoch: 2, Samples: 4384/5760, Loss: 2.131545066833496\n",
      "Epoch: 2, Samples: 4416/5760, Loss: 1.5789294242858887\n",
      "Epoch: 2, Samples: 4448/5760, Loss: 1.9128419160842896\n",
      "Epoch: 2, Samples: 4480/5760, Loss: 1.4262237548828125\n",
      "Epoch: 2, Samples: 4512/5760, Loss: 1.7425280809402466\n",
      "Epoch: 2, Samples: 4544/5760, Loss: 1.7028028964996338\n",
      "Epoch: 2, Samples: 4576/5760, Loss: 1.5249665975570679\n",
      "Epoch: 2, Samples: 4608/5760, Loss: 1.3495221138000488\n",
      "Epoch: 2, Samples: 4640/5760, Loss: 1.6745049953460693\n",
      "Epoch: 2, Samples: 4672/5760, Loss: 1.738592267036438\n",
      "Epoch: 2, Samples: 4704/5760, Loss: 1.6619069576263428\n",
      "Epoch: 2, Samples: 4736/5760, Loss: 1.5388901233673096\n",
      "Epoch: 2, Samples: 4768/5760, Loss: 1.5606167316436768\n",
      "Epoch: 2, Samples: 4800/5760, Loss: 1.4307705163955688\n",
      "Epoch: 2, Samples: 4832/5760, Loss: 1.4935568571090698\n",
      "Epoch: 2, Samples: 4864/5760, Loss: 1.3497192859649658\n",
      "Epoch: 2, Samples: 4896/5760, Loss: 1.817996859550476\n",
      "Epoch: 2, Samples: 4928/5760, Loss: 1.2558077573776245\n",
      "Epoch: 2, Samples: 4960/5760, Loss: 1.6200578212738037\n",
      "Epoch: 2, Samples: 4992/5760, Loss: 1.475886583328247\n",
      "Epoch: 2, Samples: 5024/5760, Loss: 1.7100749015808105\n",
      "Epoch: 2, Samples: 5056/5760, Loss: 1.590869665145874\n",
      "Epoch: 2, Samples: 5088/5760, Loss: 1.7064449787139893\n",
      "Epoch: 2, Samples: 5120/5760, Loss: 1.661342740058899\n",
      "Epoch: 2, Samples: 5152/5760, Loss: 1.8831725120544434\n",
      "Epoch: 2, Samples: 5184/5760, Loss: 1.3858660459518433\n",
      "Epoch: 2, Samples: 5216/5760, Loss: 1.7859498262405396\n",
      "Epoch: 2, Samples: 5248/5760, Loss: 1.5449903011322021\n",
      "Epoch: 2, Samples: 5280/5760, Loss: 1.9042388200759888\n",
      "Epoch: 2, Samples: 5312/5760, Loss: 1.4281795024871826\n",
      "Epoch: 2, Samples: 5344/5760, Loss: 1.5033233165740967\n",
      "Epoch: 2, Samples: 5376/5760, Loss: 1.4339679479599\n",
      "Epoch: 2, Samples: 5408/5760, Loss: 1.4462538957595825\n",
      "Epoch: 2, Samples: 5440/5760, Loss: 1.4613038301467896\n",
      "Epoch: 2, Samples: 5472/5760, Loss: 1.7625548839569092\n",
      "Epoch: 2, Samples: 5504/5760, Loss: 1.7039284706115723\n",
      "Epoch: 2, Samples: 5536/5760, Loss: 1.7921968698501587\n",
      "Epoch: 2, Samples: 5568/5760, Loss: 1.5588600635528564\n",
      "Epoch: 2, Samples: 5600/5760, Loss: 1.7699037790298462\n",
      "Epoch: 2, Samples: 5632/5760, Loss: 1.530213475227356\n",
      "Epoch: 2, Samples: 5664/5760, Loss: 1.535512924194336\n",
      "Epoch: 2, Samples: 5696/5760, Loss: 1.6214025020599365\n",
      "Epoch: 2, Samples: 5728/5760, Loss: 1.1385059356689453\n",
      "\n",
      "Epoch: 2\n",
      "Training set: Average loss: 1.9516\n",
      "Validation set: Average loss: 1.5117, Accuracy: 581/818 (71%)\n",
      "Saving model (epoch 2) with lowest validation loss: 1.511686004125155\n",
      "Epoch: 3, Samples: 0/5760, Loss: 1.2343552112579346\n",
      "Epoch: 3, Samples: 32/5760, Loss: 1.4260509014129639\n",
      "Epoch: 3, Samples: 64/5760, Loss: 1.6207317113876343\n",
      "Epoch: 3, Samples: 96/5760, Loss: 1.3250046968460083\n",
      "Epoch: 3, Samples: 128/5760, Loss: 1.4848473072052002\n",
      "Epoch: 3, Samples: 160/5760, Loss: 1.6514304876327515\n",
      "Epoch: 3, Samples: 192/5760, Loss: 1.5481370687484741\n",
      "Epoch: 3, Samples: 224/5760, Loss: 1.6102579832077026\n",
      "Epoch: 3, Samples: 256/5760, Loss: 1.2805848121643066\n",
      "Epoch: 3, Samples: 288/5760, Loss: 1.1030718088150024\n",
      "Epoch: 3, Samples: 320/5760, Loss: 1.6683167219161987\n",
      "Epoch: 3, Samples: 352/5760, Loss: 1.469099760055542\n",
      "Epoch: 3, Samples: 384/5760, Loss: 1.612862229347229\n",
      "Epoch: 3, Samples: 416/5760, Loss: 1.372392177581787\n",
      "Epoch: 3, Samples: 448/5760, Loss: 1.0036704540252686\n",
      "Epoch: 3, Samples: 480/5760, Loss: 1.6063346862792969\n",
      "Epoch: 3, Samples: 512/5760, Loss: 1.3466376066207886\n",
      "Epoch: 3, Samples: 544/5760, Loss: 1.5129616260528564\n",
      "Epoch: 3, Samples: 576/5760, Loss: 1.2869781255722046\n",
      "Epoch: 3, Samples: 608/5760, Loss: 1.417267918586731\n",
      "Epoch: 3, Samples: 640/5760, Loss: 1.4858883619308472\n",
      "Epoch: 3, Samples: 672/5760, Loss: 1.458173155784607\n",
      "Epoch: 3, Samples: 704/5760, Loss: 1.5428563356399536\n",
      "Epoch: 3, Samples: 736/5760, Loss: 1.510445237159729\n",
      "Epoch: 3, Samples: 768/5760, Loss: 1.2703133821487427\n",
      "Epoch: 3, Samples: 800/5760, Loss: 1.2024531364440918\n",
      "Epoch: 3, Samples: 832/5760, Loss: 1.2980819940567017\n",
      "Epoch: 3, Samples: 864/5760, Loss: 1.4341117143630981\n",
      "Epoch: 3, Samples: 896/5760, Loss: 1.6843153238296509\n",
      "Epoch: 3, Samples: 928/5760, Loss: 1.372689962387085\n",
      "Epoch: 3, Samples: 960/5760, Loss: 1.1759430170059204\n",
      "Epoch: 3, Samples: 992/5760, Loss: 1.4457764625549316\n",
      "Epoch: 3, Samples: 1024/5760, Loss: 1.1411248445510864\n",
      "Epoch: 3, Samples: 1056/5760, Loss: 1.06557035446167\n",
      "Epoch: 3, Samples: 1088/5760, Loss: 1.506506085395813\n",
      "Epoch: 3, Samples: 1120/5760, Loss: 1.146950602531433\n",
      "Epoch: 3, Samples: 1152/5760, Loss: 1.3286793231964111\n",
      "Epoch: 3, Samples: 1184/5760, Loss: 1.1955671310424805\n",
      "Epoch: 3, Samples: 1216/5760, Loss: 1.2737185955047607\n",
      "Epoch: 3, Samples: 1248/5760, Loss: 1.312018871307373\n",
      "Epoch: 3, Samples: 1280/5760, Loss: 1.2136812210083008\n",
      "Epoch: 3, Samples: 1312/5760, Loss: 1.0925731658935547\n",
      "Epoch: 3, Samples: 1344/5760, Loss: 1.3606393337249756\n",
      "Epoch: 3, Samples: 1376/5760, Loss: 1.024296760559082\n",
      "Epoch: 3, Samples: 1408/5760, Loss: 1.4600900411605835\n",
      "Epoch: 3, Samples: 1440/5760, Loss: 1.4365581274032593\n",
      "Epoch: 3, Samples: 1472/5760, Loss: 1.2863343954086304\n",
      "Epoch: 3, Samples: 1504/5760, Loss: 1.0805383920669556\n",
      "Epoch: 3, Samples: 1536/5760, Loss: 1.1593918800354004\n",
      "Epoch: 3, Samples: 1568/5760, Loss: 1.3852224349975586\n",
      "Epoch: 3, Samples: 1600/5760, Loss: 1.3089672327041626\n",
      "Epoch: 3, Samples: 1632/5760, Loss: 1.5414313077926636\n",
      "Epoch: 3, Samples: 1664/5760, Loss: 1.4966192245483398\n",
      "Epoch: 3, Samples: 1696/5760, Loss: 1.4099010229110718\n",
      "Epoch: 3, Samples: 1728/5760, Loss: 1.556273102760315\n",
      "Epoch: 3, Samples: 1760/5760, Loss: 1.0662657022476196\n",
      "Epoch: 3, Samples: 1792/5760, Loss: 1.1268538236618042\n",
      "Epoch: 3, Samples: 1824/5760, Loss: 1.29681396484375\n",
      "Epoch: 3, Samples: 1856/5760, Loss: 1.053248643875122\n",
      "Epoch: 3, Samples: 1888/5760, Loss: 1.125781536102295\n",
      "Epoch: 3, Samples: 1920/5760, Loss: 1.1578158140182495\n",
      "Epoch: 3, Samples: 1952/5760, Loss: 1.1857333183288574\n",
      "Epoch: 3, Samples: 1984/5760, Loss: 1.153817892074585\n",
      "Epoch: 3, Samples: 2016/5760, Loss: 1.4843807220458984\n",
      "Epoch: 3, Samples: 2048/5760, Loss: 1.2441154718399048\n",
      "Epoch: 3, Samples: 2080/5760, Loss: 1.3255693912506104\n",
      "Epoch: 3, Samples: 2112/5760, Loss: 1.2381888628005981\n",
      "Epoch: 3, Samples: 2144/5760, Loss: 1.1583951711654663\n",
      "Epoch: 3, Samples: 2176/5760, Loss: 1.2100046873092651\n",
      "Epoch: 3, Samples: 2208/5760, Loss: 1.3506704568862915\n",
      "Epoch: 3, Samples: 2240/5760, Loss: 1.2923108339309692\n",
      "Epoch: 3, Samples: 2272/5760, Loss: 1.2331463098526\n",
      "Epoch: 3, Samples: 2304/5760, Loss: 0.9777047634124756\n",
      "Epoch: 3, Samples: 2336/5760, Loss: 1.6836631298065186\n",
      "Epoch: 3, Samples: 2368/5760, Loss: 1.2094080448150635\n",
      "Epoch: 3, Samples: 2400/5760, Loss: 1.441053867340088\n",
      "Epoch: 3, Samples: 2432/5760, Loss: 1.281030297279358\n",
      "Epoch: 3, Samples: 2464/5760, Loss: 0.8845386505126953\n",
      "Epoch: 3, Samples: 2496/5760, Loss: 1.2374653816223145\n",
      "Epoch: 3, Samples: 2528/5760, Loss: 1.3274273872375488\n",
      "Epoch: 3, Samples: 2560/5760, Loss: 1.4843780994415283\n",
      "Epoch: 3, Samples: 2592/5760, Loss: 1.186081886291504\n",
      "Epoch: 3, Samples: 2624/5760, Loss: 1.353820562362671\n",
      "Epoch: 3, Samples: 2656/5760, Loss: 1.2288966178894043\n",
      "Epoch: 3, Samples: 2688/5760, Loss: 1.1838583946228027\n",
      "Epoch: 3, Samples: 2720/5760, Loss: 1.1475352048873901\n",
      "Epoch: 3, Samples: 2752/5760, Loss: 1.149681806564331\n",
      "Epoch: 3, Samples: 2784/5760, Loss: 1.0252413749694824\n",
      "Epoch: 3, Samples: 2816/5760, Loss: 0.9626920819282532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Samples: 2848/5760, Loss: 1.389174461364746\n",
      "Epoch: 3, Samples: 2880/5760, Loss: 1.3209861516952515\n",
      "Epoch: 3, Samples: 2912/5760, Loss: 1.0813963413238525\n",
      "Epoch: 3, Samples: 2944/5760, Loss: 1.0065653324127197\n",
      "Epoch: 3, Samples: 2976/5760, Loss: 0.916588544845581\n",
      "Epoch: 3, Samples: 3008/5760, Loss: 1.0686306953430176\n",
      "Epoch: 3, Samples: 3040/5760, Loss: 1.2468087673187256\n",
      "Epoch: 3, Samples: 3072/5760, Loss: 1.2468163967132568\n",
      "Epoch: 3, Samples: 3104/5760, Loss: 1.0820444822311401\n",
      "Epoch: 3, Samples: 3136/5760, Loss: 0.9128859043121338\n",
      "Epoch: 3, Samples: 3168/5760, Loss: 0.9123457670211792\n",
      "Epoch: 3, Samples: 3200/5760, Loss: 0.9715058207511902\n",
      "Epoch: 3, Samples: 3232/5760, Loss: 1.247359275817871\n",
      "Epoch: 3, Samples: 3264/5760, Loss: 0.9941120743751526\n",
      "Epoch: 3, Samples: 3296/5760, Loss: 1.5476223230361938\n",
      "Epoch: 3, Samples: 3328/5760, Loss: 0.8413565754890442\n",
      "Epoch: 3, Samples: 3360/5760, Loss: 1.2352427244186401\n",
      "Epoch: 3, Samples: 3392/5760, Loss: 1.2584253549575806\n",
      "Epoch: 3, Samples: 3424/5760, Loss: 0.9342769384384155\n",
      "Epoch: 3, Samples: 3456/5760, Loss: 1.0434515476226807\n",
      "Epoch: 3, Samples: 3488/5760, Loss: 1.0189497470855713\n",
      "Epoch: 3, Samples: 3520/5760, Loss: 0.9053009748458862\n",
      "Epoch: 3, Samples: 3552/5760, Loss: 1.1822068691253662\n",
      "Epoch: 3, Samples: 3584/5760, Loss: 0.9192396402359009\n",
      "Epoch: 3, Samples: 3616/5760, Loss: 1.331390142440796\n",
      "Epoch: 3, Samples: 3648/5760, Loss: 0.9985015392303467\n",
      "Epoch: 3, Samples: 3680/5760, Loss: 0.8713117241859436\n",
      "Epoch: 3, Samples: 3712/5760, Loss: 1.410404086112976\n",
      "Epoch: 3, Samples: 3744/5760, Loss: 0.8910428285598755\n",
      "Epoch: 3, Samples: 3776/5760, Loss: 0.9173413515090942\n",
      "Epoch: 3, Samples: 3808/5760, Loss: 1.2561944723129272\n",
      "Epoch: 3, Samples: 3840/5760, Loss: 1.093583583831787\n",
      "Epoch: 3, Samples: 3872/5760, Loss: 1.0342823266983032\n",
      "Epoch: 3, Samples: 3904/5760, Loss: 1.4531234502792358\n",
      "Epoch: 3, Samples: 3936/5760, Loss: 1.1308553218841553\n",
      "Epoch: 3, Samples: 3968/5760, Loss: 0.8616100549697876\n",
      "Epoch: 3, Samples: 4000/5760, Loss: 1.1570578813552856\n",
      "Epoch: 3, Samples: 4032/5760, Loss: 1.16713285446167\n",
      "Epoch: 3, Samples: 4064/5760, Loss: 0.9760338068008423\n",
      "Epoch: 3, Samples: 4096/5760, Loss: 0.7901091575622559\n",
      "Epoch: 3, Samples: 4128/5760, Loss: 0.9410673379898071\n",
      "Epoch: 3, Samples: 4160/5760, Loss: 0.8246368169784546\n",
      "Epoch: 3, Samples: 4192/5760, Loss: 1.0443634986877441\n",
      "Epoch: 3, Samples: 4224/5760, Loss: 0.8066160082817078\n",
      "Epoch: 3, Samples: 4256/5760, Loss: 1.1745715141296387\n",
      "Epoch: 3, Samples: 4288/5760, Loss: 0.8960119485855103\n",
      "Epoch: 3, Samples: 4320/5760, Loss: 1.4342983961105347\n",
      "Epoch: 3, Samples: 4352/5760, Loss: 1.2348588705062866\n",
      "Epoch: 3, Samples: 4384/5760, Loss: 1.3950847387313843\n",
      "Epoch: 3, Samples: 4416/5760, Loss: 1.0527821779251099\n",
      "Epoch: 3, Samples: 4448/5760, Loss: 0.7474000453948975\n",
      "Epoch: 3, Samples: 4480/5760, Loss: 1.20409095287323\n",
      "Epoch: 3, Samples: 4512/5760, Loss: 0.776445746421814\n",
      "Epoch: 3, Samples: 4544/5760, Loss: 1.138771414756775\n",
      "Epoch: 3, Samples: 4576/5760, Loss: 0.8730725049972534\n",
      "Epoch: 3, Samples: 4608/5760, Loss: 0.9034826755523682\n",
      "Epoch: 3, Samples: 4640/5760, Loss: 1.0094826221466064\n",
      "Epoch: 3, Samples: 4672/5760, Loss: 1.1723374128341675\n",
      "Epoch: 3, Samples: 4704/5760, Loss: 1.1925387382507324\n",
      "Epoch: 3, Samples: 4736/5760, Loss: 0.9810409545898438\n",
      "Epoch: 3, Samples: 4768/5760, Loss: 1.2359519004821777\n",
      "Epoch: 3, Samples: 4800/5760, Loss: 0.9579110145568848\n",
      "Epoch: 3, Samples: 4832/5760, Loss: 1.0347434282302856\n",
      "Epoch: 3, Samples: 4864/5760, Loss: 0.8907706141471863\n",
      "Epoch: 3, Samples: 4896/5760, Loss: 1.0970386266708374\n",
      "Epoch: 3, Samples: 4928/5760, Loss: 1.2093005180358887\n",
      "Epoch: 3, Samples: 4960/5760, Loss: 1.0595835447311401\n",
      "Epoch: 3, Samples: 4992/5760, Loss: 1.2139949798583984\n",
      "Epoch: 3, Samples: 5024/5760, Loss: 1.038412094116211\n",
      "Epoch: 3, Samples: 5056/5760, Loss: 0.978154182434082\n",
      "Epoch: 3, Samples: 5088/5760, Loss: 0.957988977432251\n",
      "Epoch: 3, Samples: 5120/5760, Loss: 0.9359564185142517\n",
      "Epoch: 3, Samples: 5152/5760, Loss: 1.0419092178344727\n",
      "Epoch: 3, Samples: 5184/5760, Loss: 1.0389914512634277\n",
      "Epoch: 3, Samples: 5216/5760, Loss: 1.0331004858016968\n",
      "Epoch: 3, Samples: 5248/5760, Loss: 0.9221481084823608\n",
      "Epoch: 3, Samples: 5280/5760, Loss: 1.4853768348693848\n",
      "Epoch: 3, Samples: 5312/5760, Loss: 0.7161096334457397\n",
      "Epoch: 3, Samples: 5344/5760, Loss: 0.9989012479782104\n",
      "Epoch: 3, Samples: 5376/5760, Loss: 1.0158817768096924\n",
      "Epoch: 3, Samples: 5408/5760, Loss: 0.8683109879493713\n",
      "Epoch: 3, Samples: 5440/5760, Loss: 1.2479349374771118\n",
      "Epoch: 3, Samples: 5472/5760, Loss: 1.1917355060577393\n",
      "Epoch: 3, Samples: 5504/5760, Loss: 1.007180094718933\n",
      "Epoch: 3, Samples: 5536/5760, Loss: 1.0758284330368042\n",
      "Epoch: 3, Samples: 5568/5760, Loss: 1.15665602684021\n",
      "Epoch: 3, Samples: 5600/5760, Loss: 0.8605751991271973\n",
      "Epoch: 3, Samples: 5632/5760, Loss: 1.111421823501587\n",
      "Epoch: 3, Samples: 5664/5760, Loss: 0.7083775997161865\n",
      "Epoch: 3, Samples: 5696/5760, Loss: 1.3368803262710571\n",
      "Epoch: 3, Samples: 5728/5760, Loss: 1.4060630798339844\n",
      "\n",
      "Epoch: 3\n",
      "Training set: Average loss: 1.1887\n",
      "Validation set: Average loss: 1.0216, Accuracy: 654/818 (80%)\n",
      "Saving model (epoch 3) with lowest validation loss: 1.0215822206093714\n",
      "Epoch: 4, Samples: 0/5760, Loss: 0.908156156539917\n",
      "Epoch: 4, Samples: 32/5760, Loss: 0.7800906896591187\n",
      "Epoch: 4, Samples: 64/5760, Loss: 0.8546576499938965\n",
      "Epoch: 4, Samples: 96/5760, Loss: 1.006467342376709\n",
      "Epoch: 4, Samples: 128/5760, Loss: 0.872357964515686\n",
      "Epoch: 4, Samples: 160/5760, Loss: 0.8548834919929504\n",
      "Epoch: 4, Samples: 192/5760, Loss: 0.729688286781311\n",
      "Epoch: 4, Samples: 224/5760, Loss: 0.7590875029563904\n",
      "Epoch: 4, Samples: 256/5760, Loss: 1.0314314365386963\n",
      "Epoch: 4, Samples: 288/5760, Loss: 1.0483030080795288\n",
      "Epoch: 4, Samples: 320/5760, Loss: 0.9926596879959106\n",
      "Epoch: 4, Samples: 352/5760, Loss: 0.7821125984191895\n",
      "Epoch: 4, Samples: 384/5760, Loss: 0.8957691192626953\n",
      "Epoch: 4, Samples: 416/5760, Loss: 0.9055001735687256\n",
      "Epoch: 4, Samples: 448/5760, Loss: 0.8939006328582764\n",
      "Epoch: 4, Samples: 480/5760, Loss: 0.6317046284675598\n",
      "Epoch: 4, Samples: 512/5760, Loss: 0.722705066204071\n",
      "Epoch: 4, Samples: 544/5760, Loss: 0.6592116355895996\n",
      "Epoch: 4, Samples: 576/5760, Loss: 1.1356430053710938\n",
      "Epoch: 4, Samples: 608/5760, Loss: 0.9784289002418518\n",
      "Epoch: 4, Samples: 640/5760, Loss: 0.6585603356361389\n",
      "Epoch: 4, Samples: 672/5760, Loss: 1.052974820137024\n",
      "Epoch: 4, Samples: 704/5760, Loss: 0.9950574040412903\n",
      "Epoch: 4, Samples: 736/5760, Loss: 1.1517341136932373\n",
      "Epoch: 4, Samples: 768/5760, Loss: 0.7277961373329163\n",
      "Epoch: 4, Samples: 800/5760, Loss: 0.7649349570274353\n",
      "Epoch: 4, Samples: 832/5760, Loss: 0.7046106457710266\n",
      "Epoch: 4, Samples: 864/5760, Loss: 0.7037805318832397\n",
      "Epoch: 4, Samples: 896/5760, Loss: 0.7498154640197754\n",
      "Epoch: 4, Samples: 928/5760, Loss: 0.7903849482536316\n",
      "Epoch: 4, Samples: 960/5760, Loss: 1.1721965074539185\n",
      "Epoch: 4, Samples: 992/5760, Loss: 0.6341142654418945\n",
      "Epoch: 4, Samples: 1024/5760, Loss: 0.7961399555206299\n",
      "Epoch: 4, Samples: 1056/5760, Loss: 0.7356523275375366\n",
      "Epoch: 4, Samples: 1088/5760, Loss: 0.6939194202423096\n",
      "Epoch: 4, Samples: 1120/5760, Loss: 0.7543047070503235\n",
      "Epoch: 4, Samples: 1152/5760, Loss: 0.5644969344139099\n",
      "Epoch: 4, Samples: 1184/5760, Loss: 0.7655256390571594\n",
      "Epoch: 4, Samples: 1216/5760, Loss: 0.8104732036590576\n",
      "Epoch: 4, Samples: 1248/5760, Loss: 0.6570446491241455\n",
      "Epoch: 4, Samples: 1280/5760, Loss: 0.7503896951675415\n",
      "Epoch: 4, Samples: 1312/5760, Loss: 0.8432060480117798\n",
      "Epoch: 4, Samples: 1344/5760, Loss: 1.0200246572494507\n",
      "Epoch: 4, Samples: 1376/5760, Loss: 0.838013768196106\n",
      "Epoch: 4, Samples: 1408/5760, Loss: 1.0582878589630127\n",
      "Epoch: 4, Samples: 1440/5760, Loss: 0.8953558206558228\n",
      "Epoch: 4, Samples: 1472/5760, Loss: 0.8501185178756714\n",
      "Epoch: 4, Samples: 1504/5760, Loss: 1.3750792741775513\n",
      "Epoch: 4, Samples: 1536/5760, Loss: 0.8451841473579407\n",
      "Epoch: 4, Samples: 1568/5760, Loss: 0.8268036842346191\n",
      "Epoch: 4, Samples: 1600/5760, Loss: 0.9507421255111694\n",
      "Epoch: 4, Samples: 1632/5760, Loss: 0.5406770706176758\n",
      "Epoch: 4, Samples: 1664/5760, Loss: 0.6813266277313232\n",
      "Epoch: 4, Samples: 1696/5760, Loss: 0.7930070161819458\n",
      "Epoch: 4, Samples: 1728/5760, Loss: 0.9303077459335327\n",
      "Epoch: 4, Samples: 1760/5760, Loss: 0.8713065385818481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Samples: 1792/5760, Loss: 1.061971664428711\n",
      "Epoch: 4, Samples: 1824/5760, Loss: 0.8917094469070435\n",
      "Epoch: 4, Samples: 1856/5760, Loss: 0.6979020237922668\n",
      "Epoch: 4, Samples: 1888/5760, Loss: 0.8591164350509644\n",
      "Epoch: 4, Samples: 1920/5760, Loss: 0.7758579254150391\n",
      "Epoch: 4, Samples: 1952/5760, Loss: 0.7583854794502258\n",
      "Epoch: 4, Samples: 1984/5760, Loss: 0.6241658329963684\n",
      "Epoch: 4, Samples: 2016/5760, Loss: 0.7651427984237671\n",
      "Epoch: 4, Samples: 2048/5760, Loss: 0.9982950687408447\n",
      "Epoch: 4, Samples: 2080/5760, Loss: 0.6870539784431458\n",
      "Epoch: 4, Samples: 2112/5760, Loss: 0.8380646109580994\n",
      "Epoch: 4, Samples: 2144/5760, Loss: 0.8127431869506836\n",
      "Epoch: 4, Samples: 2176/5760, Loss: 0.6558507084846497\n",
      "Epoch: 4, Samples: 2208/5760, Loss: 0.6821129322052002\n",
      "Epoch: 4, Samples: 2240/5760, Loss: 0.7763908505439758\n",
      "Epoch: 4, Samples: 2272/5760, Loss: 0.8784489631652832\n",
      "Epoch: 4, Samples: 2304/5760, Loss: 0.8791190385818481\n",
      "Epoch: 4, Samples: 2336/5760, Loss: 0.6374443769454956\n",
      "Epoch: 4, Samples: 2368/5760, Loss: 0.6651936769485474\n",
      "Epoch: 4, Samples: 2400/5760, Loss: 0.4885287880897522\n",
      "Epoch: 4, Samples: 2432/5760, Loss: 0.9362167716026306\n",
      "Epoch: 4, Samples: 2464/5760, Loss: 0.63458651304245\n",
      "Epoch: 4, Samples: 2496/5760, Loss: 0.7082676887512207\n",
      "Epoch: 4, Samples: 2528/5760, Loss: 0.8733327984809875\n",
      "Epoch: 4, Samples: 2560/5760, Loss: 0.6839467883110046\n",
      "Epoch: 4, Samples: 2592/5760, Loss: 1.0638362169265747\n",
      "Epoch: 4, Samples: 2624/5760, Loss: 0.9411605596542358\n",
      "Epoch: 4, Samples: 2656/5760, Loss: 0.7389751672744751\n",
      "Epoch: 4, Samples: 2688/5760, Loss: 0.8978166580200195\n",
      "Epoch: 4, Samples: 2720/5760, Loss: 0.8530699610710144\n",
      "Epoch: 4, Samples: 2752/5760, Loss: 0.604157567024231\n",
      "Epoch: 4, Samples: 2784/5760, Loss: 0.8339693546295166\n",
      "Epoch: 4, Samples: 2816/5760, Loss: 0.9661440253257751\n",
      "Epoch: 4, Samples: 2848/5760, Loss: 0.9589162468910217\n",
      "Epoch: 4, Samples: 2880/5760, Loss: 0.9093475937843323\n",
      "Epoch: 4, Samples: 2912/5760, Loss: 0.914691686630249\n",
      "Epoch: 4, Samples: 2944/5760, Loss: 0.662266731262207\n",
      "Epoch: 4, Samples: 2976/5760, Loss: 0.6904473304748535\n",
      "Epoch: 4, Samples: 3008/5760, Loss: 0.6408952474594116\n",
      "Epoch: 4, Samples: 3040/5760, Loss: 1.1089924573898315\n",
      "Epoch: 4, Samples: 3072/5760, Loss: 0.7203511595726013\n",
      "Epoch: 4, Samples: 3104/5760, Loss: 0.8355547785758972\n",
      "Epoch: 4, Samples: 3136/5760, Loss: 0.6224020719528198\n",
      "Epoch: 4, Samples: 3168/5760, Loss: 0.6855486035346985\n",
      "Epoch: 4, Samples: 3200/5760, Loss: 0.8316560387611389\n",
      "Epoch: 4, Samples: 3232/5760, Loss: 0.9696051478385925\n",
      "Epoch: 4, Samples: 3264/5760, Loss: 0.7457895874977112\n",
      "Epoch: 4, Samples: 3296/5760, Loss: 0.7742034792900085\n",
      "Epoch: 4, Samples: 3328/5760, Loss: 0.7857491970062256\n",
      "Epoch: 4, Samples: 3360/5760, Loss: 0.8752342462539673\n",
      "Epoch: 4, Samples: 3392/5760, Loss: 0.5678552389144897\n",
      "Epoch: 4, Samples: 3424/5760, Loss: 0.6961978673934937\n",
      "Epoch: 4, Samples: 3456/5760, Loss: 0.7081166505813599\n",
      "Epoch: 4, Samples: 3488/5760, Loss: 0.7530373930931091\n",
      "Epoch: 4, Samples: 3520/5760, Loss: 0.7141953110694885\n",
      "Epoch: 4, Samples: 3552/5760, Loss: 0.9090694189071655\n",
      "Epoch: 4, Samples: 3584/5760, Loss: 0.681193470954895\n",
      "Epoch: 4, Samples: 3616/5760, Loss: 0.6501132249832153\n",
      "Epoch: 4, Samples: 3648/5760, Loss: 1.0161707401275635\n",
      "Epoch: 4, Samples: 3680/5760, Loss: 0.6458219289779663\n",
      "Epoch: 4, Samples: 3712/5760, Loss: 0.9197739958763123\n",
      "Epoch: 4, Samples: 3744/5760, Loss: 0.679603099822998\n",
      "Epoch: 4, Samples: 3776/5760, Loss: 0.733311653137207\n",
      "Epoch: 4, Samples: 3808/5760, Loss: 0.7575852870941162\n",
      "Epoch: 4, Samples: 3840/5760, Loss: 0.7143435478210449\n",
      "Epoch: 4, Samples: 3872/5760, Loss: 0.9161506295204163\n",
      "Epoch: 4, Samples: 3904/5760, Loss: 0.5914710164070129\n",
      "Epoch: 4, Samples: 3936/5760, Loss: 1.1045348644256592\n",
      "Epoch: 4, Samples: 3968/5760, Loss: 0.9525105953216553\n",
      "Epoch: 4, Samples: 4000/5760, Loss: 0.7830609679222107\n",
      "Epoch: 4, Samples: 4032/5760, Loss: 0.6744310259819031\n",
      "Epoch: 4, Samples: 4064/5760, Loss: 0.7916969060897827\n",
      "Epoch: 4, Samples: 4096/5760, Loss: 0.9029951095581055\n",
      "Epoch: 4, Samples: 4128/5760, Loss: 0.8804331421852112\n",
      "Epoch: 4, Samples: 4160/5760, Loss: 0.8472837209701538\n",
      "Epoch: 4, Samples: 4192/5760, Loss: 0.40724241733551025\n",
      "Epoch: 4, Samples: 4224/5760, Loss: 0.776503324508667\n",
      "Epoch: 4, Samples: 4256/5760, Loss: 0.7018019556999207\n",
      "Epoch: 4, Samples: 4288/5760, Loss: 0.7395908236503601\n",
      "Epoch: 4, Samples: 4320/5760, Loss: 0.7823166251182556\n",
      "Epoch: 4, Samples: 4352/5760, Loss: 0.7866197228431702\n",
      "Epoch: 4, Samples: 4384/5760, Loss: 0.8275822997093201\n",
      "Epoch: 4, Samples: 4416/5760, Loss: 0.787899911403656\n",
      "Epoch: 4, Samples: 4448/5760, Loss: 0.7124758362770081\n",
      "Epoch: 4, Samples: 4480/5760, Loss: 0.38543060421943665\n",
      "Epoch: 4, Samples: 4512/5760, Loss: 0.49762511253356934\n",
      "Epoch: 4, Samples: 4544/5760, Loss: 0.7354099750518799\n",
      "Epoch: 4, Samples: 4576/5760, Loss: 0.6894442439079285\n",
      "Epoch: 4, Samples: 4608/5760, Loss: 0.9685877561569214\n",
      "Epoch: 4, Samples: 4640/5760, Loss: 0.838196873664856\n",
      "Epoch: 4, Samples: 4672/5760, Loss: 0.877935528755188\n",
      "Epoch: 4, Samples: 4704/5760, Loss: 0.8565114140510559\n",
      "Epoch: 4, Samples: 4736/5760, Loss: 0.7303882837295532\n",
      "Epoch: 4, Samples: 4768/5760, Loss: 0.7824958562850952\n",
      "Epoch: 4, Samples: 4800/5760, Loss: 0.653090238571167\n",
      "Epoch: 4, Samples: 4832/5760, Loss: 0.7532600164413452\n",
      "Epoch: 4, Samples: 4864/5760, Loss: 0.763375997543335\n",
      "Epoch: 4, Samples: 4896/5760, Loss: 0.8073767423629761\n",
      "Epoch: 4, Samples: 4928/5760, Loss: 0.7706507444381714\n",
      "Epoch: 4, Samples: 4960/5760, Loss: 0.6118948459625244\n",
      "Epoch: 4, Samples: 4992/5760, Loss: 0.6119236946105957\n",
      "Epoch: 4, Samples: 5024/5760, Loss: 0.8310446739196777\n",
      "Epoch: 4, Samples: 5056/5760, Loss: 0.6734175682067871\n",
      "Epoch: 4, Samples: 5088/5760, Loss: 0.7257913947105408\n",
      "Epoch: 4, Samples: 5120/5760, Loss: 0.7870535850524902\n",
      "Epoch: 4, Samples: 5152/5760, Loss: 0.6081119179725647\n",
      "Epoch: 4, Samples: 5184/5760, Loss: 0.8355835676193237\n",
      "Epoch: 4, Samples: 5216/5760, Loss: 0.7937424778938293\n",
      "Epoch: 4, Samples: 5248/5760, Loss: 0.540851354598999\n",
      "Epoch: 4, Samples: 5280/5760, Loss: 0.7661058306694031\n",
      "Epoch: 4, Samples: 5312/5760, Loss: 0.7771469354629517\n",
      "Epoch: 4, Samples: 5344/5760, Loss: 0.6313402056694031\n",
      "Epoch: 4, Samples: 5376/5760, Loss: 1.0217705965042114\n",
      "Epoch: 4, Samples: 5408/5760, Loss: 0.8178359270095825\n",
      "Epoch: 4, Samples: 5440/5760, Loss: 0.5429893732070923\n",
      "Epoch: 4, Samples: 5472/5760, Loss: 0.7085054516792297\n",
      "Epoch: 4, Samples: 5504/5760, Loss: 0.6597263813018799\n",
      "Epoch: 4, Samples: 5536/5760, Loss: 0.7835825085639954\n",
      "Epoch: 4, Samples: 5568/5760, Loss: 0.8729109764099121\n",
      "Epoch: 4, Samples: 5600/5760, Loss: 0.8351088762283325\n",
      "Epoch: 4, Samples: 5632/5760, Loss: 0.6229797005653381\n",
      "Epoch: 4, Samples: 5664/5760, Loss: 0.698273777961731\n",
      "Epoch: 4, Samples: 5696/5760, Loss: 0.4801388084888458\n",
      "Epoch: 4, Samples: 5728/5760, Loss: 1.0503427982330322\n",
      "\n",
      "Epoch: 4\n",
      "Training set: Average loss: 0.7934\n",
      "Validation set: Average loss: 0.7871, Accuracy: 700/818 (86%)\n",
      "Saving model (epoch 4) with lowest validation loss: 0.7871259152889252\n",
      "Epoch: 5, Samples: 0/5760, Loss: 0.48525169491767883\n",
      "Epoch: 5, Samples: 32/5760, Loss: 0.5955396890640259\n",
      "Epoch: 5, Samples: 64/5760, Loss: 0.6554951071739197\n",
      "Epoch: 5, Samples: 96/5760, Loss: 0.6200188994407654\n",
      "Epoch: 5, Samples: 128/5760, Loss: 0.6336666345596313\n",
      "Epoch: 5, Samples: 160/5760, Loss: 0.6133962869644165\n",
      "Epoch: 5, Samples: 192/5760, Loss: 0.4535902440547943\n",
      "Epoch: 5, Samples: 224/5760, Loss: 0.678451657295227\n",
      "Epoch: 5, Samples: 256/5760, Loss: 0.6922826766967773\n",
      "Epoch: 5, Samples: 288/5760, Loss: 0.5346633195877075\n",
      "Epoch: 5, Samples: 320/5760, Loss: 0.5022004246711731\n",
      "Epoch: 5, Samples: 352/5760, Loss: 0.5555870532989502\n",
      "Epoch: 5, Samples: 384/5760, Loss: 0.5658895373344421\n",
      "Epoch: 5, Samples: 416/5760, Loss: 0.5662366151809692\n",
      "Epoch: 5, Samples: 448/5760, Loss: 0.663970947265625\n",
      "Epoch: 5, Samples: 480/5760, Loss: 0.6419247388839722\n",
      "Epoch: 5, Samples: 512/5760, Loss: 0.7230530381202698\n",
      "Epoch: 5, Samples: 544/5760, Loss: 0.4930611550807953\n",
      "Epoch: 5, Samples: 576/5760, Loss: 0.7387553453445435\n",
      "Epoch: 5, Samples: 608/5760, Loss: 0.4110937714576721\n",
      "Epoch: 5, Samples: 640/5760, Loss: 0.7896292209625244\n",
      "Epoch: 5, Samples: 672/5760, Loss: 0.6730427145957947\n",
      "Epoch: 5, Samples: 704/5760, Loss: 0.7799965739250183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Samples: 736/5760, Loss: 0.41472959518432617\n",
      "Epoch: 5, Samples: 768/5760, Loss: 0.5530381202697754\n",
      "Epoch: 5, Samples: 800/5760, Loss: 0.7262091636657715\n",
      "Epoch: 5, Samples: 832/5760, Loss: 0.5988219976425171\n",
      "Epoch: 5, Samples: 864/5760, Loss: 0.6585223078727722\n",
      "Epoch: 5, Samples: 896/5760, Loss: 0.5941503047943115\n",
      "Epoch: 5, Samples: 928/5760, Loss: 0.7722218036651611\n",
      "Epoch: 5, Samples: 960/5760, Loss: 0.5985637903213501\n",
      "Epoch: 5, Samples: 992/5760, Loss: 0.5016716718673706\n",
      "Epoch: 5, Samples: 1024/5760, Loss: 0.5642602443695068\n",
      "Epoch: 5, Samples: 1056/5760, Loss: 0.8711889982223511\n",
      "Epoch: 5, Samples: 1088/5760, Loss: 0.4953811764717102\n",
      "Epoch: 5, Samples: 1120/5760, Loss: 0.7216495275497437\n",
      "Epoch: 5, Samples: 1152/5760, Loss: 0.4387741684913635\n",
      "Epoch: 5, Samples: 1184/5760, Loss: 0.5326858758926392\n",
      "Epoch: 5, Samples: 1216/5760, Loss: 0.5029650926589966\n",
      "Epoch: 5, Samples: 1248/5760, Loss: 0.6658204793930054\n",
      "Epoch: 5, Samples: 1280/5760, Loss: 0.28247812390327454\n",
      "Epoch: 5, Samples: 1312/5760, Loss: 0.6373389959335327\n",
      "Epoch: 5, Samples: 1344/5760, Loss: 0.43627893924713135\n",
      "Epoch: 5, Samples: 1376/5760, Loss: 1.1890679597854614\n",
      "Epoch: 5, Samples: 1408/5760, Loss: 0.5201241970062256\n",
      "Epoch: 5, Samples: 1440/5760, Loss: 0.6066499948501587\n",
      "Epoch: 5, Samples: 1472/5760, Loss: 0.6500036120414734\n",
      "Epoch: 5, Samples: 1504/5760, Loss: 0.4282820224761963\n",
      "Epoch: 5, Samples: 1536/5760, Loss: 0.5329337120056152\n",
      "Epoch: 5, Samples: 1568/5760, Loss: 0.5206268429756165\n",
      "Epoch: 5, Samples: 1600/5760, Loss: 0.4884721636772156\n",
      "Epoch: 5, Samples: 1632/5760, Loss: 0.5573694109916687\n",
      "Epoch: 5, Samples: 1664/5760, Loss: 0.5187113285064697\n",
      "Epoch: 5, Samples: 1696/5760, Loss: 0.5877339839935303\n",
      "Epoch: 5, Samples: 1728/5760, Loss: 0.5059179663658142\n",
      "Epoch: 5, Samples: 1760/5760, Loss: 0.5850697159767151\n",
      "Epoch: 5, Samples: 1792/5760, Loss: 0.5539294481277466\n",
      "Epoch: 5, Samples: 1824/5760, Loss: 0.610737681388855\n",
      "Epoch: 5, Samples: 1856/5760, Loss: 0.5248497724533081\n",
      "Epoch: 5, Samples: 1888/5760, Loss: 0.5502927303314209\n",
      "Epoch: 5, Samples: 1920/5760, Loss: 0.7850444316864014\n",
      "Epoch: 5, Samples: 1952/5760, Loss: 0.5737093687057495\n",
      "Epoch: 5, Samples: 1984/5760, Loss: 0.42862066626548767\n",
      "Epoch: 5, Samples: 2016/5760, Loss: 0.7306020259857178\n",
      "Epoch: 5, Samples: 2048/5760, Loss: 0.6603087186813354\n",
      "Epoch: 5, Samples: 2080/5760, Loss: 0.6850858926773071\n",
      "Epoch: 5, Samples: 2112/5760, Loss: 0.6133524179458618\n",
      "Epoch: 5, Samples: 2144/5760, Loss: 0.6300399899482727\n",
      "Epoch: 5, Samples: 2176/5760, Loss: 0.615922749042511\n",
      "Epoch: 5, Samples: 2208/5760, Loss: 0.5202459096908569\n",
      "Epoch: 5, Samples: 2240/5760, Loss: 0.4204837679862976\n",
      "Epoch: 5, Samples: 2272/5760, Loss: 0.7970860004425049\n",
      "Epoch: 5, Samples: 2304/5760, Loss: 0.6922981142997742\n",
      "Epoch: 5, Samples: 2336/5760, Loss: 0.7205358147621155\n",
      "Epoch: 5, Samples: 2368/5760, Loss: 0.49902158975601196\n",
      "Epoch: 5, Samples: 2400/5760, Loss: 0.5400789976119995\n",
      "Epoch: 5, Samples: 2432/5760, Loss: 0.6110262870788574\n",
      "Epoch: 5, Samples: 2464/5760, Loss: 0.5829581618309021\n",
      "Epoch: 5, Samples: 2496/5760, Loss: 0.548642098903656\n",
      "Epoch: 5, Samples: 2528/5760, Loss: 0.539932370185852\n",
      "Epoch: 5, Samples: 2560/5760, Loss: 0.6909211874008179\n",
      "Epoch: 5, Samples: 2592/5760, Loss: 0.6059888005256653\n",
      "Epoch: 5, Samples: 2624/5760, Loss: 0.5139950513839722\n",
      "Epoch: 5, Samples: 2656/5760, Loss: 0.5087802410125732\n",
      "Epoch: 5, Samples: 2688/5760, Loss: 0.39974844455718994\n",
      "Epoch: 5, Samples: 2720/5760, Loss: 0.5474385023117065\n",
      "Epoch: 5, Samples: 2752/5760, Loss: 0.6388161182403564\n",
      "Epoch: 5, Samples: 2784/5760, Loss: 0.6613770723342896\n",
      "Epoch: 5, Samples: 2816/5760, Loss: 0.4839983582496643\n",
      "Epoch: 5, Samples: 2848/5760, Loss: 0.5957262516021729\n",
      "Epoch: 5, Samples: 2880/5760, Loss: 0.4895416498184204\n",
      "Epoch: 5, Samples: 2912/5760, Loss: 0.581976056098938\n",
      "Epoch: 5, Samples: 2944/5760, Loss: 0.6579602956771851\n",
      "Epoch: 5, Samples: 2976/5760, Loss: 0.32234418392181396\n",
      "Epoch: 5, Samples: 3008/5760, Loss: 0.5236753225326538\n",
      "Epoch: 5, Samples: 3040/5760, Loss: 0.5226801633834839\n",
      "Epoch: 5, Samples: 3072/5760, Loss: 0.7050215601921082\n",
      "Epoch: 5, Samples: 3104/5760, Loss: 0.3897095322608948\n",
      "Epoch: 5, Samples: 3136/5760, Loss: 0.4386133551597595\n",
      "Epoch: 5, Samples: 3168/5760, Loss: 0.598242998123169\n",
      "Epoch: 5, Samples: 3200/5760, Loss: 0.5540589094161987\n",
      "Epoch: 5, Samples: 3232/5760, Loss: 0.5565745830535889\n",
      "Epoch: 5, Samples: 3264/5760, Loss: 0.4364352822303772\n",
      "Epoch: 5, Samples: 3296/5760, Loss: 0.3247760832309723\n",
      "Epoch: 5, Samples: 3328/5760, Loss: 0.396159291267395\n",
      "Epoch: 5, Samples: 3360/5760, Loss: 0.4710198640823364\n",
      "Epoch: 5, Samples: 3392/5760, Loss: 0.6430711150169373\n",
      "Epoch: 5, Samples: 3424/5760, Loss: 0.6078310012817383\n",
      "Epoch: 5, Samples: 3456/5760, Loss: 0.9065635800361633\n",
      "Epoch: 5, Samples: 3488/5760, Loss: 0.5284545421600342\n",
      "Epoch: 5, Samples: 3520/5760, Loss: 0.6467171907424927\n",
      "Epoch: 5, Samples: 3552/5760, Loss: 0.6244106292724609\n",
      "Epoch: 5, Samples: 3584/5760, Loss: 0.4281634986400604\n",
      "Epoch: 5, Samples: 3616/5760, Loss: 0.546700119972229\n",
      "Epoch: 5, Samples: 3648/5760, Loss: 0.4257180988788605\n",
      "Epoch: 5, Samples: 3680/5760, Loss: 0.5164235830307007\n",
      "Epoch: 5, Samples: 3712/5760, Loss: 0.8378878235816956\n",
      "Epoch: 5, Samples: 3744/5760, Loss: 0.4172046184539795\n",
      "Epoch: 5, Samples: 3776/5760, Loss: 0.5753259658813477\n",
      "Epoch: 5, Samples: 3808/5760, Loss: 0.6854758262634277\n",
      "Epoch: 5, Samples: 3840/5760, Loss: 0.4112510681152344\n",
      "Epoch: 5, Samples: 3872/5760, Loss: 0.47314825654029846\n",
      "Epoch: 5, Samples: 3904/5760, Loss: 0.655454158782959\n",
      "Epoch: 5, Samples: 3936/5760, Loss: 0.4922664761543274\n",
      "Epoch: 5, Samples: 3968/5760, Loss: 0.4613751471042633\n",
      "Epoch: 5, Samples: 4000/5760, Loss: 0.5789127945899963\n",
      "Epoch: 5, Samples: 4032/5760, Loss: 0.6459436416625977\n",
      "Epoch: 5, Samples: 4064/5760, Loss: 0.635766863822937\n",
      "Epoch: 5, Samples: 4096/5760, Loss: 0.4256482720375061\n",
      "Epoch: 5, Samples: 4128/5760, Loss: 0.39696717262268066\n",
      "Epoch: 5, Samples: 4160/5760, Loss: 0.6566991209983826\n",
      "Epoch: 5, Samples: 4192/5760, Loss: 0.33658140897750854\n",
      "Epoch: 5, Samples: 4224/5760, Loss: 0.5754513144493103\n",
      "Epoch: 5, Samples: 4256/5760, Loss: 0.6807106137275696\n",
      "Epoch: 5, Samples: 4288/5760, Loss: 0.6292630434036255\n",
      "Epoch: 5, Samples: 4320/5760, Loss: 0.36528223752975464\n",
      "Epoch: 5, Samples: 4352/5760, Loss: 0.620577871799469\n",
      "Epoch: 5, Samples: 4384/5760, Loss: 0.5482473969459534\n",
      "Epoch: 5, Samples: 4416/5760, Loss: 0.42673492431640625\n",
      "Epoch: 5, Samples: 4448/5760, Loss: 0.499314546585083\n",
      "Epoch: 5, Samples: 4480/5760, Loss: 0.3194606900215149\n",
      "Epoch: 5, Samples: 4512/5760, Loss: 0.7312073707580566\n",
      "Epoch: 5, Samples: 4544/5760, Loss: 0.6841431260108948\n",
      "Epoch: 5, Samples: 4576/5760, Loss: 0.6359013319015503\n",
      "Epoch: 5, Samples: 4608/5760, Loss: 0.29765585064888\n",
      "Epoch: 5, Samples: 4640/5760, Loss: 0.6272838115692139\n",
      "Epoch: 5, Samples: 4672/5760, Loss: 0.28112170100212097\n",
      "Epoch: 5, Samples: 4704/5760, Loss: 0.4582264721393585\n",
      "Epoch: 5, Samples: 4736/5760, Loss: 0.6059478521347046\n",
      "Epoch: 5, Samples: 4768/5760, Loss: 0.48278701305389404\n",
      "Epoch: 5, Samples: 4800/5760, Loss: 0.4790050983428955\n",
      "Epoch: 5, Samples: 4832/5760, Loss: 0.5606913566589355\n",
      "Epoch: 5, Samples: 4864/5760, Loss: 0.39865463972091675\n",
      "Epoch: 5, Samples: 4896/5760, Loss: 0.5097032785415649\n",
      "Epoch: 5, Samples: 4928/5760, Loss: 0.589606523513794\n",
      "Epoch: 5, Samples: 4960/5760, Loss: 0.4618023633956909\n",
      "Epoch: 5, Samples: 4992/5760, Loss: 0.3469636142253876\n",
      "Epoch: 5, Samples: 5024/5760, Loss: 0.41666439175605774\n",
      "Epoch: 5, Samples: 5056/5760, Loss: 0.5031607151031494\n",
      "Epoch: 5, Samples: 5088/5760, Loss: 0.3960706889629364\n",
      "Epoch: 5, Samples: 5120/5760, Loss: 0.2562147378921509\n",
      "Epoch: 5, Samples: 5152/5760, Loss: 0.3574064075946808\n",
      "Epoch: 5, Samples: 5184/5760, Loss: 0.39445507526397705\n",
      "Epoch: 5, Samples: 5216/5760, Loss: 0.7494235038757324\n",
      "Epoch: 5, Samples: 5248/5760, Loss: 0.4338061809539795\n",
      "Epoch: 5, Samples: 5280/5760, Loss: 0.6010547876358032\n",
      "Epoch: 5, Samples: 5312/5760, Loss: 0.4849576950073242\n",
      "Epoch: 5, Samples: 5344/5760, Loss: 0.5103648900985718\n",
      "Epoch: 5, Samples: 5376/5760, Loss: 0.43295109272003174\n",
      "Epoch: 5, Samples: 5408/5760, Loss: 0.4438113868236542\n",
      "Epoch: 5, Samples: 5440/5760, Loss: 0.690039873123169\n",
      "Epoch: 5, Samples: 5472/5760, Loss: 0.2632443308830261\n",
      "Epoch: 5, Samples: 5504/5760, Loss: 0.32052189111709595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Samples: 5536/5760, Loss: 0.4946536421775818\n",
      "Epoch: 5, Samples: 5568/5760, Loss: 0.5761688947677612\n",
      "Epoch: 5, Samples: 5600/5760, Loss: 0.6311002969741821\n",
      "Epoch: 5, Samples: 5632/5760, Loss: 0.5873763561248779\n",
      "Epoch: 5, Samples: 5664/5760, Loss: 0.4904439449310303\n",
      "Epoch: 5, Samples: 5696/5760, Loss: 0.6014081239700317\n",
      "Epoch: 5, Samples: 5728/5760, Loss: 2.332237720489502\n",
      "\n",
      "Epoch: 5\n",
      "Training set: Average loss: 0.5651\n",
      "Validation set: Average loss: 0.6562, Accuracy: 703/818 (86%)\n",
      "Saving model (epoch 5) with lowest validation loss: 0.6561657431033942\n",
      "Epoch: 6, Samples: 0/5760, Loss: 0.5845206379890442\n",
      "Epoch: 6, Samples: 32/5760, Loss: 0.5330395698547363\n",
      "Epoch: 6, Samples: 64/5760, Loss: 0.3673168420791626\n",
      "Epoch: 6, Samples: 96/5760, Loss: 0.4101771116256714\n",
      "Epoch: 6, Samples: 128/5760, Loss: 0.3635111451148987\n",
      "Epoch: 6, Samples: 160/5760, Loss: 0.47895562648773193\n",
      "Epoch: 6, Samples: 192/5760, Loss: 0.3407299816608429\n",
      "Epoch: 6, Samples: 224/5760, Loss: 0.6375648379325867\n",
      "Epoch: 6, Samples: 256/5760, Loss: 0.2921789288520813\n",
      "Epoch: 6, Samples: 288/5760, Loss: 0.5102465152740479\n",
      "Epoch: 6, Samples: 320/5760, Loss: 0.5639041066169739\n",
      "Epoch: 6, Samples: 352/5760, Loss: 0.4545019268989563\n",
      "Epoch: 6, Samples: 384/5760, Loss: 0.44637131690979004\n",
      "Epoch: 6, Samples: 416/5760, Loss: 0.4641980826854706\n",
      "Epoch: 6, Samples: 448/5760, Loss: 0.45722198486328125\n",
      "Epoch: 6, Samples: 480/5760, Loss: 0.6086969971656799\n",
      "Epoch: 6, Samples: 512/5760, Loss: 0.36988067626953125\n",
      "Epoch: 6, Samples: 544/5760, Loss: 0.573432207107544\n",
      "Epoch: 6, Samples: 576/5760, Loss: 0.28502702713012695\n",
      "Epoch: 6, Samples: 608/5760, Loss: 0.44369590282440186\n",
      "Epoch: 6, Samples: 640/5760, Loss: 0.5952942371368408\n",
      "Epoch: 6, Samples: 672/5760, Loss: 0.3896688222885132\n",
      "Epoch: 6, Samples: 704/5760, Loss: 0.7714053392410278\n",
      "Epoch: 6, Samples: 736/5760, Loss: 0.3544169068336487\n",
      "Epoch: 6, Samples: 768/5760, Loss: 0.5216583609580994\n",
      "Epoch: 6, Samples: 800/5760, Loss: 0.4403400123119354\n",
      "Epoch: 6, Samples: 832/5760, Loss: 0.4600456953048706\n",
      "Epoch: 6, Samples: 864/5760, Loss: 0.3314885199069977\n",
      "Epoch: 6, Samples: 896/5760, Loss: 0.3395330607891083\n",
      "Epoch: 6, Samples: 928/5760, Loss: 0.4235358238220215\n",
      "Epoch: 6, Samples: 960/5760, Loss: 0.5458101034164429\n",
      "Epoch: 6, Samples: 992/5760, Loss: 0.29211223125457764\n",
      "Epoch: 6, Samples: 1024/5760, Loss: 0.35428696870803833\n",
      "Epoch: 6, Samples: 1056/5760, Loss: 0.3909313976764679\n",
      "Epoch: 6, Samples: 1088/5760, Loss: 0.49615296721458435\n",
      "Epoch: 6, Samples: 1120/5760, Loss: 0.3448294699192047\n",
      "Epoch: 6, Samples: 1152/5760, Loss: 0.2913079261779785\n",
      "Epoch: 6, Samples: 1184/5760, Loss: 0.22015441954135895\n",
      "Epoch: 6, Samples: 1216/5760, Loss: 0.5225403904914856\n",
      "Epoch: 6, Samples: 1248/5760, Loss: 0.39171141386032104\n",
      "Epoch: 6, Samples: 1280/5760, Loss: 0.4063747823238373\n",
      "Epoch: 6, Samples: 1312/5760, Loss: 0.5121089220046997\n",
      "Epoch: 6, Samples: 1344/5760, Loss: 0.2442047894001007\n",
      "Epoch: 6, Samples: 1376/5760, Loss: 0.440414696931839\n",
      "Epoch: 6, Samples: 1408/5760, Loss: 0.5530251264572144\n",
      "Epoch: 6, Samples: 1440/5760, Loss: 0.43498045206069946\n",
      "Epoch: 6, Samples: 1472/5760, Loss: 0.4120746850967407\n",
      "Epoch: 6, Samples: 1504/5760, Loss: 0.34237223863601685\n",
      "Epoch: 6, Samples: 1536/5760, Loss: 0.43674272298812866\n",
      "Epoch: 6, Samples: 1568/5760, Loss: 0.4608558714389801\n",
      "Epoch: 6, Samples: 1600/5760, Loss: 0.3945310115814209\n",
      "Epoch: 6, Samples: 1632/5760, Loss: 0.4418804347515106\n",
      "Epoch: 6, Samples: 1664/5760, Loss: 0.28182852268218994\n",
      "Epoch: 6, Samples: 1696/5760, Loss: 0.43984442949295044\n",
      "Epoch: 6, Samples: 1728/5760, Loss: 0.33551034331321716\n",
      "Epoch: 6, Samples: 1760/5760, Loss: 0.3958311080932617\n",
      "Epoch: 6, Samples: 1792/5760, Loss: 0.45229580998420715\n",
      "Epoch: 6, Samples: 1824/5760, Loss: 0.3396044969558716\n",
      "Epoch: 6, Samples: 1856/5760, Loss: 0.3947691321372986\n",
      "Epoch: 6, Samples: 1888/5760, Loss: 0.4310937225818634\n",
      "Epoch: 6, Samples: 1920/5760, Loss: 0.3478776216506958\n",
      "Epoch: 6, Samples: 1952/5760, Loss: 0.4025622606277466\n",
      "Epoch: 6, Samples: 1984/5760, Loss: 0.547925591468811\n",
      "Epoch: 6, Samples: 2016/5760, Loss: 0.44295424222946167\n",
      "Epoch: 6, Samples: 2048/5760, Loss: 0.4515652358531952\n",
      "Epoch: 6, Samples: 2080/5760, Loss: 0.47123032808303833\n",
      "Epoch: 6, Samples: 2112/5760, Loss: 0.5615344047546387\n",
      "Epoch: 6, Samples: 2144/5760, Loss: 0.424001008272171\n",
      "Epoch: 6, Samples: 2176/5760, Loss: 0.38671377301216125\n",
      "Epoch: 6, Samples: 2208/5760, Loss: 0.3485347628593445\n",
      "Epoch: 6, Samples: 2240/5760, Loss: 0.3040011525154114\n",
      "Epoch: 6, Samples: 2272/5760, Loss: 0.2605319023132324\n",
      "Epoch: 6, Samples: 2304/5760, Loss: 0.5150771737098694\n",
      "Epoch: 6, Samples: 2336/5760, Loss: 0.41664400696754456\n",
      "Epoch: 6, Samples: 2368/5760, Loss: 0.4792976975440979\n",
      "Epoch: 6, Samples: 2400/5760, Loss: 0.2415163218975067\n",
      "Epoch: 6, Samples: 2432/5760, Loss: 0.38747331500053406\n",
      "Epoch: 6, Samples: 2464/5760, Loss: 0.5470626354217529\n",
      "Epoch: 6, Samples: 2496/5760, Loss: 0.5338026285171509\n",
      "Epoch: 6, Samples: 2528/5760, Loss: 0.31600746512413025\n",
      "Epoch: 6, Samples: 2560/5760, Loss: 0.39514997601509094\n",
      "Epoch: 6, Samples: 2592/5760, Loss: 0.2234925776720047\n",
      "Epoch: 6, Samples: 2624/5760, Loss: 0.5417700409889221\n",
      "Epoch: 6, Samples: 2656/5760, Loss: 0.46612077951431274\n",
      "Epoch: 6, Samples: 2688/5760, Loss: 0.27471405267715454\n",
      "Epoch: 6, Samples: 2720/5760, Loss: 0.36283189058303833\n",
      "Epoch: 6, Samples: 2752/5760, Loss: 0.37329789996147156\n",
      "Epoch: 6, Samples: 2784/5760, Loss: 0.36324647068977356\n",
      "Epoch: 6, Samples: 2816/5760, Loss: 0.5258370637893677\n",
      "Epoch: 6, Samples: 2848/5760, Loss: 0.5731491446495056\n",
      "Epoch: 6, Samples: 2880/5760, Loss: 0.3497530519962311\n",
      "Epoch: 6, Samples: 2912/5760, Loss: 0.5801377296447754\n",
      "Epoch: 6, Samples: 2944/5760, Loss: 0.3861387372016907\n",
      "Epoch: 6, Samples: 2976/5760, Loss: 0.5420984029769897\n",
      "Epoch: 6, Samples: 3008/5760, Loss: 0.45125022530555725\n",
      "Epoch: 6, Samples: 3040/5760, Loss: 0.6441754698753357\n",
      "Epoch: 6, Samples: 3072/5760, Loss: 0.44532325863838196\n",
      "Epoch: 6, Samples: 3104/5760, Loss: 0.38208436965942383\n",
      "Epoch: 6, Samples: 3136/5760, Loss: 0.4169086813926697\n",
      "Epoch: 6, Samples: 3168/5760, Loss: 0.40922313928604126\n",
      "Epoch: 6, Samples: 3200/5760, Loss: 0.5466141700744629\n",
      "Epoch: 6, Samples: 3232/5760, Loss: 0.5470731854438782\n",
      "Epoch: 6, Samples: 3264/5760, Loss: 0.42252683639526367\n",
      "Epoch: 6, Samples: 3296/5760, Loss: 0.3695104420185089\n",
      "Epoch: 6, Samples: 3328/5760, Loss: 0.7206952571868896\n",
      "Epoch: 6, Samples: 3360/5760, Loss: 0.35713160037994385\n",
      "Epoch: 6, Samples: 3392/5760, Loss: 0.410671591758728\n",
      "Epoch: 6, Samples: 3424/5760, Loss: 0.30445918440818787\n",
      "Epoch: 6, Samples: 3456/5760, Loss: 0.5261296033859253\n",
      "Epoch: 6, Samples: 3488/5760, Loss: 0.2951151430606842\n",
      "Epoch: 6, Samples: 3520/5760, Loss: 0.2527730464935303\n",
      "Epoch: 6, Samples: 3552/5760, Loss: 0.40898197889328003\n",
      "Epoch: 6, Samples: 3584/5760, Loss: 0.3287852704524994\n",
      "Epoch: 6, Samples: 3616/5760, Loss: 0.4408456087112427\n",
      "Epoch: 6, Samples: 3648/5760, Loss: 0.40108227729797363\n",
      "Epoch: 6, Samples: 3680/5760, Loss: 0.40600600838661194\n",
      "Epoch: 6, Samples: 3712/5760, Loss: 0.5203585624694824\n",
      "Epoch: 6, Samples: 3744/5760, Loss: 0.405645489692688\n",
      "Epoch: 6, Samples: 3776/5760, Loss: 0.31448304653167725\n",
      "Epoch: 6, Samples: 3808/5760, Loss: 0.4639708995819092\n",
      "Epoch: 6, Samples: 3840/5760, Loss: 0.3816230893135071\n",
      "Epoch: 6, Samples: 3872/5760, Loss: 0.4031651020050049\n",
      "Epoch: 6, Samples: 3904/5760, Loss: 0.4337615370750427\n",
      "Epoch: 6, Samples: 3936/5760, Loss: 0.2672975957393646\n",
      "Epoch: 6, Samples: 3968/5760, Loss: 0.4014670252799988\n",
      "Epoch: 6, Samples: 4000/5760, Loss: 0.42770829796791077\n",
      "Epoch: 6, Samples: 4032/5760, Loss: 0.4150451719760895\n",
      "Epoch: 6, Samples: 4064/5760, Loss: 0.520340085029602\n",
      "Epoch: 6, Samples: 4096/5760, Loss: 0.3985356390476227\n",
      "Epoch: 6, Samples: 4128/5760, Loss: 0.390459805727005\n",
      "Epoch: 6, Samples: 4160/5760, Loss: 0.5163784027099609\n",
      "Epoch: 6, Samples: 4192/5760, Loss: 0.35807865858078003\n",
      "Epoch: 6, Samples: 4224/5760, Loss: 0.27290278673171997\n",
      "Epoch: 6, Samples: 4256/5760, Loss: 0.29768455028533936\n",
      "Epoch: 6, Samples: 4288/5760, Loss: 0.27979618310928345\n",
      "Epoch: 6, Samples: 4320/5760, Loss: 0.37525779008865356\n",
      "Epoch: 6, Samples: 4352/5760, Loss: 0.2603289783000946\n",
      "Epoch: 6, Samples: 4384/5760, Loss: 0.2721374034881592\n",
      "Epoch: 6, Samples: 4416/5760, Loss: 0.27900487184524536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Samples: 4448/5760, Loss: 0.6412831544876099\n",
      "Epoch: 6, Samples: 4480/5760, Loss: 0.32380402088165283\n",
      "Epoch: 6, Samples: 4512/5760, Loss: 0.3831734359264374\n",
      "Epoch: 6, Samples: 4544/5760, Loss: 0.5336311459541321\n",
      "Epoch: 6, Samples: 4576/5760, Loss: 0.3951811194419861\n",
      "Epoch: 6, Samples: 4608/5760, Loss: 0.4996044933795929\n",
      "Epoch: 6, Samples: 4640/5760, Loss: 0.36981940269470215\n",
      "Epoch: 6, Samples: 4672/5760, Loss: 0.3192439377307892\n",
      "Epoch: 6, Samples: 4704/5760, Loss: 0.3796100914478302\n",
      "Epoch: 6, Samples: 4736/5760, Loss: 0.2914509177207947\n",
      "Epoch: 6, Samples: 4768/5760, Loss: 0.3975112736225128\n",
      "Epoch: 6, Samples: 4800/5760, Loss: 0.48599204421043396\n",
      "Epoch: 6, Samples: 4832/5760, Loss: 0.20128899812698364\n",
      "Epoch: 6, Samples: 4864/5760, Loss: 0.4817490875720978\n",
      "Epoch: 6, Samples: 4896/5760, Loss: 0.5158380270004272\n",
      "Epoch: 6, Samples: 4928/5760, Loss: 0.4409309923648834\n",
      "Epoch: 6, Samples: 4960/5760, Loss: 0.4734492897987366\n",
      "Epoch: 6, Samples: 4992/5760, Loss: 0.3736436367034912\n",
      "Epoch: 6, Samples: 5024/5760, Loss: 0.2328665405511856\n",
      "Epoch: 6, Samples: 5056/5760, Loss: 0.40591543912887573\n",
      "Epoch: 6, Samples: 5088/5760, Loss: 0.17938090860843658\n",
      "Epoch: 6, Samples: 5120/5760, Loss: 0.4025615453720093\n",
      "Epoch: 6, Samples: 5152/5760, Loss: 0.20973698794841766\n",
      "Epoch: 6, Samples: 5184/5760, Loss: 0.3368816375732422\n",
      "Epoch: 6, Samples: 5216/5760, Loss: 0.3735560476779938\n",
      "Epoch: 6, Samples: 5248/5760, Loss: 0.30490145087242126\n",
      "Epoch: 6, Samples: 5280/5760, Loss: 0.576449990272522\n",
      "Epoch: 6, Samples: 5312/5760, Loss: 0.38013002276420593\n",
      "Epoch: 6, Samples: 5344/5760, Loss: 0.4110519587993622\n",
      "Epoch: 6, Samples: 5376/5760, Loss: 0.6191174983978271\n",
      "Epoch: 6, Samples: 5408/5760, Loss: 0.31318414211273193\n",
      "Epoch: 6, Samples: 5440/5760, Loss: 0.32795479893684387\n",
      "Epoch: 6, Samples: 5472/5760, Loss: 0.5701202154159546\n",
      "Epoch: 6, Samples: 5504/5760, Loss: 0.22795365750789642\n",
      "Epoch: 6, Samples: 5536/5760, Loss: 0.41631263494491577\n",
      "Epoch: 6, Samples: 5568/5760, Loss: 0.30188027024269104\n",
      "Epoch: 6, Samples: 5600/5760, Loss: 0.2982698380947113\n",
      "Epoch: 6, Samples: 5632/5760, Loss: 0.23092854022979736\n",
      "Epoch: 6, Samples: 5664/5760, Loss: 0.3682553768157959\n",
      "Epoch: 6, Samples: 5696/5760, Loss: 0.47298553586006165\n",
      "Epoch: 6, Samples: 5728/5760, Loss: 1.9340810775756836\n",
      "\n",
      "Epoch: 6\n",
      "Training set: Average loss: 0.4206\n",
      "Validation set: Average loss: 0.5704, Accuracy: 717/818 (88%)\n",
      "Saving model (epoch 6) with lowest validation loss: 0.570374860213353\n",
      "Epoch: 7, Samples: 0/5760, Loss: 0.3252565264701843\n",
      "Epoch: 7, Samples: 32/5760, Loss: 0.27908340096473694\n",
      "Epoch: 7, Samples: 64/5760, Loss: 0.25391924381256104\n",
      "Epoch: 7, Samples: 96/5760, Loss: 0.3227938115596771\n",
      "Epoch: 7, Samples: 128/5760, Loss: 0.28333407640457153\n",
      "Epoch: 7, Samples: 160/5760, Loss: 0.421225368976593\n",
      "Epoch: 7, Samples: 192/5760, Loss: 0.40990176796913147\n",
      "Epoch: 7, Samples: 224/5760, Loss: 0.381652295589447\n",
      "Epoch: 7, Samples: 256/5760, Loss: 0.2742385268211365\n",
      "Epoch: 7, Samples: 288/5760, Loss: 0.2919703722000122\n",
      "Epoch: 7, Samples: 320/5760, Loss: 0.4154849052429199\n",
      "Epoch: 7, Samples: 352/5760, Loss: 0.380218505859375\n",
      "Epoch: 7, Samples: 384/5760, Loss: 0.46262532472610474\n",
      "Epoch: 7, Samples: 416/5760, Loss: 0.23324762284755707\n",
      "Epoch: 7, Samples: 448/5760, Loss: 0.24439211189746857\n",
      "Epoch: 7, Samples: 480/5760, Loss: 0.24314382672309875\n",
      "Epoch: 7, Samples: 512/5760, Loss: 0.24836114048957825\n",
      "Epoch: 7, Samples: 544/5760, Loss: 0.2368568629026413\n",
      "Epoch: 7, Samples: 576/5760, Loss: 0.39427322149276733\n",
      "Epoch: 7, Samples: 608/5760, Loss: 0.3470546007156372\n",
      "Epoch: 7, Samples: 640/5760, Loss: 0.38824403285980225\n",
      "Epoch: 7, Samples: 672/5760, Loss: 0.29298102855682373\n",
      "Epoch: 7, Samples: 704/5760, Loss: 0.39865344762802124\n",
      "Epoch: 7, Samples: 736/5760, Loss: 0.2815115749835968\n",
      "Epoch: 7, Samples: 768/5760, Loss: 0.3073405921459198\n",
      "Epoch: 7, Samples: 800/5760, Loss: 0.24260225892066956\n",
      "Epoch: 7, Samples: 832/5760, Loss: 0.21944688260555267\n",
      "Epoch: 7, Samples: 864/5760, Loss: 0.21397173404693604\n",
      "Epoch: 7, Samples: 896/5760, Loss: 0.25532978773117065\n",
      "Epoch: 7, Samples: 928/5760, Loss: 0.2276463359594345\n",
      "Epoch: 7, Samples: 960/5760, Loss: 0.46668773889541626\n",
      "Epoch: 7, Samples: 992/5760, Loss: 0.2651739716529846\n",
      "Epoch: 7, Samples: 1024/5760, Loss: 0.21797621250152588\n",
      "Epoch: 7, Samples: 1056/5760, Loss: 0.22090385854244232\n",
      "Epoch: 7, Samples: 1088/5760, Loss: 0.3052878975868225\n",
      "Epoch: 7, Samples: 1120/5760, Loss: 0.19286009669303894\n",
      "Epoch: 7, Samples: 1152/5760, Loss: 0.3778128921985626\n",
      "Epoch: 7, Samples: 1184/5760, Loss: 0.4268769919872284\n",
      "Epoch: 7, Samples: 1216/5760, Loss: 0.3643076419830322\n",
      "Epoch: 7, Samples: 1248/5760, Loss: 0.3737710118293762\n",
      "Epoch: 7, Samples: 1280/5760, Loss: 0.31847822666168213\n",
      "Epoch: 7, Samples: 1312/5760, Loss: 0.2759447991847992\n",
      "Epoch: 7, Samples: 1344/5760, Loss: 0.25271138548851013\n",
      "Epoch: 7, Samples: 1376/5760, Loss: 0.4305216670036316\n",
      "Epoch: 7, Samples: 1408/5760, Loss: 0.25181764364242554\n",
      "Epoch: 7, Samples: 1440/5760, Loss: 0.3851034343242645\n",
      "Epoch: 7, Samples: 1472/5760, Loss: 0.1908537894487381\n",
      "Epoch: 7, Samples: 1504/5760, Loss: 0.35308778285980225\n",
      "Epoch: 7, Samples: 1536/5760, Loss: 0.3519754707813263\n",
      "Epoch: 7, Samples: 1568/5760, Loss: 0.42297232151031494\n",
      "Epoch: 7, Samples: 1600/5760, Loss: 0.24216721951961517\n",
      "Epoch: 7, Samples: 1632/5760, Loss: 0.3141644597053528\n",
      "Epoch: 7, Samples: 1664/5760, Loss: 0.30208975076675415\n",
      "Epoch: 7, Samples: 1696/5760, Loss: 0.1684303879737854\n",
      "Epoch: 7, Samples: 1728/5760, Loss: 0.27832603454589844\n",
      "Epoch: 7, Samples: 1760/5760, Loss: 0.27046412229537964\n",
      "Epoch: 7, Samples: 1792/5760, Loss: 0.42023909091949463\n",
      "Epoch: 7, Samples: 1824/5760, Loss: 0.29618847370147705\n",
      "Epoch: 7, Samples: 1856/5760, Loss: 0.43816661834716797\n",
      "Epoch: 7, Samples: 1888/5760, Loss: 0.26041412353515625\n",
      "Epoch: 7, Samples: 1920/5760, Loss: 0.41768577694892883\n",
      "Epoch: 7, Samples: 1952/5760, Loss: 0.2797386944293976\n",
      "Epoch: 7, Samples: 1984/5760, Loss: 0.19018986821174622\n",
      "Epoch: 7, Samples: 2016/5760, Loss: 0.38576921820640564\n",
      "Epoch: 7, Samples: 2048/5760, Loss: 0.2348441481590271\n",
      "Epoch: 7, Samples: 2080/5760, Loss: 0.24985599517822266\n",
      "Epoch: 7, Samples: 2112/5760, Loss: 0.19403614103794098\n",
      "Epoch: 7, Samples: 2144/5760, Loss: 0.24457482993602753\n",
      "Epoch: 7, Samples: 2176/5760, Loss: 0.29887986183166504\n",
      "Epoch: 7, Samples: 2208/5760, Loss: 0.22533167898654938\n",
      "Epoch: 7, Samples: 2240/5760, Loss: 0.35416722297668457\n",
      "Epoch: 7, Samples: 2272/5760, Loss: 0.3255581557750702\n",
      "Epoch: 7, Samples: 2304/5760, Loss: 0.3515188992023468\n",
      "Epoch: 7, Samples: 2336/5760, Loss: 0.2691926956176758\n",
      "Epoch: 7, Samples: 2368/5760, Loss: 0.2934170961380005\n",
      "Epoch: 7, Samples: 2400/5760, Loss: 0.26827725768089294\n",
      "Epoch: 7, Samples: 2432/5760, Loss: 0.2511301040649414\n",
      "Epoch: 7, Samples: 2464/5760, Loss: 0.30432772636413574\n",
      "Epoch: 7, Samples: 2496/5760, Loss: 0.23241238296031952\n",
      "Epoch: 7, Samples: 2528/5760, Loss: 0.3709883689880371\n",
      "Epoch: 7, Samples: 2560/5760, Loss: 0.2943054735660553\n",
      "Epoch: 7, Samples: 2592/5760, Loss: 0.3685125708580017\n",
      "Epoch: 7, Samples: 2624/5760, Loss: 0.37840351462364197\n",
      "Epoch: 7, Samples: 2656/5760, Loss: 0.2390555590391159\n",
      "Epoch: 7, Samples: 2688/5760, Loss: 0.2761997580528259\n",
      "Epoch: 7, Samples: 2720/5760, Loss: 0.4082501232624054\n",
      "Epoch: 7, Samples: 2752/5760, Loss: 0.2565940022468567\n",
      "Epoch: 7, Samples: 2784/5760, Loss: 0.2752150297164917\n",
      "Epoch: 7, Samples: 2816/5760, Loss: 0.373353511095047\n",
      "Epoch: 7, Samples: 2848/5760, Loss: 0.2728131115436554\n",
      "Epoch: 7, Samples: 2880/5760, Loss: 0.23526109755039215\n",
      "Epoch: 7, Samples: 2912/5760, Loss: 0.28220346570014954\n",
      "Epoch: 7, Samples: 2944/5760, Loss: 0.5303349494934082\n",
      "Epoch: 7, Samples: 2976/5760, Loss: 0.4015983045101166\n",
      "Epoch: 7, Samples: 3008/5760, Loss: 0.27285629510879517\n",
      "Epoch: 7, Samples: 3040/5760, Loss: 0.24394792318344116\n",
      "Epoch: 7, Samples: 3072/5760, Loss: 0.41423022747039795\n",
      "Epoch: 7, Samples: 3104/5760, Loss: 0.25820133090019226\n",
      "Epoch: 7, Samples: 3136/5760, Loss: 0.22875674068927765\n",
      "Epoch: 7, Samples: 3168/5760, Loss: 0.32333770394325256\n",
      "Epoch: 7, Samples: 3200/5760, Loss: 0.35023757815361023\n",
      "Epoch: 7, Samples: 3232/5760, Loss: 0.22541525959968567\n",
      "Epoch: 7, Samples: 3264/5760, Loss: 0.15959911048412323\n",
      "Epoch: 7, Samples: 3296/5760, Loss: 0.3387027084827423\n",
      "Epoch: 7, Samples: 3328/5760, Loss: 0.3208547830581665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Samples: 3360/5760, Loss: 0.18171849846839905\n",
      "Epoch: 7, Samples: 3392/5760, Loss: 0.30656567215919495\n",
      "Epoch: 7, Samples: 3424/5760, Loss: 0.29848170280456543\n",
      "Epoch: 7, Samples: 3456/5760, Loss: 0.2893270254135132\n",
      "Epoch: 7, Samples: 3488/5760, Loss: 0.48345106840133667\n",
      "Epoch: 7, Samples: 3520/5760, Loss: 0.41430243849754333\n",
      "Epoch: 7, Samples: 3552/5760, Loss: 0.37402963638305664\n",
      "Epoch: 7, Samples: 3584/5760, Loss: 0.22263090312480927\n",
      "Epoch: 7, Samples: 3616/5760, Loss: 0.2736845910549164\n",
      "Epoch: 7, Samples: 3648/5760, Loss: 0.2219909429550171\n",
      "Epoch: 7, Samples: 3680/5760, Loss: 0.27394112944602966\n",
      "Epoch: 7, Samples: 3712/5760, Loss: 0.3282443881034851\n",
      "Epoch: 7, Samples: 3744/5760, Loss: 0.29712024331092834\n",
      "Epoch: 7, Samples: 3776/5760, Loss: 0.37326493859291077\n",
      "Epoch: 7, Samples: 3808/5760, Loss: 0.2254086136817932\n",
      "Epoch: 7, Samples: 3840/5760, Loss: 0.35395580530166626\n",
      "Epoch: 7, Samples: 3872/5760, Loss: 0.3357926309108734\n",
      "Epoch: 7, Samples: 3904/5760, Loss: 0.22344817221164703\n",
      "Epoch: 7, Samples: 3936/5760, Loss: 0.22996583580970764\n",
      "Epoch: 7, Samples: 3968/5760, Loss: 0.2611845135688782\n",
      "Epoch: 7, Samples: 4000/5760, Loss: 0.27965274453163147\n",
      "Epoch: 7, Samples: 4032/5760, Loss: 0.2693302631378174\n",
      "Epoch: 7, Samples: 4064/5760, Loss: 0.32065731287002563\n",
      "Epoch: 7, Samples: 4096/5760, Loss: 0.29663172364234924\n",
      "Epoch: 7, Samples: 4128/5760, Loss: 0.25532397627830505\n",
      "Epoch: 7, Samples: 4160/5760, Loss: 0.27455905079841614\n",
      "Epoch: 7, Samples: 4192/5760, Loss: 0.371934711933136\n",
      "Epoch: 7, Samples: 4224/5760, Loss: 0.19237980246543884\n",
      "Epoch: 7, Samples: 4256/5760, Loss: 0.21491125226020813\n",
      "Epoch: 7, Samples: 4288/5760, Loss: 0.2529158592224121\n",
      "Epoch: 7, Samples: 4320/5760, Loss: 0.39858078956604004\n",
      "Epoch: 7, Samples: 4352/5760, Loss: 0.17805488407611847\n",
      "Epoch: 7, Samples: 4384/5760, Loss: 0.517240047454834\n",
      "Epoch: 7, Samples: 4416/5760, Loss: 0.2966936528682709\n",
      "Epoch: 7, Samples: 4448/5760, Loss: 0.19511014223098755\n",
      "Epoch: 7, Samples: 4480/5760, Loss: 0.27791520953178406\n",
      "Epoch: 7, Samples: 4512/5760, Loss: 0.3369920253753662\n",
      "Epoch: 7, Samples: 4544/5760, Loss: 0.19006864726543427\n",
      "Epoch: 7, Samples: 4576/5760, Loss: 0.2748982608318329\n",
      "Epoch: 7, Samples: 4608/5760, Loss: 0.3713301718235016\n",
      "Epoch: 7, Samples: 4640/5760, Loss: 0.34815818071365356\n",
      "Epoch: 7, Samples: 4672/5760, Loss: 0.33127501606941223\n",
      "Epoch: 7, Samples: 4704/5760, Loss: 0.24986174702644348\n",
      "Epoch: 7, Samples: 4736/5760, Loss: 0.3163422644138336\n",
      "Epoch: 7, Samples: 4768/5760, Loss: 0.3610146939754486\n",
      "Epoch: 7, Samples: 4800/5760, Loss: 0.41330957412719727\n",
      "Epoch: 7, Samples: 4832/5760, Loss: 0.2577301561832428\n",
      "Epoch: 7, Samples: 4864/5760, Loss: 0.3205277919769287\n",
      "Epoch: 7, Samples: 4896/5760, Loss: 0.26384902000427246\n",
      "Epoch: 7, Samples: 4928/5760, Loss: 0.2690615952014923\n",
      "Epoch: 7, Samples: 4960/5760, Loss: 0.46458297967910767\n",
      "Epoch: 7, Samples: 4992/5760, Loss: 0.2650618553161621\n",
      "Epoch: 7, Samples: 5024/5760, Loss: 0.29091331362724304\n",
      "Epoch: 7, Samples: 5056/5760, Loss: 0.27337685227394104\n",
      "Epoch: 7, Samples: 5088/5760, Loss: 0.4482343792915344\n",
      "Epoch: 7, Samples: 5120/5760, Loss: 0.3864419460296631\n",
      "Epoch: 7, Samples: 5152/5760, Loss: 0.38406503200531006\n",
      "Epoch: 7, Samples: 5184/5760, Loss: 0.3533562421798706\n",
      "Epoch: 7, Samples: 5216/5760, Loss: 0.22825798392295837\n",
      "Epoch: 7, Samples: 5248/5760, Loss: 0.31492146849632263\n",
      "Epoch: 7, Samples: 5280/5760, Loss: 0.5028897523880005\n",
      "Epoch: 7, Samples: 5312/5760, Loss: 0.2217816859483719\n",
      "Epoch: 7, Samples: 5344/5760, Loss: 0.25919225811958313\n",
      "Epoch: 7, Samples: 5376/5760, Loss: 0.30002090334892273\n",
      "Epoch: 7, Samples: 5408/5760, Loss: 0.17955118417739868\n",
      "Epoch: 7, Samples: 5440/5760, Loss: 0.33910298347473145\n",
      "Epoch: 7, Samples: 5472/5760, Loss: 0.27280595898628235\n",
      "Epoch: 7, Samples: 5504/5760, Loss: 0.2985268235206604\n",
      "Epoch: 7, Samples: 5536/5760, Loss: 0.22828614711761475\n",
      "Epoch: 7, Samples: 5568/5760, Loss: 0.4356934428215027\n",
      "Epoch: 7, Samples: 5600/5760, Loss: 0.2921169102191925\n",
      "Epoch: 7, Samples: 5632/5760, Loss: 0.1962272673845291\n",
      "Epoch: 7, Samples: 5664/5760, Loss: 0.25269031524658203\n",
      "Epoch: 7, Samples: 5696/5760, Loss: 0.18088705837726593\n",
      "Epoch: 7, Samples: 5728/5760, Loss: 1.1567745208740234\n",
      "\n",
      "Epoch: 7\n",
      "Training set: Average loss: 0.3087\n",
      "Validation set: Average loss: 0.5215, Accuracy: 728/818 (89%)\n",
      "Saving model (epoch 7) with lowest validation loss: 0.521536383491296\n",
      "Epoch: 8, Samples: 0/5760, Loss: 0.2790040969848633\n",
      "Epoch: 8, Samples: 32/5760, Loss: 0.4386516809463501\n",
      "Epoch: 8, Samples: 64/5760, Loss: 0.26572734117507935\n",
      "Epoch: 8, Samples: 96/5760, Loss: 0.37804752588272095\n",
      "Epoch: 8, Samples: 128/5760, Loss: 0.2159542292356491\n",
      "Epoch: 8, Samples: 160/5760, Loss: 0.12497077882289886\n",
      "Epoch: 8, Samples: 192/5760, Loss: 0.11627352237701416\n",
      "Epoch: 8, Samples: 224/5760, Loss: 0.2583440840244293\n",
      "Epoch: 8, Samples: 256/5760, Loss: 0.22799938917160034\n",
      "Epoch: 8, Samples: 288/5760, Loss: 0.26108232140541077\n",
      "Epoch: 8, Samples: 320/5760, Loss: 0.3450794816017151\n",
      "Epoch: 8, Samples: 352/5760, Loss: 0.2565838396549225\n",
      "Epoch: 8, Samples: 384/5760, Loss: 0.14767952263355255\n",
      "Epoch: 8, Samples: 416/5760, Loss: 0.3397417664527893\n",
      "Epoch: 8, Samples: 448/5760, Loss: 0.24854116141796112\n",
      "Epoch: 8, Samples: 480/5760, Loss: 0.284992516040802\n",
      "Epoch: 8, Samples: 512/5760, Loss: 0.2583702802658081\n",
      "Epoch: 8, Samples: 544/5760, Loss: 0.35068029165267944\n",
      "Epoch: 8, Samples: 576/5760, Loss: 0.2345440834760666\n",
      "Epoch: 8, Samples: 608/5760, Loss: 0.17668119072914124\n",
      "Epoch: 8, Samples: 640/5760, Loss: 0.22078779339790344\n",
      "Epoch: 8, Samples: 672/5760, Loss: 0.2783910632133484\n",
      "Epoch: 8, Samples: 704/5760, Loss: 0.26848796010017395\n",
      "Epoch: 8, Samples: 736/5760, Loss: 0.16119354963302612\n",
      "Epoch: 8, Samples: 768/5760, Loss: 0.22325772047042847\n",
      "Epoch: 8, Samples: 800/5760, Loss: 0.2059195339679718\n",
      "Epoch: 8, Samples: 832/5760, Loss: 0.14865154027938843\n",
      "Epoch: 8, Samples: 864/5760, Loss: 0.1876588761806488\n",
      "Epoch: 8, Samples: 896/5760, Loss: 0.2621798515319824\n",
      "Epoch: 8, Samples: 928/5760, Loss: 0.16200657188892365\n",
      "Epoch: 8, Samples: 960/5760, Loss: 0.34792399406433105\n",
      "Epoch: 8, Samples: 992/5760, Loss: 0.22838851809501648\n",
      "Epoch: 8, Samples: 1024/5760, Loss: 0.23137670755386353\n",
      "Epoch: 8, Samples: 1056/5760, Loss: 0.2618858218193054\n",
      "Epoch: 8, Samples: 1088/5760, Loss: 0.29065966606140137\n",
      "Epoch: 8, Samples: 1120/5760, Loss: 0.2037827968597412\n",
      "Epoch: 8, Samples: 1152/5760, Loss: 0.30055609345436096\n",
      "Epoch: 8, Samples: 1184/5760, Loss: 0.28174346685409546\n",
      "Epoch: 8, Samples: 1216/5760, Loss: 0.18775680661201477\n",
      "Epoch: 8, Samples: 1248/5760, Loss: 0.23010246455669403\n",
      "Epoch: 8, Samples: 1280/5760, Loss: 0.15363550186157227\n",
      "Epoch: 8, Samples: 1312/5760, Loss: 0.29831463098526\n",
      "Epoch: 8, Samples: 1344/5760, Loss: 0.2707696557044983\n",
      "Epoch: 8, Samples: 1376/5760, Loss: 0.16145706176757812\n",
      "Epoch: 8, Samples: 1408/5760, Loss: 0.22256731986999512\n",
      "Epoch: 8, Samples: 1440/5760, Loss: 0.27492064237594604\n",
      "Epoch: 8, Samples: 1472/5760, Loss: 0.16892725229263306\n",
      "Epoch: 8, Samples: 1504/5760, Loss: 0.3574696481227875\n",
      "Epoch: 8, Samples: 1536/5760, Loss: 0.20899397134780884\n",
      "Epoch: 8, Samples: 1568/5760, Loss: 0.25267040729522705\n",
      "Epoch: 8, Samples: 1600/5760, Loss: 0.2414369285106659\n",
      "Epoch: 8, Samples: 1632/5760, Loss: 0.305362731218338\n",
      "Epoch: 8, Samples: 1664/5760, Loss: 0.25629034638404846\n",
      "Epoch: 8, Samples: 1696/5760, Loss: 0.3103508949279785\n",
      "Epoch: 8, Samples: 1728/5760, Loss: 0.18617601692676544\n",
      "Epoch: 8, Samples: 1760/5760, Loss: 0.29450276494026184\n",
      "Epoch: 8, Samples: 1792/5760, Loss: 0.2043016105890274\n",
      "Epoch: 8, Samples: 1824/5760, Loss: 0.2477518916130066\n",
      "Epoch: 8, Samples: 1856/5760, Loss: 0.11591529846191406\n",
      "Epoch: 8, Samples: 1888/5760, Loss: 0.2851308286190033\n",
      "Epoch: 8, Samples: 1920/5760, Loss: 0.19452787935733795\n",
      "Epoch: 8, Samples: 1952/5760, Loss: 0.14860184490680695\n",
      "Epoch: 8, Samples: 1984/5760, Loss: 0.3861987292766571\n",
      "Epoch: 8, Samples: 2016/5760, Loss: 0.1459500789642334\n",
      "Epoch: 8, Samples: 2048/5760, Loss: 0.3339384198188782\n",
      "Epoch: 8, Samples: 2080/5760, Loss: 0.26047560572624207\n",
      "Epoch: 8, Samples: 2112/5760, Loss: 0.21795743703842163\n",
      "Epoch: 8, Samples: 2144/5760, Loss: 0.19316954910755157\n",
      "Epoch: 8, Samples: 2176/5760, Loss: 0.23446939885616302\n",
      "Epoch: 8, Samples: 2208/5760, Loss: 0.32439661026000977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Samples: 2240/5760, Loss: 0.16730955243110657\n",
      "Epoch: 8, Samples: 2272/5760, Loss: 0.26540911197662354\n",
      "Epoch: 8, Samples: 2304/5760, Loss: 0.18198974430561066\n",
      "Epoch: 8, Samples: 2336/5760, Loss: 0.32649099826812744\n",
      "Epoch: 8, Samples: 2368/5760, Loss: 0.1438852846622467\n",
      "Epoch: 8, Samples: 2400/5760, Loss: 0.21786776185035706\n",
      "Epoch: 8, Samples: 2432/5760, Loss: 0.2323424369096756\n",
      "Epoch: 8, Samples: 2464/5760, Loss: 0.2971328794956207\n",
      "Epoch: 8, Samples: 2496/5760, Loss: 0.19237932562828064\n",
      "Epoch: 8, Samples: 2528/5760, Loss: 0.15053094923496246\n",
      "Epoch: 8, Samples: 2560/5760, Loss: 0.1734405905008316\n",
      "Epoch: 8, Samples: 2592/5760, Loss: 0.2283155620098114\n",
      "Epoch: 8, Samples: 2624/5760, Loss: 0.26790234446525574\n",
      "Epoch: 8, Samples: 2656/5760, Loss: 0.13663923740386963\n",
      "Epoch: 8, Samples: 2688/5760, Loss: 0.18067236244678497\n",
      "Epoch: 8, Samples: 2720/5760, Loss: 0.265108585357666\n",
      "Epoch: 8, Samples: 2752/5760, Loss: 0.30478793382644653\n",
      "Epoch: 8, Samples: 2784/5760, Loss: 0.16807109117507935\n",
      "Epoch: 8, Samples: 2816/5760, Loss: 0.22477273643016815\n",
      "Epoch: 8, Samples: 2848/5760, Loss: 0.12393514811992645\n",
      "Epoch: 8, Samples: 2880/5760, Loss: 0.2690761983394623\n",
      "Epoch: 8, Samples: 2912/5760, Loss: 0.2559928297996521\n",
      "Epoch: 8, Samples: 2944/5760, Loss: 0.20659174025058746\n",
      "Epoch: 8, Samples: 2976/5760, Loss: 0.2887188792228699\n",
      "Epoch: 8, Samples: 3008/5760, Loss: 0.26245179772377014\n",
      "Epoch: 8, Samples: 3040/5760, Loss: 0.24144919216632843\n",
      "Epoch: 8, Samples: 3072/5760, Loss: 0.20573218166828156\n",
      "Epoch: 8, Samples: 3104/5760, Loss: 0.2343664914369583\n",
      "Epoch: 8, Samples: 3136/5760, Loss: 0.2092948853969574\n",
      "Epoch: 8, Samples: 3168/5760, Loss: 0.1964808851480484\n",
      "Epoch: 8, Samples: 3200/5760, Loss: 0.2848907709121704\n",
      "Epoch: 8, Samples: 3232/5760, Loss: 0.19312267005443573\n",
      "Epoch: 8, Samples: 3264/5760, Loss: 0.2384282648563385\n",
      "Epoch: 8, Samples: 3296/5760, Loss: 0.30227503180503845\n",
      "Epoch: 8, Samples: 3328/5760, Loss: 0.14055535197257996\n",
      "Epoch: 8, Samples: 3360/5760, Loss: 0.16950257122516632\n",
      "Epoch: 8, Samples: 3392/5760, Loss: 0.3547482192516327\n",
      "Epoch: 8, Samples: 3424/5760, Loss: 0.13035371899604797\n",
      "Epoch: 8, Samples: 3456/5760, Loss: 0.27287769317626953\n",
      "Epoch: 8, Samples: 3488/5760, Loss: 0.2171631157398224\n",
      "Epoch: 8, Samples: 3520/5760, Loss: 0.19076277315616608\n",
      "Epoch: 8, Samples: 3552/5760, Loss: 0.2018132507801056\n",
      "Epoch: 8, Samples: 3584/5760, Loss: 0.15264812111854553\n",
      "Epoch: 8, Samples: 3616/5760, Loss: 0.14126361906528473\n",
      "Epoch: 8, Samples: 3648/5760, Loss: 0.20483480393886566\n",
      "Epoch: 8, Samples: 3680/5760, Loss: 0.10868360102176666\n",
      "Epoch: 8, Samples: 3712/5760, Loss: 0.3312632739543915\n",
      "Epoch: 8, Samples: 3744/5760, Loss: 0.2894163727760315\n",
      "Epoch: 8, Samples: 3776/5760, Loss: 0.23746076226234436\n",
      "Epoch: 8, Samples: 3808/5760, Loss: 0.2946878969669342\n",
      "Epoch: 8, Samples: 3840/5760, Loss: 0.3196814954280853\n",
      "Epoch: 8, Samples: 3872/5760, Loss: 0.1346561461687088\n",
      "Epoch: 8, Samples: 3904/5760, Loss: 0.17131172120571136\n",
      "Epoch: 8, Samples: 3936/5760, Loss: 0.30841895937919617\n",
      "Epoch: 8, Samples: 3968/5760, Loss: 0.35969215631484985\n",
      "Epoch: 8, Samples: 4000/5760, Loss: 0.24673627316951752\n",
      "Epoch: 8, Samples: 4032/5760, Loss: 0.26588237285614014\n",
      "Epoch: 8, Samples: 4064/5760, Loss: 0.29660704731941223\n",
      "Epoch: 8, Samples: 4096/5760, Loss: 0.27034682035446167\n",
      "Epoch: 8, Samples: 4128/5760, Loss: 0.2784457206726074\n",
      "Epoch: 8, Samples: 4160/5760, Loss: 0.2941124141216278\n",
      "Epoch: 8, Samples: 4192/5760, Loss: 0.18689393997192383\n",
      "Epoch: 8, Samples: 4224/5760, Loss: 0.11035841703414917\n",
      "Epoch: 8, Samples: 4256/5760, Loss: 0.47321808338165283\n",
      "Epoch: 8, Samples: 4288/5760, Loss: 0.36132729053497314\n",
      "Epoch: 8, Samples: 4320/5760, Loss: 0.2313096523284912\n",
      "Epoch: 8, Samples: 4352/5760, Loss: 0.1934567242860794\n",
      "Epoch: 8, Samples: 4384/5760, Loss: 0.17150162160396576\n",
      "Epoch: 8, Samples: 4416/5760, Loss: 0.3414061665534973\n",
      "Epoch: 8, Samples: 4448/5760, Loss: 0.11275003850460052\n",
      "Epoch: 8, Samples: 4480/5760, Loss: 0.46768078207969666\n",
      "Epoch: 8, Samples: 4512/5760, Loss: 0.25761720538139343\n",
      "Epoch: 8, Samples: 4544/5760, Loss: 0.1557309627532959\n",
      "Epoch: 8, Samples: 4576/5760, Loss: 0.2565791606903076\n",
      "Epoch: 8, Samples: 4608/5760, Loss: 0.20129862427711487\n",
      "Epoch: 8, Samples: 4640/5760, Loss: 0.2610631585121155\n",
      "Epoch: 8, Samples: 4672/5760, Loss: 0.29181796312332153\n",
      "Epoch: 8, Samples: 4704/5760, Loss: 0.17223350703716278\n",
      "Epoch: 8, Samples: 4736/5760, Loss: 0.16884835064411163\n",
      "Epoch: 8, Samples: 4768/5760, Loss: 0.1186114251613617\n",
      "Epoch: 8, Samples: 4800/5760, Loss: 0.1550866812467575\n",
      "Epoch: 8, Samples: 4832/5760, Loss: 0.19743801653385162\n",
      "Epoch: 8, Samples: 4864/5760, Loss: 0.2546221911907196\n",
      "Epoch: 8, Samples: 4896/5760, Loss: 0.2818032503128052\n",
      "Epoch: 8, Samples: 4928/5760, Loss: 0.17566965520381927\n",
      "Epoch: 8, Samples: 4960/5760, Loss: 0.18218635022640228\n",
      "Epoch: 8, Samples: 4992/5760, Loss: 0.16630518436431885\n",
      "Epoch: 8, Samples: 5024/5760, Loss: 0.2275351583957672\n",
      "Epoch: 8, Samples: 5056/5760, Loss: 0.23748530447483063\n",
      "Epoch: 8, Samples: 5088/5760, Loss: 0.18194350600242615\n",
      "Epoch: 8, Samples: 5120/5760, Loss: 0.17913569509983063\n",
      "Epoch: 8, Samples: 5152/5760, Loss: 0.2624270021915436\n",
      "Epoch: 8, Samples: 5184/5760, Loss: 0.32454708218574524\n",
      "Epoch: 8, Samples: 5216/5760, Loss: 0.22135691344738007\n",
      "Epoch: 8, Samples: 5248/5760, Loss: 0.2361113727092743\n",
      "Epoch: 8, Samples: 5280/5760, Loss: 0.19741523265838623\n",
      "Epoch: 8, Samples: 5312/5760, Loss: 0.24334412813186646\n",
      "Epoch: 8, Samples: 5344/5760, Loss: 0.2584008574485779\n",
      "Epoch: 8, Samples: 5376/5760, Loss: 0.2859436273574829\n",
      "Epoch: 8, Samples: 5408/5760, Loss: 0.22870300710201263\n",
      "Epoch: 8, Samples: 5440/5760, Loss: 0.1000506579875946\n",
      "Epoch: 8, Samples: 5472/5760, Loss: 0.17847822606563568\n",
      "Epoch: 8, Samples: 5504/5760, Loss: 0.26464784145355225\n",
      "Epoch: 8, Samples: 5536/5760, Loss: 0.24493741989135742\n",
      "Epoch: 8, Samples: 5568/5760, Loss: 0.30180567502975464\n",
      "Epoch: 8, Samples: 5600/5760, Loss: 0.10341629385948181\n",
      "Epoch: 8, Samples: 5632/5760, Loss: 0.29958632588386536\n",
      "Epoch: 8, Samples: 5664/5760, Loss: 0.09969490766525269\n",
      "Epoch: 8, Samples: 5696/5760, Loss: 0.1920931488275528\n",
      "Epoch: 8, Samples: 5728/5760, Loss: 0.8347800970077515\n",
      "\n",
      "Epoch: 8\n",
      "Training set: Average loss: 0.2379\n",
      "Validation set: Average loss: 0.4871, Accuracy: 731/818 (89%)\n",
      "Saving model (epoch 8) with lowest validation loss: 0.4871277172978108\n",
      "Epoch: 9, Samples: 0/5760, Loss: 0.16058532893657684\n",
      "Epoch: 9, Samples: 32/5760, Loss: 0.16119708120822906\n",
      "Epoch: 9, Samples: 64/5760, Loss: 0.2557474672794342\n",
      "Epoch: 9, Samples: 96/5760, Loss: 0.1328309029340744\n",
      "Epoch: 9, Samples: 128/5760, Loss: 0.15928837656974792\n",
      "Epoch: 9, Samples: 160/5760, Loss: 0.3027145564556122\n",
      "Epoch: 9, Samples: 192/5760, Loss: 0.14628846943378448\n",
      "Epoch: 9, Samples: 224/5760, Loss: 0.14254479110240936\n",
      "Epoch: 9, Samples: 256/5760, Loss: 0.20182503759860992\n",
      "Epoch: 9, Samples: 288/5760, Loss: 0.12203849852085114\n",
      "Epoch: 9, Samples: 320/5760, Loss: 0.258760541677475\n",
      "Epoch: 9, Samples: 352/5760, Loss: 0.1304369866847992\n",
      "Epoch: 9, Samples: 384/5760, Loss: 0.10961033403873444\n",
      "Epoch: 9, Samples: 416/5760, Loss: 0.11789305508136749\n",
      "Epoch: 9, Samples: 448/5760, Loss: 0.1796179562807083\n",
      "Epoch: 9, Samples: 480/5760, Loss: 0.14135733246803284\n",
      "Epoch: 9, Samples: 512/5760, Loss: 0.28083252906799316\n",
      "Epoch: 9, Samples: 544/5760, Loss: 0.21278615295886993\n",
      "Epoch: 9, Samples: 576/5760, Loss: 0.1555873602628708\n",
      "Epoch: 9, Samples: 608/5760, Loss: 0.1808099001646042\n",
      "Epoch: 9, Samples: 640/5760, Loss: 0.24025262892246246\n",
      "Epoch: 9, Samples: 672/5760, Loss: 0.19005417823791504\n",
      "Epoch: 9, Samples: 704/5760, Loss: 0.18244677782058716\n",
      "Epoch: 9, Samples: 736/5760, Loss: 0.13732561469078064\n",
      "Epoch: 9, Samples: 768/5760, Loss: 0.3081575632095337\n",
      "Epoch: 9, Samples: 800/5760, Loss: 0.16229020059108734\n",
      "Epoch: 9, Samples: 832/5760, Loss: 0.17163005471229553\n",
      "Epoch: 9, Samples: 864/5760, Loss: 0.14845523238182068\n",
      "Epoch: 9, Samples: 896/5760, Loss: 0.16416874527931213\n",
      "Epoch: 9, Samples: 928/5760, Loss: 0.13806159794330597\n",
      "Epoch: 9, Samples: 960/5760, Loss: 0.22390508651733398\n",
      "Epoch: 9, Samples: 992/5760, Loss: 0.21939851343631744\n",
      "Epoch: 9, Samples: 1024/5760, Loss: 0.09148247539997101\n",
      "Epoch: 9, Samples: 1056/5760, Loss: 0.30957382917404175\n",
      "Epoch: 9, Samples: 1088/5760, Loss: 0.12930963933467865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Samples: 1120/5760, Loss: 0.2344968616962433\n",
      "Epoch: 9, Samples: 1152/5760, Loss: 0.15280786156654358\n",
      "Epoch: 9, Samples: 1184/5760, Loss: 0.12195378541946411\n",
      "Epoch: 9, Samples: 1216/5760, Loss: 0.20245203375816345\n",
      "Epoch: 9, Samples: 1248/5760, Loss: 0.15170283615589142\n",
      "Epoch: 9, Samples: 1280/5760, Loss: 0.1301306039094925\n",
      "Epoch: 9, Samples: 1312/5760, Loss: 0.2870613634586334\n",
      "Epoch: 9, Samples: 1344/5760, Loss: 0.2135399430990219\n",
      "Epoch: 9, Samples: 1376/5760, Loss: 0.18737336993217468\n",
      "Epoch: 9, Samples: 1408/5760, Loss: 0.20902422070503235\n",
      "Epoch: 9, Samples: 1440/5760, Loss: 0.19241389632225037\n",
      "Epoch: 9, Samples: 1472/5760, Loss: 0.14149045944213867\n",
      "Epoch: 9, Samples: 1504/5760, Loss: 0.20304815471172333\n",
      "Epoch: 9, Samples: 1536/5760, Loss: 0.14770668745040894\n",
      "Epoch: 9, Samples: 1568/5760, Loss: 0.1892329454421997\n",
      "Epoch: 9, Samples: 1600/5760, Loss: 0.16972695291042328\n",
      "Epoch: 9, Samples: 1632/5760, Loss: 0.17386573553085327\n",
      "Epoch: 9, Samples: 1664/5760, Loss: 0.2013167440891266\n",
      "Epoch: 9, Samples: 1696/5760, Loss: 0.15572775900363922\n",
      "Epoch: 9, Samples: 1728/5760, Loss: 0.14293311536312103\n",
      "Epoch: 9, Samples: 1760/5760, Loss: 0.19046473503112793\n",
      "Epoch: 9, Samples: 1792/5760, Loss: 0.13122756779193878\n",
      "Epoch: 9, Samples: 1824/5760, Loss: 0.27625101804733276\n",
      "Epoch: 9, Samples: 1856/5760, Loss: 0.16716395318508148\n",
      "Epoch: 9, Samples: 1888/5760, Loss: 0.26365381479263306\n",
      "Epoch: 9, Samples: 1920/5760, Loss: 0.15437649190425873\n",
      "Epoch: 9, Samples: 1952/5760, Loss: 0.12133046984672546\n",
      "Epoch: 9, Samples: 1984/5760, Loss: 0.1747012883424759\n",
      "Epoch: 9, Samples: 2016/5760, Loss: 0.23955857753753662\n",
      "Epoch: 9, Samples: 2048/5760, Loss: 0.13170558214187622\n",
      "Epoch: 9, Samples: 2080/5760, Loss: 0.3736824095249176\n",
      "Epoch: 9, Samples: 2112/5760, Loss: 0.2654731273651123\n",
      "Epoch: 9, Samples: 2144/5760, Loss: 0.14593303203582764\n",
      "Epoch: 9, Samples: 2176/5760, Loss: 0.12961018085479736\n",
      "Epoch: 9, Samples: 2208/5760, Loss: 0.17975665628910065\n",
      "Epoch: 9, Samples: 2240/5760, Loss: 0.21332275867462158\n",
      "Epoch: 9, Samples: 2272/5760, Loss: 0.24132929742336273\n",
      "Epoch: 9, Samples: 2304/5760, Loss: 0.12177453935146332\n",
      "Epoch: 9, Samples: 2336/5760, Loss: 0.17837578058242798\n",
      "Epoch: 9, Samples: 2368/5760, Loss: 0.15940353274345398\n",
      "Epoch: 9, Samples: 2400/5760, Loss: 0.1839589625597\n",
      "Epoch: 9, Samples: 2432/5760, Loss: 0.20078851282596588\n",
      "Epoch: 9, Samples: 2464/5760, Loss: 0.21685025095939636\n",
      "Epoch: 9, Samples: 2496/5760, Loss: 0.2845103144645691\n",
      "Epoch: 9, Samples: 2528/5760, Loss: 0.18371260166168213\n",
      "Epoch: 9, Samples: 2560/5760, Loss: 0.24153456091880798\n",
      "Epoch: 9, Samples: 2592/5760, Loss: 0.14764392375946045\n",
      "Epoch: 9, Samples: 2624/5760, Loss: 0.12248121201992035\n",
      "Epoch: 9, Samples: 2656/5760, Loss: 0.20983825623989105\n",
      "Epoch: 9, Samples: 2688/5760, Loss: 0.14565794169902802\n",
      "Epoch: 9, Samples: 2720/5760, Loss: 0.15637362003326416\n",
      "Epoch: 9, Samples: 2752/5760, Loss: 0.1534973829984665\n",
      "Epoch: 9, Samples: 2784/5760, Loss: 0.1298910677433014\n",
      "Epoch: 9, Samples: 2816/5760, Loss: 0.13998597860336304\n",
      "Epoch: 9, Samples: 2848/5760, Loss: 0.23754318058490753\n",
      "Epoch: 9, Samples: 2880/5760, Loss: 0.10112376511096954\n",
      "Epoch: 9, Samples: 2912/5760, Loss: 0.21219196915626526\n",
      "Epoch: 9, Samples: 2944/5760, Loss: 0.20671074092388153\n",
      "Epoch: 9, Samples: 2976/5760, Loss: 0.1471382975578308\n",
      "Epoch: 9, Samples: 3008/5760, Loss: 0.2601146697998047\n",
      "Epoch: 9, Samples: 3040/5760, Loss: 0.22792762517929077\n",
      "Epoch: 9, Samples: 3072/5760, Loss: 0.24404092133045197\n",
      "Epoch: 9, Samples: 3104/5760, Loss: 0.17011064291000366\n",
      "Epoch: 9, Samples: 3136/5760, Loss: 0.1402052640914917\n",
      "Epoch: 9, Samples: 3168/5760, Loss: 0.2074814885854721\n",
      "Epoch: 9, Samples: 3200/5760, Loss: 0.2911207377910614\n",
      "Epoch: 9, Samples: 3232/5760, Loss: 0.16436293721199036\n",
      "Epoch: 9, Samples: 3264/5760, Loss: 0.27618661522865295\n",
      "Epoch: 9, Samples: 3296/5760, Loss: 0.21272225677967072\n",
      "Epoch: 9, Samples: 3328/5760, Loss: 0.22777009010314941\n",
      "Epoch: 9, Samples: 3360/5760, Loss: 0.2348281443119049\n",
      "Epoch: 9, Samples: 3392/5760, Loss: 0.20799146592617035\n",
      "Epoch: 9, Samples: 3424/5760, Loss: 0.19827738404273987\n",
      "Epoch: 9, Samples: 3456/5760, Loss: 0.13641731441020966\n",
      "Epoch: 9, Samples: 3488/5760, Loss: 0.19257749617099762\n",
      "Epoch: 9, Samples: 3520/5760, Loss: 0.3009936809539795\n",
      "Epoch: 9, Samples: 3552/5760, Loss: 0.21623872220516205\n",
      "Epoch: 9, Samples: 3584/5760, Loss: 0.17319408059120178\n",
      "Epoch: 9, Samples: 3616/5760, Loss: 0.25508710741996765\n",
      "Epoch: 9, Samples: 3648/5760, Loss: 0.18666279315948486\n",
      "Epoch: 9, Samples: 3680/5760, Loss: 0.12907761335372925\n",
      "Epoch: 9, Samples: 3712/5760, Loss: 0.2835649251937866\n",
      "Epoch: 9, Samples: 3744/5760, Loss: 0.1418483406305313\n",
      "Epoch: 9, Samples: 3776/5760, Loss: 0.13859368860721588\n",
      "Epoch: 9, Samples: 3808/5760, Loss: 0.11944246292114258\n",
      "Epoch: 9, Samples: 3840/5760, Loss: 0.12032510340213776\n",
      "Epoch: 9, Samples: 3872/5760, Loss: 0.25853943824768066\n",
      "Epoch: 9, Samples: 3904/5760, Loss: 0.19587978720664978\n",
      "Epoch: 9, Samples: 3936/5760, Loss: 0.22735974192619324\n",
      "Epoch: 9, Samples: 3968/5760, Loss: 0.200006365776062\n",
      "Epoch: 9, Samples: 4000/5760, Loss: 0.236728698015213\n",
      "Epoch: 9, Samples: 4032/5760, Loss: 0.16873027384281158\n",
      "Epoch: 9, Samples: 4064/5760, Loss: 0.1425057053565979\n",
      "Epoch: 9, Samples: 4096/5760, Loss: 0.14321650564670563\n",
      "Epoch: 9, Samples: 4128/5760, Loss: 0.16573448479175568\n",
      "Epoch: 9, Samples: 4160/5760, Loss: 0.13708814978599548\n",
      "Epoch: 9, Samples: 4192/5760, Loss: 0.12383536994457245\n",
      "Epoch: 9, Samples: 4224/5760, Loss: 0.14297300577163696\n",
      "Epoch: 9, Samples: 4256/5760, Loss: 0.2115316390991211\n",
      "Epoch: 9, Samples: 4288/5760, Loss: 0.18069233000278473\n",
      "Epoch: 9, Samples: 4320/5760, Loss: 0.2710046172142029\n",
      "Epoch: 9, Samples: 4352/5760, Loss: 0.16957294940948486\n",
      "Epoch: 9, Samples: 4384/5760, Loss: 0.36895713210105896\n",
      "Epoch: 9, Samples: 4416/5760, Loss: 0.11489193141460419\n",
      "Epoch: 9, Samples: 4448/5760, Loss: 0.1555505394935608\n",
      "Epoch: 9, Samples: 4480/5760, Loss: 0.16278070211410522\n",
      "Epoch: 9, Samples: 4512/5760, Loss: 0.10455955564975739\n",
      "Epoch: 9, Samples: 4544/5760, Loss: 0.1297105997800827\n",
      "Epoch: 9, Samples: 4576/5760, Loss: 0.17403262853622437\n",
      "Epoch: 9, Samples: 4608/5760, Loss: 0.12874701619148254\n",
      "Epoch: 9, Samples: 4640/5760, Loss: 0.09622266888618469\n",
      "Epoch: 9, Samples: 4672/5760, Loss: 0.17515689134597778\n",
      "Epoch: 9, Samples: 4704/5760, Loss: 0.09917867183685303\n",
      "Epoch: 9, Samples: 4736/5760, Loss: 0.2494603991508484\n",
      "Epoch: 9, Samples: 4768/5760, Loss: 0.13352370262145996\n",
      "Epoch: 9, Samples: 4800/5760, Loss: 0.19867897033691406\n",
      "Epoch: 9, Samples: 4832/5760, Loss: 0.14067882299423218\n",
      "Epoch: 9, Samples: 4864/5760, Loss: 0.20736023783683777\n",
      "Epoch: 9, Samples: 4896/5760, Loss: 0.20651601254940033\n",
      "Epoch: 9, Samples: 4928/5760, Loss: 0.14717423915863037\n",
      "Epoch: 9, Samples: 4960/5760, Loss: 0.161101832985878\n",
      "Epoch: 9, Samples: 4992/5760, Loss: 0.14819824695587158\n",
      "Epoch: 9, Samples: 5024/5760, Loss: 0.1489427238702774\n",
      "Epoch: 9, Samples: 5056/5760, Loss: 0.13602948188781738\n",
      "Epoch: 9, Samples: 5088/5760, Loss: 0.10152322053909302\n",
      "Epoch: 9, Samples: 5120/5760, Loss: 0.256175696849823\n",
      "Epoch: 9, Samples: 5152/5760, Loss: 0.3091242015361786\n",
      "Epoch: 9, Samples: 5184/5760, Loss: 0.08977755904197693\n",
      "Epoch: 9, Samples: 5216/5760, Loss: 0.17399276793003082\n",
      "Epoch: 9, Samples: 5248/5760, Loss: 0.19497765600681305\n",
      "Epoch: 9, Samples: 5280/5760, Loss: 0.09926611185073853\n",
      "Epoch: 9, Samples: 5312/5760, Loss: 0.1646682620048523\n",
      "Epoch: 9, Samples: 5344/5760, Loss: 0.1939154416322708\n",
      "Epoch: 9, Samples: 5376/5760, Loss: 0.15440502762794495\n",
      "Epoch: 9, Samples: 5408/5760, Loss: 0.11665700376033783\n",
      "Epoch: 9, Samples: 5440/5760, Loss: 0.14269590377807617\n",
      "Epoch: 9, Samples: 5472/5760, Loss: 0.19465813040733337\n",
      "Epoch: 9, Samples: 5504/5760, Loss: 0.11595797538757324\n",
      "Epoch: 9, Samples: 5536/5760, Loss: 0.20101280510425568\n",
      "Epoch: 9, Samples: 5568/5760, Loss: 0.16306188702583313\n",
      "Epoch: 9, Samples: 5600/5760, Loss: 0.12249386310577393\n",
      "Epoch: 9, Samples: 5632/5760, Loss: 0.16494040191173553\n",
      "Epoch: 9, Samples: 5664/5760, Loss: 0.2917770743370056\n",
      "Epoch: 9, Samples: 5696/5760, Loss: 0.2306409329175949\n",
      "Epoch: 9, Samples: 5728/5760, Loss: 0.7407315969467163\n",
      "\n",
      "Epoch: 9\n",
      "Training set: Average loss: 0.1861\n",
      "Validation set: Average loss: 0.4649, Accuracy: 738/818 (90%)\n",
      "Saving model (epoch 9) with lowest validation loss: 0.4648724840237544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Samples: 0/5760, Loss: 0.1889864057302475\n",
      "Epoch: 10, Samples: 32/5760, Loss: 0.08543504774570465\n",
      "Epoch: 10, Samples: 64/5760, Loss: 0.11899419128894806\n",
      "Epoch: 10, Samples: 96/5760, Loss: 0.3188704550266266\n",
      "Epoch: 10, Samples: 128/5760, Loss: 0.21030637621879578\n",
      "Epoch: 10, Samples: 160/5760, Loss: 0.09263581037521362\n",
      "Epoch: 10, Samples: 192/5760, Loss: 0.22845694422721863\n",
      "Epoch: 10, Samples: 224/5760, Loss: 0.08812205493450165\n",
      "Epoch: 10, Samples: 256/5760, Loss: 0.1347341239452362\n",
      "Epoch: 10, Samples: 288/5760, Loss: 0.1269604116678238\n",
      "Epoch: 10, Samples: 320/5760, Loss: 0.17439040541648865\n",
      "Epoch: 10, Samples: 352/5760, Loss: 0.13901977241039276\n",
      "Epoch: 10, Samples: 384/5760, Loss: 0.2141251266002655\n",
      "Epoch: 10, Samples: 416/5760, Loss: 0.20580436289310455\n",
      "Epoch: 10, Samples: 448/5760, Loss: 0.11346891522407532\n",
      "Epoch: 10, Samples: 480/5760, Loss: 0.13860704004764557\n",
      "Epoch: 10, Samples: 512/5760, Loss: 0.1424359679222107\n",
      "Epoch: 10, Samples: 544/5760, Loss: 0.08988334238529205\n",
      "Epoch: 10, Samples: 576/5760, Loss: 0.11155867576599121\n",
      "Epoch: 10, Samples: 608/5760, Loss: 0.11987616121768951\n",
      "Epoch: 10, Samples: 640/5760, Loss: 0.208197221159935\n",
      "Epoch: 10, Samples: 672/5760, Loss: 0.14176608622074127\n",
      "Epoch: 10, Samples: 704/5760, Loss: 0.22416049242019653\n",
      "Epoch: 10, Samples: 736/5760, Loss: 0.21335554122924805\n",
      "Epoch: 10, Samples: 768/5760, Loss: 0.17596912384033203\n",
      "Epoch: 10, Samples: 800/5760, Loss: 0.15469807386398315\n",
      "Epoch: 10, Samples: 832/5760, Loss: 0.12389516830444336\n",
      "Epoch: 10, Samples: 864/5760, Loss: 0.20195357501506805\n",
      "Epoch: 10, Samples: 896/5760, Loss: 0.12721699476242065\n",
      "Epoch: 10, Samples: 928/5760, Loss: 0.12951603531837463\n",
      "Epoch: 10, Samples: 960/5760, Loss: 0.1698300540447235\n",
      "Epoch: 10, Samples: 992/5760, Loss: 0.11201688647270203\n",
      "Epoch: 10, Samples: 1024/5760, Loss: 0.12948112189769745\n",
      "Epoch: 10, Samples: 1056/5760, Loss: 0.14998134970664978\n",
      "Epoch: 10, Samples: 1088/5760, Loss: 0.07804952561855316\n",
      "Epoch: 10, Samples: 1120/5760, Loss: 0.13695645332336426\n",
      "Epoch: 10, Samples: 1152/5760, Loss: 0.11582107841968536\n",
      "Epoch: 10, Samples: 1184/5760, Loss: 0.13515686988830566\n",
      "Epoch: 10, Samples: 1216/5760, Loss: 0.11897532641887665\n",
      "Epoch: 10, Samples: 1248/5760, Loss: 0.14073868095874786\n",
      "Epoch: 10, Samples: 1280/5760, Loss: 0.15750791132450104\n",
      "Epoch: 10, Samples: 1312/5760, Loss: 0.1531876176595688\n",
      "Epoch: 10, Samples: 1344/5760, Loss: 0.15756815671920776\n",
      "Epoch: 10, Samples: 1376/5760, Loss: 0.18059813976287842\n",
      "Epoch: 10, Samples: 1408/5760, Loss: 0.04618540406227112\n",
      "Epoch: 10, Samples: 1440/5760, Loss: 0.17183078825473785\n",
      "Epoch: 10, Samples: 1472/5760, Loss: 0.09596985578536987\n",
      "Epoch: 10, Samples: 1504/5760, Loss: 0.18064093589782715\n",
      "Epoch: 10, Samples: 1536/5760, Loss: 0.14368699491024017\n",
      "Epoch: 10, Samples: 1568/5760, Loss: 0.13330018520355225\n",
      "Epoch: 10, Samples: 1600/5760, Loss: 0.12435486912727356\n",
      "Epoch: 10, Samples: 1632/5760, Loss: 0.1336333155632019\n",
      "Epoch: 10, Samples: 1664/5760, Loss: 0.08661669492721558\n",
      "Epoch: 10, Samples: 1696/5760, Loss: 0.2043670117855072\n",
      "Epoch: 10, Samples: 1728/5760, Loss: 0.12808077037334442\n",
      "Epoch: 10, Samples: 1760/5760, Loss: 0.1335979849100113\n",
      "Epoch: 10, Samples: 1792/5760, Loss: 0.13303016126155853\n",
      "Epoch: 10, Samples: 1824/5760, Loss: 0.10167428851127625\n",
      "Epoch: 10, Samples: 1856/5760, Loss: 0.1595126837491989\n",
      "Epoch: 10, Samples: 1888/5760, Loss: 0.24468347430229187\n",
      "Epoch: 10, Samples: 1920/5760, Loss: 0.10890394449234009\n",
      "Epoch: 10, Samples: 1952/5760, Loss: 0.09102697670459747\n",
      "Epoch: 10, Samples: 1984/5760, Loss: 0.08895406126976013\n",
      "Epoch: 10, Samples: 2016/5760, Loss: 0.1395474225282669\n",
      "Epoch: 10, Samples: 2048/5760, Loss: 0.22321012616157532\n",
      "Epoch: 10, Samples: 2080/5760, Loss: 0.12283924221992493\n",
      "Epoch: 10, Samples: 2112/5760, Loss: 0.12428370118141174\n",
      "Epoch: 10, Samples: 2144/5760, Loss: 0.10547792911529541\n",
      "Epoch: 10, Samples: 2176/5760, Loss: 0.1907082498073578\n",
      "Epoch: 10, Samples: 2208/5760, Loss: 0.2852366864681244\n",
      "Epoch: 10, Samples: 2240/5760, Loss: 0.22756114602088928\n",
      "Epoch: 10, Samples: 2272/5760, Loss: 0.2690286338329315\n",
      "Epoch: 10, Samples: 2304/5760, Loss: 0.08673620223999023\n",
      "Epoch: 10, Samples: 2336/5760, Loss: 0.1260790079832077\n",
      "Epoch: 10, Samples: 2368/5760, Loss: 0.31279462575912476\n",
      "Epoch: 10, Samples: 2400/5760, Loss: 0.19780713319778442\n",
      "Epoch: 10, Samples: 2432/5760, Loss: 0.19073441624641418\n",
      "Epoch: 10, Samples: 2464/5760, Loss: 0.08587698638439178\n",
      "Epoch: 10, Samples: 2496/5760, Loss: 0.12520085275173187\n",
      "Epoch: 10, Samples: 2528/5760, Loss: 0.19278576970100403\n",
      "Epoch: 10, Samples: 2560/5760, Loss: 0.0956181138753891\n",
      "Epoch: 10, Samples: 2592/5760, Loss: 0.12252628803253174\n",
      "Epoch: 10, Samples: 2624/5760, Loss: 0.12937557697296143\n",
      "Epoch: 10, Samples: 2656/5760, Loss: 0.12441101670265198\n",
      "Epoch: 10, Samples: 2688/5760, Loss: 0.13306479156017303\n",
      "Epoch: 10, Samples: 2720/5760, Loss: 0.10423165559768677\n",
      "Epoch: 10, Samples: 2752/5760, Loss: 0.0969657152891159\n",
      "Epoch: 10, Samples: 2784/5760, Loss: 0.11816798150539398\n",
      "Epoch: 10, Samples: 2816/5760, Loss: 0.13088425993919373\n",
      "Epoch: 10, Samples: 2848/5760, Loss: 0.10922811925411224\n",
      "Epoch: 10, Samples: 2880/5760, Loss: 0.07321110367774963\n",
      "Epoch: 10, Samples: 2912/5760, Loss: 0.1572846621274948\n",
      "Epoch: 10, Samples: 2944/5760, Loss: 0.15943531692028046\n",
      "Epoch: 10, Samples: 2976/5760, Loss: 0.11839722096920013\n",
      "Epoch: 10, Samples: 3008/5760, Loss: 0.12326084077358246\n",
      "Epoch: 10, Samples: 3040/5760, Loss: 0.11049188673496246\n",
      "Epoch: 10, Samples: 3072/5760, Loss: 0.09656284749507904\n",
      "Epoch: 10, Samples: 3104/5760, Loss: 0.1273733526468277\n",
      "Epoch: 10, Samples: 3136/5760, Loss: 0.09019458293914795\n",
      "Epoch: 10, Samples: 3168/5760, Loss: 0.1284296214580536\n",
      "Epoch: 10, Samples: 3200/5760, Loss: 0.11023488640785217\n",
      "Epoch: 10, Samples: 3232/5760, Loss: 0.10422024130821228\n",
      "Epoch: 10, Samples: 3264/5760, Loss: 0.15383613109588623\n",
      "Epoch: 10, Samples: 3296/5760, Loss: 0.14964936673641205\n",
      "Epoch: 10, Samples: 3328/5760, Loss: 0.05712252855300903\n",
      "Epoch: 10, Samples: 3360/5760, Loss: 0.23251298069953918\n",
      "Epoch: 10, Samples: 3392/5760, Loss: 0.1931833028793335\n",
      "Epoch: 10, Samples: 3424/5760, Loss: 0.2319902628660202\n",
      "Epoch: 10, Samples: 3456/5760, Loss: 0.14482960104942322\n",
      "Epoch: 10, Samples: 3488/5760, Loss: 0.1611643135547638\n",
      "Epoch: 10, Samples: 3520/5760, Loss: 0.17953339219093323\n",
      "Epoch: 10, Samples: 3552/5760, Loss: 0.11193673312664032\n",
      "Epoch: 10, Samples: 3584/5760, Loss: 0.15912820398807526\n",
      "Epoch: 10, Samples: 3616/5760, Loss: 0.14538563787937164\n",
      "Epoch: 10, Samples: 3648/5760, Loss: 0.17972694337368011\n",
      "Epoch: 10, Samples: 3680/5760, Loss: 0.10754860937595367\n",
      "Epoch: 10, Samples: 3712/5760, Loss: 0.10407218337059021\n",
      "Epoch: 10, Samples: 3744/5760, Loss: 0.1620439887046814\n",
      "Epoch: 10, Samples: 3776/5760, Loss: 0.11477398872375488\n",
      "Epoch: 10, Samples: 3808/5760, Loss: 0.18763381242752075\n",
      "Epoch: 10, Samples: 3840/5760, Loss: 0.08360713720321655\n",
      "Epoch: 10, Samples: 3872/5760, Loss: 0.10857453942298889\n",
      "Epoch: 10, Samples: 3904/5760, Loss: 0.2228204756975174\n",
      "Epoch: 10, Samples: 3936/5760, Loss: 0.09229418635368347\n",
      "Epoch: 10, Samples: 3968/5760, Loss: 0.0955142080783844\n",
      "Epoch: 10, Samples: 4000/5760, Loss: 0.14674487709999084\n",
      "Epoch: 10, Samples: 4032/5760, Loss: 0.1736941635608673\n",
      "Epoch: 10, Samples: 4064/5760, Loss: 0.10562418401241302\n",
      "Epoch: 10, Samples: 4096/5760, Loss: 0.21949885785579681\n",
      "Epoch: 10, Samples: 4128/5760, Loss: 0.09493473172187805\n",
      "Epoch: 10, Samples: 4160/5760, Loss: 0.30222171545028687\n",
      "Epoch: 10, Samples: 4192/5760, Loss: 0.12639158964157104\n",
      "Epoch: 10, Samples: 4224/5760, Loss: 0.15681380033493042\n",
      "Epoch: 10, Samples: 4256/5760, Loss: 0.1140318363904953\n",
      "Epoch: 10, Samples: 4288/5760, Loss: 0.15020467340946198\n",
      "Epoch: 10, Samples: 4320/5760, Loss: 0.15450046956539154\n",
      "Epoch: 10, Samples: 4352/5760, Loss: 0.15807126462459564\n",
      "Epoch: 10, Samples: 4384/5760, Loss: 0.08550228178501129\n",
      "Epoch: 10, Samples: 4416/5760, Loss: 0.2071162611246109\n",
      "Epoch: 10, Samples: 4448/5760, Loss: 0.09338846802711487\n",
      "Epoch: 10, Samples: 4480/5760, Loss: 0.07088382542133331\n",
      "Epoch: 10, Samples: 4512/5760, Loss: 0.12611103057861328\n",
      "Epoch: 10, Samples: 4544/5760, Loss: 0.21239373087882996\n",
      "Epoch: 10, Samples: 4576/5760, Loss: 0.11945492029190063\n",
      "Epoch: 10, Samples: 4608/5760, Loss: 0.12469100952148438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Samples: 4640/5760, Loss: 0.14536786079406738\n",
      "Epoch: 10, Samples: 4672/5760, Loss: 0.13474629819393158\n",
      "Epoch: 10, Samples: 4704/5760, Loss: 0.21596644818782806\n",
      "Epoch: 10, Samples: 4736/5760, Loss: 0.13270153105258942\n",
      "Epoch: 10, Samples: 4768/5760, Loss: 0.15276503562927246\n",
      "Epoch: 10, Samples: 4800/5760, Loss: 0.1914878487586975\n",
      "Epoch: 10, Samples: 4832/5760, Loss: 0.15268905460834503\n",
      "Epoch: 10, Samples: 4864/5760, Loss: 0.10762226581573486\n",
      "Epoch: 10, Samples: 4896/5760, Loss: 0.13380692899227142\n",
      "Epoch: 10, Samples: 4928/5760, Loss: 0.0773959755897522\n",
      "Epoch: 10, Samples: 4960/5760, Loss: 0.14261431992053986\n",
      "Epoch: 10, Samples: 4992/5760, Loss: 0.1608085036277771\n",
      "Epoch: 10, Samples: 5024/5760, Loss: 0.1251104772090912\n",
      "Epoch: 10, Samples: 5056/5760, Loss: 0.09906163811683655\n",
      "Epoch: 10, Samples: 5088/5760, Loss: 0.1556214988231659\n",
      "Epoch: 10, Samples: 5120/5760, Loss: 0.10138770937919617\n",
      "Epoch: 10, Samples: 5152/5760, Loss: 0.0735277533531189\n",
      "Epoch: 10, Samples: 5184/5760, Loss: 0.16399356722831726\n",
      "Epoch: 10, Samples: 5216/5760, Loss: 0.1159619688987732\n",
      "Epoch: 10, Samples: 5248/5760, Loss: 0.10244320333003998\n",
      "Epoch: 10, Samples: 5280/5760, Loss: 0.17910215258598328\n",
      "Epoch: 10, Samples: 5312/5760, Loss: 0.13726188242435455\n",
      "Epoch: 10, Samples: 5344/5760, Loss: 0.17273348569869995\n",
      "Epoch: 10, Samples: 5376/5760, Loss: 0.07736852765083313\n",
      "Epoch: 10, Samples: 5408/5760, Loss: 0.07941846549510956\n",
      "Epoch: 10, Samples: 5440/5760, Loss: 0.15921808779239655\n",
      "Epoch: 10, Samples: 5472/5760, Loss: 0.08622796833515167\n",
      "Epoch: 10, Samples: 5504/5760, Loss: 0.10438403487205505\n",
      "Epoch: 10, Samples: 5536/5760, Loss: 0.14077386260032654\n",
      "Epoch: 10, Samples: 5568/5760, Loss: 0.23035627603530884\n",
      "Epoch: 10, Samples: 5600/5760, Loss: 0.16561587154865265\n",
      "Epoch: 10, Samples: 5632/5760, Loss: 0.11538270115852356\n",
      "Epoch: 10, Samples: 5664/5760, Loss: 0.13761752843856812\n",
      "Epoch: 10, Samples: 5696/5760, Loss: 0.1276910901069641\n",
      "Epoch: 10, Samples: 5728/5760, Loss: 1.6009430885314941\n",
      "\n",
      "Epoch: 10\n",
      "Training set: Average loss: 0.1516\n",
      "Validation set: Average loss: 0.4278, Accuracy: 737/818 (90%)\n",
      "Saving model (epoch 10) with lowest validation loss: 0.4278373099290408\n",
      "Epoch: 11, Samples: 0/5760, Loss: 0.08193786442279816\n",
      "Epoch: 11, Samples: 32/5760, Loss: 0.100222647190094\n",
      "Epoch: 11, Samples: 64/5760, Loss: 0.11062745749950409\n",
      "Epoch: 11, Samples: 96/5760, Loss: 0.09513591229915619\n",
      "Epoch: 11, Samples: 128/5760, Loss: 0.13784907758235931\n",
      "Epoch: 11, Samples: 160/5760, Loss: 0.07824717462062836\n",
      "Epoch: 11, Samples: 192/5760, Loss: 0.15776509046554565\n",
      "Epoch: 11, Samples: 224/5760, Loss: 0.10987405478954315\n",
      "Epoch: 11, Samples: 256/5760, Loss: 0.09410293400287628\n",
      "Epoch: 11, Samples: 288/5760, Loss: 0.19603276252746582\n",
      "Epoch: 11, Samples: 320/5760, Loss: 0.14858444035053253\n",
      "Epoch: 11, Samples: 352/5760, Loss: 0.11637052893638611\n",
      "Epoch: 11, Samples: 384/5760, Loss: 0.12155178189277649\n",
      "Epoch: 11, Samples: 416/5760, Loss: 0.10883176326751709\n",
      "Epoch: 11, Samples: 448/5760, Loss: 0.12116669118404388\n",
      "Epoch: 11, Samples: 480/5760, Loss: 0.13566453754901886\n",
      "Epoch: 11, Samples: 512/5760, Loss: 0.19483768939971924\n",
      "Epoch: 11, Samples: 544/5760, Loss: 0.1433079093694687\n",
      "Epoch: 11, Samples: 576/5760, Loss: 0.10938942432403564\n",
      "Epoch: 11, Samples: 608/5760, Loss: 0.07553346455097198\n",
      "Epoch: 11, Samples: 640/5760, Loss: 0.08070927858352661\n",
      "Epoch: 11, Samples: 672/5760, Loss: 0.0701478123664856\n",
      "Epoch: 11, Samples: 704/5760, Loss: 0.11011041700839996\n",
      "Epoch: 11, Samples: 736/5760, Loss: 0.102658212184906\n",
      "Epoch: 11, Samples: 768/5760, Loss: 0.09611640870571136\n",
      "Epoch: 11, Samples: 800/5760, Loss: 0.1372285634279251\n",
      "Epoch: 11, Samples: 832/5760, Loss: 0.16403329372406006\n",
      "Epoch: 11, Samples: 864/5760, Loss: 0.1306939572095871\n",
      "Epoch: 11, Samples: 896/5760, Loss: 0.08740776777267456\n",
      "Epoch: 11, Samples: 928/5760, Loss: 0.07609999179840088\n",
      "Epoch: 11, Samples: 960/5760, Loss: 0.29049980640411377\n",
      "Epoch: 11, Samples: 992/5760, Loss: 0.1523793786764145\n",
      "Epoch: 11, Samples: 1024/5760, Loss: 0.08632634580135345\n",
      "Epoch: 11, Samples: 1056/5760, Loss: 0.11784631013870239\n",
      "Epoch: 11, Samples: 1088/5760, Loss: 0.1302967220544815\n",
      "Epoch: 11, Samples: 1120/5760, Loss: 0.11044469475746155\n",
      "Epoch: 11, Samples: 1152/5760, Loss: 0.1222662627696991\n",
      "Epoch: 11, Samples: 1184/5760, Loss: 0.06761839985847473\n",
      "Epoch: 11, Samples: 1216/5760, Loss: 0.09395752847194672\n",
      "Epoch: 11, Samples: 1248/5760, Loss: 0.09381966292858124\n",
      "Epoch: 11, Samples: 1280/5760, Loss: 0.14744813740253448\n",
      "Epoch: 11, Samples: 1312/5760, Loss: 0.04717549681663513\n",
      "Epoch: 11, Samples: 1344/5760, Loss: 0.07792544364929199\n",
      "Epoch: 11, Samples: 1376/5760, Loss: 0.16938722133636475\n",
      "Epoch: 11, Samples: 1408/5760, Loss: 0.11456602811813354\n",
      "Epoch: 11, Samples: 1440/5760, Loss: 0.13365603983402252\n",
      "Epoch: 11, Samples: 1472/5760, Loss: 0.10198073089122772\n",
      "Epoch: 11, Samples: 1504/5760, Loss: 0.16356025636196136\n",
      "Epoch: 11, Samples: 1536/5760, Loss: 0.1603783518075943\n",
      "Epoch: 11, Samples: 1568/5760, Loss: 0.0681227445602417\n",
      "Epoch: 11, Samples: 1600/5760, Loss: 0.09565016627311707\n",
      "Epoch: 11, Samples: 1632/5760, Loss: 0.16944462060928345\n",
      "Epoch: 11, Samples: 1664/5760, Loss: 0.15126807987689972\n",
      "Epoch: 11, Samples: 1696/5760, Loss: 0.22414174675941467\n",
      "Epoch: 11, Samples: 1728/5760, Loss: 0.0984635204076767\n",
      "Epoch: 11, Samples: 1760/5760, Loss: 0.18544670939445496\n",
      "Epoch: 11, Samples: 1792/5760, Loss: 0.10295191407203674\n",
      "Epoch: 11, Samples: 1824/5760, Loss: 0.09084805846214294\n",
      "Epoch: 11, Samples: 1856/5760, Loss: 0.1300586760044098\n",
      "Epoch: 11, Samples: 1888/5760, Loss: 0.10844263434410095\n",
      "Epoch: 11, Samples: 1920/5760, Loss: 0.13795508444309235\n",
      "Epoch: 11, Samples: 1952/5760, Loss: 0.11980678141117096\n",
      "Epoch: 11, Samples: 1984/5760, Loss: 0.0957597941160202\n",
      "Epoch: 11, Samples: 2016/5760, Loss: 0.18323250114917755\n",
      "Epoch: 11, Samples: 2048/5760, Loss: 0.14858464896678925\n",
      "Epoch: 11, Samples: 2080/5760, Loss: 0.14620555937290192\n",
      "Epoch: 11, Samples: 2112/5760, Loss: 0.23603282868862152\n",
      "Epoch: 11, Samples: 2144/5760, Loss: 0.07848383486270905\n",
      "Epoch: 11, Samples: 2176/5760, Loss: 0.0721302479505539\n",
      "Epoch: 11, Samples: 2208/5760, Loss: 0.10806623101234436\n",
      "Epoch: 11, Samples: 2240/5760, Loss: 0.13503104448318481\n",
      "Epoch: 11, Samples: 2272/5760, Loss: 0.11317233741283417\n",
      "Epoch: 11, Samples: 2304/5760, Loss: 0.23020632565021515\n",
      "Epoch: 11, Samples: 2336/5760, Loss: 0.10797755420207977\n",
      "Epoch: 11, Samples: 2368/5760, Loss: 0.08842898905277252\n",
      "Epoch: 11, Samples: 2400/5760, Loss: 0.09125024080276489\n",
      "Epoch: 11, Samples: 2432/5760, Loss: 0.15374092757701874\n",
      "Epoch: 11, Samples: 2464/5760, Loss: 0.09456594288349152\n",
      "Epoch: 11, Samples: 2496/5760, Loss: 0.11237005889415741\n",
      "Epoch: 11, Samples: 2528/5760, Loss: 0.08545470237731934\n",
      "Epoch: 11, Samples: 2560/5760, Loss: 0.11637397110462189\n",
      "Epoch: 11, Samples: 2592/5760, Loss: 0.06193903088569641\n",
      "Epoch: 11, Samples: 2624/5760, Loss: 0.14285193383693695\n",
      "Epoch: 11, Samples: 2656/5760, Loss: 0.07769273221492767\n",
      "Epoch: 11, Samples: 2688/5760, Loss: 0.10601532459259033\n",
      "Epoch: 11, Samples: 2720/5760, Loss: 0.1257701814174652\n",
      "Epoch: 11, Samples: 2752/5760, Loss: 0.22711025178432465\n",
      "Epoch: 11, Samples: 2784/5760, Loss: 0.06525732576847076\n",
      "Epoch: 11, Samples: 2816/5760, Loss: 0.1651359349489212\n",
      "Epoch: 11, Samples: 2848/5760, Loss: 0.08310122787952423\n",
      "Epoch: 11, Samples: 2880/5760, Loss: 0.10811416804790497\n",
      "Epoch: 11, Samples: 2912/5760, Loss: 0.13328568637371063\n",
      "Epoch: 11, Samples: 2944/5760, Loss: 0.11275878548622131\n",
      "Epoch: 11, Samples: 2976/5760, Loss: 0.23325254023075104\n",
      "Epoch: 11, Samples: 3008/5760, Loss: 0.1748122125864029\n",
      "Epoch: 11, Samples: 3040/5760, Loss: 0.1348176747560501\n",
      "Epoch: 11, Samples: 3072/5760, Loss: 0.2681279480457306\n",
      "Epoch: 11, Samples: 3104/5760, Loss: 0.07290628552436829\n",
      "Epoch: 11, Samples: 3136/5760, Loss: 0.13659627735614777\n",
      "Epoch: 11, Samples: 3168/5760, Loss: 0.10997538268566132\n",
      "Epoch: 11, Samples: 3200/5760, Loss: 0.11124451458454132\n",
      "Epoch: 11, Samples: 3232/5760, Loss: 0.06156401336193085\n",
      "Epoch: 11, Samples: 3264/5760, Loss: 0.06078183650970459\n",
      "Epoch: 11, Samples: 3296/5760, Loss: 0.0871507078409195\n",
      "Epoch: 11, Samples: 3328/5760, Loss: 0.07381649315357208\n",
      "Epoch: 11, Samples: 3360/5760, Loss: 0.09480956196784973\n",
      "Epoch: 11, Samples: 3392/5760, Loss: 0.14856334030628204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Samples: 3424/5760, Loss: 0.15800054371356964\n",
      "Epoch: 11, Samples: 3456/5760, Loss: 0.11739584803581238\n",
      "Epoch: 11, Samples: 3488/5760, Loss: 0.09964872896671295\n",
      "Epoch: 11, Samples: 3520/5760, Loss: 0.19168102741241455\n",
      "Epoch: 11, Samples: 3552/5760, Loss: 0.06218495965003967\n",
      "Epoch: 11, Samples: 3584/5760, Loss: 0.1946091502904892\n",
      "Epoch: 11, Samples: 3616/5760, Loss: 0.16863372921943665\n",
      "Epoch: 11, Samples: 3648/5760, Loss: 0.10014839470386505\n",
      "Epoch: 11, Samples: 3680/5760, Loss: 0.05194343626499176\n",
      "Epoch: 11, Samples: 3712/5760, Loss: 0.10914731025695801\n",
      "Epoch: 11, Samples: 3744/5760, Loss: 0.11282660067081451\n",
      "Epoch: 11, Samples: 3776/5760, Loss: 0.1052650511264801\n",
      "Epoch: 11, Samples: 3808/5760, Loss: 0.08604958653450012\n",
      "Epoch: 11, Samples: 3840/5760, Loss: 0.07458855211734772\n",
      "Epoch: 11, Samples: 3872/5760, Loss: 0.059209778904914856\n",
      "Epoch: 11, Samples: 3904/5760, Loss: 0.08189661800861359\n",
      "Epoch: 11, Samples: 3936/5760, Loss: 0.1555972546339035\n",
      "Epoch: 11, Samples: 3968/5760, Loss: 0.06101837754249573\n",
      "Epoch: 11, Samples: 4000/5760, Loss: 0.09734733402729034\n",
      "Epoch: 11, Samples: 4032/5760, Loss: 0.14030246436595917\n",
      "Epoch: 11, Samples: 4064/5760, Loss: 0.09371498227119446\n",
      "Epoch: 11, Samples: 4096/5760, Loss: 0.06683807075023651\n",
      "Epoch: 11, Samples: 4128/5760, Loss: 0.09095102548599243\n",
      "Epoch: 11, Samples: 4160/5760, Loss: 0.15699248015880585\n",
      "Epoch: 11, Samples: 4192/5760, Loss: 0.16333948075771332\n",
      "Epoch: 11, Samples: 4224/5760, Loss: 0.08157901465892792\n",
      "Epoch: 11, Samples: 4256/5760, Loss: 0.15523912012577057\n",
      "Epoch: 11, Samples: 4288/5760, Loss: 0.056478992104530334\n",
      "Epoch: 11, Samples: 4320/5760, Loss: 0.13931553065776825\n",
      "Epoch: 11, Samples: 4352/5760, Loss: 0.1428646743297577\n",
      "Epoch: 11, Samples: 4384/5760, Loss: 0.06329074501991272\n",
      "Epoch: 11, Samples: 4416/5760, Loss: 0.0767059177160263\n",
      "Epoch: 11, Samples: 4448/5760, Loss: 0.12128287553787231\n",
      "Epoch: 11, Samples: 4480/5760, Loss: 0.13291294872760773\n",
      "Epoch: 11, Samples: 4512/5760, Loss: 0.06377513706684113\n",
      "Epoch: 11, Samples: 4544/5760, Loss: 0.09059731662273407\n",
      "Epoch: 11, Samples: 4576/5760, Loss: 0.18411561846733093\n",
      "Epoch: 11, Samples: 4608/5760, Loss: 0.12069492042064667\n",
      "Epoch: 11, Samples: 4640/5760, Loss: 0.1419883817434311\n",
      "Epoch: 11, Samples: 4672/5760, Loss: 0.0708790272474289\n",
      "Epoch: 11, Samples: 4704/5760, Loss: 0.0892324298620224\n",
      "Epoch: 11, Samples: 4736/5760, Loss: 0.12117849290370941\n",
      "Epoch: 11, Samples: 4768/5760, Loss: 0.13641738891601562\n",
      "Epoch: 11, Samples: 4800/5760, Loss: 0.0905870646238327\n",
      "Epoch: 11, Samples: 4832/5760, Loss: 0.1492697298526764\n",
      "Epoch: 11, Samples: 4864/5760, Loss: 0.15162506699562073\n",
      "Epoch: 11, Samples: 4896/5760, Loss: 0.09052315354347229\n",
      "Epoch: 11, Samples: 4928/5760, Loss: 0.13475656509399414\n",
      "Epoch: 11, Samples: 4960/5760, Loss: 0.07941170036792755\n",
      "Epoch: 11, Samples: 4992/5760, Loss: 0.12118121981620789\n",
      "Epoch: 11, Samples: 5024/5760, Loss: 0.2440904974937439\n",
      "Epoch: 11, Samples: 5056/5760, Loss: 0.12696506083011627\n",
      "Epoch: 11, Samples: 5088/5760, Loss: 0.08535365760326385\n",
      "Epoch: 11, Samples: 5120/5760, Loss: 0.10418125987052917\n",
      "Epoch: 11, Samples: 5152/5760, Loss: 0.14635483920574188\n",
      "Epoch: 11, Samples: 5184/5760, Loss: 0.15372925996780396\n",
      "Epoch: 11, Samples: 5216/5760, Loss: 0.06203463673591614\n",
      "Epoch: 11, Samples: 5248/5760, Loss: 0.17560149729251862\n",
      "Epoch: 11, Samples: 5280/5760, Loss: 0.08897557854652405\n",
      "Epoch: 11, Samples: 5312/5760, Loss: 0.07428677380084991\n",
      "Epoch: 11, Samples: 5344/5760, Loss: 0.1309950351715088\n",
      "Epoch: 11, Samples: 5376/5760, Loss: 0.22397735714912415\n",
      "Epoch: 11, Samples: 5408/5760, Loss: 0.0829852819442749\n",
      "Epoch: 11, Samples: 5440/5760, Loss: 0.06503431499004364\n",
      "Epoch: 11, Samples: 5472/5760, Loss: 0.15026801824569702\n",
      "Epoch: 11, Samples: 5504/5760, Loss: 0.11939716339111328\n",
      "Epoch: 11, Samples: 5536/5760, Loss: 0.07197012007236481\n",
      "Epoch: 11, Samples: 5568/5760, Loss: 0.23808786273002625\n",
      "Epoch: 11, Samples: 5600/5760, Loss: 0.09364672005176544\n",
      "Epoch: 11, Samples: 5632/5760, Loss: 0.07281017303466797\n",
      "Epoch: 11, Samples: 5664/5760, Loss: 0.1255934238433838\n",
      "Epoch: 11, Samples: 5696/5760, Loss: 0.1548740565776825\n",
      "Epoch: 11, Samples: 5728/5760, Loss: 1.793398141860962\n",
      "\n",
      "Epoch: 11\n",
      "Training set: Average loss: 0.1296\n",
      "Validation set: Average loss: 0.4121, Accuracy: 738/818 (90%)\n",
      "Saving model (epoch 11) with lowest validation loss: 0.4121058067450157\n",
      "Epoch: 12, Samples: 0/5760, Loss: 0.1900145709514618\n",
      "Epoch: 12, Samples: 32/5760, Loss: 0.16774769127368927\n",
      "Epoch: 12, Samples: 64/5760, Loss: 0.08716586232185364\n",
      "Epoch: 12, Samples: 96/5760, Loss: 0.18947051465511322\n",
      "Epoch: 12, Samples: 128/5760, Loss: 0.09906670451164246\n",
      "Epoch: 12, Samples: 160/5760, Loss: 0.10469572246074677\n",
      "Epoch: 12, Samples: 192/5760, Loss: 0.10677829384803772\n",
      "Epoch: 12, Samples: 224/5760, Loss: 0.08395305275917053\n",
      "Epoch: 12, Samples: 256/5760, Loss: 0.0661986917257309\n",
      "Epoch: 12, Samples: 288/5760, Loss: 0.08797378838062286\n",
      "Epoch: 12, Samples: 320/5760, Loss: 0.06498424708843231\n",
      "Epoch: 12, Samples: 352/5760, Loss: 0.21943295001983643\n",
      "Epoch: 12, Samples: 384/5760, Loss: 0.10373769700527191\n",
      "Epoch: 12, Samples: 416/5760, Loss: 0.130002960562706\n",
      "Epoch: 12, Samples: 448/5760, Loss: 0.0499708354473114\n",
      "Epoch: 12, Samples: 480/5760, Loss: 0.11688743531703949\n",
      "Epoch: 12, Samples: 512/5760, Loss: 0.07937240600585938\n",
      "Epoch: 12, Samples: 544/5760, Loss: 0.05714079737663269\n",
      "Epoch: 12, Samples: 576/5760, Loss: 0.13003729283809662\n",
      "Epoch: 12, Samples: 608/5760, Loss: 0.06383827328681946\n",
      "Epoch: 12, Samples: 640/5760, Loss: 0.09538476169109344\n",
      "Epoch: 12, Samples: 672/5760, Loss: 0.068783700466156\n",
      "Epoch: 12, Samples: 704/5760, Loss: 0.04846411943435669\n",
      "Epoch: 12, Samples: 736/5760, Loss: 0.13662749528884888\n",
      "Epoch: 12, Samples: 768/5760, Loss: 0.0708073228597641\n",
      "Epoch: 12, Samples: 800/5760, Loss: 0.1023036539554596\n",
      "Epoch: 12, Samples: 832/5760, Loss: 0.09913600981235504\n",
      "Epoch: 12, Samples: 864/5760, Loss: 0.11807429790496826\n",
      "Epoch: 12, Samples: 896/5760, Loss: 0.05578818917274475\n",
      "Epoch: 12, Samples: 928/5760, Loss: 0.19222496449947357\n",
      "Epoch: 12, Samples: 960/5760, Loss: 0.06947711110115051\n",
      "Epoch: 12, Samples: 992/5760, Loss: 0.08561255037784576\n",
      "Epoch: 12, Samples: 1024/5760, Loss: 0.10006330907344818\n",
      "Epoch: 12, Samples: 1056/5760, Loss: 0.061630427837371826\n",
      "Epoch: 12, Samples: 1088/5760, Loss: 0.07840368151664734\n",
      "Epoch: 12, Samples: 1120/5760, Loss: 0.08392170071601868\n",
      "Epoch: 12, Samples: 1152/5760, Loss: 0.07346732914447784\n",
      "Epoch: 12, Samples: 1184/5760, Loss: 0.2156011462211609\n",
      "Epoch: 12, Samples: 1216/5760, Loss: 0.05838245153427124\n",
      "Epoch: 12, Samples: 1248/5760, Loss: 0.14335088431835175\n",
      "Epoch: 12, Samples: 1280/5760, Loss: 0.10306088626384735\n",
      "Epoch: 12, Samples: 1312/5760, Loss: 0.11594070494174957\n",
      "Epoch: 12, Samples: 1344/5760, Loss: 0.11174137890338898\n",
      "Epoch: 12, Samples: 1376/5760, Loss: 0.10689190030097961\n",
      "Epoch: 12, Samples: 1408/5760, Loss: 0.0859091728925705\n",
      "Epoch: 12, Samples: 1440/5760, Loss: 0.12293961644172668\n",
      "Epoch: 12, Samples: 1472/5760, Loss: 0.12890386581420898\n",
      "Epoch: 12, Samples: 1504/5760, Loss: 0.10841824114322662\n",
      "Epoch: 12, Samples: 1536/5760, Loss: 0.09525835514068604\n",
      "Epoch: 12, Samples: 1568/5760, Loss: 0.09720459580421448\n",
      "Epoch: 12, Samples: 1600/5760, Loss: 0.09824053943157196\n",
      "Epoch: 12, Samples: 1632/5760, Loss: 0.07178136706352234\n",
      "Epoch: 12, Samples: 1664/5760, Loss: 0.07933749258518219\n",
      "Epoch: 12, Samples: 1696/5760, Loss: 0.10627679526805878\n",
      "Epoch: 12, Samples: 1728/5760, Loss: 0.13591310381889343\n",
      "Epoch: 12, Samples: 1760/5760, Loss: 0.1261352300643921\n",
      "Epoch: 12, Samples: 1792/5760, Loss: 0.11914308369159698\n",
      "Epoch: 12, Samples: 1824/5760, Loss: 0.07223375141620636\n",
      "Epoch: 12, Samples: 1856/5760, Loss: 0.10871516168117523\n",
      "Epoch: 12, Samples: 1888/5760, Loss: 0.14343184232711792\n",
      "Epoch: 12, Samples: 1920/5760, Loss: 0.04395744204521179\n",
      "Epoch: 12, Samples: 1952/5760, Loss: 0.08258739113807678\n",
      "Epoch: 12, Samples: 1984/5760, Loss: 0.13800914585590363\n",
      "Epoch: 12, Samples: 2016/5760, Loss: 0.10742449760437012\n",
      "Epoch: 12, Samples: 2048/5760, Loss: 0.04340827465057373\n",
      "Epoch: 12, Samples: 2080/5760, Loss: 0.1487535685300827\n",
      "Epoch: 12, Samples: 2112/5760, Loss: 0.06269823014736176\n",
      "Epoch: 12, Samples: 2144/5760, Loss: 0.06721118092536926\n",
      "Epoch: 12, Samples: 2176/5760, Loss: 0.13595204055309296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Samples: 2208/5760, Loss: 0.08212630450725555\n",
      "Epoch: 12, Samples: 2240/5760, Loss: 0.09568329155445099\n",
      "Epoch: 12, Samples: 2272/5760, Loss: 0.06224021315574646\n",
      "Epoch: 12, Samples: 2304/5760, Loss: 0.14718478918075562\n",
      "Epoch: 12, Samples: 2336/5760, Loss: 0.10543239116668701\n",
      "Epoch: 12, Samples: 2368/5760, Loss: 0.06414012610912323\n",
      "Epoch: 12, Samples: 2400/5760, Loss: 0.10738863050937653\n",
      "Epoch: 12, Samples: 2432/5760, Loss: 0.06974752247333527\n",
      "Epoch: 12, Samples: 2464/5760, Loss: 0.1147683709859848\n",
      "Epoch: 12, Samples: 2496/5760, Loss: 0.13759103417396545\n",
      "Epoch: 12, Samples: 2528/5760, Loss: 0.12108314037322998\n",
      "Epoch: 12, Samples: 2560/5760, Loss: 0.07943126559257507\n",
      "Epoch: 12, Samples: 2592/5760, Loss: 0.09561890363693237\n",
      "Epoch: 12, Samples: 2624/5760, Loss: 0.10845814645290375\n",
      "Epoch: 12, Samples: 2656/5760, Loss: 0.09269778430461884\n",
      "Epoch: 12, Samples: 2688/5760, Loss: 0.21381400525569916\n",
      "Epoch: 12, Samples: 2720/5760, Loss: 0.10206694900989532\n",
      "Epoch: 12, Samples: 2752/5760, Loss: 0.09798704087734222\n",
      "Epoch: 12, Samples: 2784/5760, Loss: 0.15431542694568634\n",
      "Epoch: 12, Samples: 2816/5760, Loss: 0.11995530128479004\n",
      "Epoch: 12, Samples: 2848/5760, Loss: 0.05091513693332672\n",
      "Epoch: 12, Samples: 2880/5760, Loss: 0.14455634355545044\n",
      "Epoch: 12, Samples: 2912/5760, Loss: 0.1170504242181778\n",
      "Epoch: 12, Samples: 2944/5760, Loss: 0.050062164664268494\n",
      "Epoch: 12, Samples: 2976/5760, Loss: 0.06479999423027039\n",
      "Epoch: 12, Samples: 3008/5760, Loss: 0.24908775091171265\n",
      "Epoch: 12, Samples: 3040/5760, Loss: 0.15248920023441315\n",
      "Epoch: 12, Samples: 3072/5760, Loss: 0.17218117415905\n",
      "Epoch: 12, Samples: 3104/5760, Loss: 0.061032891273498535\n",
      "Epoch: 12, Samples: 3136/5760, Loss: 0.2134929895401001\n",
      "Epoch: 12, Samples: 3168/5760, Loss: 0.07265821099281311\n",
      "Epoch: 12, Samples: 3200/5760, Loss: 0.07172289490699768\n",
      "Epoch: 12, Samples: 3232/5760, Loss: 0.1448076218366623\n",
      "Epoch: 12, Samples: 3264/5760, Loss: 0.07251523435115814\n",
      "Epoch: 12, Samples: 3296/5760, Loss: 0.08409768342971802\n",
      "Epoch: 12, Samples: 3328/5760, Loss: 0.09011587500572205\n",
      "Epoch: 12, Samples: 3360/5760, Loss: 0.09685918688774109\n",
      "Epoch: 12, Samples: 3392/5760, Loss: 0.14245644211769104\n",
      "Epoch: 12, Samples: 3424/5760, Loss: 0.14940382540225983\n",
      "Epoch: 12, Samples: 3456/5760, Loss: 0.14741455018520355\n",
      "Epoch: 12, Samples: 3488/5760, Loss: 0.08405062556266785\n",
      "Epoch: 12, Samples: 3520/5760, Loss: 0.08788096904754639\n",
      "Epoch: 12, Samples: 3552/5760, Loss: 0.06104722619056702\n",
      "Epoch: 12, Samples: 3584/5760, Loss: 0.059162065386772156\n",
      "Epoch: 12, Samples: 3616/5760, Loss: 0.10112020373344421\n",
      "Epoch: 12, Samples: 3648/5760, Loss: 0.0843392014503479\n",
      "Epoch: 12, Samples: 3680/5760, Loss: 0.12918788194656372\n",
      "Epoch: 12, Samples: 3712/5760, Loss: 0.1268930733203888\n",
      "Epoch: 12, Samples: 3744/5760, Loss: 0.09574602544307709\n",
      "Epoch: 12, Samples: 3776/5760, Loss: 0.08313873410224915\n",
      "Epoch: 12, Samples: 3808/5760, Loss: 0.11451198160648346\n",
      "Epoch: 12, Samples: 3840/5760, Loss: 0.09267784655094147\n",
      "Epoch: 12, Samples: 3872/5760, Loss: 0.09437276422977448\n",
      "Epoch: 12, Samples: 3904/5760, Loss: 0.14931216835975647\n",
      "Epoch: 12, Samples: 3936/5760, Loss: 0.11494582891464233\n",
      "Epoch: 12, Samples: 3968/5760, Loss: 0.08534872531890869\n",
      "Epoch: 12, Samples: 4000/5760, Loss: 0.09784017503261566\n",
      "Epoch: 12, Samples: 4032/5760, Loss: 0.07535393536090851\n",
      "Epoch: 12, Samples: 4064/5760, Loss: 0.08459442853927612\n",
      "Epoch: 12, Samples: 4096/5760, Loss: 0.16240885853767395\n",
      "Epoch: 12, Samples: 4128/5760, Loss: 0.08348117768764496\n",
      "Epoch: 12, Samples: 4160/5760, Loss: 0.1348830610513687\n",
      "Epoch: 12, Samples: 4192/5760, Loss: 0.09595026075839996\n",
      "Epoch: 12, Samples: 4224/5760, Loss: 0.0871082991361618\n",
      "Epoch: 12, Samples: 4256/5760, Loss: 0.07263194024562836\n",
      "Epoch: 12, Samples: 4288/5760, Loss: 0.07900038361549377\n",
      "Epoch: 12, Samples: 4320/5760, Loss: 0.09222105145454407\n",
      "Epoch: 12, Samples: 4352/5760, Loss: 0.0769677460193634\n",
      "Epoch: 12, Samples: 4384/5760, Loss: 0.10403542220592499\n",
      "Epoch: 12, Samples: 4416/5760, Loss: 0.06996852159500122\n",
      "Epoch: 12, Samples: 4448/5760, Loss: 0.1112409383058548\n",
      "Epoch: 12, Samples: 4480/5760, Loss: 0.07546409964561462\n",
      "Epoch: 12, Samples: 4512/5760, Loss: 0.07828240096569061\n",
      "Epoch: 12, Samples: 4544/5760, Loss: 0.12172124534845352\n",
      "Epoch: 12, Samples: 4576/5760, Loss: 0.07700398564338684\n",
      "Epoch: 12, Samples: 4608/5760, Loss: 0.08080415427684784\n",
      "Epoch: 12, Samples: 4640/5760, Loss: 0.133609801530838\n",
      "Epoch: 12, Samples: 4672/5760, Loss: 0.08562558889389038\n",
      "Epoch: 12, Samples: 4704/5760, Loss: 0.0855310708284378\n",
      "Epoch: 12, Samples: 4736/5760, Loss: 0.07872171700000763\n",
      "Epoch: 12, Samples: 4768/5760, Loss: 0.08492489159107208\n",
      "Epoch: 12, Samples: 4800/5760, Loss: 0.11236381530761719\n",
      "Epoch: 12, Samples: 4832/5760, Loss: 0.09369394183158875\n",
      "Epoch: 12, Samples: 4864/5760, Loss: 0.12751427292823792\n",
      "Epoch: 12, Samples: 4896/5760, Loss: 0.05267535150051117\n",
      "Epoch: 12, Samples: 4928/5760, Loss: 0.07795429229736328\n",
      "Epoch: 12, Samples: 4960/5760, Loss: 0.08084435760974884\n",
      "Epoch: 12, Samples: 4992/5760, Loss: 0.04293215274810791\n",
      "Epoch: 12, Samples: 5024/5760, Loss: 0.11215609312057495\n",
      "Epoch: 12, Samples: 5056/5760, Loss: 0.07686914503574371\n",
      "Epoch: 12, Samples: 5088/5760, Loss: 0.08915327489376068\n",
      "Epoch: 12, Samples: 5120/5760, Loss: 0.06975531578063965\n",
      "Epoch: 12, Samples: 5152/5760, Loss: 0.03555697202682495\n",
      "Epoch: 12, Samples: 5184/5760, Loss: 0.08679696917533875\n",
      "Epoch: 12, Samples: 5216/5760, Loss: 0.07431212067604065\n",
      "Epoch: 12, Samples: 5248/5760, Loss: 0.10716553032398224\n",
      "Epoch: 12, Samples: 5280/5760, Loss: 0.09394070506095886\n",
      "Epoch: 12, Samples: 5312/5760, Loss: 0.07938173413276672\n",
      "Epoch: 12, Samples: 5344/5760, Loss: 0.05262765288352966\n",
      "Epoch: 12, Samples: 5376/5760, Loss: 0.08339764177799225\n",
      "Epoch: 12, Samples: 5408/5760, Loss: 0.08894850313663483\n",
      "Epoch: 12, Samples: 5440/5760, Loss: 0.06389600038528442\n",
      "Epoch: 12, Samples: 5472/5760, Loss: 0.1399160623550415\n",
      "Epoch: 12, Samples: 5504/5760, Loss: 0.08430302143096924\n",
      "Epoch: 12, Samples: 5536/5760, Loss: 0.13826440274715424\n",
      "Epoch: 12, Samples: 5568/5760, Loss: 0.13243167102336884\n",
      "Epoch: 12, Samples: 5600/5760, Loss: 0.10329970717430115\n",
      "Epoch: 12, Samples: 5632/5760, Loss: 0.13979658484458923\n",
      "Epoch: 12, Samples: 5664/5760, Loss: 0.07172337174415588\n",
      "Epoch: 12, Samples: 5696/5760, Loss: 0.044490307569503784\n",
      "Epoch: 12, Samples: 5728/5760, Loss: 1.2988171577453613\n",
      "\n",
      "Epoch: 12\n",
      "Training set: Average loss: 0.1077\n",
      "Validation set: Average loss: 0.4084, Accuracy: 742/818 (91%)\n",
      "Saving model (epoch 12) with lowest validation loss: 0.40842815603201205\n",
      "Epoch: 13, Samples: 0/5760, Loss: 0.07857021689414978\n",
      "Epoch: 13, Samples: 32/5760, Loss: 0.10882729291915894\n",
      "Epoch: 13, Samples: 64/5760, Loss: 0.08269141614437103\n",
      "Epoch: 13, Samples: 96/5760, Loss: 0.07476150989532471\n",
      "Epoch: 13, Samples: 128/5760, Loss: 0.03800727427005768\n",
      "Epoch: 13, Samples: 160/5760, Loss: 0.05151720345020294\n",
      "Epoch: 13, Samples: 192/5760, Loss: 0.1490541696548462\n",
      "Epoch: 13, Samples: 224/5760, Loss: 0.07504637539386749\n",
      "Epoch: 13, Samples: 256/5760, Loss: 0.05812983214855194\n",
      "Epoch: 13, Samples: 288/5760, Loss: 0.06408928334712982\n",
      "Epoch: 13, Samples: 320/5760, Loss: 0.12315341830253601\n",
      "Epoch: 13, Samples: 352/5760, Loss: 0.11334511637687683\n",
      "Epoch: 13, Samples: 384/5760, Loss: 0.09121477603912354\n",
      "Epoch: 13, Samples: 416/5760, Loss: 0.09041106700897217\n",
      "Epoch: 13, Samples: 448/5760, Loss: 0.05812652409076691\n",
      "Epoch: 13, Samples: 480/5760, Loss: 0.04484434425830841\n",
      "Epoch: 13, Samples: 512/5760, Loss: 0.050138697028160095\n",
      "Epoch: 13, Samples: 544/5760, Loss: 0.0998840481042862\n",
      "Epoch: 13, Samples: 576/5760, Loss: 0.1057615578174591\n",
      "Epoch: 13, Samples: 608/5760, Loss: 0.09273163974285126\n",
      "Epoch: 13, Samples: 640/5760, Loss: 0.056059449911117554\n",
      "Epoch: 13, Samples: 672/5760, Loss: 0.10212334990501404\n",
      "Epoch: 13, Samples: 704/5760, Loss: 0.20235583186149597\n",
      "Epoch: 13, Samples: 736/5760, Loss: 0.08880116045475006\n",
      "Epoch: 13, Samples: 768/5760, Loss: 0.08848588168621063\n",
      "Epoch: 13, Samples: 800/5760, Loss: 0.1340257078409195\n",
      "Epoch: 13, Samples: 832/5760, Loss: 0.08558973670005798\n",
      "Epoch: 13, Samples: 864/5760, Loss: 0.10155877470970154\n",
      "Epoch: 13, Samples: 896/5760, Loss: 0.05615803599357605\n",
      "Epoch: 13, Samples: 928/5760, Loss: 0.06314980983734131\n",
      "Epoch: 13, Samples: 960/5760, Loss: 0.07910224795341492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Samples: 992/5760, Loss: 0.09647127985954285\n",
      "Epoch: 13, Samples: 1024/5760, Loss: 0.11105640232563019\n",
      "Epoch: 13, Samples: 1056/5760, Loss: 0.12511248886585236\n",
      "Epoch: 13, Samples: 1088/5760, Loss: 0.04586893320083618\n",
      "Epoch: 13, Samples: 1120/5760, Loss: 0.04906769096851349\n",
      "Epoch: 13, Samples: 1152/5760, Loss: 0.0490032434463501\n",
      "Epoch: 13, Samples: 1184/5760, Loss: 0.07682090997695923\n",
      "Epoch: 13, Samples: 1216/5760, Loss: 0.07269036769866943\n",
      "Epoch: 13, Samples: 1248/5760, Loss: 0.08756814897060394\n",
      "Epoch: 13, Samples: 1280/5760, Loss: 0.0628562867641449\n",
      "Epoch: 13, Samples: 1312/5760, Loss: 0.09955571591854095\n",
      "Epoch: 13, Samples: 1344/5760, Loss: 0.08223721385002136\n",
      "Epoch: 13, Samples: 1376/5760, Loss: 0.05411018431186676\n",
      "Epoch: 13, Samples: 1408/5760, Loss: 0.1350221037864685\n",
      "Epoch: 13, Samples: 1440/5760, Loss: 0.11415986716747284\n",
      "Epoch: 13, Samples: 1472/5760, Loss: 0.04382216930389404\n",
      "Epoch: 13, Samples: 1504/5760, Loss: 0.07536527514457703\n",
      "Epoch: 13, Samples: 1536/5760, Loss: 0.0905979722738266\n",
      "Epoch: 13, Samples: 1568/5760, Loss: 0.06141394376754761\n",
      "Epoch: 13, Samples: 1600/5760, Loss: 0.06056436896324158\n",
      "Epoch: 13, Samples: 1632/5760, Loss: 0.06572453677654266\n",
      "Epoch: 13, Samples: 1664/5760, Loss: 0.08909964561462402\n",
      "Epoch: 13, Samples: 1696/5760, Loss: 0.11279886960983276\n",
      "Epoch: 13, Samples: 1728/5760, Loss: 0.06289288401603699\n",
      "Epoch: 13, Samples: 1760/5760, Loss: 0.30148470401763916\n",
      "Epoch: 13, Samples: 1792/5760, Loss: 0.07780177891254425\n",
      "Epoch: 13, Samples: 1824/5760, Loss: 0.05190590023994446\n",
      "Epoch: 13, Samples: 1856/5760, Loss: 0.12014013528823853\n",
      "Epoch: 13, Samples: 1888/5760, Loss: 0.07232663035392761\n",
      "Epoch: 13, Samples: 1920/5760, Loss: 0.08505022525787354\n",
      "Epoch: 13, Samples: 1952/5760, Loss: 0.0981617271900177\n",
      "Epoch: 13, Samples: 1984/5760, Loss: 0.10369919240474701\n",
      "Epoch: 13, Samples: 2016/5760, Loss: 0.10509809851646423\n",
      "Epoch: 13, Samples: 2048/5760, Loss: 0.051165640354156494\n",
      "Epoch: 13, Samples: 2080/5760, Loss: 0.090357705950737\n",
      "Epoch: 13, Samples: 2112/5760, Loss: 0.07353225350379944\n",
      "Epoch: 13, Samples: 2144/5760, Loss: 0.03704294562339783\n",
      "Epoch: 13, Samples: 2176/5760, Loss: 0.05434456467628479\n",
      "Epoch: 13, Samples: 2208/5760, Loss: 0.0818357765674591\n",
      "Epoch: 13, Samples: 2240/5760, Loss: 0.05604583024978638\n",
      "Epoch: 13, Samples: 2272/5760, Loss: 0.06330078840255737\n",
      "Epoch: 13, Samples: 2304/5760, Loss: 0.10741418600082397\n",
      "Epoch: 13, Samples: 2336/5760, Loss: 0.06963346898555756\n",
      "Epoch: 13, Samples: 2368/5760, Loss: 0.06990940868854523\n",
      "Epoch: 13, Samples: 2400/5760, Loss: 0.058203428983688354\n",
      "Epoch: 13, Samples: 2432/5760, Loss: 0.08961300551891327\n",
      "Epoch: 13, Samples: 2464/5760, Loss: 0.07234896719455719\n",
      "Epoch: 13, Samples: 2496/5760, Loss: 0.0970035046339035\n",
      "Epoch: 13, Samples: 2528/5760, Loss: 0.06587620079517365\n",
      "Epoch: 13, Samples: 2560/5760, Loss: 0.10021775960922241\n",
      "Epoch: 13, Samples: 2592/5760, Loss: 0.08698447048664093\n",
      "Epoch: 13, Samples: 2624/5760, Loss: 0.08611102402210236\n",
      "Epoch: 13, Samples: 2656/5760, Loss: 0.0375799834728241\n",
      "Epoch: 13, Samples: 2688/5760, Loss: 0.04802359640598297\n",
      "Epoch: 13, Samples: 2720/5760, Loss: 0.08637616038322449\n",
      "Epoch: 13, Samples: 2752/5760, Loss: 0.06241852045059204\n",
      "Epoch: 13, Samples: 2784/5760, Loss: 0.04396688938140869\n",
      "Epoch: 13, Samples: 2816/5760, Loss: 0.058751389384269714\n",
      "Epoch: 13, Samples: 2848/5760, Loss: 0.07177388668060303\n",
      "Epoch: 13, Samples: 2880/5760, Loss: 0.07526014745235443\n",
      "Epoch: 13, Samples: 2912/5760, Loss: 0.09521669149398804\n",
      "Epoch: 13, Samples: 2944/5760, Loss: 0.05866134166717529\n",
      "Epoch: 13, Samples: 2976/5760, Loss: 0.047432079911231995\n",
      "Epoch: 13, Samples: 3008/5760, Loss: 0.13344180583953857\n",
      "Epoch: 13, Samples: 3040/5760, Loss: 0.04734860360622406\n",
      "Epoch: 13, Samples: 3072/5760, Loss: 0.14752031862735748\n",
      "Epoch: 13, Samples: 3104/5760, Loss: 0.08602151274681091\n",
      "Epoch: 13, Samples: 3136/5760, Loss: 0.07872410118579865\n",
      "Epoch: 13, Samples: 3168/5760, Loss: 0.04601156711578369\n",
      "Epoch: 13, Samples: 3200/5760, Loss: 0.06087800860404968\n",
      "Epoch: 13, Samples: 3232/5760, Loss: 0.12041893601417542\n",
      "Epoch: 13, Samples: 3264/5760, Loss: 0.0781477689743042\n",
      "Epoch: 13, Samples: 3296/5760, Loss: 0.06248730421066284\n",
      "Epoch: 13, Samples: 3328/5760, Loss: 0.04510389268398285\n",
      "Epoch: 13, Samples: 3360/5760, Loss: 0.05041816830635071\n",
      "Epoch: 13, Samples: 3392/5760, Loss: 0.06401844322681427\n",
      "Epoch: 13, Samples: 3424/5760, Loss: 0.08761276304721832\n",
      "Epoch: 13, Samples: 3456/5760, Loss: 0.07894439995288849\n",
      "Epoch: 13, Samples: 3488/5760, Loss: 0.10348951816558838\n",
      "Epoch: 13, Samples: 3520/5760, Loss: 0.08412669599056244\n",
      "Epoch: 13, Samples: 3552/5760, Loss: 0.06909164786338806\n",
      "Epoch: 13, Samples: 3584/5760, Loss: 0.05086508393287659\n",
      "Epoch: 13, Samples: 3616/5760, Loss: 0.10641063749790192\n",
      "Epoch: 13, Samples: 3648/5760, Loss: 0.06453698873519897\n",
      "Epoch: 13, Samples: 3680/5760, Loss: 0.08879804611206055\n",
      "Epoch: 13, Samples: 3712/5760, Loss: 0.11038424074649811\n",
      "Epoch: 13, Samples: 3744/5760, Loss: 0.10370460152626038\n",
      "Epoch: 13, Samples: 3776/5760, Loss: 0.0744047462940216\n",
      "Epoch: 13, Samples: 3808/5760, Loss: 0.20305822789669037\n",
      "Epoch: 13, Samples: 3840/5760, Loss: 0.19720976054668427\n",
      "Epoch: 13, Samples: 3872/5760, Loss: 0.08795340359210968\n",
      "Epoch: 13, Samples: 3904/5760, Loss: 0.08351287245750427\n",
      "Epoch: 13, Samples: 3936/5760, Loss: 0.0668078064918518\n",
      "Epoch: 13, Samples: 3968/5760, Loss: 0.1652286946773529\n",
      "Epoch: 13, Samples: 4000/5760, Loss: 0.07822789251804352\n",
      "Epoch: 13, Samples: 4032/5760, Loss: 0.07671630382537842\n",
      "Epoch: 13, Samples: 4064/5760, Loss: 0.0918712317943573\n",
      "Epoch: 13, Samples: 4096/5760, Loss: 0.06142023205757141\n",
      "Epoch: 13, Samples: 4128/5760, Loss: 0.08114305138587952\n",
      "Epoch: 13, Samples: 4160/5760, Loss: 0.10212822258472443\n",
      "Epoch: 13, Samples: 4192/5760, Loss: 0.12316760420799255\n",
      "Epoch: 13, Samples: 4224/5760, Loss: 0.047506123781204224\n",
      "Epoch: 13, Samples: 4256/5760, Loss: 0.08786001801490784\n",
      "Epoch: 13, Samples: 4288/5760, Loss: 0.07273797690868378\n",
      "Epoch: 13, Samples: 4320/5760, Loss: 0.07246099412441254\n",
      "Epoch: 13, Samples: 4352/5760, Loss: 0.06005513668060303\n",
      "Epoch: 13, Samples: 4384/5760, Loss: 0.04251369833946228\n",
      "Epoch: 13, Samples: 4416/5760, Loss: 0.07376348972320557\n",
      "Epoch: 13, Samples: 4448/5760, Loss: 0.04875047504901886\n",
      "Epoch: 13, Samples: 4480/5760, Loss: 0.08545920252799988\n",
      "Epoch: 13, Samples: 4512/5760, Loss: 0.06489306688308716\n",
      "Epoch: 13, Samples: 4544/5760, Loss: 0.05723676085472107\n",
      "Epoch: 13, Samples: 4576/5760, Loss: 0.09176087379455566\n",
      "Epoch: 13, Samples: 4608/5760, Loss: 0.06960073113441467\n",
      "Epoch: 13, Samples: 4640/5760, Loss: 0.06303703784942627\n",
      "Epoch: 13, Samples: 4672/5760, Loss: 0.150601327419281\n",
      "Epoch: 13, Samples: 4704/5760, Loss: 0.039336949586868286\n",
      "Epoch: 13, Samples: 4736/5760, Loss: 0.06006024777889252\n",
      "Epoch: 13, Samples: 4768/5760, Loss: 0.055611640214920044\n",
      "Epoch: 13, Samples: 4800/5760, Loss: 0.10872393846511841\n",
      "Epoch: 13, Samples: 4832/5760, Loss: 0.07836943864822388\n",
      "Epoch: 13, Samples: 4864/5760, Loss: 0.08103522658348083\n",
      "Epoch: 13, Samples: 4896/5760, Loss: 0.06808948516845703\n",
      "Epoch: 13, Samples: 4928/5760, Loss: 0.1285855919122696\n",
      "Epoch: 13, Samples: 4960/5760, Loss: 0.05360320210456848\n",
      "Epoch: 13, Samples: 4992/5760, Loss: 0.07288968563079834\n",
      "Epoch: 13, Samples: 5024/5760, Loss: 0.07363048195838928\n",
      "Epoch: 13, Samples: 5056/5760, Loss: 0.14558565616607666\n",
      "Epoch: 13, Samples: 5088/5760, Loss: 0.13241809606552124\n",
      "Epoch: 13, Samples: 5120/5760, Loss: 0.1269208937883377\n",
      "Epoch: 13, Samples: 5152/5760, Loss: 0.060905277729034424\n",
      "Epoch: 13, Samples: 5184/5760, Loss: 0.07549995183944702\n",
      "Epoch: 13, Samples: 5216/5760, Loss: 0.20483751595020294\n",
      "Epoch: 13, Samples: 5248/5760, Loss: 0.10332396626472473\n",
      "Epoch: 13, Samples: 5280/5760, Loss: 0.08136022090911865\n",
      "Epoch: 13, Samples: 5312/5760, Loss: 0.09920583665370941\n",
      "Epoch: 13, Samples: 5344/5760, Loss: 0.05807662010192871\n",
      "Epoch: 13, Samples: 5376/5760, Loss: 0.053450047969818115\n",
      "Epoch: 13, Samples: 5408/5760, Loss: 0.08357344567775726\n",
      "Epoch: 13, Samples: 5440/5760, Loss: 0.09700174629688263\n",
      "Epoch: 13, Samples: 5472/5760, Loss: 0.06059403717517853\n",
      "Epoch: 13, Samples: 5504/5760, Loss: 0.05598585307598114\n",
      "Epoch: 13, Samples: 5536/5760, Loss: 0.10841244459152222\n",
      "Epoch: 13, Samples: 5568/5760, Loss: 0.0632423609495163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Samples: 5600/5760, Loss: 0.06994706392288208\n",
      "Epoch: 13, Samples: 5632/5760, Loss: 0.09491951763629913\n",
      "Epoch: 13, Samples: 5664/5760, Loss: 0.11895479261875153\n",
      "Epoch: 13, Samples: 5696/5760, Loss: 0.0723094791173935\n",
      "Epoch: 13, Samples: 5728/5760, Loss: 1.267011284828186\n",
      "\n",
      "Epoch: 13\n",
      "Training set: Average loss: 0.0908\n",
      "Validation set: Average loss: 0.4084, Accuracy: 748/818 (91%)\n",
      "Saving model (epoch 13) with lowest validation loss: 0.40840844810009\n",
      "Epoch: 14, Samples: 0/5760, Loss: 0.0620688796043396\n",
      "Epoch: 14, Samples: 32/5760, Loss: 0.08220688998699188\n",
      "Epoch: 14, Samples: 64/5760, Loss: 0.047863930463790894\n",
      "Epoch: 14, Samples: 96/5760, Loss: 0.04913422465324402\n",
      "Epoch: 14, Samples: 128/5760, Loss: 0.09186720848083496\n",
      "Epoch: 14, Samples: 160/5760, Loss: 0.052261292934417725\n",
      "Epoch: 14, Samples: 192/5760, Loss: 0.11607018113136292\n",
      "Epoch: 14, Samples: 224/5760, Loss: 0.051137834787368774\n",
      "Epoch: 14, Samples: 256/5760, Loss: 0.06564377248287201\n",
      "Epoch: 14, Samples: 288/5760, Loss: 0.07578198611736298\n",
      "Epoch: 14, Samples: 320/5760, Loss: 0.05367560684680939\n",
      "Epoch: 14, Samples: 352/5760, Loss: 0.07391445338726044\n",
      "Epoch: 14, Samples: 384/5760, Loss: 0.05009101331233978\n",
      "Epoch: 14, Samples: 416/5760, Loss: 0.06296852231025696\n",
      "Epoch: 14, Samples: 448/5760, Loss: 0.09359149634838104\n",
      "Epoch: 14, Samples: 480/5760, Loss: 0.09218943119049072\n",
      "Epoch: 14, Samples: 512/5760, Loss: 0.05755108594894409\n",
      "Epoch: 14, Samples: 544/5760, Loss: 0.07716688513755798\n",
      "Epoch: 14, Samples: 576/5760, Loss: 0.08637817203998566\n",
      "Epoch: 14, Samples: 608/5760, Loss: 0.09950976073741913\n",
      "Epoch: 14, Samples: 640/5760, Loss: 0.06805376708507538\n",
      "Epoch: 14, Samples: 672/5760, Loss: 0.06044241786003113\n",
      "Epoch: 14, Samples: 704/5760, Loss: 0.05914139747619629\n",
      "Epoch: 14, Samples: 736/5760, Loss: 0.07452933490276337\n",
      "Epoch: 14, Samples: 768/5760, Loss: 0.12879444658756256\n",
      "Epoch: 14, Samples: 800/5760, Loss: 0.03618171811103821\n",
      "Epoch: 14, Samples: 832/5760, Loss: 0.06584785878658295\n",
      "Epoch: 14, Samples: 864/5760, Loss: 0.04865753650665283\n",
      "Epoch: 14, Samples: 896/5760, Loss: 0.057425618171691895\n",
      "Epoch: 14, Samples: 928/5760, Loss: 0.05089119076728821\n",
      "Epoch: 14, Samples: 960/5760, Loss: 0.04703770577907562\n",
      "Epoch: 14, Samples: 992/5760, Loss: 0.05148047208786011\n",
      "Epoch: 14, Samples: 1024/5760, Loss: 0.0856681764125824\n",
      "Epoch: 14, Samples: 1056/5760, Loss: 0.06202977895736694\n",
      "Epoch: 14, Samples: 1088/5760, Loss: 0.06705653667449951\n",
      "Epoch: 14, Samples: 1120/5760, Loss: 0.04439866542816162\n",
      "Epoch: 14, Samples: 1152/5760, Loss: 0.050155237317085266\n",
      "Epoch: 14, Samples: 1184/5760, Loss: 0.04401412606239319\n",
      "Epoch: 14, Samples: 1216/5760, Loss: 0.12014533579349518\n",
      "Epoch: 14, Samples: 1248/5760, Loss: 0.0358198881149292\n",
      "Epoch: 14, Samples: 1280/5760, Loss: 0.031169772148132324\n",
      "Epoch: 14, Samples: 1312/5760, Loss: 0.05532824993133545\n",
      "Epoch: 14, Samples: 1344/5760, Loss: 0.07293228805065155\n",
      "Epoch: 14, Samples: 1376/5760, Loss: 0.0448102205991745\n",
      "Epoch: 14, Samples: 1408/5760, Loss: 0.0762319564819336\n",
      "Epoch: 14, Samples: 1440/5760, Loss: 0.09103217720985413\n",
      "Epoch: 14, Samples: 1472/5760, Loss: 0.06848233938217163\n",
      "Epoch: 14, Samples: 1504/5760, Loss: 0.054906800389289856\n",
      "Epoch: 14, Samples: 1536/5760, Loss: 0.059325218200683594\n",
      "Epoch: 14, Samples: 1568/5760, Loss: 0.07516476511955261\n",
      "Epoch: 14, Samples: 1600/5760, Loss: 0.06671397387981415\n",
      "Epoch: 14, Samples: 1632/5760, Loss: 0.058727771043777466\n",
      "Epoch: 14, Samples: 1664/5760, Loss: 0.07128295302391052\n",
      "Epoch: 14, Samples: 1696/5760, Loss: 0.07504412531852722\n",
      "Epoch: 14, Samples: 1728/5760, Loss: 0.05765363574028015\n",
      "Epoch: 14, Samples: 1760/5760, Loss: 0.11018472909927368\n",
      "Epoch: 14, Samples: 1792/5760, Loss: 0.04468768835067749\n",
      "Epoch: 14, Samples: 1824/5760, Loss: 0.06810764968395233\n",
      "Epoch: 14, Samples: 1856/5760, Loss: 0.05795411765575409\n",
      "Epoch: 14, Samples: 1888/5760, Loss: 0.05573916435241699\n",
      "Epoch: 14, Samples: 1920/5760, Loss: 0.04803192615509033\n",
      "Epoch: 14, Samples: 1952/5760, Loss: 0.11138641834259033\n",
      "Epoch: 14, Samples: 1984/5760, Loss: 0.07048958539962769\n",
      "Epoch: 14, Samples: 2016/5760, Loss: 0.04608163237571716\n",
      "Epoch: 14, Samples: 2048/5760, Loss: 0.04859788715839386\n",
      "Epoch: 14, Samples: 2080/5760, Loss: 0.06436444818973541\n",
      "Epoch: 14, Samples: 2112/5760, Loss: 0.19874809682369232\n",
      "Epoch: 14, Samples: 2144/5760, Loss: 0.07289685308933258\n",
      "Epoch: 14, Samples: 2176/5760, Loss: 0.0549805611371994\n",
      "Epoch: 14, Samples: 2208/5760, Loss: 0.05677889287471771\n",
      "Epoch: 14, Samples: 2240/5760, Loss: 0.06150981783866882\n",
      "Epoch: 14, Samples: 2272/5760, Loss: 0.06258709728717804\n",
      "Epoch: 14, Samples: 2304/5760, Loss: 0.11243551969528198\n",
      "Epoch: 14, Samples: 2336/5760, Loss: 0.05499586462974548\n",
      "Epoch: 14, Samples: 2368/5760, Loss: 0.09987547993659973\n",
      "Epoch: 14, Samples: 2400/5760, Loss: 0.0649193674325943\n",
      "Epoch: 14, Samples: 2432/5760, Loss: 0.0535038560628891\n",
      "Epoch: 14, Samples: 2464/5760, Loss: 0.06304894387722015\n",
      "Epoch: 14, Samples: 2496/5760, Loss: 0.056677550077438354\n",
      "Epoch: 14, Samples: 2528/5760, Loss: 0.027985990047454834\n",
      "Epoch: 14, Samples: 2560/5760, Loss: 0.06701222062110901\n",
      "Epoch: 14, Samples: 2592/5760, Loss: 0.09844028949737549\n",
      "Epoch: 14, Samples: 2624/5760, Loss: 0.03148375451564789\n",
      "Epoch: 14, Samples: 2656/5760, Loss: 0.057174503803253174\n",
      "Epoch: 14, Samples: 2688/5760, Loss: 0.11700478196144104\n",
      "Epoch: 14, Samples: 2720/5760, Loss: 0.10340794920921326\n",
      "Epoch: 14, Samples: 2752/5760, Loss: 0.06964969635009766\n",
      "Epoch: 14, Samples: 2784/5760, Loss: 0.05887123942375183\n",
      "Epoch: 14, Samples: 2816/5760, Loss: 0.07762882113456726\n",
      "Epoch: 14, Samples: 2848/5760, Loss: 0.05308675765991211\n",
      "Epoch: 14, Samples: 2880/5760, Loss: 0.12710340321063995\n",
      "Epoch: 14, Samples: 2912/5760, Loss: 0.07138673961162567\n",
      "Epoch: 14, Samples: 2944/5760, Loss: 0.08839237689971924\n",
      "Epoch: 14, Samples: 2976/5760, Loss: 0.030159026384353638\n",
      "Epoch: 14, Samples: 3008/5760, Loss: 0.0526660680770874\n",
      "Epoch: 14, Samples: 3040/5760, Loss: 0.1595374345779419\n",
      "Epoch: 14, Samples: 3072/5760, Loss: 0.05023401975631714\n",
      "Epoch: 14, Samples: 3104/5760, Loss: 0.08877880871295929\n",
      "Epoch: 14, Samples: 3136/5760, Loss: 0.09484869241714478\n",
      "Epoch: 14, Samples: 3168/5760, Loss: 0.07397302985191345\n",
      "Epoch: 14, Samples: 3200/5760, Loss: 0.03822396695613861\n",
      "Epoch: 14, Samples: 3232/5760, Loss: 0.0715438574552536\n",
      "Epoch: 14, Samples: 3264/5760, Loss: 0.06917569041252136\n",
      "Epoch: 14, Samples: 3296/5760, Loss: 0.06370916962623596\n",
      "Epoch: 14, Samples: 3328/5760, Loss: 0.0549626350402832\n",
      "Epoch: 14, Samples: 3360/5760, Loss: 0.057776302099227905\n",
      "Epoch: 14, Samples: 3392/5760, Loss: 0.052700698375701904\n",
      "Epoch: 14, Samples: 3424/5760, Loss: 0.06170591711997986\n",
      "Epoch: 14, Samples: 3456/5760, Loss: 0.10099664330482483\n",
      "Epoch: 14, Samples: 3488/5760, Loss: 0.13043814897537231\n",
      "Epoch: 14, Samples: 3520/5760, Loss: 0.05546736717224121\n",
      "Epoch: 14, Samples: 3552/5760, Loss: 0.06276637315750122\n",
      "Epoch: 14, Samples: 3584/5760, Loss: 0.0679115355014801\n",
      "Epoch: 14, Samples: 3616/5760, Loss: 0.05311332643032074\n",
      "Epoch: 14, Samples: 3648/5760, Loss: 0.057486653327941895\n",
      "Epoch: 14, Samples: 3680/5760, Loss: 0.08312445878982544\n",
      "Epoch: 14, Samples: 3712/5760, Loss: 0.06835903227329254\n",
      "Epoch: 14, Samples: 3744/5760, Loss: 0.06098209321498871\n",
      "Epoch: 14, Samples: 3776/5760, Loss: 0.09676505625247955\n",
      "Epoch: 14, Samples: 3808/5760, Loss: 0.08104589581489563\n",
      "Epoch: 14, Samples: 3840/5760, Loss: 0.05477875471115112\n",
      "Epoch: 14, Samples: 3872/5760, Loss: 0.10338948667049408\n",
      "Epoch: 14, Samples: 3904/5760, Loss: 0.036495745182037354\n",
      "Epoch: 14, Samples: 3936/5760, Loss: 0.05655550956726074\n",
      "Epoch: 14, Samples: 3968/5760, Loss: 0.05769433081150055\n",
      "Epoch: 14, Samples: 4000/5760, Loss: 0.04588647186756134\n",
      "Epoch: 14, Samples: 4032/5760, Loss: 0.1118476539850235\n",
      "Epoch: 14, Samples: 4064/5760, Loss: 0.06911352276802063\n",
      "Epoch: 14, Samples: 4096/5760, Loss: 0.04119253158569336\n",
      "Epoch: 14, Samples: 4128/5760, Loss: 0.035005122423172\n",
      "Epoch: 14, Samples: 4160/5760, Loss: 0.07322447001934052\n",
      "Epoch: 14, Samples: 4192/5760, Loss: 0.061632201075553894\n",
      "Epoch: 14, Samples: 4224/5760, Loss: 0.056928470730781555\n",
      "Epoch: 14, Samples: 4256/5760, Loss: 0.07593454420566559\n",
      "Epoch: 14, Samples: 4288/5760, Loss: 0.08480888605117798\n",
      "Epoch: 14, Samples: 4320/5760, Loss: 0.05122549831867218\n",
      "Epoch: 14, Samples: 4352/5760, Loss: 0.10783183574676514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Samples: 4384/5760, Loss: 0.08835706114768982\n",
      "Epoch: 14, Samples: 4416/5760, Loss: 0.06103344261646271\n",
      "Epoch: 14, Samples: 4448/5760, Loss: 0.08584487438201904\n",
      "Epoch: 14, Samples: 4480/5760, Loss: 0.05739861726760864\n",
      "Epoch: 14, Samples: 4512/5760, Loss: 0.06808114051818848\n",
      "Epoch: 14, Samples: 4544/5760, Loss: 0.05624774098396301\n",
      "Epoch: 14, Samples: 4576/5760, Loss: 0.0841713398694992\n",
      "Epoch: 14, Samples: 4608/5760, Loss: 0.061628878116607666\n",
      "Epoch: 14, Samples: 4640/5760, Loss: 0.09659884870052338\n",
      "Epoch: 14, Samples: 4672/5760, Loss: 0.0491807758808136\n",
      "Epoch: 14, Samples: 4704/5760, Loss: 0.04335302114486694\n",
      "Epoch: 14, Samples: 4736/5760, Loss: 0.04627121984958649\n",
      "Epoch: 14, Samples: 4768/5760, Loss: 0.07388906180858612\n",
      "Epoch: 14, Samples: 4800/5760, Loss: 0.07425038516521454\n",
      "Epoch: 14, Samples: 4832/5760, Loss: 0.06812793016433716\n",
      "Epoch: 14, Samples: 4864/5760, Loss: 0.08256955444812775\n",
      "Epoch: 14, Samples: 4896/5760, Loss: 0.06626392900943756\n",
      "Epoch: 14, Samples: 4928/5760, Loss: 0.14779041707515717\n",
      "Epoch: 14, Samples: 4960/5760, Loss: 0.049722880125045776\n",
      "Epoch: 14, Samples: 4992/5760, Loss: 0.04785841703414917\n",
      "Epoch: 14, Samples: 5024/5760, Loss: 0.08700168132781982\n",
      "Epoch: 14, Samples: 5056/5760, Loss: 0.05419839918613434\n",
      "Epoch: 14, Samples: 5088/5760, Loss: 0.09679044783115387\n",
      "Epoch: 14, Samples: 5120/5760, Loss: 0.08570128679275513\n",
      "Epoch: 14, Samples: 5152/5760, Loss: 0.08412107825279236\n",
      "Epoch: 14, Samples: 5184/5760, Loss: 0.043827593326568604\n",
      "Epoch: 14, Samples: 5216/5760, Loss: 0.0493653267621994\n",
      "Epoch: 14, Samples: 5248/5760, Loss: 0.04817621409893036\n",
      "Epoch: 14, Samples: 5280/5760, Loss: 0.06243051588535309\n",
      "Epoch: 14, Samples: 5312/5760, Loss: 0.09625893831253052\n",
      "Epoch: 14, Samples: 5344/5760, Loss: 0.10879190266132355\n",
      "Epoch: 14, Samples: 5376/5760, Loss: 0.057733118534088135\n",
      "Epoch: 14, Samples: 5408/5760, Loss: 0.0799790769815445\n",
      "Epoch: 14, Samples: 5440/5760, Loss: 0.0779103934764862\n",
      "Epoch: 14, Samples: 5472/5760, Loss: 0.13338951766490936\n",
      "Epoch: 14, Samples: 5504/5760, Loss: 0.06016361713409424\n",
      "Epoch: 14, Samples: 5536/5760, Loss: 0.07753217220306396\n",
      "Epoch: 14, Samples: 5568/5760, Loss: 0.0837457925081253\n",
      "Epoch: 14, Samples: 5600/5760, Loss: 0.05638064444065094\n",
      "Epoch: 14, Samples: 5632/5760, Loss: 0.06850570440292358\n",
      "Epoch: 14, Samples: 5664/5760, Loss: 0.06874632835388184\n",
      "Epoch: 14, Samples: 5696/5760, Loss: 0.055661603808403015\n",
      "Epoch: 14, Samples: 5728/5760, Loss: 0.8340537548065186\n",
      "\n",
      "Epoch: 14\n",
      "Training set: Average loss: 0.0744\n",
      "Validation set: Average loss: 0.3793, Accuracy: 745/818 (91%)\n",
      "Saving model (epoch 14) with lowest validation loss: 0.3792673120131859\n",
      "Epoch: 15, Samples: 0/5760, Loss: 0.07500365376472473\n",
      "Epoch: 15, Samples: 32/5760, Loss: 0.11648480594158173\n",
      "Epoch: 15, Samples: 64/5760, Loss: 0.03843723237514496\n",
      "Epoch: 15, Samples: 96/5760, Loss: 0.05689242482185364\n",
      "Epoch: 15, Samples: 128/5760, Loss: 0.038393184542655945\n",
      "Epoch: 15, Samples: 160/5760, Loss: 0.06389836966991425\n",
      "Epoch: 15, Samples: 192/5760, Loss: 0.14239199459552765\n",
      "Epoch: 15, Samples: 224/5760, Loss: 0.06263697147369385\n",
      "Epoch: 15, Samples: 256/5760, Loss: 0.1085202544927597\n",
      "Epoch: 15, Samples: 288/5760, Loss: 0.04688026010990143\n",
      "Epoch: 15, Samples: 320/5760, Loss: 0.05281367897987366\n",
      "Epoch: 15, Samples: 352/5760, Loss: 0.11363546550273895\n",
      "Epoch: 15, Samples: 384/5760, Loss: 0.061027348041534424\n",
      "Epoch: 15, Samples: 416/5760, Loss: 0.05568501353263855\n",
      "Epoch: 15, Samples: 448/5760, Loss: 0.047296762466430664\n",
      "Epoch: 15, Samples: 480/5760, Loss: 0.06834372878074646\n",
      "Epoch: 15, Samples: 512/5760, Loss: 0.05583369731903076\n",
      "Epoch: 15, Samples: 544/5760, Loss: 0.08977605402469635\n",
      "Epoch: 15, Samples: 576/5760, Loss: 0.05018240213394165\n",
      "Epoch: 15, Samples: 608/5760, Loss: 0.05142013728618622\n",
      "Epoch: 15, Samples: 640/5760, Loss: 0.08877252042293549\n",
      "Epoch: 15, Samples: 672/5760, Loss: 0.05062210559844971\n",
      "Epoch: 15, Samples: 704/5760, Loss: 0.0838259607553482\n",
      "Epoch: 15, Samples: 736/5760, Loss: 0.05660399794578552\n",
      "Epoch: 15, Samples: 768/5760, Loss: 0.0949222594499588\n",
      "Epoch: 15, Samples: 800/5760, Loss: 0.05775080621242523\n",
      "Epoch: 15, Samples: 832/5760, Loss: 0.044522613286972046\n",
      "Epoch: 15, Samples: 864/5760, Loss: 0.04869872331619263\n",
      "Epoch: 15, Samples: 896/5760, Loss: 0.052923351526260376\n",
      "Epoch: 15, Samples: 928/5760, Loss: 0.07780428230762482\n",
      "Epoch: 15, Samples: 960/5760, Loss: 0.06496526300907135\n",
      "Epoch: 15, Samples: 992/5760, Loss: 0.04263916611671448\n",
      "Epoch: 15, Samples: 1024/5760, Loss: 0.06168752908706665\n",
      "Epoch: 15, Samples: 1056/5760, Loss: 0.07644423842430115\n",
      "Epoch: 15, Samples: 1088/5760, Loss: 0.06243519484996796\n",
      "Epoch: 15, Samples: 1120/5760, Loss: 0.061345696449279785\n",
      "Epoch: 15, Samples: 1152/5760, Loss: 0.049529582262039185\n",
      "Epoch: 15, Samples: 1184/5760, Loss: 0.05894365906715393\n",
      "Epoch: 15, Samples: 1216/5760, Loss: 0.04830944538116455\n",
      "Epoch: 15, Samples: 1248/5760, Loss: 0.10113248229026794\n",
      "Epoch: 15, Samples: 1280/5760, Loss: 0.033714428544044495\n",
      "Epoch: 15, Samples: 1312/5760, Loss: 0.06944641470909119\n",
      "Epoch: 15, Samples: 1344/5760, Loss: 0.053515225648880005\n",
      "Epoch: 15, Samples: 1376/5760, Loss: 0.08473190665245056\n",
      "Epoch: 15, Samples: 1408/5760, Loss: 0.04393438994884491\n",
      "Epoch: 15, Samples: 1440/5760, Loss: 0.043106332421302795\n",
      "Epoch: 15, Samples: 1472/5760, Loss: 0.06464718282222748\n",
      "Epoch: 15, Samples: 1504/5760, Loss: 0.03431059420108795\n",
      "Epoch: 15, Samples: 1536/5760, Loss: 0.0633971244096756\n",
      "Epoch: 15, Samples: 1568/5760, Loss: 0.06304372847080231\n",
      "Epoch: 15, Samples: 1600/5760, Loss: 0.038567766547203064\n",
      "Epoch: 15, Samples: 1632/5760, Loss: 0.05942130088806152\n",
      "Epoch: 15, Samples: 1664/5760, Loss: 0.07092952728271484\n",
      "Epoch: 15, Samples: 1696/5760, Loss: 0.05505755543708801\n",
      "Epoch: 15, Samples: 1728/5760, Loss: 0.07112711668014526\n",
      "Epoch: 15, Samples: 1760/5760, Loss: 0.039854466915130615\n",
      "Epoch: 15, Samples: 1792/5760, Loss: 0.043493568897247314\n",
      "Epoch: 15, Samples: 1824/5760, Loss: 0.05189685523509979\n",
      "Epoch: 15, Samples: 1856/5760, Loss: 0.051153361797332764\n",
      "Epoch: 15, Samples: 1888/5760, Loss: 0.060898080468177795\n",
      "Epoch: 15, Samples: 1920/5760, Loss: 0.03503504395484924\n",
      "Epoch: 15, Samples: 1952/5760, Loss: 0.06721045076847076\n",
      "Epoch: 15, Samples: 1984/5760, Loss: 0.05726289749145508\n",
      "Epoch: 15, Samples: 2016/5760, Loss: 0.07710272073745728\n",
      "Epoch: 15, Samples: 2048/5760, Loss: 0.04067492485046387\n",
      "Epoch: 15, Samples: 2080/5760, Loss: 0.10440382361412048\n",
      "Epoch: 15, Samples: 2112/5760, Loss: 0.06454125046730042\n",
      "Epoch: 15, Samples: 2144/5760, Loss: 0.054368630051612854\n",
      "Epoch: 15, Samples: 2176/5760, Loss: 0.10945653915405273\n",
      "Epoch: 15, Samples: 2208/5760, Loss: 0.07684394717216492\n",
      "Epoch: 15, Samples: 2240/5760, Loss: 0.02971397340297699\n",
      "Epoch: 15, Samples: 2272/5760, Loss: 0.05333782732486725\n",
      "Epoch: 15, Samples: 2304/5760, Loss: 0.05984318256378174\n",
      "Epoch: 15, Samples: 2336/5760, Loss: 0.07485504448413849\n",
      "Epoch: 15, Samples: 2368/5760, Loss: 0.03718273341655731\n",
      "Epoch: 15, Samples: 2400/5760, Loss: 0.09373502433300018\n",
      "Epoch: 15, Samples: 2432/5760, Loss: 0.05841703712940216\n",
      "Epoch: 15, Samples: 2464/5760, Loss: 0.05844278633594513\n",
      "Epoch: 15, Samples: 2496/5760, Loss: 0.0796947330236435\n",
      "Epoch: 15, Samples: 2528/5760, Loss: 0.03714607656002045\n",
      "Epoch: 15, Samples: 2560/5760, Loss: 0.08970403671264648\n",
      "Epoch: 15, Samples: 2592/5760, Loss: 0.07603086531162262\n",
      "Epoch: 15, Samples: 2624/5760, Loss: 0.06976665556430817\n",
      "Epoch: 15, Samples: 2656/5760, Loss: 0.04950876533985138\n",
      "Epoch: 15, Samples: 2688/5760, Loss: 0.07939349114894867\n",
      "Epoch: 15, Samples: 2720/5760, Loss: 0.057252317667007446\n",
      "Epoch: 15, Samples: 2752/5760, Loss: 0.025459110736846924\n",
      "Epoch: 15, Samples: 2784/5760, Loss: 0.057591378688812256\n",
      "Epoch: 15, Samples: 2816/5760, Loss: 0.028854072093963623\n",
      "Epoch: 15, Samples: 2848/5760, Loss: 0.043557941913604736\n",
      "Epoch: 15, Samples: 2880/5760, Loss: 0.10197232663631439\n",
      "Epoch: 15, Samples: 2912/5760, Loss: 0.03136962652206421\n",
      "Epoch: 15, Samples: 2944/5760, Loss: 0.03917756676673889\n",
      "Epoch: 15, Samples: 2976/5760, Loss: 0.054101377725601196\n",
      "Epoch: 15, Samples: 3008/5760, Loss: 0.06614966690540314\n",
      "Epoch: 15, Samples: 3040/5760, Loss: 0.060445636510849\n",
      "Epoch: 15, Samples: 3072/5760, Loss: 0.035348594188690186\n",
      "Epoch: 15, Samples: 3104/5760, Loss: 0.05980539321899414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Samples: 3136/5760, Loss: 0.07074475288391113\n",
      "Epoch: 15, Samples: 3168/5760, Loss: 0.02876165509223938\n",
      "Epoch: 15, Samples: 3200/5760, Loss: 0.07904922962188721\n",
      "Epoch: 15, Samples: 3232/5760, Loss: 0.06416784226894379\n",
      "Epoch: 15, Samples: 3264/5760, Loss: 0.07274758815765381\n",
      "Epoch: 15, Samples: 3296/5760, Loss: 0.05363556742668152\n",
      "Epoch: 15, Samples: 3328/5760, Loss: 0.036273419857025146\n",
      "Epoch: 15, Samples: 3360/5760, Loss: 0.04399131238460541\n",
      "Epoch: 15, Samples: 3392/5760, Loss: 0.0634319931268692\n",
      "Epoch: 15, Samples: 3424/5760, Loss: 0.06921035051345825\n",
      "Epoch: 15, Samples: 3456/5760, Loss: 0.07118827104568481\n",
      "Epoch: 15, Samples: 3488/5760, Loss: 0.037470296025276184\n",
      "Epoch: 15, Samples: 3520/5760, Loss: 0.04213954508304596\n",
      "Epoch: 15, Samples: 3552/5760, Loss: 0.05747193098068237\n",
      "Epoch: 15, Samples: 3584/5760, Loss: 0.05740933120250702\n",
      "Epoch: 15, Samples: 3616/5760, Loss: 0.05544520914554596\n",
      "Epoch: 15, Samples: 3648/5760, Loss: 0.09002611041069031\n",
      "Epoch: 15, Samples: 3680/5760, Loss: 0.03992193937301636\n",
      "Epoch: 15, Samples: 3712/5760, Loss: 0.05305834114551544\n",
      "Epoch: 15, Samples: 3744/5760, Loss: 0.11121828854084015\n",
      "Epoch: 15, Samples: 3776/5760, Loss: 0.05056767165660858\n",
      "Epoch: 15, Samples: 3808/5760, Loss: 0.05162365734577179\n",
      "Epoch: 15, Samples: 3840/5760, Loss: 0.043263018131256104\n",
      "Epoch: 15, Samples: 3872/5760, Loss: 0.0542839914560318\n",
      "Epoch: 15, Samples: 3904/5760, Loss: 0.05146336555480957\n",
      "Epoch: 15, Samples: 3936/5760, Loss: 0.03978118300437927\n",
      "Epoch: 15, Samples: 3968/5760, Loss: 0.07219283282756805\n",
      "Epoch: 15, Samples: 4000/5760, Loss: 0.04595746099948883\n",
      "Epoch: 15, Samples: 4032/5760, Loss: 0.046159058809280396\n",
      "Epoch: 15, Samples: 4064/5760, Loss: 0.028782188892364502\n",
      "Epoch: 15, Samples: 4096/5760, Loss: 0.06739474833011627\n",
      "Epoch: 15, Samples: 4128/5760, Loss: 0.05925045907497406\n",
      "Epoch: 15, Samples: 4160/5760, Loss: 0.0904216319322586\n",
      "Epoch: 15, Samples: 4192/5760, Loss: 0.04801274836063385\n",
      "Epoch: 15, Samples: 4224/5760, Loss: 0.05075354874134064\n",
      "Epoch: 15, Samples: 4256/5760, Loss: 0.05826418101787567\n",
      "Epoch: 15, Samples: 4288/5760, Loss: 0.06642931699752808\n",
      "Epoch: 15, Samples: 4320/5760, Loss: 0.06298300623893738\n",
      "Epoch: 15, Samples: 4352/5760, Loss: 0.0684138685464859\n",
      "Epoch: 15, Samples: 4384/5760, Loss: 0.04555237293243408\n",
      "Epoch: 15, Samples: 4416/5760, Loss: 0.04328733682632446\n",
      "Epoch: 15, Samples: 4448/5760, Loss: 0.11465969681739807\n",
      "Epoch: 15, Samples: 4480/5760, Loss: 0.049149930477142334\n",
      "Epoch: 15, Samples: 4512/5760, Loss: 0.06091904640197754\n",
      "Epoch: 15, Samples: 4544/5760, Loss: 0.0404595285654068\n",
      "Epoch: 15, Samples: 4576/5760, Loss: 0.047631680965423584\n",
      "Epoch: 15, Samples: 4608/5760, Loss: 0.07785478234291077\n",
      "Epoch: 15, Samples: 4640/5760, Loss: 0.0713544636964798\n",
      "Epoch: 15, Samples: 4672/5760, Loss: 0.09001137316226959\n",
      "Epoch: 15, Samples: 4704/5760, Loss: 0.06738802790641785\n",
      "Epoch: 15, Samples: 4736/5760, Loss: 0.043569132685661316\n",
      "Epoch: 15, Samples: 4768/5760, Loss: 0.03852856159210205\n",
      "Epoch: 15, Samples: 4800/5760, Loss: 0.0629594624042511\n",
      "Epoch: 15, Samples: 4832/5760, Loss: 0.04138706624507904\n",
      "Epoch: 15, Samples: 4864/5760, Loss: 0.03208106756210327\n",
      "Epoch: 15, Samples: 4896/5760, Loss: 0.05159918963909149\n",
      "Epoch: 15, Samples: 4928/5760, Loss: 0.0791592001914978\n",
      "Epoch: 15, Samples: 4960/5760, Loss: 0.08993415534496307\n",
      "Epoch: 15, Samples: 4992/5760, Loss: 0.057800114154815674\n",
      "Epoch: 15, Samples: 5024/5760, Loss: 0.05915333330631256\n",
      "Epoch: 15, Samples: 5056/5760, Loss: 0.04832926392555237\n",
      "Epoch: 15, Samples: 5088/5760, Loss: 0.08140580356121063\n",
      "Epoch: 15, Samples: 5120/5760, Loss: 0.0542495995759964\n",
      "Epoch: 15, Samples: 5152/5760, Loss: 0.10365217924118042\n",
      "Epoch: 15, Samples: 5184/5760, Loss: 0.059776321053504944\n",
      "Epoch: 15, Samples: 5216/5760, Loss: 0.08589096367359161\n",
      "Epoch: 15, Samples: 5248/5760, Loss: 0.05408217012882233\n",
      "Epoch: 15, Samples: 5280/5760, Loss: 0.06592991948127747\n",
      "Epoch: 15, Samples: 5312/5760, Loss: 0.04005920886993408\n",
      "Epoch: 15, Samples: 5344/5760, Loss: 0.040519848465919495\n",
      "Epoch: 15, Samples: 5376/5760, Loss: 0.0328422486782074\n",
      "Epoch: 15, Samples: 5408/5760, Loss: 0.09801438450813293\n",
      "Epoch: 15, Samples: 5440/5760, Loss: 0.055083245038986206\n",
      "Epoch: 15, Samples: 5472/5760, Loss: 0.12362019717693329\n",
      "Epoch: 15, Samples: 5504/5760, Loss: 0.05359466373920441\n",
      "Epoch: 15, Samples: 5536/5760, Loss: 0.081407830119133\n",
      "Epoch: 15, Samples: 5568/5760, Loss: 0.08946898579597473\n",
      "Epoch: 15, Samples: 5600/5760, Loss: 0.07374538481235504\n",
      "Epoch: 15, Samples: 5632/5760, Loss: 0.08311016857624054\n",
      "Epoch: 15, Samples: 5664/5760, Loss: 0.054564476013183594\n",
      "Epoch: 15, Samples: 5696/5760, Loss: 0.0496528297662735\n",
      "Epoch: 15, Samples: 5728/5760, Loss: 0.608061671257019\n",
      "\n",
      "Epoch: 15\n",
      "Training set: Average loss: 0.0646\n",
      "Validation set: Average loss: 0.3904, Accuracy: 747/818 (91%)\n",
      "Epoch: 16, Samples: 0/5760, Loss: 0.05951964855194092\n",
      "Epoch: 16, Samples: 32/5760, Loss: 0.042982637882232666\n",
      "Epoch: 16, Samples: 64/5760, Loss: 0.05451805889606476\n",
      "Epoch: 16, Samples: 96/5760, Loss: 0.017942965030670166\n",
      "Epoch: 16, Samples: 128/5760, Loss: 0.07635627686977386\n",
      "Epoch: 16, Samples: 160/5760, Loss: 0.07137012481689453\n",
      "Epoch: 16, Samples: 192/5760, Loss: 0.05305729806423187\n",
      "Epoch: 16, Samples: 224/5760, Loss: 0.04101867973804474\n",
      "Epoch: 16, Samples: 256/5760, Loss: 0.08120886981487274\n",
      "Epoch: 16, Samples: 288/5760, Loss: 0.054533496499061584\n",
      "Epoch: 16, Samples: 320/5760, Loss: 0.048839882016181946\n",
      "Epoch: 16, Samples: 352/5760, Loss: 0.05115637183189392\n",
      "Epoch: 16, Samples: 384/5760, Loss: 0.03189544379711151\n",
      "Epoch: 16, Samples: 416/5760, Loss: 0.0428009033203125\n",
      "Epoch: 16, Samples: 448/5760, Loss: 0.05117662250995636\n",
      "Epoch: 16, Samples: 480/5760, Loss: 0.035614803433418274\n",
      "Epoch: 16, Samples: 512/5760, Loss: 0.05669696629047394\n",
      "Epoch: 16, Samples: 544/5760, Loss: 0.0491635799407959\n",
      "Epoch: 16, Samples: 576/5760, Loss: 0.06321507692337036\n",
      "Epoch: 16, Samples: 608/5760, Loss: 0.02939620614051819\n",
      "Epoch: 16, Samples: 640/5760, Loss: 0.040710628032684326\n",
      "Epoch: 16, Samples: 672/5760, Loss: 0.07309208810329437\n",
      "Epoch: 16, Samples: 704/5760, Loss: 0.057905226945877075\n",
      "Epoch: 16, Samples: 736/5760, Loss: 0.035354986786842346\n",
      "Epoch: 16, Samples: 768/5760, Loss: 0.0604395866394043\n",
      "Epoch: 16, Samples: 800/5760, Loss: 0.07378819584846497\n",
      "Epoch: 16, Samples: 832/5760, Loss: 0.04577445983886719\n",
      "Epoch: 16, Samples: 864/5760, Loss: 0.055521294474601746\n",
      "Epoch: 16, Samples: 896/5760, Loss: 0.065916046500206\n",
      "Epoch: 16, Samples: 928/5760, Loss: 0.05351091921329498\n",
      "Epoch: 16, Samples: 960/5760, Loss: 0.0821685642004013\n",
      "Epoch: 16, Samples: 992/5760, Loss: 0.0582742840051651\n",
      "Epoch: 16, Samples: 1024/5760, Loss: 0.05307985842227936\n",
      "Epoch: 16, Samples: 1056/5760, Loss: 0.05458363890647888\n",
      "Epoch: 16, Samples: 1088/5760, Loss: 0.042380452156066895\n",
      "Epoch: 16, Samples: 1120/5760, Loss: 0.0438164621591568\n",
      "Epoch: 16, Samples: 1152/5760, Loss: 0.04400758445262909\n",
      "Epoch: 16, Samples: 1184/5760, Loss: 0.04886952042579651\n",
      "Epoch: 16, Samples: 1216/5760, Loss: 0.03575950860977173\n",
      "Epoch: 16, Samples: 1248/5760, Loss: 0.032886236906051636\n",
      "Epoch: 16, Samples: 1280/5760, Loss: 0.023221075534820557\n",
      "Epoch: 16, Samples: 1312/5760, Loss: 0.067538782954216\n",
      "Epoch: 16, Samples: 1344/5760, Loss: 0.08490213751792908\n",
      "Epoch: 16, Samples: 1376/5760, Loss: 0.03815680742263794\n",
      "Epoch: 16, Samples: 1408/5760, Loss: 0.06693756580352783\n",
      "Epoch: 16, Samples: 1440/5760, Loss: 0.035614460706710815\n",
      "Epoch: 16, Samples: 1472/5760, Loss: 0.0714254081249237\n",
      "Epoch: 16, Samples: 1504/5760, Loss: 0.050277337431907654\n",
      "Epoch: 16, Samples: 1536/5760, Loss: 0.1069246456027031\n",
      "Epoch: 16, Samples: 1568/5760, Loss: 0.0891992598772049\n",
      "Epoch: 16, Samples: 1600/5760, Loss: 0.0553896427154541\n",
      "Epoch: 16, Samples: 1632/5760, Loss: 0.06791111826896667\n",
      "Epoch: 16, Samples: 1664/5760, Loss: 0.06644237041473389\n",
      "Epoch: 16, Samples: 1696/5760, Loss: 0.06006743013858795\n",
      "Epoch: 16, Samples: 1728/5760, Loss: 0.07130266726016998\n",
      "Epoch: 16, Samples: 1760/5760, Loss: 0.10751573741436005\n",
      "Epoch: 16, Samples: 1792/5760, Loss: 0.036874085664749146\n",
      "Epoch: 16, Samples: 1824/5760, Loss: 0.052326276898384094\n",
      "Epoch: 16, Samples: 1856/5760, Loss: 0.04686042666435242\n",
      "Epoch: 16, Samples: 1888/5760, Loss: 0.047378793358802795\n",
      "Epoch: 16, Samples: 1920/5760, Loss: 0.06456291675567627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Samples: 1952/5760, Loss: 0.028997987508773804\n",
      "Epoch: 16, Samples: 1984/5760, Loss: 0.045222654938697815\n",
      "Epoch: 16, Samples: 2016/5760, Loss: 0.03911393880844116\n",
      "Epoch: 16, Samples: 2048/5760, Loss: 0.029383808374404907\n",
      "Epoch: 16, Samples: 2080/5760, Loss: 0.06435325741767883\n",
      "Epoch: 16, Samples: 2112/5760, Loss: 0.0364413857460022\n",
      "Epoch: 16, Samples: 2144/5760, Loss: 0.03273773193359375\n",
      "Epoch: 16, Samples: 2176/5760, Loss: 0.02923676371574402\n",
      "Epoch: 16, Samples: 2208/5760, Loss: 0.046971604228019714\n",
      "Epoch: 16, Samples: 2240/5760, Loss: 0.11490656435489655\n",
      "Epoch: 16, Samples: 2272/5760, Loss: 0.041240036487579346\n",
      "Epoch: 16, Samples: 2304/5760, Loss: 0.03551235795021057\n",
      "Epoch: 16, Samples: 2336/5760, Loss: 0.05797789990901947\n",
      "Epoch: 16, Samples: 2368/5760, Loss: 0.055067047476768494\n",
      "Epoch: 16, Samples: 2400/5760, Loss: 0.042353272438049316\n",
      "Epoch: 16, Samples: 2432/5760, Loss: 0.06974929571151733\n",
      "Epoch: 16, Samples: 2464/5760, Loss: 0.033122241497039795\n",
      "Epoch: 16, Samples: 2496/5760, Loss: 0.05604511499404907\n",
      "Epoch: 16, Samples: 2528/5760, Loss: 0.06380291283130646\n",
      "Epoch: 16, Samples: 2560/5760, Loss: 0.05140887200832367\n",
      "Epoch: 16, Samples: 2592/5760, Loss: 0.04381828010082245\n",
      "Epoch: 16, Samples: 2624/5760, Loss: 0.05823497474193573\n",
      "Epoch: 16, Samples: 2656/5760, Loss: 0.06173062324523926\n",
      "Epoch: 16, Samples: 2688/5760, Loss: 0.08112314343452454\n",
      "Epoch: 16, Samples: 2720/5760, Loss: 0.07108095288276672\n",
      "Epoch: 16, Samples: 2752/5760, Loss: 0.03367315232753754\n",
      "Epoch: 16, Samples: 2784/5760, Loss: 0.0774359256029129\n",
      "Epoch: 16, Samples: 2816/5760, Loss: 0.052775606513023376\n",
      "Epoch: 16, Samples: 2848/5760, Loss: 0.03557717800140381\n",
      "Epoch: 16, Samples: 2880/5760, Loss: 0.03912043571472168\n",
      "Epoch: 16, Samples: 2912/5760, Loss: 0.022003859281539917\n",
      "Epoch: 16, Samples: 2944/5760, Loss: 0.03463137149810791\n",
      "Epoch: 16, Samples: 2976/5760, Loss: 0.047822922468185425\n",
      "Epoch: 16, Samples: 3008/5760, Loss: 0.08249561488628387\n",
      "Epoch: 16, Samples: 3040/5760, Loss: 0.05196589231491089\n",
      "Epoch: 16, Samples: 3072/5760, Loss: 0.04895326495170593\n",
      "Epoch: 16, Samples: 3104/5760, Loss: 0.10168503224849701\n",
      "Epoch: 16, Samples: 3136/5760, Loss: 0.06286771595478058\n",
      "Epoch: 16, Samples: 3168/5760, Loss: 0.07382680475711823\n",
      "Epoch: 16, Samples: 3200/5760, Loss: 0.028531372547149658\n",
      "Epoch: 16, Samples: 3232/5760, Loss: 0.02994990348815918\n",
      "Epoch: 16, Samples: 3264/5760, Loss: 0.06760422885417938\n",
      "Epoch: 16, Samples: 3296/5760, Loss: 0.05936190485954285\n",
      "Epoch: 16, Samples: 3328/5760, Loss: 0.04231773316860199\n",
      "Epoch: 16, Samples: 3360/5760, Loss: 0.05629396438598633\n",
      "Epoch: 16, Samples: 3392/5760, Loss: 0.02964460849761963\n",
      "Epoch: 16, Samples: 3424/5760, Loss: 0.051390960812568665\n",
      "Epoch: 16, Samples: 3456/5760, Loss: 0.05219048261642456\n",
      "Epoch: 16, Samples: 3488/5760, Loss: 0.06153859198093414\n",
      "Epoch: 16, Samples: 3520/5760, Loss: 0.09367869794368744\n",
      "Epoch: 16, Samples: 3552/5760, Loss: 0.050568222999572754\n",
      "Epoch: 16, Samples: 3584/5760, Loss: 0.06516624987125397\n",
      "Epoch: 16, Samples: 3616/5760, Loss: 0.04486113786697388\n",
      "Epoch: 16, Samples: 3648/5760, Loss: 0.07965520024299622\n",
      "Epoch: 16, Samples: 3680/5760, Loss: 0.053900346159935\n",
      "Epoch: 16, Samples: 3712/5760, Loss: 0.0605844110250473\n",
      "Epoch: 16, Samples: 3744/5760, Loss: 0.0516352504491806\n",
      "Epoch: 16, Samples: 3776/5760, Loss: 0.04753027856349945\n",
      "Epoch: 16, Samples: 3808/5760, Loss: 0.06231634318828583\n",
      "Epoch: 16, Samples: 3840/5760, Loss: 0.14764252305030823\n",
      "Epoch: 16, Samples: 3872/5760, Loss: 0.06805236637592316\n",
      "Epoch: 16, Samples: 3904/5760, Loss: 0.05204567313194275\n",
      "Epoch: 16, Samples: 3936/5760, Loss: 0.05961301922798157\n",
      "Epoch: 16, Samples: 3968/5760, Loss: 0.05592724680900574\n",
      "Epoch: 16, Samples: 4000/5760, Loss: 0.04193079471588135\n",
      "Epoch: 16, Samples: 4032/5760, Loss: 0.04609547555446625\n",
      "Epoch: 16, Samples: 4064/5760, Loss: 0.03873738646507263\n",
      "Epoch: 16, Samples: 4096/5760, Loss: 0.07168427109718323\n",
      "Epoch: 16, Samples: 4128/5760, Loss: 0.0474555641412735\n",
      "Epoch: 16, Samples: 4160/5760, Loss: 0.042724162340164185\n",
      "Epoch: 16, Samples: 4192/5760, Loss: 0.06667667627334595\n",
      "Epoch: 16, Samples: 4224/5760, Loss: 0.07852521538734436\n",
      "Epoch: 16, Samples: 4256/5760, Loss: 0.04584825038909912\n",
      "Epoch: 16, Samples: 4288/5760, Loss: 0.09123097360134125\n",
      "Epoch: 16, Samples: 4320/5760, Loss: 0.016685396432876587\n",
      "Epoch: 16, Samples: 4352/5760, Loss: 0.03539615869522095\n",
      "Epoch: 16, Samples: 4384/5760, Loss: 0.04808935523033142\n",
      "Epoch: 16, Samples: 4416/5760, Loss: 0.07616959512233734\n",
      "Epoch: 16, Samples: 4448/5760, Loss: 0.08642654120922089\n",
      "Epoch: 16, Samples: 4480/5760, Loss: 0.04732251167297363\n",
      "Epoch: 16, Samples: 4512/5760, Loss: 0.03937670588493347\n",
      "Epoch: 16, Samples: 4544/5760, Loss: 0.035277724266052246\n",
      "Epoch: 16, Samples: 4576/5760, Loss: 0.04594685137271881\n",
      "Epoch: 16, Samples: 4608/5760, Loss: 0.049685657024383545\n",
      "Epoch: 16, Samples: 4640/5760, Loss: 0.0385003387928009\n",
      "Epoch: 16, Samples: 4672/5760, Loss: 0.0423235148191452\n",
      "Epoch: 16, Samples: 4704/5760, Loss: 0.04374375939369202\n",
      "Epoch: 16, Samples: 4736/5760, Loss: 0.03552624583244324\n",
      "Epoch: 16, Samples: 4768/5760, Loss: 0.0451200008392334\n",
      "Epoch: 16, Samples: 4800/5760, Loss: 0.057515352964401245\n",
      "Epoch: 16, Samples: 4832/5760, Loss: 0.032540977001190186\n",
      "Epoch: 16, Samples: 4864/5760, Loss: 0.02095162868499756\n",
      "Epoch: 16, Samples: 4896/5760, Loss: 0.038643985986709595\n",
      "Epoch: 16, Samples: 4928/5760, Loss: 0.04318992793560028\n",
      "Epoch: 16, Samples: 4960/5760, Loss: 0.06385660171508789\n",
      "Epoch: 16, Samples: 4992/5760, Loss: 0.01878809928894043\n",
      "Epoch: 16, Samples: 5024/5760, Loss: 0.03908528387546539\n",
      "Epoch: 16, Samples: 5056/5760, Loss: 0.05016529560089111\n",
      "Epoch: 16, Samples: 5088/5760, Loss: 0.031981468200683594\n",
      "Epoch: 16, Samples: 5120/5760, Loss: 0.07135534286499023\n",
      "Epoch: 16, Samples: 5152/5760, Loss: 0.058913856744766235\n",
      "Epoch: 16, Samples: 5184/5760, Loss: 0.04370655119419098\n",
      "Epoch: 16, Samples: 5216/5760, Loss: 0.08453741669654846\n",
      "Epoch: 16, Samples: 5248/5760, Loss: 0.04566106200218201\n",
      "Epoch: 16, Samples: 5280/5760, Loss: 0.041566550731658936\n",
      "Epoch: 16, Samples: 5312/5760, Loss: 0.03588831424713135\n",
      "Epoch: 16, Samples: 5344/5760, Loss: 0.032312214374542236\n",
      "Epoch: 16, Samples: 5376/5760, Loss: 0.059273943305015564\n",
      "Epoch: 16, Samples: 5408/5760, Loss: 0.04485447704792023\n",
      "Epoch: 16, Samples: 5440/5760, Loss: 0.06149563193321228\n",
      "Epoch: 16, Samples: 5472/5760, Loss: 0.044085562229156494\n",
      "Epoch: 16, Samples: 5504/5760, Loss: 0.06433956325054169\n",
      "Epoch: 16, Samples: 5536/5760, Loss: 0.05716976523399353\n",
      "Epoch: 16, Samples: 5568/5760, Loss: 0.0328199565410614\n",
      "Epoch: 16, Samples: 5600/5760, Loss: 0.04979625344276428\n",
      "Epoch: 16, Samples: 5632/5760, Loss: 0.03558427095413208\n",
      "Epoch: 16, Samples: 5664/5760, Loss: 0.05150905251502991\n",
      "Epoch: 16, Samples: 5696/5760, Loss: 0.02594122290611267\n",
      "Epoch: 16, Samples: 5728/5760, Loss: 0.7480413913726807\n",
      "\n",
      "Epoch: 16\n",
      "Training set: Average loss: 0.0569\n",
      "Validation set: Average loss: 0.3798, Accuracy: 742/818 (91%)\n",
      "Epoch: 17, Samples: 0/5760, Loss: 0.0357065349817276\n",
      "Epoch: 17, Samples: 32/5760, Loss: 0.027571991086006165\n",
      "Epoch: 17, Samples: 64/5760, Loss: 0.03461050987243652\n",
      "Epoch: 17, Samples: 96/5760, Loss: 0.07631918787956238\n",
      "Epoch: 17, Samples: 128/5760, Loss: 0.0748402327299118\n",
      "Epoch: 17, Samples: 160/5760, Loss: 0.04699668288230896\n",
      "Epoch: 17, Samples: 192/5760, Loss: 0.04937578737735748\n",
      "Epoch: 17, Samples: 224/5760, Loss: 0.0550084114074707\n",
      "Epoch: 17, Samples: 256/5760, Loss: 0.0433342307806015\n",
      "Epoch: 17, Samples: 288/5760, Loss: 0.04227036237716675\n",
      "Epoch: 17, Samples: 320/5760, Loss: 0.04795989394187927\n",
      "Epoch: 17, Samples: 352/5760, Loss: 0.05395272374153137\n",
      "Epoch: 17, Samples: 384/5760, Loss: 0.06870818138122559\n",
      "Epoch: 17, Samples: 416/5760, Loss: 0.03193551301956177\n",
      "Epoch: 17, Samples: 448/5760, Loss: 0.07760484516620636\n",
      "Epoch: 17, Samples: 480/5760, Loss: 0.04994535446166992\n",
      "Epoch: 17, Samples: 512/5760, Loss: 0.11090514063835144\n",
      "Epoch: 17, Samples: 544/5760, Loss: 0.034788280725479126\n",
      "Epoch: 17, Samples: 576/5760, Loss: 0.03403094410896301\n",
      "Epoch: 17, Samples: 608/5760, Loss: 0.04926446080207825\n",
      "Epoch: 17, Samples: 640/5760, Loss: 0.08294105529785156\n",
      "Epoch: 17, Samples: 672/5760, Loss: 0.05826893448829651\n",
      "Epoch: 17, Samples: 704/5760, Loss: 0.08498038351535797\n",
      "Epoch: 17, Samples: 736/5760, Loss: 0.031560882925987244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Samples: 768/5760, Loss: 0.04367738962173462\n",
      "Epoch: 17, Samples: 800/5760, Loss: 0.060830071568489075\n",
      "Epoch: 17, Samples: 832/5760, Loss: 0.06291152536869049\n",
      "Epoch: 17, Samples: 864/5760, Loss: 0.05647513270378113\n",
      "Epoch: 17, Samples: 896/5760, Loss: 0.02206730842590332\n",
      "Epoch: 17, Samples: 928/5760, Loss: 0.031137168407440186\n",
      "Epoch: 17, Samples: 960/5760, Loss: 0.0653136819601059\n",
      "Epoch: 17, Samples: 992/5760, Loss: 0.04793733358383179\n",
      "Epoch: 17, Samples: 1024/5760, Loss: 0.04624734818935394\n",
      "Epoch: 17, Samples: 1056/5760, Loss: 0.03394751250743866\n",
      "Epoch: 17, Samples: 1088/5760, Loss: 0.053985342383384705\n",
      "Epoch: 17, Samples: 1120/5760, Loss: 0.03862583637237549\n",
      "Epoch: 17, Samples: 1152/5760, Loss: 0.04111295938491821\n",
      "Epoch: 17, Samples: 1184/5760, Loss: 0.02308143675327301\n",
      "Epoch: 17, Samples: 1216/5760, Loss: 0.058311402797698975\n",
      "Epoch: 17, Samples: 1248/5760, Loss: 0.037927255034446716\n",
      "Epoch: 17, Samples: 1280/5760, Loss: 0.052108198404312134\n",
      "Epoch: 17, Samples: 1312/5760, Loss: 0.035560473799705505\n",
      "Epoch: 17, Samples: 1344/5760, Loss: 0.03900487720966339\n",
      "Epoch: 17, Samples: 1376/5760, Loss: 0.06213025748729706\n",
      "Epoch: 17, Samples: 1408/5760, Loss: 0.06330083310604095\n",
      "Epoch: 17, Samples: 1440/5760, Loss: 0.05146053433418274\n",
      "Epoch: 17, Samples: 1472/5760, Loss: 0.05194580554962158\n",
      "Epoch: 17, Samples: 1504/5760, Loss: 0.05051347613334656\n",
      "Epoch: 17, Samples: 1536/5760, Loss: 0.13871803879737854\n",
      "Epoch: 17, Samples: 1568/5760, Loss: 0.0421442836523056\n",
      "Epoch: 17, Samples: 1600/5760, Loss: 0.055309414863586426\n",
      "Epoch: 17, Samples: 1632/5760, Loss: 0.034604430198669434\n",
      "Epoch: 17, Samples: 1664/5760, Loss: 0.03265061974525452\n",
      "Epoch: 17, Samples: 1696/5760, Loss: 0.05598840117454529\n",
      "Epoch: 17, Samples: 1728/5760, Loss: 0.026882410049438477\n",
      "Epoch: 17, Samples: 1760/5760, Loss: 0.05030500888824463\n",
      "Epoch: 17, Samples: 1792/5760, Loss: 0.06738930940628052\n",
      "Epoch: 17, Samples: 1824/5760, Loss: 0.04760943353176117\n",
      "Epoch: 17, Samples: 1856/5760, Loss: 0.03580598533153534\n",
      "Epoch: 17, Samples: 1888/5760, Loss: 0.06294754147529602\n",
      "Epoch: 17, Samples: 1920/5760, Loss: 0.03858238458633423\n",
      "Epoch: 17, Samples: 1952/5760, Loss: 0.03765815496444702\n",
      "Epoch: 17, Samples: 1984/5760, Loss: 0.031723082065582275\n",
      "Epoch: 17, Samples: 2016/5760, Loss: 0.03251594305038452\n",
      "Epoch: 17, Samples: 2048/5760, Loss: 0.04736652970314026\n",
      "Epoch: 17, Samples: 2080/5760, Loss: 0.03813007473945618\n",
      "Epoch: 17, Samples: 2112/5760, Loss: 0.05368192493915558\n",
      "Epoch: 17, Samples: 2144/5760, Loss: 0.04563508927822113\n",
      "Epoch: 17, Samples: 2176/5760, Loss: 0.03508743643760681\n",
      "Epoch: 17, Samples: 2208/5760, Loss: 0.025076180696487427\n",
      "Epoch: 17, Samples: 2240/5760, Loss: 0.0934310257434845\n",
      "Epoch: 17, Samples: 2272/5760, Loss: 0.07722312211990356\n",
      "Epoch: 17, Samples: 2304/5760, Loss: 0.0807739794254303\n",
      "Epoch: 17, Samples: 2336/5760, Loss: 0.045278728008270264\n",
      "Epoch: 17, Samples: 2368/5760, Loss: 0.03361991047859192\n",
      "Epoch: 17, Samples: 2400/5760, Loss: 0.050111159682273865\n",
      "Epoch: 17, Samples: 2432/5760, Loss: 0.05748268961906433\n",
      "Epoch: 17, Samples: 2464/5760, Loss: 0.05908825993537903\n",
      "Epoch: 17, Samples: 2496/5760, Loss: 0.03690624237060547\n",
      "Epoch: 17, Samples: 2528/5760, Loss: 0.027965113520622253\n",
      "Epoch: 17, Samples: 2560/5760, Loss: 0.03417709469795227\n",
      "Epoch: 17, Samples: 2592/5760, Loss: 0.026325345039367676\n",
      "Epoch: 17, Samples: 2624/5760, Loss: 0.052940428256988525\n",
      "Epoch: 17, Samples: 2656/5760, Loss: 0.045931681990623474\n",
      "Epoch: 17, Samples: 2688/5760, Loss: 0.05090104043483734\n",
      "Epoch: 17, Samples: 2720/5760, Loss: 0.03697890043258667\n",
      "Epoch: 17, Samples: 2752/5760, Loss: 0.04945473372936249\n",
      "Epoch: 17, Samples: 2784/5760, Loss: 0.036553025245666504\n",
      "Epoch: 17, Samples: 2816/5760, Loss: 0.05084654688835144\n",
      "Epoch: 17, Samples: 2848/5760, Loss: 0.05768156051635742\n",
      "Epoch: 17, Samples: 2880/5760, Loss: 0.055044397711753845\n",
      "Epoch: 17, Samples: 2912/5760, Loss: 0.030790984630584717\n",
      "Epoch: 17, Samples: 2944/5760, Loss: 0.04885813593864441\n",
      "Epoch: 17, Samples: 2976/5760, Loss: 0.11044436693191528\n",
      "Epoch: 17, Samples: 3008/5760, Loss: 0.0477849543094635\n",
      "Epoch: 17, Samples: 3040/5760, Loss: 0.07173401117324829\n",
      "Epoch: 17, Samples: 3072/5760, Loss: 0.026276588439941406\n",
      "Epoch: 17, Samples: 3104/5760, Loss: 0.06486088037490845\n",
      "Epoch: 17, Samples: 3136/5760, Loss: 0.037934720516204834\n",
      "Epoch: 17, Samples: 3168/5760, Loss: 0.04177543520927429\n",
      "Epoch: 17, Samples: 3200/5760, Loss: 0.0455586314201355\n",
      "Epoch: 17, Samples: 3232/5760, Loss: 0.08136765658855438\n",
      "Epoch: 17, Samples: 3264/5760, Loss: 0.04774639010429382\n",
      "Epoch: 17, Samples: 3296/5760, Loss: 0.06708300113677979\n",
      "Epoch: 17, Samples: 3328/5760, Loss: 0.038783490657806396\n",
      "Epoch: 17, Samples: 3360/5760, Loss: 0.07234469056129456\n",
      "Epoch: 17, Samples: 3392/5760, Loss: 0.035676345229148865\n",
      "Epoch: 17, Samples: 3424/5760, Loss: 0.05629856884479523\n",
      "Epoch: 17, Samples: 3456/5760, Loss: 0.03609055280685425\n",
      "Epoch: 17, Samples: 3488/5760, Loss: 0.04155983030796051\n",
      "Epoch: 17, Samples: 3520/5760, Loss: 0.04699814319610596\n",
      "Epoch: 17, Samples: 3552/5760, Loss: 0.05837160348892212\n",
      "Epoch: 17, Samples: 3584/5760, Loss: 0.04863482713699341\n",
      "Epoch: 17, Samples: 3616/5760, Loss: 0.02264818549156189\n",
      "Epoch: 17, Samples: 3648/5760, Loss: 0.027904614806175232\n",
      "Epoch: 17, Samples: 3680/5760, Loss: 0.03999689221382141\n",
      "Epoch: 17, Samples: 3712/5760, Loss: 0.1074228435754776\n",
      "Epoch: 17, Samples: 3744/5760, Loss: 0.04232163727283478\n",
      "Epoch: 17, Samples: 3776/5760, Loss: 0.04543599486351013\n",
      "Epoch: 17, Samples: 3808/5760, Loss: 0.022120565176010132\n",
      "Epoch: 17, Samples: 3840/5760, Loss: 0.03449632227420807\n",
      "Epoch: 17, Samples: 3872/5760, Loss: 0.055624157190322876\n",
      "Epoch: 17, Samples: 3904/5760, Loss: 0.030028462409973145\n",
      "Epoch: 17, Samples: 3936/5760, Loss: 0.03472912311553955\n",
      "Epoch: 17, Samples: 3968/5760, Loss: 0.05067393183708191\n",
      "Epoch: 17, Samples: 4000/5760, Loss: 0.04914046823978424\n",
      "Epoch: 17, Samples: 4032/5760, Loss: 0.04695942997932434\n",
      "Epoch: 17, Samples: 4064/5760, Loss: 0.04622022807598114\n",
      "Epoch: 17, Samples: 4096/5760, Loss: 0.037991851568222046\n",
      "Epoch: 17, Samples: 4128/5760, Loss: 0.049086347222328186\n",
      "Epoch: 17, Samples: 4160/5760, Loss: 0.028488874435424805\n",
      "Epoch: 17, Samples: 4192/5760, Loss: 0.061128973960876465\n",
      "Epoch: 17, Samples: 4224/5760, Loss: 0.0388510525226593\n",
      "Epoch: 17, Samples: 4256/5760, Loss: 0.03619931638240814\n",
      "Epoch: 17, Samples: 4288/5760, Loss: 0.037514135241508484\n",
      "Epoch: 17, Samples: 4320/5760, Loss: 0.03421691060066223\n",
      "Epoch: 17, Samples: 4352/5760, Loss: 0.033719658851623535\n",
      "Epoch: 17, Samples: 4384/5760, Loss: 0.05025537312030792\n",
      "Epoch: 17, Samples: 4416/5760, Loss: 0.026907145977020264\n",
      "Epoch: 17, Samples: 4448/5760, Loss: 0.029717326164245605\n",
      "Epoch: 17, Samples: 4480/5760, Loss: 0.056347161531448364\n",
      "Epoch: 17, Samples: 4512/5760, Loss: 0.06462368369102478\n",
      "Epoch: 17, Samples: 4544/5760, Loss: 0.061837539076805115\n",
      "Epoch: 17, Samples: 4576/5760, Loss: 0.11143337190151215\n",
      "Epoch: 17, Samples: 4608/5760, Loss: 0.04513421654701233\n",
      "Epoch: 17, Samples: 4640/5760, Loss: 0.05335576832294464\n",
      "Epoch: 17, Samples: 4672/5760, Loss: 0.03129550814628601\n",
      "Epoch: 17, Samples: 4704/5760, Loss: 0.04008173942565918\n",
      "Epoch: 17, Samples: 4736/5760, Loss: 0.06441415846347809\n",
      "Epoch: 17, Samples: 4768/5760, Loss: 0.05256074666976929\n",
      "Epoch: 17, Samples: 4800/5760, Loss: 0.047558367252349854\n",
      "Epoch: 17, Samples: 4832/5760, Loss: 0.06413410604000092\n",
      "Epoch: 17, Samples: 4864/5760, Loss: 0.0649983137845993\n",
      "Epoch: 17, Samples: 4896/5760, Loss: 0.043382734060287476\n",
      "Epoch: 17, Samples: 4928/5760, Loss: 0.03650718927383423\n",
      "Epoch: 17, Samples: 4960/5760, Loss: 0.05965961515903473\n",
      "Epoch: 17, Samples: 4992/5760, Loss: 0.04769162833690643\n",
      "Epoch: 17, Samples: 5024/5760, Loss: 0.05316242575645447\n",
      "Epoch: 17, Samples: 5056/5760, Loss: 0.029472649097442627\n",
      "Epoch: 17, Samples: 5088/5760, Loss: 0.04393079876899719\n",
      "Epoch: 17, Samples: 5120/5760, Loss: 0.055684223771095276\n",
      "Epoch: 17, Samples: 5152/5760, Loss: 0.0389675498008728\n",
      "Epoch: 17, Samples: 5184/5760, Loss: 0.039510056376457214\n",
      "Epoch: 17, Samples: 5216/5760, Loss: 0.0652124434709549\n",
      "Epoch: 17, Samples: 5248/5760, Loss: 0.04134836792945862\n",
      "Epoch: 17, Samples: 5280/5760, Loss: 0.04854792356491089\n",
      "Epoch: 17, Samples: 5312/5760, Loss: 0.02864021062850952\n",
      "Epoch: 17, Samples: 5344/5760, Loss: 0.0518946647644043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Samples: 5376/5760, Loss: 0.028358444571495056\n",
      "Epoch: 17, Samples: 5408/5760, Loss: 0.04551161825656891\n",
      "Epoch: 17, Samples: 5440/5760, Loss: 0.03957560658454895\n",
      "Epoch: 17, Samples: 5472/5760, Loss: 0.045583054423332214\n",
      "Epoch: 17, Samples: 5504/5760, Loss: 0.05205217003822327\n",
      "Epoch: 17, Samples: 5536/5760, Loss: 0.05671510100364685\n",
      "Epoch: 17, Samples: 5568/5760, Loss: 0.037909865379333496\n",
      "Epoch: 17, Samples: 5600/5760, Loss: 0.05548939108848572\n",
      "Epoch: 17, Samples: 5632/5760, Loss: 0.060259416699409485\n",
      "Epoch: 17, Samples: 5664/5760, Loss: 0.03683832287788391\n",
      "Epoch: 17, Samples: 5696/5760, Loss: 0.0393255352973938\n",
      "Epoch: 17, Samples: 5728/5760, Loss: 0.6068018674850464\n",
      "\n",
      "Epoch: 17\n",
      "Training set: Average loss: 0.0519\n",
      "Validation set: Average loss: 0.3786, Accuracy: 742/818 (91%)\n",
      "Saving model (epoch 17) with lowest validation loss: 0.37861751593076265\n",
      "Epoch: 18, Samples: 0/5760, Loss: 0.048359259963035583\n",
      "Epoch: 18, Samples: 32/5760, Loss: 0.046552225947380066\n",
      "Epoch: 18, Samples: 64/5760, Loss: 0.05186404287815094\n",
      "Epoch: 18, Samples: 96/5760, Loss: 0.040772706270217896\n",
      "Epoch: 18, Samples: 128/5760, Loss: 0.04133635759353638\n",
      "Epoch: 18, Samples: 160/5760, Loss: 0.04719819128513336\n",
      "Epoch: 18, Samples: 192/5760, Loss: 0.026021212339401245\n",
      "Epoch: 18, Samples: 224/5760, Loss: 0.053834810853004456\n",
      "Epoch: 18, Samples: 256/5760, Loss: 0.031580954790115356\n",
      "Epoch: 18, Samples: 288/5760, Loss: 0.04263238608837128\n",
      "Epoch: 18, Samples: 320/5760, Loss: 0.055036112666130066\n",
      "Epoch: 18, Samples: 352/5760, Loss: 0.035986244678497314\n",
      "Epoch: 18, Samples: 384/5760, Loss: 0.05455535650253296\n",
      "Epoch: 18, Samples: 416/5760, Loss: 0.026347070932388306\n",
      "Epoch: 18, Samples: 448/5760, Loss: 0.030130356550216675\n",
      "Epoch: 18, Samples: 480/5760, Loss: 0.043987810611724854\n",
      "Epoch: 18, Samples: 512/5760, Loss: 0.04507288336753845\n",
      "Epoch: 18, Samples: 544/5760, Loss: 0.04302199184894562\n",
      "Epoch: 18, Samples: 576/5760, Loss: 0.043838873505592346\n",
      "Epoch: 18, Samples: 608/5760, Loss: 0.028763800859451294\n",
      "Epoch: 18, Samples: 640/5760, Loss: 0.038248851895332336\n",
      "Epoch: 18, Samples: 672/5760, Loss: 0.0325181782245636\n",
      "Epoch: 18, Samples: 704/5760, Loss: 0.06556184589862823\n",
      "Epoch: 18, Samples: 736/5760, Loss: 0.060232728719711304\n",
      "Epoch: 18, Samples: 768/5760, Loss: 0.03261220455169678\n",
      "Epoch: 18, Samples: 800/5760, Loss: 0.09657536447048187\n",
      "Epoch: 18, Samples: 832/5760, Loss: 0.05861873924732208\n",
      "Epoch: 18, Samples: 864/5760, Loss: 0.026795506477355957\n",
      "Epoch: 18, Samples: 896/5760, Loss: 0.04643276333808899\n",
      "Epoch: 18, Samples: 928/5760, Loss: 0.046146243810653687\n",
      "Epoch: 18, Samples: 960/5760, Loss: 0.039856672286987305\n",
      "Epoch: 18, Samples: 992/5760, Loss: 0.03369109332561493\n",
      "Epoch: 18, Samples: 1024/5760, Loss: 0.03227885067462921\n",
      "Epoch: 18, Samples: 1056/5760, Loss: 0.038496315479278564\n",
      "Epoch: 18, Samples: 1088/5760, Loss: 0.04325130581855774\n",
      "Epoch: 18, Samples: 1120/5760, Loss: 0.0361403226852417\n",
      "Epoch: 18, Samples: 1152/5760, Loss: 0.04638391733169556\n",
      "Epoch: 18, Samples: 1184/5760, Loss: 0.02946867048740387\n",
      "Epoch: 18, Samples: 1216/5760, Loss: 0.09814244508743286\n",
      "Epoch: 18, Samples: 1248/5760, Loss: 0.04705852270126343\n",
      "Epoch: 18, Samples: 1280/5760, Loss: 0.029077917337417603\n",
      "Epoch: 18, Samples: 1312/5760, Loss: 0.0531124621629715\n",
      "Epoch: 18, Samples: 1344/5760, Loss: 0.06055466830730438\n",
      "Epoch: 18, Samples: 1376/5760, Loss: 0.04611092805862427\n",
      "Epoch: 18, Samples: 1408/5760, Loss: 0.04536965489387512\n",
      "Epoch: 18, Samples: 1440/5760, Loss: 0.04195575416088104\n",
      "Epoch: 18, Samples: 1472/5760, Loss: 0.04618053138256073\n",
      "Epoch: 18, Samples: 1504/5760, Loss: 0.04032672941684723\n",
      "Epoch: 18, Samples: 1536/5760, Loss: 0.022698163986206055\n",
      "Epoch: 18, Samples: 1568/5760, Loss: 0.026669591665267944\n",
      "Epoch: 18, Samples: 1600/5760, Loss: 0.04686453938484192\n",
      "Epoch: 18, Samples: 1632/5760, Loss: 0.0349128395318985\n",
      "Epoch: 18, Samples: 1664/5760, Loss: 0.0360649973154068\n",
      "Epoch: 18, Samples: 1696/5760, Loss: 0.04489363729953766\n",
      "Epoch: 18, Samples: 1728/5760, Loss: 0.0461517870426178\n",
      "Epoch: 18, Samples: 1760/5760, Loss: 0.02587069571018219\n",
      "Epoch: 18, Samples: 1792/5760, Loss: 0.05829727649688721\n",
      "Epoch: 18, Samples: 1824/5760, Loss: 0.03337728977203369\n",
      "Epoch: 18, Samples: 1856/5760, Loss: 0.03961864113807678\n",
      "Epoch: 18, Samples: 1888/5760, Loss: 0.04240071773529053\n",
      "Epoch: 18, Samples: 1920/5760, Loss: 0.021739065647125244\n",
      "Epoch: 18, Samples: 1952/5760, Loss: 0.0416015088558197\n",
      "Epoch: 18, Samples: 1984/5760, Loss: 0.03205569088459015\n",
      "Epoch: 18, Samples: 2016/5760, Loss: 0.07726176083087921\n",
      "Epoch: 18, Samples: 2048/5760, Loss: 0.03028784692287445\n",
      "Epoch: 18, Samples: 2080/5760, Loss: 0.027440905570983887\n",
      "Epoch: 18, Samples: 2112/5760, Loss: 0.07986390590667725\n",
      "Epoch: 18, Samples: 2144/5760, Loss: 0.032914936542510986\n",
      "Epoch: 18, Samples: 2176/5760, Loss: 0.03069719672203064\n",
      "Epoch: 18, Samples: 2208/5760, Loss: 0.03294077515602112\n",
      "Epoch: 18, Samples: 2240/5760, Loss: 0.02944810688495636\n",
      "Epoch: 18, Samples: 2272/5760, Loss: 0.020660877227783203\n",
      "Epoch: 18, Samples: 2304/5760, Loss: 0.042684972286224365\n",
      "Epoch: 18, Samples: 2336/5760, Loss: 0.02453559637069702\n",
      "Epoch: 18, Samples: 2368/5760, Loss: 0.019841164350509644\n",
      "Epoch: 18, Samples: 2400/5760, Loss: 0.03375992178916931\n",
      "Epoch: 18, Samples: 2432/5760, Loss: 0.045910269021987915\n",
      "Epoch: 18, Samples: 2464/5760, Loss: 0.06652048230171204\n",
      "Epoch: 18, Samples: 2496/5760, Loss: 0.0529516339302063\n",
      "Epoch: 18, Samples: 2528/5760, Loss: 0.03205054998397827\n",
      "Epoch: 18, Samples: 2560/5760, Loss: 0.040809690952301025\n",
      "Epoch: 18, Samples: 2592/5760, Loss: 0.03676480054855347\n",
      "Epoch: 18, Samples: 2624/5760, Loss: 0.05454924702644348\n",
      "Epoch: 18, Samples: 2656/5760, Loss: 0.0422382652759552\n",
      "Epoch: 18, Samples: 2688/5760, Loss: 0.03131526708602905\n",
      "Epoch: 18, Samples: 2720/5760, Loss: 0.023463845252990723\n",
      "Epoch: 18, Samples: 2752/5760, Loss: 0.03848443925380707\n",
      "Epoch: 18, Samples: 2784/5760, Loss: 0.030573636293411255\n",
      "Epoch: 18, Samples: 2816/5760, Loss: 0.0731259286403656\n",
      "Epoch: 18, Samples: 2848/5760, Loss: 0.06596837937831879\n",
      "Epoch: 18, Samples: 2880/5760, Loss: 0.03155772387981415\n",
      "Epoch: 18, Samples: 2912/5760, Loss: 0.020984739065170288\n",
      "Epoch: 18, Samples: 2944/5760, Loss: 0.02822020649909973\n",
      "Epoch: 18, Samples: 2976/5760, Loss: 0.07785914838314056\n",
      "Epoch: 18, Samples: 3008/5760, Loss: 0.03046610951423645\n",
      "Epoch: 18, Samples: 3040/5760, Loss: 0.044640228152275085\n",
      "Epoch: 18, Samples: 3072/5760, Loss: 0.039612218737602234\n",
      "Epoch: 18, Samples: 3104/5760, Loss: 0.035746246576309204\n",
      "Epoch: 18, Samples: 3136/5760, Loss: 0.03844007849693298\n",
      "Epoch: 18, Samples: 3168/5760, Loss: 0.022203415632247925\n",
      "Epoch: 18, Samples: 3200/5760, Loss: 0.06057924032211304\n",
      "Epoch: 18, Samples: 3232/5760, Loss: 0.0333261638879776\n",
      "Epoch: 18, Samples: 3264/5760, Loss: 0.01898258924484253\n",
      "Epoch: 18, Samples: 3296/5760, Loss: 0.04656076431274414\n",
      "Epoch: 18, Samples: 3328/5760, Loss: 0.030815675854682922\n",
      "Epoch: 18, Samples: 3360/5760, Loss: 0.05495038628578186\n",
      "Epoch: 18, Samples: 3392/5760, Loss: 0.033065855503082275\n",
      "Epoch: 18, Samples: 3424/5760, Loss: 0.043491214513778687\n",
      "Epoch: 18, Samples: 3456/5760, Loss: 0.024098604917526245\n",
      "Epoch: 18, Samples: 3488/5760, Loss: 0.02198392152786255\n",
      "Epoch: 18, Samples: 3520/5760, Loss: 0.04565553367137909\n",
      "Epoch: 18, Samples: 3552/5760, Loss: 0.08320298790931702\n",
      "Epoch: 18, Samples: 3584/5760, Loss: 0.028469234704971313\n",
      "Epoch: 18, Samples: 3616/5760, Loss: 0.04609537124633789\n",
      "Epoch: 18, Samples: 3648/5760, Loss: 0.07112613320350647\n",
      "Epoch: 18, Samples: 3680/5760, Loss: 0.07207110524177551\n",
      "Epoch: 18, Samples: 3712/5760, Loss: 0.029296860098838806\n",
      "Epoch: 18, Samples: 3744/5760, Loss: 0.061611711978912354\n",
      "Epoch: 18, Samples: 3776/5760, Loss: 0.02633512020111084\n",
      "Epoch: 18, Samples: 3808/5760, Loss: 0.027084827423095703\n",
      "Epoch: 18, Samples: 3840/5760, Loss: 0.0362527072429657\n",
      "Epoch: 18, Samples: 3872/5760, Loss: 0.07136546075344086\n",
      "Epoch: 18, Samples: 3904/5760, Loss: 0.05753423273563385\n",
      "Epoch: 18, Samples: 3936/5760, Loss: 0.03884989023208618\n",
      "Epoch: 18, Samples: 3968/5760, Loss: 0.04157102108001709\n",
      "Epoch: 18, Samples: 4000/5760, Loss: 0.04717087745666504\n",
      "Epoch: 18, Samples: 4032/5760, Loss: 0.03262469172477722\n",
      "Epoch: 18, Samples: 4064/5760, Loss: 0.0245358943939209\n",
      "Epoch: 18, Samples: 4096/5760, Loss: 0.01772931218147278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Samples: 4128/5760, Loss: 0.04663887619972229\n",
      "Epoch: 18, Samples: 4160/5760, Loss: 0.05747923254966736\n",
      "Epoch: 18, Samples: 4192/5760, Loss: 0.04137301445007324\n",
      "Epoch: 18, Samples: 4224/5760, Loss: 0.0332798957824707\n",
      "Epoch: 18, Samples: 4256/5760, Loss: 0.04092651605606079\n",
      "Epoch: 18, Samples: 4288/5760, Loss: 0.02904278039932251\n",
      "Epoch: 18, Samples: 4320/5760, Loss: 0.04945322871208191\n",
      "Epoch: 18, Samples: 4352/5760, Loss: 0.037542641162872314\n",
      "Epoch: 18, Samples: 4384/5760, Loss: 0.039708495140075684\n",
      "Epoch: 18, Samples: 4416/5760, Loss: 0.0300864577293396\n",
      "Epoch: 18, Samples: 4448/5760, Loss: 0.044102609157562256\n",
      "Epoch: 18, Samples: 4480/5760, Loss: 0.02951619029045105\n",
      "Epoch: 18, Samples: 4512/5760, Loss: 0.042853742837905884\n",
      "Epoch: 18, Samples: 4544/5760, Loss: 0.03842034935951233\n",
      "Epoch: 18, Samples: 4576/5760, Loss: 0.030622124671936035\n",
      "Epoch: 18, Samples: 4608/5760, Loss: 0.03376707434654236\n",
      "Epoch: 18, Samples: 4640/5760, Loss: 0.02014613151550293\n",
      "Epoch: 18, Samples: 4672/5760, Loss: 0.03314179182052612\n",
      "Epoch: 18, Samples: 4704/5760, Loss: 0.03108188509941101\n",
      "Epoch: 18, Samples: 4736/5760, Loss: 0.027268245816230774\n",
      "Epoch: 18, Samples: 4768/5760, Loss: 0.0449090301990509\n",
      "Epoch: 18, Samples: 4800/5760, Loss: 0.05086131393909454\n",
      "Epoch: 18, Samples: 4832/5760, Loss: 0.026274681091308594\n",
      "Epoch: 18, Samples: 4864/5760, Loss: 0.043641477823257446\n",
      "Epoch: 18, Samples: 4896/5760, Loss: 0.025267481803894043\n",
      "Epoch: 18, Samples: 4928/5760, Loss: 0.05774475634098053\n",
      "Epoch: 18, Samples: 4960/5760, Loss: 0.030680030584335327\n",
      "Epoch: 18, Samples: 4992/5760, Loss: 0.04325875639915466\n",
      "Epoch: 18, Samples: 5024/5760, Loss: 0.046065554022789\n",
      "Epoch: 18, Samples: 5056/5760, Loss: 0.0365602970123291\n",
      "Epoch: 18, Samples: 5088/5760, Loss: 0.05403760075569153\n",
      "Epoch: 18, Samples: 5120/5760, Loss: 0.026548266410827637\n",
      "Epoch: 18, Samples: 5152/5760, Loss: 0.028067201375961304\n",
      "Epoch: 18, Samples: 5184/5760, Loss: 0.04069218039512634\n",
      "Epoch: 18, Samples: 5216/5760, Loss: 0.03770463168621063\n",
      "Epoch: 18, Samples: 5248/5760, Loss: 0.025197625160217285\n",
      "Epoch: 18, Samples: 5280/5760, Loss: 0.05793417990207672\n",
      "Epoch: 18, Samples: 5312/5760, Loss: 0.030991703271865845\n",
      "Epoch: 18, Samples: 5344/5760, Loss: 0.03568632900714874\n",
      "Epoch: 18, Samples: 5376/5760, Loss: 0.02843824028968811\n",
      "Epoch: 18, Samples: 5408/5760, Loss: 0.0461648553609848\n",
      "Epoch: 18, Samples: 5440/5760, Loss: 0.017339348793029785\n",
      "Epoch: 18, Samples: 5472/5760, Loss: 0.051574066281318665\n",
      "Epoch: 18, Samples: 5504/5760, Loss: 0.09185945987701416\n",
      "Epoch: 18, Samples: 5536/5760, Loss: 0.04828992486000061\n",
      "Epoch: 18, Samples: 5568/5760, Loss: 0.03330865502357483\n",
      "Epoch: 18, Samples: 5600/5760, Loss: 0.047624796628952026\n",
      "Epoch: 18, Samples: 5632/5760, Loss: 0.026923388242721558\n",
      "Epoch: 18, Samples: 5664/5760, Loss: 0.09957079589366913\n",
      "Epoch: 18, Samples: 5696/5760, Loss: 0.055566996335983276\n",
      "Epoch: 18, Samples: 5728/5760, Loss: 0.8004794120788574\n",
      "\n",
      "Epoch: 18\n",
      "Training set: Average loss: 0.0458\n",
      "Validation set: Average loss: 0.3737, Accuracy: 745/818 (91%)\n",
      "Saving model (epoch 18) with lowest validation loss: 0.3736540600657463\n",
      "Epoch: 19, Samples: 0/5760, Loss: 0.035759150981903076\n",
      "Epoch: 19, Samples: 32/5760, Loss: 0.021541118621826172\n",
      "Epoch: 19, Samples: 64/5760, Loss: 0.04723347723484039\n",
      "Epoch: 19, Samples: 96/5760, Loss: 0.02234169840812683\n",
      "Epoch: 19, Samples: 128/5760, Loss: 0.026819854974746704\n",
      "Epoch: 19, Samples: 160/5760, Loss: 0.032099172472953796\n",
      "Epoch: 19, Samples: 192/5760, Loss: 0.030965209007263184\n",
      "Epoch: 19, Samples: 224/5760, Loss: 0.02986033260822296\n",
      "Epoch: 19, Samples: 256/5760, Loss: 0.03474000096321106\n",
      "Epoch: 19, Samples: 288/5760, Loss: 0.048594698309898376\n",
      "Epoch: 19, Samples: 320/5760, Loss: 0.042642444372177124\n",
      "Epoch: 19, Samples: 352/5760, Loss: 0.030826717615127563\n",
      "Epoch: 19, Samples: 384/5760, Loss: 0.0408954918384552\n",
      "Epoch: 19, Samples: 416/5760, Loss: 0.03408297896385193\n",
      "Epoch: 19, Samples: 448/5760, Loss: 0.04599982500076294\n",
      "Epoch: 19, Samples: 480/5760, Loss: 0.041257768869400024\n",
      "Epoch: 19, Samples: 512/5760, Loss: 0.04563795030117035\n",
      "Epoch: 19, Samples: 544/5760, Loss: 0.026377946138381958\n",
      "Epoch: 19, Samples: 576/5760, Loss: 0.03404244780540466\n",
      "Epoch: 19, Samples: 608/5760, Loss: 0.04139977693557739\n",
      "Epoch: 19, Samples: 640/5760, Loss: 0.0617823600769043\n",
      "Epoch: 19, Samples: 672/5760, Loss: 0.026161938905715942\n",
      "Epoch: 19, Samples: 704/5760, Loss: 0.024205654859542847\n",
      "Epoch: 19, Samples: 736/5760, Loss: 0.047335028648376465\n",
      "Epoch: 19, Samples: 768/5760, Loss: 0.07716983556747437\n",
      "Epoch: 19, Samples: 800/5760, Loss: 0.029768139123916626\n",
      "Epoch: 19, Samples: 832/5760, Loss: 0.04882834851741791\n",
      "Epoch: 19, Samples: 864/5760, Loss: 0.07845215499401093\n",
      "Epoch: 19, Samples: 896/5760, Loss: 0.048980653285980225\n",
      "Epoch: 19, Samples: 928/5760, Loss: 0.039043307304382324\n",
      "Epoch: 19, Samples: 960/5760, Loss: 0.05498422682285309\n",
      "Epoch: 19, Samples: 992/5760, Loss: 0.06474977731704712\n",
      "Epoch: 19, Samples: 1024/5760, Loss: 0.033166706562042236\n",
      "Epoch: 19, Samples: 1056/5760, Loss: 0.020620912313461304\n",
      "Epoch: 19, Samples: 1088/5760, Loss: 0.015589475631713867\n",
      "Epoch: 19, Samples: 1120/5760, Loss: 0.022465437650680542\n",
      "Epoch: 19, Samples: 1152/5760, Loss: 0.027916282415390015\n",
      "Epoch: 19, Samples: 1184/5760, Loss: 0.019583940505981445\n",
      "Epoch: 19, Samples: 1216/5760, Loss: 0.04126483201980591\n",
      "Epoch: 19, Samples: 1248/5760, Loss: 0.03069281578063965\n",
      "Epoch: 19, Samples: 1280/5760, Loss: 0.020194441080093384\n",
      "Epoch: 19, Samples: 1312/5760, Loss: 0.03068159520626068\n",
      "Epoch: 19, Samples: 1344/5760, Loss: 0.03374616801738739\n",
      "Epoch: 19, Samples: 1376/5760, Loss: 0.03413453698158264\n",
      "Epoch: 19, Samples: 1408/5760, Loss: 0.025644272565841675\n",
      "Epoch: 19, Samples: 1440/5760, Loss: 0.031120702624320984\n",
      "Epoch: 19, Samples: 1472/5760, Loss: 0.028392761945724487\n",
      "Epoch: 19, Samples: 1504/5760, Loss: 0.037587448954582214\n",
      "Epoch: 19, Samples: 1536/5760, Loss: 0.029259487986564636\n",
      "Epoch: 19, Samples: 1568/5760, Loss: 0.04356452822685242\n",
      "Epoch: 19, Samples: 1600/5760, Loss: 0.030888468027114868\n",
      "Epoch: 19, Samples: 1632/5760, Loss: 0.04233713448047638\n",
      "Epoch: 19, Samples: 1664/5760, Loss: 0.03235480189323425\n",
      "Epoch: 19, Samples: 1696/5760, Loss: 0.03833495080471039\n",
      "Epoch: 19, Samples: 1728/5760, Loss: 0.03163394331932068\n",
      "Epoch: 19, Samples: 1760/5760, Loss: 0.045751333236694336\n",
      "Epoch: 19, Samples: 1792/5760, Loss: 0.03567773103713989\n",
      "Epoch: 19, Samples: 1824/5760, Loss: 0.034482717514038086\n",
      "Epoch: 19, Samples: 1856/5760, Loss: 0.04148605465888977\n",
      "Epoch: 19, Samples: 1888/5760, Loss: 0.029399678111076355\n",
      "Epoch: 19, Samples: 1920/5760, Loss: 0.048177555203437805\n",
      "Epoch: 19, Samples: 1952/5760, Loss: 0.05545388162136078\n",
      "Epoch: 19, Samples: 1984/5760, Loss: 0.03904774785041809\n",
      "Epoch: 19, Samples: 2016/5760, Loss: 0.0402357280254364\n",
      "Epoch: 19, Samples: 2048/5760, Loss: 0.03755101561546326\n",
      "Epoch: 19, Samples: 2080/5760, Loss: 0.037450700998306274\n",
      "Epoch: 19, Samples: 2112/5760, Loss: 0.05260293185710907\n",
      "Epoch: 19, Samples: 2144/5760, Loss: 0.05999062955379486\n",
      "Epoch: 19, Samples: 2176/5760, Loss: 0.03100988268852234\n",
      "Epoch: 19, Samples: 2208/5760, Loss: 0.03699292242527008\n",
      "Epoch: 19, Samples: 2240/5760, Loss: 0.0411834716796875\n",
      "Epoch: 19, Samples: 2272/5760, Loss: 0.06360819935798645\n",
      "Epoch: 19, Samples: 2304/5760, Loss: 0.046139299869537354\n",
      "Epoch: 19, Samples: 2336/5760, Loss: 0.030726850032806396\n",
      "Epoch: 19, Samples: 2368/5760, Loss: 0.029617398977279663\n",
      "Epoch: 19, Samples: 2400/5760, Loss: 0.020485997200012207\n",
      "Epoch: 19, Samples: 2432/5760, Loss: 0.03611353039741516\n",
      "Epoch: 19, Samples: 2464/5760, Loss: 0.0303499698638916\n",
      "Epoch: 19, Samples: 2496/5760, Loss: 0.01869991421699524\n",
      "Epoch: 19, Samples: 2528/5760, Loss: 0.0320894718170166\n",
      "Epoch: 19, Samples: 2560/5760, Loss: 0.030630528926849365\n",
      "Epoch: 19, Samples: 2592/5760, Loss: 0.04090245068073273\n",
      "Epoch: 19, Samples: 2624/5760, Loss: 0.025463759899139404\n",
      "Epoch: 19, Samples: 2656/5760, Loss: 0.03117460012435913\n",
      "Epoch: 19, Samples: 2688/5760, Loss: 0.018755346536636353\n",
      "Epoch: 19, Samples: 2720/5760, Loss: 0.03067636489868164\n",
      "Epoch: 19, Samples: 2752/5760, Loss: 0.055832624435424805\n",
      "Epoch: 19, Samples: 2784/5760, Loss: 0.021870553493499756\n",
      "Epoch: 19, Samples: 2816/5760, Loss: 0.03331023454666138\n",
      "Epoch: 19, Samples: 2848/5760, Loss: 0.050152942538261414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Samples: 2880/5760, Loss: 0.028379857540130615\n",
      "Epoch: 19, Samples: 2912/5760, Loss: 0.0207158625125885\n",
      "Epoch: 19, Samples: 2944/5760, Loss: 0.01900172233581543\n",
      "Epoch: 19, Samples: 2976/5760, Loss: 0.020801663398742676\n",
      "Epoch: 19, Samples: 3008/5760, Loss: 0.024902328848838806\n",
      "Epoch: 19, Samples: 3040/5760, Loss: 0.025035589933395386\n",
      "Epoch: 19, Samples: 3072/5760, Loss: 0.018229961395263672\n",
      "Epoch: 19, Samples: 3104/5760, Loss: 0.03226381540298462\n",
      "Epoch: 19, Samples: 3136/5760, Loss: 0.04820513725280762\n",
      "Epoch: 19, Samples: 3168/5760, Loss: 0.038138002157211304\n",
      "Epoch: 19, Samples: 3200/5760, Loss: 0.055559247732162476\n",
      "Epoch: 19, Samples: 3232/5760, Loss: 0.04104435443878174\n",
      "Epoch: 19, Samples: 3264/5760, Loss: 0.04426330327987671\n",
      "Epoch: 19, Samples: 3296/5760, Loss: 0.04007025063037872\n",
      "Epoch: 19, Samples: 3328/5760, Loss: 0.052050307393074036\n",
      "Epoch: 19, Samples: 3360/5760, Loss: 0.03072628378868103\n",
      "Epoch: 19, Samples: 3392/5760, Loss: 0.02752894163131714\n",
      "Epoch: 19, Samples: 3424/5760, Loss: 0.02189105749130249\n",
      "Epoch: 19, Samples: 3456/5760, Loss: 0.02925574779510498\n",
      "Epoch: 19, Samples: 3488/5760, Loss: 0.052387386560440063\n",
      "Epoch: 19, Samples: 3520/5760, Loss: 0.028250455856323242\n",
      "Epoch: 19, Samples: 3552/5760, Loss: 0.04392226040363312\n",
      "Epoch: 19, Samples: 3584/5760, Loss: 0.017158865928649902\n",
      "Epoch: 19, Samples: 3616/5760, Loss: 0.03400745987892151\n",
      "Epoch: 19, Samples: 3648/5760, Loss: 0.05229906737804413\n",
      "Epoch: 19, Samples: 3680/5760, Loss: 0.04008921980857849\n",
      "Epoch: 19, Samples: 3712/5760, Loss: 0.0377446711063385\n",
      "Epoch: 19, Samples: 3744/5760, Loss: 0.03641429543495178\n",
      "Epoch: 19, Samples: 3776/5760, Loss: 0.024880200624465942\n",
      "Epoch: 19, Samples: 3808/5760, Loss: 0.0335962176322937\n",
      "Epoch: 19, Samples: 3840/5760, Loss: 0.05144387483596802\n",
      "Epoch: 19, Samples: 3872/5760, Loss: 0.022824108600616455\n",
      "Epoch: 19, Samples: 3904/5760, Loss: 0.03439891338348389\n",
      "Epoch: 19, Samples: 3936/5760, Loss: 0.030961841344833374\n",
      "Epoch: 19, Samples: 3968/5760, Loss: 0.0601387619972229\n",
      "Epoch: 19, Samples: 4000/5760, Loss: 0.022859320044517517\n",
      "Epoch: 19, Samples: 4032/5760, Loss: 0.05480003356933594\n",
      "Epoch: 19, Samples: 4064/5760, Loss: 0.02516883611679077\n",
      "Epoch: 19, Samples: 4096/5760, Loss: 0.037777066230773926\n",
      "Epoch: 19, Samples: 4128/5760, Loss: 0.05378596484661102\n",
      "Epoch: 19, Samples: 4160/5760, Loss: 0.02090105414390564\n",
      "Epoch: 19, Samples: 4192/5760, Loss: 0.03881281614303589\n",
      "Epoch: 19, Samples: 4224/5760, Loss: 0.06444123387336731\n",
      "Epoch: 19, Samples: 4256/5760, Loss: 0.01800784468650818\n",
      "Epoch: 19, Samples: 4288/5760, Loss: 0.04717950522899628\n",
      "Epoch: 19, Samples: 4320/5760, Loss: 0.04086217284202576\n",
      "Epoch: 19, Samples: 4352/5760, Loss: 0.028137996792793274\n",
      "Epoch: 19, Samples: 4384/5760, Loss: 0.024530142545700073\n",
      "Epoch: 19, Samples: 4416/5760, Loss: 0.026285171508789062\n",
      "Epoch: 19, Samples: 4448/5760, Loss: 0.03255072236061096\n",
      "Epoch: 19, Samples: 4480/5760, Loss: 0.06381580233573914\n",
      "Epoch: 19, Samples: 4512/5760, Loss: 0.04797975718975067\n",
      "Epoch: 19, Samples: 4544/5760, Loss: 0.03981490433216095\n",
      "Epoch: 19, Samples: 4576/5760, Loss: 0.017352551221847534\n",
      "Epoch: 19, Samples: 4608/5760, Loss: 0.038537174463272095\n",
      "Epoch: 19, Samples: 4640/5760, Loss: 0.04718692600727081\n",
      "Epoch: 19, Samples: 4672/5760, Loss: 0.034646958112716675\n",
      "Epoch: 19, Samples: 4704/5760, Loss: 0.06599153578281403\n",
      "Epoch: 19, Samples: 4736/5760, Loss: 0.054815635085105896\n",
      "Epoch: 19, Samples: 4768/5760, Loss: 0.031079068779945374\n",
      "Epoch: 19, Samples: 4800/5760, Loss: 0.03417395055294037\n",
      "Epoch: 19, Samples: 4832/5760, Loss: 0.048441410064697266\n",
      "Epoch: 19, Samples: 4864/5760, Loss: 0.026681572198867798\n",
      "Epoch: 19, Samples: 4896/5760, Loss: 0.03461951017379761\n",
      "Epoch: 19, Samples: 4928/5760, Loss: 0.02955368161201477\n",
      "Epoch: 19, Samples: 4960/5760, Loss: 0.07742948830127716\n",
      "Epoch: 19, Samples: 4992/5760, Loss: 0.04934774339199066\n",
      "Epoch: 19, Samples: 5024/5760, Loss: 0.025406599044799805\n",
      "Epoch: 19, Samples: 5056/5760, Loss: 0.04193492233753204\n",
      "Epoch: 19, Samples: 5088/5760, Loss: 0.03192886710166931\n",
      "Epoch: 19, Samples: 5120/5760, Loss: 0.03614245355129242\n",
      "Epoch: 19, Samples: 5152/5760, Loss: 0.04100733995437622\n",
      "Epoch: 19, Samples: 5184/5760, Loss: 0.03939200937747955\n",
      "Epoch: 19, Samples: 5216/5760, Loss: 0.024620860815048218\n",
      "Epoch: 19, Samples: 5248/5760, Loss: 0.03937220573425293\n",
      "Epoch: 19, Samples: 5280/5760, Loss: 0.03157983720302582\n",
      "Epoch: 19, Samples: 5312/5760, Loss: 0.05745744705200195\n",
      "Epoch: 19, Samples: 5344/5760, Loss: 0.051754504442214966\n",
      "Epoch: 19, Samples: 5376/5760, Loss: 0.02106596529483795\n",
      "Epoch: 19, Samples: 5408/5760, Loss: 0.03193122148513794\n",
      "Epoch: 19, Samples: 5440/5760, Loss: 0.036142125725746155\n",
      "Epoch: 19, Samples: 5472/5760, Loss: 0.04305045306682587\n",
      "Epoch: 19, Samples: 5504/5760, Loss: 0.04269865155220032\n",
      "Epoch: 19, Samples: 5536/5760, Loss: 0.04353258013725281\n",
      "Epoch: 19, Samples: 5568/5760, Loss: 0.02340736985206604\n",
      "Epoch: 19, Samples: 5600/5760, Loss: 0.039663419127464294\n",
      "Epoch: 19, Samples: 5632/5760, Loss: 0.021167367696762085\n",
      "Epoch: 19, Samples: 5664/5760, Loss: 0.038247376680374146\n",
      "Epoch: 19, Samples: 5696/5760, Loss: 0.028274834156036377\n",
      "Epoch: 19, Samples: 5728/5760, Loss: 1.007803201675415\n",
      "\n",
      "Epoch: 19\n",
      "Training set: Average loss: 0.0423\n",
      "Validation set: Average loss: 0.3645, Accuracy: 748/818 (91%)\n",
      "Saving model (epoch 19) with lowest validation loss: 0.36452645206680667\n",
      "Epoch: 20, Samples: 0/5760, Loss: 0.041072458028793335\n",
      "Epoch: 20, Samples: 32/5760, Loss: 0.03735317289829254\n",
      "Epoch: 20, Samples: 64/5760, Loss: 0.03420856595039368\n",
      "Epoch: 20, Samples: 96/5760, Loss: 0.030071020126342773\n",
      "Epoch: 20, Samples: 128/5760, Loss: 0.03281944990158081\n",
      "Epoch: 20, Samples: 160/5760, Loss: 0.03168606758117676\n",
      "Epoch: 20, Samples: 192/5760, Loss: 0.047757238149642944\n",
      "Epoch: 20, Samples: 224/5760, Loss: 0.032129496335983276\n",
      "Epoch: 20, Samples: 256/5760, Loss: 0.03354310989379883\n",
      "Epoch: 20, Samples: 288/5760, Loss: 0.05175478756427765\n",
      "Epoch: 20, Samples: 320/5760, Loss: 0.04946702718734741\n",
      "Epoch: 20, Samples: 352/5760, Loss: 0.0279446542263031\n",
      "Epoch: 20, Samples: 384/5760, Loss: 0.045670658349990845\n",
      "Epoch: 20, Samples: 416/5760, Loss: 0.025309979915618896\n",
      "Epoch: 20, Samples: 448/5760, Loss: 0.03758072853088379\n",
      "Epoch: 20, Samples: 480/5760, Loss: 0.03988778591156006\n",
      "Epoch: 20, Samples: 512/5760, Loss: 0.03310742974281311\n",
      "Epoch: 20, Samples: 544/5760, Loss: 0.019689947366714478\n",
      "Epoch: 20, Samples: 576/5760, Loss: 0.029466748237609863\n",
      "Epoch: 20, Samples: 608/5760, Loss: 0.03657269477844238\n",
      "Epoch: 20, Samples: 640/5760, Loss: 0.06838589906692505\n",
      "Epoch: 20, Samples: 672/5760, Loss: 0.02048531174659729\n",
      "Epoch: 20, Samples: 704/5760, Loss: 0.029673218727111816\n",
      "Epoch: 20, Samples: 736/5760, Loss: 0.04444539546966553\n",
      "Epoch: 20, Samples: 768/5760, Loss: 0.025437980890274048\n",
      "Epoch: 20, Samples: 800/5760, Loss: 0.032512396574020386\n",
      "Epoch: 20, Samples: 832/5760, Loss: 0.03220447897911072\n",
      "Epoch: 20, Samples: 864/5760, Loss: 0.036373674869537354\n",
      "Epoch: 20, Samples: 896/5760, Loss: 0.05138656497001648\n",
      "Epoch: 20, Samples: 928/5760, Loss: 0.02001512050628662\n",
      "Epoch: 20, Samples: 960/5760, Loss: 0.05276687443256378\n",
      "Epoch: 20, Samples: 992/5760, Loss: 0.04301883280277252\n",
      "Epoch: 20, Samples: 1024/5760, Loss: 0.03677523136138916\n",
      "Epoch: 20, Samples: 1056/5760, Loss: 0.031123757362365723\n",
      "Epoch: 20, Samples: 1088/5760, Loss: 0.018259316682815552\n",
      "Epoch: 20, Samples: 1120/5760, Loss: 0.07893747091293335\n",
      "Epoch: 20, Samples: 1152/5760, Loss: 0.029070481657981873\n",
      "Epoch: 20, Samples: 1184/5760, Loss: 0.06195908784866333\n",
      "Epoch: 20, Samples: 1216/5760, Loss: 0.04140561819076538\n",
      "Epoch: 20, Samples: 1248/5760, Loss: 0.040683403611183167\n",
      "Epoch: 20, Samples: 1280/5760, Loss: 0.0542522668838501\n",
      "Epoch: 20, Samples: 1312/5760, Loss: 0.030082792043685913\n",
      "Epoch: 20, Samples: 1344/5760, Loss: 0.03450369834899902\n",
      "Epoch: 20, Samples: 1376/5760, Loss: 0.029438167810440063\n",
      "Epoch: 20, Samples: 1408/5760, Loss: 0.028545960783958435\n",
      "Epoch: 20, Samples: 1440/5760, Loss: 0.020711183547973633\n",
      "Epoch: 20, Samples: 1472/5760, Loss: 0.04554043710231781\n",
      "Epoch: 20, Samples: 1504/5760, Loss: 0.023799806833267212\n",
      "Epoch: 20, Samples: 1536/5760, Loss: 0.034493327140808105\n",
      "Epoch: 20, Samples: 1568/5760, Loss: 0.02053546905517578\n",
      "Epoch: 20, Samples: 1600/5760, Loss: 0.022503405809402466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Samples: 1632/5760, Loss: 0.029690295457839966\n",
      "Epoch: 20, Samples: 1664/5760, Loss: 0.029485493898391724\n",
      "Epoch: 20, Samples: 1696/5760, Loss: 0.024581879377365112\n",
      "Epoch: 20, Samples: 1728/5760, Loss: 0.046485766768455505\n",
      "Epoch: 20, Samples: 1760/5760, Loss: 0.04492811858654022\n",
      "Epoch: 20, Samples: 1792/5760, Loss: 0.03403368592262268\n",
      "Epoch: 20, Samples: 1824/5760, Loss: 0.04210343956947327\n",
      "Epoch: 20, Samples: 1856/5760, Loss: 0.06127651035785675\n",
      "Epoch: 20, Samples: 1888/5760, Loss: 0.025457188487052917\n",
      "Epoch: 20, Samples: 1920/5760, Loss: 0.03518654406070709\n",
      "Epoch: 20, Samples: 1952/5760, Loss: 0.018165618181228638\n",
      "Epoch: 20, Samples: 1984/5760, Loss: 0.03250545263290405\n",
      "Epoch: 20, Samples: 2016/5760, Loss: 0.042834147810935974\n",
      "Epoch: 20, Samples: 2048/5760, Loss: 0.030228853225708008\n",
      "Epoch: 20, Samples: 2080/5760, Loss: 0.03885647654533386\n",
      "Epoch: 20, Samples: 2112/5760, Loss: 0.03810058534145355\n",
      "Epoch: 20, Samples: 2144/5760, Loss: 0.03143572807312012\n",
      "Epoch: 20, Samples: 2176/5760, Loss: 0.028469502925872803\n",
      "Epoch: 20, Samples: 2208/5760, Loss: 0.03893716633319855\n",
      "Epoch: 20, Samples: 2240/5760, Loss: 0.027462035417556763\n",
      "Epoch: 20, Samples: 2272/5760, Loss: 0.044298335909843445\n",
      "Epoch: 20, Samples: 2304/5760, Loss: 0.04532964527606964\n",
      "Epoch: 20, Samples: 2336/5760, Loss: 0.05669662356376648\n",
      "Epoch: 20, Samples: 2368/5760, Loss: 0.026880711317062378\n",
      "Epoch: 20, Samples: 2400/5760, Loss: 0.03197455406188965\n",
      "Epoch: 20, Samples: 2432/5760, Loss: 0.03393736481666565\n",
      "Epoch: 20, Samples: 2464/5760, Loss: 0.06667768955230713\n",
      "Epoch: 20, Samples: 2496/5760, Loss: 0.03529614210128784\n",
      "Epoch: 20, Samples: 2528/5760, Loss: 0.03604355454444885\n",
      "Epoch: 20, Samples: 2560/5760, Loss: 0.0232430100440979\n",
      "Epoch: 20, Samples: 2592/5760, Loss: 0.01833444833755493\n",
      "Epoch: 20, Samples: 2624/5760, Loss: 0.027010351419448853\n",
      "Epoch: 20, Samples: 2656/5760, Loss: 0.017321527004241943\n",
      "Epoch: 20, Samples: 2688/5760, Loss: 0.028926074504852295\n",
      "Epoch: 20, Samples: 2720/5760, Loss: 0.024317771196365356\n",
      "Epoch: 20, Samples: 2752/5760, Loss: 0.06136082112789154\n",
      "Epoch: 20, Samples: 2784/5760, Loss: 0.03749701380729675\n",
      "Epoch: 20, Samples: 2816/5760, Loss: 0.022046804428100586\n",
      "Epoch: 20, Samples: 2848/5760, Loss: 0.0295637845993042\n",
      "Epoch: 20, Samples: 2880/5760, Loss: 0.017535895109176636\n",
      "Epoch: 20, Samples: 2912/5760, Loss: 0.023847877979278564\n",
      "Epoch: 20, Samples: 2944/5760, Loss: 0.038879022002220154\n",
      "Epoch: 20, Samples: 2976/5760, Loss: 0.02651667594909668\n",
      "Epoch: 20, Samples: 3008/5760, Loss: 0.02064266800880432\n",
      "Epoch: 20, Samples: 3040/5760, Loss: 0.03169649839401245\n",
      "Epoch: 20, Samples: 3072/5760, Loss: 0.03043496608734131\n",
      "Epoch: 20, Samples: 3104/5760, Loss: 0.026212722063064575\n",
      "Epoch: 20, Samples: 3136/5760, Loss: 0.032541424036026\n",
      "Epoch: 20, Samples: 3168/5760, Loss: 0.028560757637023926\n",
      "Epoch: 20, Samples: 3200/5760, Loss: 0.02671867609024048\n",
      "Epoch: 20, Samples: 3232/5760, Loss: 0.049344658851623535\n",
      "Epoch: 20, Samples: 3264/5760, Loss: 0.03606957197189331\n",
      "Epoch: 20, Samples: 3296/5760, Loss: 0.028194904327392578\n",
      "Epoch: 20, Samples: 3328/5760, Loss: 0.044364944100379944\n",
      "Epoch: 20, Samples: 3360/5760, Loss: 0.021492332220077515\n",
      "Epoch: 20, Samples: 3392/5760, Loss: 0.04421399533748627\n",
      "Epoch: 20, Samples: 3424/5760, Loss: 0.0278586745262146\n",
      "Epoch: 20, Samples: 3456/5760, Loss: 0.03273075819015503\n",
      "Epoch: 20, Samples: 3488/5760, Loss: 0.01728537678718567\n",
      "Epoch: 20, Samples: 3520/5760, Loss: 0.02181193232536316\n",
      "Epoch: 20, Samples: 3552/5760, Loss: 0.0212441086769104\n",
      "Epoch: 20, Samples: 3584/5760, Loss: 0.0215110182762146\n",
      "Epoch: 20, Samples: 3616/5760, Loss: 0.02860325574874878\n",
      "Epoch: 20, Samples: 3648/5760, Loss: 0.042124807834625244\n",
      "Epoch: 20, Samples: 3680/5760, Loss: 0.029324233531951904\n",
      "Epoch: 20, Samples: 3712/5760, Loss: 0.04395619034767151\n",
      "Epoch: 20, Samples: 3744/5760, Loss: 0.027734339237213135\n",
      "Epoch: 20, Samples: 3776/5760, Loss: 0.037619829177856445\n",
      "Epoch: 20, Samples: 3808/5760, Loss: 0.020764708518981934\n",
      "Epoch: 20, Samples: 3840/5760, Loss: 0.04425407946109772\n",
      "Epoch: 20, Samples: 3872/5760, Loss: 0.02598428726196289\n",
      "Epoch: 20, Samples: 3904/5760, Loss: 0.02304081618785858\n",
      "Epoch: 20, Samples: 3936/5760, Loss: 0.028205037117004395\n",
      "Epoch: 20, Samples: 3968/5760, Loss: 0.028205424547195435\n",
      "Epoch: 20, Samples: 4000/5760, Loss: 0.03492435812950134\n",
      "Epoch: 20, Samples: 4032/5760, Loss: 0.020533084869384766\n",
      "Epoch: 20, Samples: 4064/5760, Loss: 0.030442506074905396\n",
      "Epoch: 20, Samples: 4096/5760, Loss: 0.028470218181610107\n",
      "Epoch: 20, Samples: 4128/5760, Loss: 0.04202046990394592\n",
      "Epoch: 20, Samples: 4160/5760, Loss: 0.03143009543418884\n",
      "Epoch: 20, Samples: 4192/5760, Loss: 0.01928865909576416\n",
      "Epoch: 20, Samples: 4224/5760, Loss: 0.03266781568527222\n",
      "Epoch: 20, Samples: 4256/5760, Loss: 0.044877052307128906\n",
      "Epoch: 20, Samples: 4288/5760, Loss: 0.02564266324043274\n",
      "Epoch: 20, Samples: 4320/5760, Loss: 0.042627960443496704\n",
      "Epoch: 20, Samples: 4352/5760, Loss: 0.025478899478912354\n",
      "Epoch: 20, Samples: 4384/5760, Loss: 0.01874944567680359\n",
      "Epoch: 20, Samples: 4416/5760, Loss: 0.02227790653705597\n",
      "Epoch: 20, Samples: 4448/5760, Loss: 0.030715852975845337\n",
      "Epoch: 20, Samples: 4480/5760, Loss: 0.019028276205062866\n",
      "Epoch: 20, Samples: 4512/5760, Loss: 0.031229570508003235\n",
      "Epoch: 20, Samples: 4544/5760, Loss: 0.04410070180892944\n",
      "Epoch: 20, Samples: 4576/5760, Loss: 0.023106426000595093\n",
      "Epoch: 20, Samples: 4608/5760, Loss: 0.019539743661880493\n",
      "Epoch: 20, Samples: 4640/5760, Loss: 0.04818360507488251\n",
      "Epoch: 20, Samples: 4672/5760, Loss: 0.029973477125167847\n",
      "Epoch: 20, Samples: 4704/5760, Loss: 0.01668459177017212\n",
      "Epoch: 20, Samples: 4736/5760, Loss: 0.05256284773349762\n",
      "Epoch: 20, Samples: 4768/5760, Loss: 0.03141367435455322\n",
      "Epoch: 20, Samples: 4800/5760, Loss: 0.049710556864738464\n",
      "Epoch: 20, Samples: 4832/5760, Loss: 0.0292767733335495\n",
      "Epoch: 20, Samples: 4864/5760, Loss: 0.025321006774902344\n",
      "Epoch: 20, Samples: 4896/5760, Loss: 0.05858111381530762\n",
      "Epoch: 20, Samples: 4928/5760, Loss: 0.047387003898620605\n",
      "Epoch: 20, Samples: 4960/5760, Loss: 0.024858295917510986\n",
      "Epoch: 20, Samples: 4992/5760, Loss: 0.022446930408477783\n",
      "Epoch: 20, Samples: 5024/5760, Loss: 0.0595427006483078\n",
      "Epoch: 20, Samples: 5056/5760, Loss: 0.02835497260093689\n",
      "Epoch: 20, Samples: 5088/5760, Loss: 0.026305824518203735\n",
      "Epoch: 20, Samples: 5120/5760, Loss: 0.020616888999938965\n",
      "Epoch: 20, Samples: 5152/5760, Loss: 0.025722920894622803\n",
      "Epoch: 20, Samples: 5184/5760, Loss: 0.029896706342697144\n",
      "Epoch: 20, Samples: 5216/5760, Loss: 0.026389360427856445\n",
      "Epoch: 20, Samples: 5248/5760, Loss: 0.05790013074874878\n",
      "Epoch: 20, Samples: 5280/5760, Loss: 0.03396916389465332\n",
      "Epoch: 20, Samples: 5312/5760, Loss: 0.023039937019348145\n",
      "Epoch: 20, Samples: 5344/5760, Loss: 0.023663192987442017\n",
      "Epoch: 20, Samples: 5376/5760, Loss: 0.026455402374267578\n",
      "Epoch: 20, Samples: 5408/5760, Loss: 0.03478686511516571\n",
      "Epoch: 20, Samples: 5440/5760, Loss: 0.027585089206695557\n",
      "Epoch: 20, Samples: 5472/5760, Loss: 0.032067298889160156\n",
      "Epoch: 20, Samples: 5504/5760, Loss: 0.04893516004085541\n",
      "Epoch: 20, Samples: 5536/5760, Loss: 0.03197470307350159\n",
      "Epoch: 20, Samples: 5568/5760, Loss: 0.04358474910259247\n",
      "Epoch: 20, Samples: 5600/5760, Loss: 0.025480926036834717\n",
      "Epoch: 20, Samples: 5632/5760, Loss: 0.027696430683135986\n",
      "Epoch: 20, Samples: 5664/5760, Loss: 0.026537448167800903\n",
      "Epoch: 20, Samples: 5696/5760, Loss: 0.0653756707906723\n",
      "Epoch: 20, Samples: 5728/5760, Loss: 0.4385526180267334\n",
      "\n",
      "Epoch: 20\n",
      "Training set: Average loss: 0.0360\n",
      "Validation set: Average loss: 0.3684, Accuracy: 747/818 (91%)\n",
      "Epoch: 21, Samples: 0/5760, Loss: 0.024528294801712036\n",
      "Epoch: 21, Samples: 32/5760, Loss: 0.03331461548805237\n",
      "Epoch: 21, Samples: 64/5760, Loss: 0.02981436252593994\n",
      "Epoch: 21, Samples: 96/5760, Loss: 0.04077380895614624\n",
      "Epoch: 21, Samples: 128/5760, Loss: 0.0286257266998291\n",
      "Epoch: 21, Samples: 160/5760, Loss: 0.03975662589073181\n",
      "Epoch: 21, Samples: 192/5760, Loss: 0.019781798124313354\n",
      "Epoch: 21, Samples: 224/5760, Loss: 0.023060888051986694\n",
      "Epoch: 21, Samples: 256/5760, Loss: 0.02750334143638611\n",
      "Epoch: 21, Samples: 288/5760, Loss: 0.019269078969955444\n",
      "Epoch: 21, Samples: 320/5760, Loss: 0.049629926681518555\n",
      "Epoch: 21, Samples: 352/5760, Loss: 0.030644625425338745\n",
      "Epoch: 21, Samples: 384/5760, Loss: 0.04470789432525635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Samples: 416/5760, Loss: 0.03250366449356079\n",
      "Epoch: 21, Samples: 448/5760, Loss: 0.047360360622406006\n",
      "Epoch: 21, Samples: 480/5760, Loss: 0.013302832841873169\n",
      "Epoch: 21, Samples: 512/5760, Loss: 0.027065575122833252\n",
      "Epoch: 21, Samples: 544/5760, Loss: 0.03025609254837036\n",
      "Epoch: 21, Samples: 576/5760, Loss: 0.03147625923156738\n",
      "Epoch: 21, Samples: 608/5760, Loss: 0.036446258425712585\n",
      "Epoch: 21, Samples: 640/5760, Loss: 0.03101325035095215\n",
      "Epoch: 21, Samples: 672/5760, Loss: 0.013442128896713257\n",
      "Epoch: 21, Samples: 704/5760, Loss: 0.02891293168067932\n",
      "Epoch: 21, Samples: 736/5760, Loss: 0.10867303609848022\n",
      "Epoch: 21, Samples: 768/5760, Loss: 0.019471168518066406\n",
      "Epoch: 21, Samples: 800/5760, Loss: 0.024113774299621582\n",
      "Epoch: 21, Samples: 832/5760, Loss: 0.028972148895263672\n",
      "Epoch: 21, Samples: 864/5760, Loss: 0.0384131520986557\n",
      "Epoch: 21, Samples: 896/5760, Loss: 0.016635149717330933\n",
      "Epoch: 21, Samples: 928/5760, Loss: 0.021768391132354736\n",
      "Epoch: 21, Samples: 960/5760, Loss: 0.039869219064712524\n",
      "Epoch: 21, Samples: 992/5760, Loss: 0.020099103450775146\n",
      "Epoch: 21, Samples: 1024/5760, Loss: 0.019097119569778442\n",
      "Epoch: 21, Samples: 1056/5760, Loss: 0.032299429178237915\n",
      "Epoch: 21, Samples: 1088/5760, Loss: 0.02048131823539734\n",
      "Epoch: 21, Samples: 1120/5760, Loss: 0.01657170057296753\n",
      "Epoch: 21, Samples: 1152/5760, Loss: 0.049132898449897766\n",
      "Epoch: 21, Samples: 1184/5760, Loss: 0.02689078450202942\n",
      "Epoch: 21, Samples: 1216/5760, Loss: 0.03928151726722717\n",
      "Epoch: 21, Samples: 1248/5760, Loss: 0.01594218611717224\n",
      "Epoch: 21, Samples: 1280/5760, Loss: 0.02412077784538269\n",
      "Epoch: 21, Samples: 1312/5760, Loss: 0.032775938510894775\n",
      "Epoch: 21, Samples: 1344/5760, Loss: 0.07192488014698029\n",
      "Epoch: 21, Samples: 1376/5760, Loss: 0.026172727346420288\n",
      "Epoch: 21, Samples: 1408/5760, Loss: 0.03405621647834778\n",
      "Epoch: 21, Samples: 1440/5760, Loss: 0.020985543727874756\n",
      "Epoch: 21, Samples: 1472/5760, Loss: 0.01789143681526184\n",
      "Epoch: 21, Samples: 1504/5760, Loss: 0.032328903675079346\n",
      "Epoch: 21, Samples: 1536/5760, Loss: 0.03295294940471649\n",
      "Epoch: 21, Samples: 1568/5760, Loss: 0.015272766351699829\n",
      "Epoch: 21, Samples: 1600/5760, Loss: 0.03253568708896637\n",
      "Epoch: 21, Samples: 1632/5760, Loss: 0.017679482698440552\n",
      "Epoch: 21, Samples: 1664/5760, Loss: 0.038114339113235474\n",
      "Epoch: 21, Samples: 1696/5760, Loss: 0.03219226002693176\n",
      "Epoch: 21, Samples: 1728/5760, Loss: 0.033940643072128296\n",
      "Epoch: 21, Samples: 1760/5760, Loss: 0.02608257532119751\n",
      "Epoch: 21, Samples: 1792/5760, Loss: 0.041963204741477966\n",
      "Epoch: 21, Samples: 1824/5760, Loss: 0.05003991723060608\n",
      "Epoch: 21, Samples: 1856/5760, Loss: 0.024751335382461548\n",
      "Epoch: 21, Samples: 1888/5760, Loss: 0.07684437930583954\n",
      "Epoch: 21, Samples: 1920/5760, Loss: 0.03249168395996094\n",
      "Epoch: 21, Samples: 1952/5760, Loss: 0.017460405826568604\n",
      "Epoch: 21, Samples: 1984/5760, Loss: 0.024100661277770996\n",
      "Epoch: 21, Samples: 2016/5760, Loss: 0.029538482427597046\n",
      "Epoch: 21, Samples: 2048/5760, Loss: 0.05937011539936066\n",
      "Epoch: 21, Samples: 2080/5760, Loss: 0.027032092213630676\n",
      "Epoch: 21, Samples: 2112/5760, Loss: 0.059650227427482605\n",
      "Epoch: 21, Samples: 2144/5760, Loss: 0.021430939435958862\n",
      "Epoch: 21, Samples: 2176/5760, Loss: 0.024729788303375244\n",
      "Epoch: 21, Samples: 2208/5760, Loss: 0.047626301646232605\n",
      "Epoch: 21, Samples: 2240/5760, Loss: 0.023020416498184204\n",
      "Epoch: 21, Samples: 2272/5760, Loss: 0.01826608180999756\n",
      "Epoch: 21, Samples: 2304/5760, Loss: 0.03889620304107666\n",
      "Epoch: 21, Samples: 2336/5760, Loss: 0.02956274151802063\n",
      "Epoch: 21, Samples: 2368/5760, Loss: 0.020607948303222656\n",
      "Epoch: 21, Samples: 2400/5760, Loss: 0.05229984223842621\n",
      "Epoch: 21, Samples: 2432/5760, Loss: 0.020070314407348633\n",
      "Epoch: 21, Samples: 2464/5760, Loss: 0.033633410930633545\n",
      "Epoch: 21, Samples: 2496/5760, Loss: 0.024536818265914917\n",
      "Epoch: 21, Samples: 2528/5760, Loss: 0.026167184114456177\n",
      "Epoch: 21, Samples: 2560/5760, Loss: 0.05512186884880066\n",
      "Epoch: 21, Samples: 2592/5760, Loss: 0.02173691987991333\n",
      "Epoch: 21, Samples: 2624/5760, Loss: 0.027434319257736206\n",
      "Epoch: 21, Samples: 2656/5760, Loss: 0.04405093193054199\n",
      "Epoch: 21, Samples: 2688/5760, Loss: 0.027918875217437744\n",
      "Epoch: 21, Samples: 2720/5760, Loss: 0.026107579469680786\n",
      "Epoch: 21, Samples: 2752/5760, Loss: 0.04230755567550659\n",
      "Epoch: 21, Samples: 2784/5760, Loss: 0.03644338250160217\n",
      "Epoch: 21, Samples: 2816/5760, Loss: 0.020815938711166382\n",
      "Epoch: 21, Samples: 2848/5760, Loss: 0.02371099591255188\n",
      "Epoch: 21, Samples: 2880/5760, Loss: 0.028107047080993652\n",
      "Epoch: 21, Samples: 2912/5760, Loss: 0.029392987489700317\n",
      "Epoch: 21, Samples: 2944/5760, Loss: 0.02980133891105652\n",
      "Epoch: 21, Samples: 2976/5760, Loss: 0.02374938130378723\n",
      "Epoch: 21, Samples: 3008/5760, Loss: 0.035271793603897095\n",
      "Epoch: 21, Samples: 3040/5760, Loss: 0.019533604383468628\n",
      "Epoch: 21, Samples: 3072/5760, Loss: 0.04840785264968872\n",
      "Epoch: 21, Samples: 3104/5760, Loss: 0.01876544952392578\n",
      "Epoch: 21, Samples: 3136/5760, Loss: 0.06890369951725006\n",
      "Epoch: 21, Samples: 3168/5760, Loss: 0.03156761825084686\n",
      "Epoch: 21, Samples: 3200/5760, Loss: 0.07385735213756561\n",
      "Epoch: 21, Samples: 3232/5760, Loss: 0.05780608952045441\n",
      "Epoch: 21, Samples: 3264/5760, Loss: 0.026640653610229492\n",
      "Epoch: 21, Samples: 3296/5760, Loss: 0.011934250593185425\n",
      "Epoch: 21, Samples: 3328/5760, Loss: 0.033238157629966736\n",
      "Epoch: 21, Samples: 3360/5760, Loss: 0.04278191924095154\n",
      "Epoch: 21, Samples: 3392/5760, Loss: 0.015678226947784424\n",
      "Epoch: 21, Samples: 3424/5760, Loss: 0.02928239107131958\n",
      "Epoch: 21, Samples: 3456/5760, Loss: 0.04152044653892517\n",
      "Epoch: 21, Samples: 3488/5760, Loss: 0.016736119985580444\n",
      "Epoch: 21, Samples: 3520/5760, Loss: 0.023348718881607056\n",
      "Epoch: 21, Samples: 3552/5760, Loss: 0.037191241979599\n",
      "Epoch: 21, Samples: 3584/5760, Loss: 0.020090162754058838\n",
      "Epoch: 21, Samples: 3616/5760, Loss: 0.03379082679748535\n",
      "Epoch: 21, Samples: 3648/5760, Loss: 0.026001334190368652\n",
      "Epoch: 21, Samples: 3680/5760, Loss: 0.023776203393936157\n",
      "Epoch: 21, Samples: 3712/5760, Loss: 0.034773409366607666\n",
      "Epoch: 21, Samples: 3744/5760, Loss: 0.014250427484512329\n",
      "Epoch: 21, Samples: 3776/5760, Loss: 0.027821317315101624\n",
      "Epoch: 21, Samples: 3808/5760, Loss: 0.02407042682170868\n",
      "Epoch: 21, Samples: 3840/5760, Loss: 0.028319507837295532\n",
      "Epoch: 21, Samples: 3872/5760, Loss: 0.02391722798347473\n",
      "Epoch: 21, Samples: 3904/5760, Loss: 0.026353836059570312\n",
      "Epoch: 21, Samples: 3936/5760, Loss: 0.03167203068733215\n",
      "Epoch: 21, Samples: 3968/5760, Loss: 0.04501296579837799\n",
      "Epoch: 21, Samples: 4000/5760, Loss: 0.04931516945362091\n",
      "Epoch: 21, Samples: 4032/5760, Loss: 0.03390970826148987\n",
      "Epoch: 21, Samples: 4064/5760, Loss: 0.032853543758392334\n",
      "Epoch: 21, Samples: 4096/5760, Loss: 0.03935503959655762\n",
      "Epoch: 21, Samples: 4128/5760, Loss: 0.045291900634765625\n",
      "Epoch: 21, Samples: 4160/5760, Loss: 0.02481284737586975\n",
      "Epoch: 21, Samples: 4192/5760, Loss: 0.04439839720726013\n",
      "Epoch: 21, Samples: 4224/5760, Loss: 0.03372538089752197\n",
      "Epoch: 21, Samples: 4256/5760, Loss: 0.025465697050094604\n",
      "Epoch: 21, Samples: 4288/5760, Loss: 0.028437286615371704\n",
      "Epoch: 21, Samples: 4320/5760, Loss: 0.027547627687454224\n",
      "Epoch: 21, Samples: 4352/5760, Loss: 0.030878663063049316\n",
      "Epoch: 21, Samples: 4384/5760, Loss: 0.022662490606307983\n",
      "Epoch: 21, Samples: 4416/5760, Loss: 0.023378223180770874\n",
      "Epoch: 21, Samples: 4448/5760, Loss: 0.03327447175979614\n",
      "Epoch: 21, Samples: 4480/5760, Loss: 0.10683085024356842\n",
      "Epoch: 21, Samples: 4512/5760, Loss: 0.03582528233528137\n",
      "Epoch: 21, Samples: 4544/5760, Loss: 0.018724292516708374\n",
      "Epoch: 21, Samples: 4576/5760, Loss: 0.023631900548934937\n",
      "Epoch: 21, Samples: 4608/5760, Loss: 0.04244022071361542\n",
      "Epoch: 21, Samples: 4640/5760, Loss: 0.031665459275245667\n",
      "Epoch: 21, Samples: 4672/5760, Loss: 0.031653761863708496\n",
      "Epoch: 21, Samples: 4704/5760, Loss: 0.035627275705337524\n",
      "Epoch: 21, Samples: 4736/5760, Loss: 0.03274106979370117\n",
      "Epoch: 21, Samples: 4768/5760, Loss: 0.013679057359695435\n",
      "Epoch: 21, Samples: 4800/5760, Loss: 0.04060916602611542\n",
      "Epoch: 21, Samples: 4832/5760, Loss: 0.02197679877281189\n",
      "Epoch: 21, Samples: 4864/5760, Loss: 0.01582828164100647\n",
      "Epoch: 21, Samples: 4896/5760, Loss: 0.024343401193618774\n",
      "Epoch: 21, Samples: 4928/5760, Loss: 0.038861170411109924\n",
      "Epoch: 21, Samples: 4960/5760, Loss: 0.04365155100822449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Samples: 4992/5760, Loss: 0.039662882685661316\n",
      "Epoch: 21, Samples: 5024/5760, Loss: 0.03499966859817505\n",
      "Epoch: 21, Samples: 5056/5760, Loss: 0.024082094430923462\n",
      "Epoch: 21, Samples: 5088/5760, Loss: 0.04413585364818573\n",
      "Epoch: 21, Samples: 5120/5760, Loss: 0.031635165214538574\n",
      "Epoch: 21, Samples: 5152/5760, Loss: 0.02213403582572937\n",
      "Epoch: 21, Samples: 5184/5760, Loss: 0.030261337757110596\n",
      "Epoch: 21, Samples: 5216/5760, Loss: 0.027329012751579285\n",
      "Epoch: 21, Samples: 5248/5760, Loss: 0.034791991114616394\n",
      "Epoch: 21, Samples: 5280/5760, Loss: 0.019099891185760498\n",
      "Epoch: 21, Samples: 5312/5760, Loss: 0.02596914768218994\n",
      "Epoch: 21, Samples: 5344/5760, Loss: 0.05097067356109619\n",
      "Epoch: 21, Samples: 5376/5760, Loss: 0.0362924188375473\n",
      "Epoch: 21, Samples: 5408/5760, Loss: 0.052233368158340454\n",
      "Epoch: 21, Samples: 5440/5760, Loss: 0.021940603852272034\n",
      "Epoch: 21, Samples: 5472/5760, Loss: 0.02847599983215332\n",
      "Epoch: 21, Samples: 5504/5760, Loss: 0.04590712487697601\n",
      "Epoch: 21, Samples: 5536/5760, Loss: 0.031822532415390015\n",
      "Epoch: 21, Samples: 5568/5760, Loss: 0.018096745014190674\n",
      "Epoch: 21, Samples: 5600/5760, Loss: 0.027062028646469116\n",
      "Epoch: 21, Samples: 5632/5760, Loss: 0.03459005057811737\n",
      "Epoch: 21, Samples: 5664/5760, Loss: 0.029649049043655396\n",
      "Epoch: 21, Samples: 5696/5760, Loss: 0.03345838189125061\n",
      "Epoch: 21, Samples: 5728/5760, Loss: 0.8232605457305908\n",
      "\n",
      "Epoch: 21\n",
      "Training set: Average loss: 0.0367\n",
      "Validation set: Average loss: 0.3501, Accuracy: 747/818 (91%)\n",
      "Saving model (epoch 21) with lowest validation loss: 0.3501043990254402\n",
      "Epoch: 22, Samples: 0/5760, Loss: 0.03146204352378845\n",
      "Epoch: 22, Samples: 32/5760, Loss: 0.018951132893562317\n",
      "Epoch: 22, Samples: 64/5760, Loss: 0.03160795569419861\n",
      "Epoch: 22, Samples: 96/5760, Loss: 0.021591156721115112\n",
      "Epoch: 22, Samples: 128/5760, Loss: 0.03088018298149109\n",
      "Epoch: 22, Samples: 160/5760, Loss: 0.03353327512741089\n",
      "Epoch: 22, Samples: 192/5760, Loss: 0.03702414035797119\n",
      "Epoch: 22, Samples: 224/5760, Loss: 0.04202240705490112\n",
      "Epoch: 22, Samples: 256/5760, Loss: 0.027874469757080078\n",
      "Epoch: 22, Samples: 288/5760, Loss: 0.02350178360939026\n",
      "Epoch: 22, Samples: 320/5760, Loss: 0.04461109638214111\n",
      "Epoch: 22, Samples: 352/5760, Loss: 0.02728748321533203\n",
      "Epoch: 22, Samples: 384/5760, Loss: 0.022412806749343872\n",
      "Epoch: 22, Samples: 416/5760, Loss: 0.029376327991485596\n",
      "Epoch: 22, Samples: 448/5760, Loss: 0.04388004541397095\n",
      "Epoch: 22, Samples: 480/5760, Loss: 0.03237445652484894\n",
      "Epoch: 22, Samples: 512/5760, Loss: 0.01642543077468872\n",
      "Epoch: 22, Samples: 544/5760, Loss: 0.07784490287303925\n",
      "Epoch: 22, Samples: 576/5760, Loss: 0.04749763011932373\n",
      "Epoch: 22, Samples: 608/5760, Loss: 0.022819340229034424\n",
      "Epoch: 22, Samples: 640/5760, Loss: 0.02328088879585266\n",
      "Epoch: 22, Samples: 672/5760, Loss: 0.030947506427764893\n",
      "Epoch: 22, Samples: 704/5760, Loss: 0.0361977219581604\n",
      "Epoch: 22, Samples: 736/5760, Loss: 0.028221547603607178\n",
      "Epoch: 22, Samples: 768/5760, Loss: 0.035725295543670654\n",
      "Epoch: 22, Samples: 800/5760, Loss: 0.021050870418548584\n",
      "Epoch: 22, Samples: 832/5760, Loss: 0.019280731678009033\n",
      "Epoch: 22, Samples: 864/5760, Loss: 0.02632009983062744\n",
      "Epoch: 22, Samples: 896/5760, Loss: 0.018864214420318604\n",
      "Epoch: 22, Samples: 928/5760, Loss: 0.024419724941253662\n",
      "Epoch: 22, Samples: 960/5760, Loss: 0.026914268732070923\n",
      "Epoch: 22, Samples: 992/5760, Loss: 0.03693222999572754\n",
      "Epoch: 22, Samples: 1024/5760, Loss: 0.03262794017791748\n",
      "Epoch: 22, Samples: 1056/5760, Loss: 0.02164861559867859\n",
      "Epoch: 22, Samples: 1088/5760, Loss: 0.041324734687805176\n",
      "Epoch: 22, Samples: 1120/5760, Loss: 0.025631040334701538\n",
      "Epoch: 22, Samples: 1152/5760, Loss: 0.019209951162338257\n",
      "Epoch: 22, Samples: 1184/5760, Loss: 0.019083380699157715\n",
      "Epoch: 22, Samples: 1216/5760, Loss: 0.04074418544769287\n",
      "Epoch: 22, Samples: 1248/5760, Loss: 0.028421491384506226\n",
      "Epoch: 22, Samples: 1280/5760, Loss: 0.022855013608932495\n",
      "Epoch: 22, Samples: 1312/5760, Loss: 0.015982449054718018\n",
      "Epoch: 22, Samples: 1344/5760, Loss: 0.03643795847892761\n",
      "Epoch: 22, Samples: 1376/5760, Loss: 0.022026032209396362\n",
      "Epoch: 22, Samples: 1408/5760, Loss: 0.022723853588104248\n",
      "Epoch: 22, Samples: 1440/5760, Loss: 0.07218955457210541\n",
      "Epoch: 22, Samples: 1472/5760, Loss: 0.024894148111343384\n",
      "Epoch: 22, Samples: 1504/5760, Loss: 0.02237492799758911\n",
      "Epoch: 22, Samples: 1536/5760, Loss: 0.021506160497665405\n",
      "Epoch: 22, Samples: 1568/5760, Loss: 0.030184000730514526\n",
      "Epoch: 22, Samples: 1600/5760, Loss: 0.019804805517196655\n",
      "Epoch: 22, Samples: 1632/5760, Loss: 0.03415830433368683\n",
      "Epoch: 22, Samples: 1664/5760, Loss: 0.03167828917503357\n",
      "Epoch: 22, Samples: 1696/5760, Loss: 0.017230987548828125\n",
      "Epoch: 22, Samples: 1728/5760, Loss: 0.023060470819473267\n",
      "Epoch: 22, Samples: 1760/5760, Loss: 0.04209357500076294\n",
      "Epoch: 22, Samples: 1792/5760, Loss: 0.02256608009338379\n",
      "Epoch: 22, Samples: 1824/5760, Loss: 0.027934014797210693\n",
      "Epoch: 22, Samples: 1856/5760, Loss: 0.07296784222126007\n",
      "Epoch: 22, Samples: 1888/5760, Loss: 0.025371074676513672\n",
      "Epoch: 22, Samples: 1920/5760, Loss: 0.025309383869171143\n",
      "Epoch: 22, Samples: 1952/5760, Loss: 0.03334875404834747\n",
      "Epoch: 22, Samples: 1984/5760, Loss: 0.022538989782333374\n",
      "Epoch: 22, Samples: 2016/5760, Loss: 0.02790197730064392\n",
      "Epoch: 22, Samples: 2048/5760, Loss: 0.02821233868598938\n",
      "Epoch: 22, Samples: 2080/5760, Loss: 0.026743143796920776\n",
      "Epoch: 22, Samples: 2112/5760, Loss: 0.026621297001838684\n",
      "Epoch: 22, Samples: 2144/5760, Loss: 0.031722187995910645\n",
      "Epoch: 22, Samples: 2176/5760, Loss: 0.0259639173746109\n",
      "Epoch: 22, Samples: 2208/5760, Loss: 0.025231629610061646\n",
      "Epoch: 22, Samples: 2240/5760, Loss: 0.02717718482017517\n",
      "Epoch: 22, Samples: 2272/5760, Loss: 0.02878531813621521\n",
      "Epoch: 22, Samples: 2304/5760, Loss: 0.030585110187530518\n",
      "Epoch: 22, Samples: 2336/5760, Loss: 0.02222001552581787\n",
      "Epoch: 22, Samples: 2368/5760, Loss: 0.02723725140094757\n",
      "Epoch: 22, Samples: 2400/5760, Loss: 0.01333153247833252\n",
      "Epoch: 22, Samples: 2432/5760, Loss: 0.021662771701812744\n",
      "Epoch: 22, Samples: 2464/5760, Loss: 0.020451724529266357\n",
      "Epoch: 22, Samples: 2496/5760, Loss: 0.023472696542739868\n",
      "Epoch: 22, Samples: 2528/5760, Loss: 0.022768348455429077\n",
      "Epoch: 22, Samples: 2560/5760, Loss: 0.029327422380447388\n",
      "Epoch: 22, Samples: 2592/5760, Loss: 0.03657747805118561\n",
      "Epoch: 22, Samples: 2624/5760, Loss: 0.01699995994567871\n",
      "Epoch: 22, Samples: 2656/5760, Loss: 0.02651187777519226\n",
      "Epoch: 22, Samples: 2688/5760, Loss: 0.015254557132720947\n",
      "Epoch: 22, Samples: 2720/5760, Loss: 0.02499479055404663\n",
      "Epoch: 22, Samples: 2752/5760, Loss: 0.05262655019760132\n",
      "Epoch: 22, Samples: 2784/5760, Loss: 0.05869871377944946\n",
      "Epoch: 22, Samples: 2816/5760, Loss: 0.02618849277496338\n",
      "Epoch: 22, Samples: 2848/5760, Loss: 0.022237122058868408\n",
      "Epoch: 22, Samples: 2880/5760, Loss: 0.030869171023368835\n",
      "Epoch: 22, Samples: 2912/5760, Loss: 0.02901482582092285\n",
      "Epoch: 22, Samples: 2944/5760, Loss: 0.04325932264328003\n",
      "Epoch: 22, Samples: 2976/5760, Loss: 0.03399398922920227\n",
      "Epoch: 22, Samples: 3008/5760, Loss: 0.05122120678424835\n",
      "Epoch: 22, Samples: 3040/5760, Loss: 0.03765392303466797\n",
      "Epoch: 22, Samples: 3072/5760, Loss: 0.034064680337905884\n",
      "Epoch: 22, Samples: 3104/5760, Loss: 0.027094751596450806\n",
      "Epoch: 22, Samples: 3136/5760, Loss: 0.01848432421684265\n",
      "Epoch: 22, Samples: 3168/5760, Loss: 0.03771071135997772\n",
      "Epoch: 22, Samples: 3200/5760, Loss: 0.018987685441970825\n",
      "Epoch: 22, Samples: 3232/5760, Loss: 0.030710160732269287\n",
      "Epoch: 22, Samples: 3264/5760, Loss: 0.022063523530960083\n",
      "Epoch: 22, Samples: 3296/5760, Loss: 0.02357889711856842\n",
      "Epoch: 22, Samples: 3328/5760, Loss: 0.02694675326347351\n",
      "Epoch: 22, Samples: 3360/5760, Loss: 0.026726260781288147\n",
      "Epoch: 22, Samples: 3392/5760, Loss: 0.03926795721054077\n",
      "Epoch: 22, Samples: 3424/5760, Loss: 0.025618255138397217\n",
      "Epoch: 22, Samples: 3456/5760, Loss: 0.03871876001358032\n",
      "Epoch: 22, Samples: 3488/5760, Loss: 0.05689302086830139\n",
      "Epoch: 22, Samples: 3520/5760, Loss: 0.03305068612098694\n",
      "Epoch: 22, Samples: 3552/5760, Loss: 0.03524848818778992\n",
      "Epoch: 22, Samples: 3584/5760, Loss: 0.01687905192375183\n",
      "Epoch: 22, Samples: 3616/5760, Loss: 0.05117209255695343\n",
      "Epoch: 22, Samples: 3648/5760, Loss: 0.03020617365837097\n",
      "Epoch: 22, Samples: 3680/5760, Loss: 0.041774213314056396\n",
      "Epoch: 22, Samples: 3712/5760, Loss: 0.03342434763908386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Samples: 3744/5760, Loss: 0.029333025217056274\n",
      "Epoch: 22, Samples: 3776/5760, Loss: 0.028126627206802368\n",
      "Epoch: 22, Samples: 3808/5760, Loss: 0.01769581437110901\n",
      "Epoch: 22, Samples: 3840/5760, Loss: 0.04559388756752014\n",
      "Epoch: 22, Samples: 3872/5760, Loss: 0.02789291739463806\n",
      "Epoch: 22, Samples: 3904/5760, Loss: 0.045481160283088684\n",
      "Epoch: 22, Samples: 3936/5760, Loss: 0.03713726997375488\n",
      "Epoch: 22, Samples: 3968/5760, Loss: 0.03316885232925415\n",
      "Epoch: 22, Samples: 4000/5760, Loss: 0.0241243839263916\n",
      "Epoch: 22, Samples: 4032/5760, Loss: 0.027254030108451843\n",
      "Epoch: 22, Samples: 4064/5760, Loss: 0.02841392159461975\n",
      "Epoch: 22, Samples: 4096/5760, Loss: 0.050738945603370667\n",
      "Epoch: 22, Samples: 4128/5760, Loss: 0.019628435373306274\n",
      "Epoch: 22, Samples: 4160/5760, Loss: 0.017563045024871826\n",
      "Epoch: 22, Samples: 4192/5760, Loss: 0.021625518798828125\n",
      "Epoch: 22, Samples: 4224/5760, Loss: 0.0423947274684906\n",
      "Epoch: 22, Samples: 4256/5760, Loss: 0.02521158754825592\n",
      "Epoch: 22, Samples: 4288/5760, Loss: 0.024044901132583618\n",
      "Epoch: 22, Samples: 4320/5760, Loss: 0.03461642563343048\n",
      "Epoch: 22, Samples: 4352/5760, Loss: 0.034065306186676025\n",
      "Epoch: 22, Samples: 4384/5760, Loss: 0.033466413617134094\n",
      "Epoch: 22, Samples: 4416/5760, Loss: 0.02821972966194153\n",
      "Epoch: 22, Samples: 4448/5760, Loss: 0.03015667200088501\n",
      "Epoch: 22, Samples: 4480/5760, Loss: 0.025857925415039062\n",
      "Epoch: 22, Samples: 4512/5760, Loss: 0.058536916971206665\n",
      "Epoch: 22, Samples: 4544/5760, Loss: 0.03280360996723175\n",
      "Epoch: 22, Samples: 4576/5760, Loss: 0.040416792035102844\n",
      "Epoch: 22, Samples: 4608/5760, Loss: 0.05817742645740509\n",
      "Epoch: 22, Samples: 4640/5760, Loss: 0.016585737466812134\n",
      "Epoch: 22, Samples: 4672/5760, Loss: 0.032287389039993286\n",
      "Epoch: 22, Samples: 4704/5760, Loss: 0.024079591035842896\n",
      "Epoch: 22, Samples: 4736/5760, Loss: 0.03246524930000305\n",
      "Epoch: 22, Samples: 4768/5760, Loss: 0.01649641990661621\n",
      "Epoch: 22, Samples: 4800/5760, Loss: 0.026427090167999268\n",
      "Epoch: 22, Samples: 4832/5760, Loss: 0.05234980583190918\n",
      "Epoch: 22, Samples: 4864/5760, Loss: 0.029750198125839233\n",
      "Epoch: 22, Samples: 4896/5760, Loss: 0.017029404640197754\n",
      "Epoch: 22, Samples: 4928/5760, Loss: 0.038873836398124695\n",
      "Epoch: 22, Samples: 4960/5760, Loss: 0.053544506430625916\n",
      "Epoch: 22, Samples: 4992/5760, Loss: 0.03074219822883606\n",
      "Epoch: 22, Samples: 5024/5760, Loss: 0.04296562075614929\n",
      "Epoch: 22, Samples: 5056/5760, Loss: 0.020339906215667725\n",
      "Epoch: 22, Samples: 5088/5760, Loss: 0.021206676959991455\n",
      "Epoch: 22, Samples: 5120/5760, Loss: 0.044498562812805176\n",
      "Epoch: 22, Samples: 5152/5760, Loss: 0.021867483854293823\n",
      "Epoch: 22, Samples: 5184/5760, Loss: 0.02920842170715332\n",
      "Epoch: 22, Samples: 5216/5760, Loss: 0.008823275566101074\n",
      "Epoch: 22, Samples: 5248/5760, Loss: 0.03200751543045044\n",
      "Epoch: 22, Samples: 5280/5760, Loss: 0.025817856192588806\n",
      "Epoch: 22, Samples: 5312/5760, Loss: 0.025108367204666138\n",
      "Epoch: 22, Samples: 5344/5760, Loss: 0.02200755476951599\n",
      "Epoch: 22, Samples: 5376/5760, Loss: 0.04857388138771057\n",
      "Epoch: 22, Samples: 5408/5760, Loss: 0.03719942271709442\n",
      "Epoch: 22, Samples: 5440/5760, Loss: 0.028347313404083252\n",
      "Epoch: 22, Samples: 5472/5760, Loss: 0.017499029636383057\n",
      "Epoch: 22, Samples: 5504/5760, Loss: 0.029004693031311035\n",
      "Epoch: 22, Samples: 5536/5760, Loss: 0.019625842571258545\n",
      "Epoch: 22, Samples: 5568/5760, Loss: 0.06105630099773407\n",
      "Epoch: 22, Samples: 5600/5760, Loss: 0.025284558534622192\n",
      "Epoch: 22, Samples: 5632/5760, Loss: 0.042325764894485474\n",
      "Epoch: 22, Samples: 5664/5760, Loss: 0.02242770791053772\n",
      "Epoch: 22, Samples: 5696/5760, Loss: 0.025184839963912964\n",
      "Epoch: 22, Samples: 5728/5760, Loss: 0.3693370819091797\n",
      "\n",
      "Epoch: 22\n",
      "Training set: Average loss: 0.0325\n",
      "Validation set: Average loss: 0.3550, Accuracy: 749/818 (92%)\n",
      "Epoch: 23, Samples: 0/5760, Loss: 0.014789789915084839\n",
      "Epoch: 23, Samples: 32/5760, Loss: 0.018432945013046265\n",
      "Epoch: 23, Samples: 64/5760, Loss: 0.029901474714279175\n",
      "Epoch: 23, Samples: 96/5760, Loss: 0.03783601522445679\n",
      "Epoch: 23, Samples: 128/5760, Loss: 0.03378470242023468\n",
      "Epoch: 23, Samples: 160/5760, Loss: 0.04481193423271179\n",
      "Epoch: 23, Samples: 192/5760, Loss: 0.029177933931350708\n",
      "Epoch: 23, Samples: 224/5760, Loss: 0.02714119851589203\n",
      "Epoch: 23, Samples: 256/5760, Loss: 0.033748552203178406\n",
      "Epoch: 23, Samples: 288/5760, Loss: 0.03284811973571777\n",
      "Epoch: 23, Samples: 320/5760, Loss: 0.02832743525505066\n",
      "Epoch: 23, Samples: 352/5760, Loss: 0.028190284967422485\n",
      "Epoch: 23, Samples: 384/5760, Loss: 0.018565863370895386\n",
      "Epoch: 23, Samples: 416/5760, Loss: 0.046047717332839966\n",
      "Epoch: 23, Samples: 448/5760, Loss: 0.025852829217910767\n",
      "Epoch: 23, Samples: 480/5760, Loss: 0.025335103273391724\n",
      "Epoch: 23, Samples: 512/5760, Loss: 0.025379806756973267\n",
      "Epoch: 23, Samples: 544/5760, Loss: 0.02529081702232361\n",
      "Epoch: 23, Samples: 576/5760, Loss: 0.030050933361053467\n",
      "Epoch: 23, Samples: 608/5760, Loss: 0.02567097544670105\n",
      "Epoch: 23, Samples: 640/5760, Loss: 0.035508543252944946\n",
      "Epoch: 23, Samples: 672/5760, Loss: 0.06455917656421661\n",
      "Epoch: 23, Samples: 704/5760, Loss: 0.027709633111953735\n",
      "Epoch: 23, Samples: 736/5760, Loss: 0.02036985754966736\n",
      "Epoch: 23, Samples: 768/5760, Loss: 0.032880187034606934\n",
      "Epoch: 23, Samples: 800/5760, Loss: 0.018545866012573242\n",
      "Epoch: 23, Samples: 832/5760, Loss: 0.02882656455039978\n",
      "Epoch: 23, Samples: 864/5760, Loss: 0.02504265308380127\n",
      "Epoch: 23, Samples: 896/5760, Loss: 0.026375621557235718\n",
      "Epoch: 23, Samples: 928/5760, Loss: 0.02522185444831848\n",
      "Epoch: 23, Samples: 960/5760, Loss: 0.018473327159881592\n",
      "Epoch: 23, Samples: 992/5760, Loss: 0.03589077293872833\n",
      "Epoch: 23, Samples: 1024/5760, Loss: 0.03356471657752991\n",
      "Epoch: 23, Samples: 1056/5760, Loss: 0.020173698663711548\n",
      "Epoch: 23, Samples: 1088/5760, Loss: 0.021400153636932373\n",
      "Epoch: 23, Samples: 1120/5760, Loss: 0.020726799964904785\n",
      "Epoch: 23, Samples: 1152/5760, Loss: 0.016766488552093506\n",
      "Epoch: 23, Samples: 1184/5760, Loss: 0.03446701169013977\n",
      "Epoch: 23, Samples: 1216/5760, Loss: 0.022209763526916504\n",
      "Epoch: 23, Samples: 1248/5760, Loss: 0.01682540774345398\n",
      "Epoch: 23, Samples: 1280/5760, Loss: 0.01265937089920044\n",
      "Epoch: 23, Samples: 1312/5760, Loss: 0.02185124158859253\n",
      "Epoch: 23, Samples: 1344/5760, Loss: 0.02175500988960266\n",
      "Epoch: 23, Samples: 1376/5760, Loss: 0.03605771064758301\n",
      "Epoch: 23, Samples: 1408/5760, Loss: 0.021856755018234253\n",
      "Epoch: 23, Samples: 1440/5760, Loss: 0.023876100778579712\n",
      "Epoch: 23, Samples: 1472/5760, Loss: 0.012680590152740479\n",
      "Epoch: 23, Samples: 1504/5760, Loss: 0.025723963975906372\n",
      "Epoch: 23, Samples: 1536/5760, Loss: 0.0387289822101593\n",
      "Epoch: 23, Samples: 1568/5760, Loss: 0.016921520233154297\n",
      "Epoch: 23, Samples: 1600/5760, Loss: 0.022211670875549316\n",
      "Epoch: 23, Samples: 1632/5760, Loss: 0.029369056224822998\n",
      "Epoch: 23, Samples: 1664/5760, Loss: 0.013178825378417969\n",
      "Epoch: 23, Samples: 1696/5760, Loss: 0.03292053937911987\n",
      "Epoch: 23, Samples: 1728/5760, Loss: 0.02545890212059021\n",
      "Epoch: 23, Samples: 1760/5760, Loss: 0.026954427361488342\n",
      "Epoch: 23, Samples: 1792/5760, Loss: 0.03822501003742218\n",
      "Epoch: 23, Samples: 1824/5760, Loss: 0.03407259285449982\n",
      "Epoch: 23, Samples: 1856/5760, Loss: 0.019902735948562622\n",
      "Epoch: 23, Samples: 1888/5760, Loss: 0.01818332076072693\n",
      "Epoch: 23, Samples: 1920/5760, Loss: 0.025092244148254395\n",
      "Epoch: 23, Samples: 1952/5760, Loss: 0.02562376856803894\n",
      "Epoch: 23, Samples: 1984/5760, Loss: 0.02419409155845642\n",
      "Epoch: 23, Samples: 2016/5760, Loss: 0.020477473735809326\n",
      "Epoch: 23, Samples: 2048/5760, Loss: 0.04743623733520508\n",
      "Epoch: 23, Samples: 2080/5760, Loss: 0.03130227327346802\n",
      "Epoch: 23, Samples: 2112/5760, Loss: 0.03217923641204834\n",
      "Epoch: 23, Samples: 2144/5760, Loss: 0.012001723051071167\n",
      "Epoch: 23, Samples: 2176/5760, Loss: 0.015293329954147339\n",
      "Epoch: 23, Samples: 2208/5760, Loss: 0.02078157663345337\n",
      "Epoch: 23, Samples: 2240/5760, Loss: 0.02273312211036682\n",
      "Epoch: 23, Samples: 2272/5760, Loss: 0.012570619583129883\n",
      "Epoch: 23, Samples: 2304/5760, Loss: 0.055430442094802856\n",
      "Epoch: 23, Samples: 2336/5760, Loss: 0.05211615562438965\n",
      "Epoch: 23, Samples: 2368/5760, Loss: 0.05806058645248413\n",
      "Epoch: 23, Samples: 2400/5760, Loss: 0.0278376042842865\n",
      "Epoch: 23, Samples: 2432/5760, Loss: 0.022447064518928528\n",
      "Epoch: 23, Samples: 2464/5760, Loss: 0.038123518228530884\n",
      "Epoch: 23, Samples: 2496/5760, Loss: 0.02102234959602356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Samples: 2528/5760, Loss: 0.029567450284957886\n",
      "Epoch: 23, Samples: 2560/5760, Loss: 0.019181936979293823\n",
      "Epoch: 23, Samples: 2592/5760, Loss: 0.04015244543552399\n",
      "Epoch: 23, Samples: 2624/5760, Loss: 0.01508745551109314\n",
      "Epoch: 23, Samples: 2656/5760, Loss: 0.029299229383468628\n",
      "Epoch: 23, Samples: 2688/5760, Loss: 0.03644213080406189\n",
      "Epoch: 23, Samples: 2720/5760, Loss: 0.019026994705200195\n",
      "Epoch: 23, Samples: 2752/5760, Loss: 0.020187854766845703\n",
      "Epoch: 23, Samples: 2784/5760, Loss: 0.02964732050895691\n",
      "Epoch: 23, Samples: 2816/5760, Loss: 0.021928727626800537\n",
      "Epoch: 23, Samples: 2848/5760, Loss: 0.017520785331726074\n",
      "Epoch: 23, Samples: 2880/5760, Loss: 0.021429896354675293\n",
      "Epoch: 23, Samples: 2912/5760, Loss: 0.022840559482574463\n",
      "Epoch: 23, Samples: 2944/5760, Loss: 0.03164859116077423\n",
      "Epoch: 23, Samples: 2976/5760, Loss: 0.03592677414417267\n",
      "Epoch: 23, Samples: 3008/5760, Loss: 0.02584901452064514\n",
      "Epoch: 23, Samples: 3040/5760, Loss: 0.039000362157821655\n",
      "Epoch: 23, Samples: 3072/5760, Loss: 0.023981034755706787\n",
      "Epoch: 23, Samples: 3104/5760, Loss: 0.019556760787963867\n",
      "Epoch: 23, Samples: 3136/5760, Loss: 0.02720656991004944\n",
      "Epoch: 23, Samples: 3168/5760, Loss: 0.02546641230583191\n",
      "Epoch: 23, Samples: 3200/5760, Loss: 0.037986695766448975\n",
      "Epoch: 23, Samples: 3232/5760, Loss: 0.03497377038002014\n",
      "Epoch: 23, Samples: 3264/5760, Loss: 0.04097723960876465\n",
      "Epoch: 23, Samples: 3296/5760, Loss: 0.014851272106170654\n",
      "Epoch: 23, Samples: 3328/5760, Loss: 0.046716779470443726\n",
      "Epoch: 23, Samples: 3360/5760, Loss: 0.01783931255340576\n",
      "Epoch: 23, Samples: 3392/5760, Loss: 0.026737332344055176\n",
      "Epoch: 23, Samples: 3424/5760, Loss: 0.021567106246948242\n",
      "Epoch: 23, Samples: 3456/5760, Loss: 0.03809778392314911\n",
      "Epoch: 23, Samples: 3488/5760, Loss: 0.016796976327896118\n",
      "Epoch: 23, Samples: 3520/5760, Loss: 0.021000266075134277\n",
      "Epoch: 23, Samples: 3552/5760, Loss: 0.02378484606742859\n",
      "Epoch: 23, Samples: 3584/5760, Loss: 0.02027648687362671\n",
      "Epoch: 23, Samples: 3616/5760, Loss: 0.02384006977081299\n",
      "Epoch: 23, Samples: 3648/5760, Loss: 0.02708989381790161\n",
      "Epoch: 23, Samples: 3680/5760, Loss: 0.01874879002571106\n",
      "Epoch: 23, Samples: 3712/5760, Loss: 0.01393735408782959\n",
      "Epoch: 23, Samples: 3744/5760, Loss: 0.03414878249168396\n",
      "Epoch: 23, Samples: 3776/5760, Loss: 0.036047905683517456\n",
      "Epoch: 23, Samples: 3808/5760, Loss: 0.025313973426818848\n",
      "Epoch: 23, Samples: 3840/5760, Loss: 0.01644456386566162\n",
      "Epoch: 23, Samples: 3872/5760, Loss: 0.017156928777694702\n",
      "Epoch: 23, Samples: 3904/5760, Loss: 0.023975133895874023\n",
      "Epoch: 23, Samples: 3936/5760, Loss: 0.023121461272239685\n",
      "Epoch: 23, Samples: 3968/5760, Loss: 0.02738240361213684\n",
      "Epoch: 23, Samples: 4000/5760, Loss: 0.02079826593399048\n",
      "Epoch: 23, Samples: 4032/5760, Loss: 0.015095770359039307\n",
      "Epoch: 23, Samples: 4064/5760, Loss: 0.021536484360694885\n",
      "Epoch: 23, Samples: 4096/5760, Loss: 0.014500826597213745\n",
      "Epoch: 23, Samples: 4128/5760, Loss: 0.03577090799808502\n",
      "Epoch: 23, Samples: 4160/5760, Loss: 0.021174192428588867\n",
      "Epoch: 23, Samples: 4192/5760, Loss: 0.02580547332763672\n",
      "Epoch: 23, Samples: 4224/5760, Loss: 0.0257425457239151\n",
      "Epoch: 23, Samples: 4256/5760, Loss: 0.013104617595672607\n",
      "Epoch: 23, Samples: 4288/5760, Loss: 0.03856050968170166\n",
      "Epoch: 23, Samples: 4320/5760, Loss: 0.02973198890686035\n",
      "Epoch: 23, Samples: 4352/5760, Loss: 0.029160737991333008\n",
      "Epoch: 23, Samples: 4384/5760, Loss: 0.025803089141845703\n",
      "Epoch: 23, Samples: 4416/5760, Loss: 0.01873597502708435\n",
      "Epoch: 23, Samples: 4448/5760, Loss: 0.022556960582733154\n",
      "Epoch: 23, Samples: 4480/5760, Loss: 0.018129408359527588\n",
      "Epoch: 23, Samples: 4512/5760, Loss: 0.01679721474647522\n",
      "Epoch: 23, Samples: 4544/5760, Loss: 0.029597938060760498\n",
      "Epoch: 23, Samples: 4576/5760, Loss: 0.019271165132522583\n",
      "Epoch: 23, Samples: 4608/5760, Loss: 0.03284147381782532\n",
      "Epoch: 23, Samples: 4640/5760, Loss: 0.03458297252655029\n",
      "Epoch: 23, Samples: 4672/5760, Loss: 0.03561106324195862\n",
      "Epoch: 23, Samples: 4704/5760, Loss: 0.05151471495628357\n",
      "Epoch: 23, Samples: 4736/5760, Loss: 0.021534860134124756\n",
      "Epoch: 23, Samples: 4768/5760, Loss: 0.019065141677856445\n",
      "Epoch: 23, Samples: 4800/5760, Loss: 0.023858606815338135\n",
      "Epoch: 23, Samples: 4832/5760, Loss: 0.01308673620223999\n",
      "Epoch: 23, Samples: 4864/5760, Loss: 0.04717208445072174\n",
      "Epoch: 23, Samples: 4896/5760, Loss: 0.020587891340255737\n",
      "Epoch: 23, Samples: 4928/5760, Loss: 0.021494120359420776\n",
      "Epoch: 23, Samples: 4960/5760, Loss: 0.027664989233016968\n",
      "Epoch: 23, Samples: 4992/5760, Loss: 0.02479928731918335\n",
      "Epoch: 23, Samples: 5024/5760, Loss: 0.04008267819881439\n",
      "Epoch: 23, Samples: 5056/5760, Loss: 0.016296863555908203\n",
      "Epoch: 23, Samples: 5088/5760, Loss: 0.021561414003372192\n",
      "Epoch: 23, Samples: 5120/5760, Loss: 0.015340417623519897\n",
      "Epoch: 23, Samples: 5152/5760, Loss: 0.030899539589881897\n",
      "Epoch: 23, Samples: 5184/5760, Loss: 0.018354937434196472\n",
      "Epoch: 23, Samples: 5216/5760, Loss: 0.02551332116127014\n",
      "Epoch: 23, Samples: 5248/5760, Loss: 0.022875577211380005\n",
      "Epoch: 23, Samples: 5280/5760, Loss: 0.016721338033676147\n",
      "Epoch: 23, Samples: 5312/5760, Loss: 0.02188393473625183\n",
      "Epoch: 23, Samples: 5344/5760, Loss: 0.02459201216697693\n",
      "Epoch: 23, Samples: 5376/5760, Loss: 0.0533660352230072\n",
      "Epoch: 23, Samples: 5408/5760, Loss: 0.014446914196014404\n",
      "Epoch: 23, Samples: 5440/5760, Loss: 0.018754184246063232\n",
      "Epoch: 23, Samples: 5472/5760, Loss: 0.020257383584976196\n",
      "Epoch: 23, Samples: 5504/5760, Loss: 0.011279672384262085\n",
      "Epoch: 23, Samples: 5536/5760, Loss: 0.02166426181793213\n",
      "Epoch: 23, Samples: 5568/5760, Loss: 0.018641293048858643\n",
      "Epoch: 23, Samples: 5600/5760, Loss: 0.04820050299167633\n",
      "Epoch: 23, Samples: 5632/5760, Loss: 0.03047958016395569\n",
      "Epoch: 23, Samples: 5664/5760, Loss: 0.038613855838775635\n",
      "Epoch: 23, Samples: 5696/5760, Loss: 0.022085487842559814\n",
      "Epoch: 23, Samples: 5728/5760, Loss: 0.1250370740890503\n",
      "\n",
      "Epoch: 23\n",
      "Training set: Average loss: 0.0271\n",
      "Validation set: Average loss: 0.3549, Accuracy: 749/818 (92%)\n",
      "Epoch: 24, Samples: 0/5760, Loss: 0.03317791223526001\n",
      "Epoch: 24, Samples: 32/5760, Loss: 0.018082618713378906\n",
      "Epoch: 24, Samples: 64/5760, Loss: 0.03821989893913269\n",
      "Epoch: 24, Samples: 96/5760, Loss: 0.01892876625061035\n",
      "Epoch: 24, Samples: 128/5760, Loss: 0.02722768485546112\n",
      "Epoch: 24, Samples: 160/5760, Loss: 0.023419976234436035\n",
      "Epoch: 24, Samples: 192/5760, Loss: 0.02081158757209778\n",
      "Epoch: 24, Samples: 224/5760, Loss: 0.025326251983642578\n",
      "Epoch: 24, Samples: 256/5760, Loss: 0.028189033269882202\n",
      "Epoch: 24, Samples: 288/5760, Loss: 0.03097441792488098\n",
      "Epoch: 24, Samples: 320/5760, Loss: 0.019948437809944153\n",
      "Epoch: 24, Samples: 352/5760, Loss: 0.028344526886940002\n",
      "Epoch: 24, Samples: 384/5760, Loss: 0.02501887083053589\n",
      "Epoch: 24, Samples: 416/5760, Loss: 0.021142005920410156\n",
      "Epoch: 24, Samples: 448/5760, Loss: 0.01904657483100891\n",
      "Epoch: 24, Samples: 480/5760, Loss: 0.04366488754749298\n",
      "Epoch: 24, Samples: 512/5760, Loss: 0.022502154111862183\n",
      "Epoch: 24, Samples: 544/5760, Loss: 0.018127471208572388\n",
      "Epoch: 24, Samples: 576/5760, Loss: 0.02041003108024597\n",
      "Epoch: 24, Samples: 608/5760, Loss: 0.0155714750289917\n",
      "Epoch: 24, Samples: 640/5760, Loss: 0.032638922333717346\n",
      "Epoch: 24, Samples: 672/5760, Loss: 0.017504647374153137\n",
      "Epoch: 24, Samples: 704/5760, Loss: 0.03323139250278473\n",
      "Epoch: 24, Samples: 736/5760, Loss: 0.0327514111995697\n",
      "Epoch: 24, Samples: 768/5760, Loss: 0.01964244246482849\n",
      "Epoch: 24, Samples: 800/5760, Loss: 0.03245234489440918\n",
      "Epoch: 24, Samples: 832/5760, Loss: 0.028401196002960205\n",
      "Epoch: 24, Samples: 864/5760, Loss: 0.014475494623184204\n",
      "Epoch: 24, Samples: 896/5760, Loss: 0.03398957848548889\n",
      "Epoch: 24, Samples: 928/5760, Loss: 0.016177594661712646\n",
      "Epoch: 24, Samples: 960/5760, Loss: 0.018362343311309814\n",
      "Epoch: 24, Samples: 992/5760, Loss: 0.02353045344352722\n",
      "Epoch: 24, Samples: 1024/5760, Loss: 0.02730002999305725\n",
      "Epoch: 24, Samples: 1056/5760, Loss: 0.03208419680595398\n",
      "Epoch: 24, Samples: 1088/5760, Loss: 0.024406224489212036\n",
      "Epoch: 24, Samples: 1120/5760, Loss: 0.01821109652519226\n",
      "Epoch: 24, Samples: 1152/5760, Loss: 0.039344415068626404\n",
      "Epoch: 24, Samples: 1184/5760, Loss: 0.03420519828796387\n",
      "Epoch: 24, Samples: 1216/5760, Loss: 0.014996051788330078\n",
      "Epoch: 24, Samples: 1248/5760, Loss: 0.021900981664657593\n",
      "Epoch: 24, Samples: 1280/5760, Loss: 0.021127313375473022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Samples: 1312/5760, Loss: 0.04130391776561737\n",
      "Epoch: 24, Samples: 1344/5760, Loss: 0.020088881254196167\n",
      "Epoch: 24, Samples: 1376/5760, Loss: 0.03610333800315857\n",
      "Epoch: 24, Samples: 1408/5760, Loss: 0.033708661794662476\n",
      "Epoch: 24, Samples: 1440/5760, Loss: 0.01557779312133789\n",
      "Epoch: 24, Samples: 1472/5760, Loss: 0.017636120319366455\n",
      "Epoch: 24, Samples: 1504/5760, Loss: 0.03101480007171631\n",
      "Epoch: 24, Samples: 1536/5760, Loss: 0.02006441354751587\n",
      "Epoch: 24, Samples: 1568/5760, Loss: 0.017431527376174927\n",
      "Epoch: 24, Samples: 1600/5760, Loss: 0.03357718884944916\n",
      "Epoch: 24, Samples: 1632/5760, Loss: 0.021069690585136414\n",
      "Epoch: 24, Samples: 1664/5760, Loss: 0.013875842094421387\n",
      "Epoch: 24, Samples: 1696/5760, Loss: 0.07198436558246613\n",
      "Epoch: 24, Samples: 1728/5760, Loss: 0.02661001682281494\n",
      "Epoch: 24, Samples: 1760/5760, Loss: 0.015314936637878418\n",
      "Epoch: 24, Samples: 1792/5760, Loss: 0.05104546248912811\n",
      "Epoch: 24, Samples: 1824/5760, Loss: 0.01693195104598999\n",
      "Epoch: 24, Samples: 1856/5760, Loss: 0.01876884698867798\n",
      "Epoch: 24, Samples: 1888/5760, Loss: 0.031205415725708008\n",
      "Epoch: 24, Samples: 1920/5760, Loss: 0.015011399984359741\n",
      "Epoch: 24, Samples: 1952/5760, Loss: 0.014400660991668701\n",
      "Epoch: 24, Samples: 1984/5760, Loss: 0.024166464805603027\n",
      "Epoch: 24, Samples: 2016/5760, Loss: 0.02824503183364868\n",
      "Epoch: 24, Samples: 2048/5760, Loss: 0.03292739391326904\n",
      "Epoch: 24, Samples: 2080/5760, Loss: 0.02406415343284607\n",
      "Epoch: 24, Samples: 2112/5760, Loss: 0.029652953147888184\n",
      "Epoch: 24, Samples: 2144/5760, Loss: 0.027577102184295654\n",
      "Epoch: 24, Samples: 2176/5760, Loss: 0.016560614109039307\n",
      "Epoch: 24, Samples: 2208/5760, Loss: 0.029376685619354248\n",
      "Epoch: 24, Samples: 2240/5760, Loss: 0.02361699938774109\n",
      "Epoch: 24, Samples: 2272/5760, Loss: 0.02063855528831482\n",
      "Epoch: 24, Samples: 2304/5760, Loss: 0.03132832050323486\n",
      "Epoch: 24, Samples: 2336/5760, Loss: 0.015741825103759766\n",
      "Epoch: 24, Samples: 2368/5760, Loss: 0.02333107590675354\n",
      "Epoch: 24, Samples: 2400/5760, Loss: 0.01697489619255066\n",
      "Epoch: 24, Samples: 2432/5760, Loss: 0.022695496678352356\n",
      "Epoch: 24, Samples: 2464/5760, Loss: 0.020401179790496826\n",
      "Epoch: 24, Samples: 2496/5760, Loss: 0.040166258811950684\n",
      "Epoch: 24, Samples: 2528/5760, Loss: 0.031020477414131165\n",
      "Epoch: 24, Samples: 2560/5760, Loss: 0.02641788125038147\n",
      "Epoch: 24, Samples: 2592/5760, Loss: 0.03362533450126648\n",
      "Epoch: 24, Samples: 2624/5760, Loss: 0.03860771656036377\n",
      "Epoch: 24, Samples: 2656/5760, Loss: 0.01125362515449524\n",
      "Epoch: 24, Samples: 2688/5760, Loss: 0.023325800895690918\n",
      "Epoch: 24, Samples: 2720/5760, Loss: 0.01958128809928894\n",
      "Epoch: 24, Samples: 2752/5760, Loss: 0.023939192295074463\n",
      "Epoch: 24, Samples: 2784/5760, Loss: 0.02913866937160492\n",
      "Epoch: 24, Samples: 2816/5760, Loss: 0.037086278200149536\n",
      "Epoch: 24, Samples: 2848/5760, Loss: 0.03933776915073395\n",
      "Epoch: 24, Samples: 2880/5760, Loss: 0.027907878160476685\n",
      "Epoch: 24, Samples: 2912/5760, Loss: 0.01908630132675171\n",
      "Epoch: 24, Samples: 2944/5760, Loss: 0.024843841791152954\n",
      "Epoch: 24, Samples: 2976/5760, Loss: 0.024277031421661377\n",
      "Epoch: 24, Samples: 3008/5760, Loss: 0.01327243447303772\n",
      "Epoch: 24, Samples: 3040/5760, Loss: 0.019288599491119385\n",
      "Epoch: 24, Samples: 3072/5760, Loss: 0.016341984272003174\n",
      "Epoch: 24, Samples: 3104/5760, Loss: 0.01696494221687317\n",
      "Epoch: 24, Samples: 3136/5760, Loss: 0.04530733823776245\n",
      "Epoch: 24, Samples: 3168/5760, Loss: 0.017024844884872437\n",
      "Epoch: 24, Samples: 3200/5760, Loss: 0.02374476194381714\n",
      "Epoch: 24, Samples: 3232/5760, Loss: 0.012733221054077148\n",
      "Epoch: 24, Samples: 3264/5760, Loss: 0.02292686700820923\n",
      "Epoch: 24, Samples: 3296/5760, Loss: 0.02382606267929077\n",
      "Epoch: 24, Samples: 3328/5760, Loss: 0.028528183698654175\n",
      "Epoch: 24, Samples: 3360/5760, Loss: 0.036854058504104614\n",
      "Epoch: 24, Samples: 3392/5760, Loss: 0.019472062587738037\n",
      "Epoch: 24, Samples: 3424/5760, Loss: 0.018999487161636353\n",
      "Epoch: 24, Samples: 3456/5760, Loss: 0.019391804933547974\n",
      "Epoch: 24, Samples: 3488/5760, Loss: 0.032141804695129395\n",
      "Epoch: 24, Samples: 3520/5760, Loss: 0.03496792912483215\n",
      "Epoch: 24, Samples: 3552/5760, Loss: 0.025455981492996216\n",
      "Epoch: 24, Samples: 3584/5760, Loss: 0.02834203839302063\n",
      "Epoch: 24, Samples: 3616/5760, Loss: 0.024434268474578857\n",
      "Epoch: 24, Samples: 3648/5760, Loss: 0.036811038851737976\n",
      "Epoch: 24, Samples: 3680/5760, Loss: 0.0172121524810791\n",
      "Epoch: 24, Samples: 3712/5760, Loss: 0.029218196868896484\n",
      "Epoch: 24, Samples: 3744/5760, Loss: 0.015433967113494873\n",
      "Epoch: 24, Samples: 3776/5760, Loss: 0.014360636472702026\n",
      "Epoch: 24, Samples: 3808/5760, Loss: 0.020261406898498535\n",
      "Epoch: 24, Samples: 3840/5760, Loss: 0.022962242364883423\n",
      "Epoch: 24, Samples: 3872/5760, Loss: 0.016888052225112915\n",
      "Epoch: 24, Samples: 3904/5760, Loss: 0.029809117317199707\n",
      "Epoch: 24, Samples: 3936/5760, Loss: 0.01408451795578003\n",
      "Epoch: 24, Samples: 3968/5760, Loss: 0.01910400390625\n",
      "Epoch: 24, Samples: 4000/5760, Loss: 0.015121757984161377\n",
      "Epoch: 24, Samples: 4032/5760, Loss: 0.029514014720916748\n",
      "Epoch: 24, Samples: 4064/5760, Loss: 0.019504129886627197\n",
      "Epoch: 24, Samples: 4096/5760, Loss: 0.030453383922576904\n",
      "Epoch: 24, Samples: 4128/5760, Loss: 0.04511471092700958\n",
      "Epoch: 24, Samples: 4160/5760, Loss: 0.02856464684009552\n",
      "Epoch: 24, Samples: 4192/5760, Loss: 0.02826935052871704\n",
      "Epoch: 24, Samples: 4224/5760, Loss: 0.034423649311065674\n",
      "Epoch: 24, Samples: 4256/5760, Loss: 0.0241682231426239\n",
      "Epoch: 24, Samples: 4288/5760, Loss: 0.017290085554122925\n",
      "Epoch: 24, Samples: 4320/5760, Loss: 0.024865388870239258\n",
      "Epoch: 24, Samples: 4352/5760, Loss: 0.024703994393348694\n",
      "Epoch: 24, Samples: 4384/5760, Loss: 0.019378751516342163\n",
      "Epoch: 24, Samples: 4416/5760, Loss: 0.013180762529373169\n",
      "Epoch: 24, Samples: 4448/5760, Loss: 0.019141018390655518\n",
      "Epoch: 24, Samples: 4480/5760, Loss: 0.03320559859275818\n",
      "Epoch: 24, Samples: 4512/5760, Loss: 0.021245554089546204\n",
      "Epoch: 24, Samples: 4544/5760, Loss: 0.03221234679222107\n",
      "Epoch: 24, Samples: 4576/5760, Loss: 0.01036214828491211\n",
      "Epoch: 24, Samples: 4608/5760, Loss: 0.015379875898361206\n",
      "Epoch: 24, Samples: 4640/5760, Loss: 0.031863629817962646\n",
      "Epoch: 24, Samples: 4672/5760, Loss: 0.034010306000709534\n",
      "Epoch: 24, Samples: 4704/5760, Loss: 0.03657501935958862\n",
      "Epoch: 24, Samples: 4736/5760, Loss: 0.03068402409553528\n",
      "Epoch: 24, Samples: 4768/5760, Loss: 0.027949795126914978\n",
      "Epoch: 24, Samples: 4800/5760, Loss: 0.05820947885513306\n",
      "Epoch: 24, Samples: 4832/5760, Loss: 0.05608607828617096\n",
      "Epoch: 24, Samples: 4864/5760, Loss: 0.026475578546524048\n",
      "Epoch: 24, Samples: 4896/5760, Loss: 0.009288877248764038\n",
      "Epoch: 24, Samples: 4928/5760, Loss: 0.029142320156097412\n",
      "Epoch: 24, Samples: 4960/5760, Loss: 0.07374221086502075\n",
      "Epoch: 24, Samples: 4992/5760, Loss: 0.03280743956565857\n",
      "Epoch: 24, Samples: 5024/5760, Loss: 0.03660900890827179\n",
      "Epoch: 24, Samples: 5056/5760, Loss: 0.024920552968978882\n",
      "Epoch: 24, Samples: 5088/5760, Loss: 0.016297906637191772\n",
      "Epoch: 24, Samples: 5120/5760, Loss: 0.022910356521606445\n",
      "Epoch: 24, Samples: 5152/5760, Loss: 0.024315059185028076\n",
      "Epoch: 24, Samples: 5184/5760, Loss: 0.019092977046966553\n",
      "Epoch: 24, Samples: 5216/5760, Loss: 0.02515503764152527\n",
      "Epoch: 24, Samples: 5248/5760, Loss: 0.020371556282043457\n",
      "Epoch: 24, Samples: 5280/5760, Loss: 0.01768261194229126\n",
      "Epoch: 24, Samples: 5312/5760, Loss: 0.01935809850692749\n",
      "Epoch: 24, Samples: 5344/5760, Loss: 0.018671929836273193\n",
      "Epoch: 24, Samples: 5376/5760, Loss: 0.024861276149749756\n",
      "Epoch: 24, Samples: 5408/5760, Loss: 0.01793038845062256\n",
      "Epoch: 24, Samples: 5440/5760, Loss: 0.015367090702056885\n",
      "Epoch: 24, Samples: 5472/5760, Loss: 0.03233996033668518\n",
      "Epoch: 24, Samples: 5504/5760, Loss: 0.014947593212127686\n",
      "Epoch: 24, Samples: 5536/5760, Loss: 0.018046677112579346\n",
      "Epoch: 24, Samples: 5568/5760, Loss: 0.015034586191177368\n",
      "Epoch: 24, Samples: 5600/5760, Loss: 0.01902526617050171\n",
      "Epoch: 24, Samples: 5632/5760, Loss: 0.017272740602493286\n",
      "Epoch: 24, Samples: 5664/5760, Loss: 0.01752781867980957\n",
      "Epoch: 24, Samples: 5696/5760, Loss: 0.02136138081550598\n",
      "Epoch: 24, Samples: 5728/5760, Loss: 0.22694134712219238\n",
      "\n",
      "Epoch: 24\n",
      "Training set: Average loss: 0.0265\n",
      "Validation set: Average loss: 0.3542, Accuracy: 745/818 (91%)\n",
      "Epoch: 25, Samples: 0/5760, Loss: 0.014211148023605347\n",
      "Epoch: 25, Samples: 32/5760, Loss: 0.025023311376571655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Samples: 64/5760, Loss: 0.02311977744102478\n",
      "Epoch: 25, Samples: 96/5760, Loss: 0.026755332946777344\n",
      "Epoch: 25, Samples: 128/5760, Loss: 0.01801547408103943\n",
      "Epoch: 25, Samples: 160/5760, Loss: 0.02017185091972351\n",
      "Epoch: 25, Samples: 192/5760, Loss: 0.026081383228302002\n",
      "Epoch: 25, Samples: 224/5760, Loss: 0.01865258812904358\n",
      "Epoch: 25, Samples: 256/5760, Loss: 0.04496239125728607\n",
      "Epoch: 25, Samples: 288/5760, Loss: 0.012622714042663574\n",
      "Epoch: 25, Samples: 320/5760, Loss: 0.03113994002342224\n",
      "Epoch: 25, Samples: 352/5760, Loss: 0.027246296405792236\n",
      "Epoch: 25, Samples: 384/5760, Loss: 0.023413240909576416\n",
      "Epoch: 25, Samples: 416/5760, Loss: 0.028673231601715088\n",
      "Epoch: 25, Samples: 448/5760, Loss: 0.019561678171157837\n",
      "Epoch: 25, Samples: 480/5760, Loss: 0.030937790870666504\n",
      "Epoch: 25, Samples: 512/5760, Loss: 0.01751229166984558\n",
      "Epoch: 25, Samples: 544/5760, Loss: 0.038854554295539856\n",
      "Epoch: 25, Samples: 576/5760, Loss: 0.01504373550415039\n",
      "Epoch: 25, Samples: 608/5760, Loss: 0.027239695191383362\n",
      "Epoch: 25, Samples: 640/5760, Loss: 0.038691937923431396\n",
      "Epoch: 25, Samples: 672/5760, Loss: 0.07069173455238342\n",
      "Epoch: 25, Samples: 704/5760, Loss: 0.016284644603729248\n",
      "Epoch: 25, Samples: 736/5760, Loss: 0.027122527360916138\n",
      "Epoch: 25, Samples: 768/5760, Loss: 0.02474844455718994\n",
      "Epoch: 25, Samples: 800/5760, Loss: 0.011671483516693115\n",
      "Epoch: 25, Samples: 832/5760, Loss: 0.01991412043571472\n",
      "Epoch: 25, Samples: 864/5760, Loss: 0.017869949340820312\n",
      "Epoch: 25, Samples: 896/5760, Loss: 0.0261152982711792\n",
      "Epoch: 25, Samples: 928/5760, Loss: 0.019857048988342285\n",
      "Epoch: 25, Samples: 960/5760, Loss: 0.015394061803817749\n",
      "Epoch: 25, Samples: 992/5760, Loss: 0.02745753526687622\n",
      "Epoch: 25, Samples: 1024/5760, Loss: 0.0177442729473114\n",
      "Epoch: 25, Samples: 1056/5760, Loss: 0.017535120248794556\n",
      "Epoch: 25, Samples: 1088/5760, Loss: 0.03392215073108673\n",
      "Epoch: 25, Samples: 1120/5760, Loss: 0.016263514757156372\n",
      "Epoch: 25, Samples: 1152/5760, Loss: 0.013744831085205078\n",
      "Epoch: 25, Samples: 1184/5760, Loss: 0.02293771505355835\n",
      "Epoch: 25, Samples: 1216/5760, Loss: 0.020770549774169922\n",
      "Epoch: 25, Samples: 1248/5760, Loss: 0.018169164657592773\n",
      "Epoch: 25, Samples: 1280/5760, Loss: 0.027847319841384888\n",
      "Epoch: 25, Samples: 1312/5760, Loss: 0.025959491729736328\n",
      "Epoch: 25, Samples: 1344/5760, Loss: 0.03185509145259857\n",
      "Epoch: 25, Samples: 1376/5760, Loss: 0.021449685096740723\n",
      "Epoch: 25, Samples: 1408/5760, Loss: 0.02274024486541748\n",
      "Epoch: 25, Samples: 1440/5760, Loss: 0.011123746633529663\n",
      "Epoch: 25, Samples: 1472/5760, Loss: 0.014288127422332764\n",
      "Epoch: 25, Samples: 1504/5760, Loss: 0.026594310998916626\n",
      "Epoch: 25, Samples: 1536/5760, Loss: 0.040847644209861755\n",
      "Epoch: 25, Samples: 1568/5760, Loss: 0.03379184007644653\n",
      "Epoch: 25, Samples: 1600/5760, Loss: 0.02553647756576538\n",
      "Epoch: 25, Samples: 1632/5760, Loss: 0.018793433904647827\n",
      "Epoch: 25, Samples: 1664/5760, Loss: 0.0215894877910614\n",
      "Epoch: 25, Samples: 1696/5760, Loss: 0.016152411699295044\n",
      "Epoch: 25, Samples: 1728/5760, Loss: 0.02159610390663147\n",
      "Epoch: 25, Samples: 1760/5760, Loss: 0.03530341386795044\n",
      "Epoch: 25, Samples: 1792/5760, Loss: 0.02334737777709961\n",
      "Epoch: 25, Samples: 1824/5760, Loss: 0.021060049533843994\n",
      "Epoch: 25, Samples: 1856/5760, Loss: 0.013788431882858276\n",
      "Epoch: 25, Samples: 1888/5760, Loss: 0.019027799367904663\n",
      "Epoch: 25, Samples: 1920/5760, Loss: 0.02328115701675415\n",
      "Epoch: 25, Samples: 1952/5760, Loss: 0.014636009931564331\n",
      "Epoch: 25, Samples: 1984/5760, Loss: 0.021323412656784058\n",
      "Epoch: 25, Samples: 2016/5760, Loss: 0.02097606658935547\n",
      "Epoch: 25, Samples: 2048/5760, Loss: 0.016495585441589355\n",
      "Epoch: 25, Samples: 2080/5760, Loss: 0.020950496196746826\n",
      "Epoch: 25, Samples: 2112/5760, Loss: 0.026072710752487183\n",
      "Epoch: 25, Samples: 2144/5760, Loss: 0.02105054259300232\n",
      "Epoch: 25, Samples: 2176/5760, Loss: 0.018247276544570923\n",
      "Epoch: 25, Samples: 2208/5760, Loss: 0.012267142534255981\n",
      "Epoch: 25, Samples: 2240/5760, Loss: 0.02230265736579895\n",
      "Epoch: 25, Samples: 2272/5760, Loss: 0.016372650861740112\n",
      "Epoch: 25, Samples: 2304/5760, Loss: 0.030099838972091675\n",
      "Epoch: 25, Samples: 2336/5760, Loss: 0.019183427095413208\n",
      "Epoch: 25, Samples: 2368/5760, Loss: 0.019980788230895996\n",
      "Epoch: 25, Samples: 2400/5760, Loss: 0.026956558227539062\n",
      "Epoch: 25, Samples: 2432/5760, Loss: 0.015466421842575073\n",
      "Epoch: 25, Samples: 2464/5760, Loss: 0.028753578662872314\n",
      "Epoch: 25, Samples: 2496/5760, Loss: 0.02856484055519104\n",
      "Epoch: 25, Samples: 2528/5760, Loss: 0.031582772731781006\n",
      "Epoch: 25, Samples: 2560/5760, Loss: 0.023532181978225708\n",
      "Epoch: 25, Samples: 2592/5760, Loss: 0.021764755249023438\n",
      "Epoch: 25, Samples: 2624/5760, Loss: 0.012916892766952515\n",
      "Epoch: 25, Samples: 2656/5760, Loss: 0.016237646341323853\n",
      "Epoch: 25, Samples: 2688/5760, Loss: 0.035296112298965454\n",
      "Epoch: 25, Samples: 2720/5760, Loss: 0.03388833999633789\n",
      "Epoch: 25, Samples: 2752/5760, Loss: 0.02208244800567627\n",
      "Epoch: 25, Samples: 2784/5760, Loss: 0.03370058536529541\n",
      "Epoch: 25, Samples: 2816/5760, Loss: 0.024706125259399414\n",
      "Epoch: 25, Samples: 2848/5760, Loss: 0.03689002990722656\n",
      "Epoch: 25, Samples: 2880/5760, Loss: 0.016728848218917847\n",
      "Epoch: 25, Samples: 2912/5760, Loss: 0.01774469017982483\n",
      "Epoch: 25, Samples: 2944/5760, Loss: 0.02482697367668152\n",
      "Epoch: 25, Samples: 2976/5760, Loss: 0.029648050665855408\n",
      "Epoch: 25, Samples: 3008/5760, Loss: 0.025187045335769653\n",
      "Epoch: 25, Samples: 3040/5760, Loss: 0.02038714289665222\n",
      "Epoch: 25, Samples: 3072/5760, Loss: 0.04607841372489929\n",
      "Epoch: 25, Samples: 3104/5760, Loss: 0.014891117811203003\n",
      "Epoch: 25, Samples: 3136/5760, Loss: 0.023370206356048584\n",
      "Epoch: 25, Samples: 3168/5760, Loss: 0.05537740886211395\n",
      "Epoch: 25, Samples: 3200/5760, Loss: 0.030575871467590332\n",
      "Epoch: 25, Samples: 3232/5760, Loss: 0.020565032958984375\n",
      "Epoch: 25, Samples: 3264/5760, Loss: 0.016764700412750244\n",
      "Epoch: 25, Samples: 3296/5760, Loss: 0.019822627305984497\n",
      "Epoch: 25, Samples: 3328/5760, Loss: 0.024940431118011475\n",
      "Epoch: 25, Samples: 3360/5760, Loss: 0.028320282697677612\n",
      "Epoch: 25, Samples: 3392/5760, Loss: 0.033976584672927856\n",
      "Epoch: 25, Samples: 3424/5760, Loss: 0.018137484788894653\n",
      "Epoch: 25, Samples: 3456/5760, Loss: 0.02168920636177063\n",
      "Epoch: 25, Samples: 3488/5760, Loss: 0.021964281797409058\n",
      "Epoch: 25, Samples: 3520/5760, Loss: 0.017168760299682617\n",
      "Epoch: 25, Samples: 3552/5760, Loss: 0.024488389492034912\n",
      "Epoch: 25, Samples: 3584/5760, Loss: 0.03617900609970093\n",
      "Epoch: 25, Samples: 3616/5760, Loss: 0.02877773344516754\n",
      "Epoch: 25, Samples: 3648/5760, Loss: 0.03231152892112732\n",
      "Epoch: 25, Samples: 3680/5760, Loss: 0.04580879211425781\n",
      "Epoch: 25, Samples: 3712/5760, Loss: 0.027454808354377747\n",
      "Epoch: 25, Samples: 3744/5760, Loss: 0.020774662494659424\n",
      "Epoch: 25, Samples: 3776/5760, Loss: 0.036010608077049255\n",
      "Epoch: 25, Samples: 3808/5760, Loss: 0.0324983149766922\n",
      "Epoch: 25, Samples: 3840/5760, Loss: 0.02450200915336609\n",
      "Epoch: 25, Samples: 3872/5760, Loss: 0.015522778034210205\n",
      "Epoch: 25, Samples: 3904/5760, Loss: 0.016568750143051147\n",
      "Epoch: 25, Samples: 3936/5760, Loss: 0.021545618772506714\n",
      "Epoch: 25, Samples: 3968/5760, Loss: 0.020618349313735962\n",
      "Epoch: 25, Samples: 4000/5760, Loss: 0.040146589279174805\n",
      "Epoch: 25, Samples: 4032/5760, Loss: 0.012787163257598877\n",
      "Epoch: 25, Samples: 4064/5760, Loss: 0.019417107105255127\n",
      "Epoch: 25, Samples: 4096/5760, Loss: 0.021120190620422363\n",
      "Epoch: 25, Samples: 4128/5760, Loss: 0.016125082969665527\n",
      "Epoch: 25, Samples: 4160/5760, Loss: 0.026495754718780518\n",
      "Epoch: 25, Samples: 4192/5760, Loss: 0.011800259351730347\n",
      "Epoch: 25, Samples: 4224/5760, Loss: 0.05355481803417206\n",
      "Epoch: 25, Samples: 4256/5760, Loss: 0.021718323230743408\n",
      "Epoch: 25, Samples: 4288/5760, Loss: 0.014633327722549438\n",
      "Epoch: 25, Samples: 4320/5760, Loss: 0.012043088674545288\n",
      "Epoch: 25, Samples: 4352/5760, Loss: 0.009654909372329712\n",
      "Epoch: 25, Samples: 4384/5760, Loss: 0.019948691129684448\n",
      "Epoch: 25, Samples: 4416/5760, Loss: 0.02374330163002014\n",
      "Epoch: 25, Samples: 4448/5760, Loss: 0.02248436212539673\n",
      "Epoch: 25, Samples: 4480/5760, Loss: 0.02589118480682373\n",
      "Epoch: 25, Samples: 4512/5760, Loss: 0.025359943509101868\n",
      "Epoch: 25, Samples: 4544/5760, Loss: 0.014768362045288086\n",
      "Epoch: 25, Samples: 4576/5760, Loss: 0.02131292223930359\n",
      "Epoch: 25, Samples: 4608/5760, Loss: 0.025550544261932373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Samples: 4640/5760, Loss: 0.020002901554107666\n",
      "Epoch: 25, Samples: 4672/5760, Loss: 0.02240997552871704\n",
      "Epoch: 25, Samples: 4704/5760, Loss: 0.016898423433303833\n",
      "Epoch: 25, Samples: 4736/5760, Loss: 0.028932154178619385\n",
      "Epoch: 25, Samples: 4768/5760, Loss: 0.020553842186927795\n",
      "Epoch: 25, Samples: 4800/5760, Loss: 0.015321671962738037\n",
      "Epoch: 25, Samples: 4832/5760, Loss: 0.025782883167266846\n",
      "Epoch: 25, Samples: 4864/5760, Loss: 0.04770690202713013\n",
      "Epoch: 25, Samples: 4896/5760, Loss: 0.012623846530914307\n",
      "Epoch: 25, Samples: 4928/5760, Loss: 0.024726390838623047\n",
      "Epoch: 25, Samples: 4960/5760, Loss: 0.014734208583831787\n",
      "Epoch: 25, Samples: 4992/5760, Loss: 0.01808875799179077\n",
      "Epoch: 25, Samples: 5024/5760, Loss: 0.018382996320724487\n",
      "Epoch: 25, Samples: 5056/5760, Loss: 0.020791053771972656\n",
      "Epoch: 25, Samples: 5088/5760, Loss: 0.012363195419311523\n",
      "Epoch: 25, Samples: 5120/5760, Loss: 0.030624523758888245\n",
      "Epoch: 25, Samples: 5152/5760, Loss: 0.0170803964138031\n",
      "Epoch: 25, Samples: 5184/5760, Loss: 0.022167950868606567\n",
      "Epoch: 25, Samples: 5216/5760, Loss: 0.01541745662689209\n",
      "Epoch: 25, Samples: 5248/5760, Loss: 0.043567657470703125\n",
      "Epoch: 25, Samples: 5280/5760, Loss: 0.037086158990859985\n",
      "Epoch: 25, Samples: 5312/5760, Loss: 0.031619638204574585\n",
      "Epoch: 25, Samples: 5344/5760, Loss: 0.012751162052154541\n",
      "Epoch: 25, Samples: 5376/5760, Loss: 0.01761329174041748\n",
      "Epoch: 25, Samples: 5408/5760, Loss: 0.018137425184249878\n",
      "Epoch: 25, Samples: 5440/5760, Loss: 0.012657076120376587\n",
      "Epoch: 25, Samples: 5472/5760, Loss: 0.018030285835266113\n",
      "Epoch: 25, Samples: 5504/5760, Loss: 0.04209381341934204\n",
      "Epoch: 25, Samples: 5536/5760, Loss: 0.015197068452835083\n",
      "Epoch: 25, Samples: 5568/5760, Loss: 0.017051279544830322\n",
      "Epoch: 25, Samples: 5600/5760, Loss: 0.022599399089813232\n",
      "Epoch: 25, Samples: 5632/5760, Loss: 0.02646365761756897\n",
      "Epoch: 25, Samples: 5664/5760, Loss: 0.020070821046829224\n",
      "Epoch: 25, Samples: 5696/5760, Loss: 0.01610437035560608\n",
      "Epoch: 25, Samples: 5728/5760, Loss: 1.0362054109573364\n",
      "\n",
      "Epoch: 25\n",
      "Training set: Average loss: 0.0293\n",
      "Validation set: Average loss: 0.3528, Accuracy: 744/818 (91%)\n",
      "Epoch: 26, Samples: 0/5760, Loss: 0.021693319082260132\n",
      "Epoch: 26, Samples: 32/5760, Loss: 0.027013301849365234\n",
      "Epoch: 26, Samples: 64/5760, Loss: 0.01641407608985901\n",
      "Epoch: 26, Samples: 96/5760, Loss: 0.018987447023391724\n",
      "Epoch: 26, Samples: 128/5760, Loss: 0.02266913652420044\n",
      "Epoch: 26, Samples: 160/5760, Loss: 0.01805129647254944\n",
      "Epoch: 26, Samples: 192/5760, Loss: 0.01661357283592224\n",
      "Epoch: 26, Samples: 224/5760, Loss: 0.024434775114059448\n",
      "Epoch: 26, Samples: 256/5760, Loss: 0.021498143672943115\n",
      "Epoch: 26, Samples: 288/5760, Loss: 0.024219661951065063\n",
      "Epoch: 26, Samples: 320/5760, Loss: 0.02852928638458252\n",
      "Epoch: 26, Samples: 352/5760, Loss: 0.03034225106239319\n",
      "Epoch: 26, Samples: 384/5760, Loss: 0.02764788269996643\n",
      "Epoch: 26, Samples: 416/5760, Loss: 0.016670286655426025\n",
      "Epoch: 26, Samples: 448/5760, Loss: 0.016931861639022827\n",
      "Epoch: 26, Samples: 480/5760, Loss: 0.02060854434967041\n",
      "Epoch: 26, Samples: 512/5760, Loss: 0.010282188653945923\n",
      "Epoch: 26, Samples: 544/5760, Loss: 0.01549580693244934\n",
      "Epoch: 26, Samples: 576/5760, Loss: 0.014443248510360718\n",
      "Epoch: 26, Samples: 608/5760, Loss: 0.014552414417266846\n",
      "Epoch: 26, Samples: 640/5760, Loss: 0.03052002191543579\n",
      "Epoch: 26, Samples: 672/5760, Loss: 0.024705499410629272\n",
      "Epoch: 26, Samples: 704/5760, Loss: 0.037482231855392456\n",
      "Epoch: 26, Samples: 736/5760, Loss: 0.017541706562042236\n",
      "Epoch: 26, Samples: 768/5760, Loss: 0.015604257583618164\n",
      "Epoch: 26, Samples: 800/5760, Loss: 0.029568061232566833\n",
      "Epoch: 26, Samples: 832/5760, Loss: 0.031083926558494568\n",
      "Epoch: 26, Samples: 864/5760, Loss: 0.027210712432861328\n",
      "Epoch: 26, Samples: 896/5760, Loss: 0.015034258365631104\n",
      "Epoch: 26, Samples: 928/5760, Loss: 0.015891164541244507\n",
      "Epoch: 26, Samples: 960/5760, Loss: 0.03602980077266693\n",
      "Epoch: 26, Samples: 992/5760, Loss: 0.0207844078540802\n",
      "Epoch: 26, Samples: 1024/5760, Loss: 0.01701214909553528\n",
      "Epoch: 26, Samples: 1056/5760, Loss: 0.03459326922893524\n",
      "Epoch: 26, Samples: 1088/5760, Loss: 0.007769405841827393\n",
      "Epoch: 26, Samples: 1120/5760, Loss: 0.029514163732528687\n",
      "Epoch: 26, Samples: 1152/5760, Loss: 0.016445040702819824\n",
      "Epoch: 26, Samples: 1184/5760, Loss: 0.021737873554229736\n",
      "Epoch: 26, Samples: 1216/5760, Loss: 0.02775813639163971\n",
      "Epoch: 26, Samples: 1248/5760, Loss: 0.01996472477912903\n",
      "Epoch: 26, Samples: 1280/5760, Loss: 0.023282095789909363\n",
      "Epoch: 26, Samples: 1312/5760, Loss: 0.011327952146530151\n",
      "Epoch: 26, Samples: 1344/5760, Loss: 0.027320384979248047\n",
      "Epoch: 26, Samples: 1376/5760, Loss: 0.03359496593475342\n",
      "Epoch: 26, Samples: 1408/5760, Loss: 0.01738789677619934\n",
      "Epoch: 26, Samples: 1440/5760, Loss: 0.018807291984558105\n",
      "Epoch: 26, Samples: 1472/5760, Loss: 0.019622772932052612\n",
      "Epoch: 26, Samples: 1504/5760, Loss: 0.013904005289077759\n",
      "Epoch: 26, Samples: 1536/5760, Loss: 0.01858299970626831\n",
      "Epoch: 26, Samples: 1568/5760, Loss: 0.027624696493148804\n",
      "Epoch: 26, Samples: 1600/5760, Loss: 0.03145623207092285\n",
      "Epoch: 26, Samples: 1632/5760, Loss: 0.02394711971282959\n",
      "Epoch: 26, Samples: 1664/5760, Loss: 0.02364569902420044\n",
      "Epoch: 26, Samples: 1696/5760, Loss: 0.025332599878311157\n",
      "Epoch: 26, Samples: 1728/5760, Loss: 0.01777857542037964\n",
      "Epoch: 26, Samples: 1760/5760, Loss: 0.022678494453430176\n",
      "Epoch: 26, Samples: 1792/5760, Loss: 0.021158069372177124\n",
      "Epoch: 26, Samples: 1824/5760, Loss: 0.020840048789978027\n",
      "Epoch: 26, Samples: 1856/5760, Loss: 0.01965591311454773\n",
      "Epoch: 26, Samples: 1888/5760, Loss: 0.01665148138999939\n",
      "Epoch: 26, Samples: 1920/5760, Loss: 0.009219050407409668\n",
      "Epoch: 26, Samples: 1952/5760, Loss: 0.0298251211643219\n",
      "Epoch: 26, Samples: 1984/5760, Loss: 0.02429330348968506\n",
      "Epoch: 26, Samples: 2016/5760, Loss: 0.03019091486930847\n",
      "Epoch: 26, Samples: 2048/5760, Loss: 0.03818678855895996\n",
      "Epoch: 26, Samples: 2080/5760, Loss: 0.016585439443588257\n",
      "Epoch: 26, Samples: 2112/5760, Loss: 0.02629554271697998\n",
      "Epoch: 26, Samples: 2144/5760, Loss: 0.01701909303665161\n",
      "Epoch: 26, Samples: 2176/5760, Loss: 0.018784373998641968\n",
      "Epoch: 26, Samples: 2208/5760, Loss: 0.017123788595199585\n",
      "Epoch: 26, Samples: 2240/5760, Loss: 0.024986237287521362\n",
      "Epoch: 26, Samples: 2272/5760, Loss: 0.02749386429786682\n",
      "Epoch: 26, Samples: 2304/5760, Loss: 0.01213720440864563\n",
      "Epoch: 26, Samples: 2336/5760, Loss: 0.016849905252456665\n",
      "Epoch: 26, Samples: 2368/5760, Loss: 0.024062633514404297\n",
      "Epoch: 26, Samples: 2400/5760, Loss: 0.04189777374267578\n",
      "Epoch: 26, Samples: 2432/5760, Loss: 0.026125311851501465\n",
      "Epoch: 26, Samples: 2464/5760, Loss: 0.08077937364578247\n",
      "Epoch: 26, Samples: 2496/5760, Loss: 0.023310363292694092\n",
      "Epoch: 26, Samples: 2528/5760, Loss: 0.010814964771270752\n",
      "Epoch: 26, Samples: 2560/5760, Loss: 0.029206812381744385\n",
      "Epoch: 26, Samples: 2592/5760, Loss: 0.01587352156639099\n",
      "Epoch: 26, Samples: 2624/5760, Loss: 0.01506417989730835\n",
      "Epoch: 26, Samples: 2656/5760, Loss: 0.02529217302799225\n",
      "Epoch: 26, Samples: 2688/5760, Loss: 0.01671317219734192\n",
      "Epoch: 26, Samples: 2720/5760, Loss: 0.02649754285812378\n",
      "Epoch: 26, Samples: 2752/5760, Loss: 0.03830716013908386\n",
      "Epoch: 26, Samples: 2784/5760, Loss: 0.01900830864906311\n",
      "Epoch: 26, Samples: 2816/5760, Loss: 0.02467268705368042\n",
      "Epoch: 26, Samples: 2848/5760, Loss: 0.019221097230911255\n",
      "Epoch: 26, Samples: 2880/5760, Loss: 0.015632688999176025\n",
      "Epoch: 26, Samples: 2912/5760, Loss: 0.016797184944152832\n",
      "Epoch: 26, Samples: 2944/5760, Loss: 0.017668068408966064\n",
      "Epoch: 26, Samples: 2976/5760, Loss: 0.02490118145942688\n",
      "Epoch: 26, Samples: 3008/5760, Loss: 0.018015295267105103\n",
      "Epoch: 26, Samples: 3040/5760, Loss: 0.031042546033859253\n",
      "Epoch: 26, Samples: 3072/5760, Loss: 0.01980939507484436\n",
      "Epoch: 26, Samples: 3104/5760, Loss: 0.0208246111869812\n",
      "Epoch: 26, Samples: 3136/5760, Loss: 0.018836289644241333\n",
      "Epoch: 26, Samples: 3168/5760, Loss: 0.021602988243103027\n",
      "Epoch: 26, Samples: 3200/5760, Loss: 0.023337408900260925\n",
      "Epoch: 26, Samples: 3232/5760, Loss: 0.03501741588115692\n",
      "Epoch: 26, Samples: 3264/5760, Loss: 0.028176158666610718\n",
      "Epoch: 26, Samples: 3296/5760, Loss: 0.026601165533065796\n",
      "Epoch: 26, Samples: 3328/5760, Loss: 0.026187121868133545\n",
      "Epoch: 26, Samples: 3360/5760, Loss: 0.017854154109954834\n",
      "Epoch: 26, Samples: 3392/5760, Loss: 0.03329780697822571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Samples: 3424/5760, Loss: 0.033089011907577515\n",
      "Epoch: 26, Samples: 3456/5760, Loss: 0.01677611470222473\n",
      "Epoch: 26, Samples: 3488/5760, Loss: 0.037887006998062134\n",
      "Epoch: 26, Samples: 3520/5760, Loss: 0.02081584930419922\n",
      "Epoch: 26, Samples: 3552/5760, Loss: 0.02799321711063385\n",
      "Epoch: 26, Samples: 3584/5760, Loss: 0.040339693427085876\n",
      "Epoch: 26, Samples: 3616/5760, Loss: 0.01850336790084839\n",
      "Epoch: 26, Samples: 3648/5760, Loss: 0.023430317640304565\n",
      "Epoch: 26, Samples: 3680/5760, Loss: 0.03082279860973358\n",
      "Epoch: 26, Samples: 3712/5760, Loss: 0.024282991886138916\n",
      "Epoch: 26, Samples: 3744/5760, Loss: 0.013401657342910767\n",
      "Epoch: 26, Samples: 3776/5760, Loss: 0.014428108930587769\n",
      "Epoch: 26, Samples: 3808/5760, Loss: 0.00983816385269165\n",
      "Epoch: 26, Samples: 3840/5760, Loss: 0.01547732949256897\n",
      "Epoch: 26, Samples: 3872/5760, Loss: 0.015394419431686401\n",
      "Epoch: 26, Samples: 3904/5760, Loss: 0.03364478051662445\n",
      "Epoch: 26, Samples: 3936/5760, Loss: 0.04752837121486664\n",
      "Epoch: 26, Samples: 3968/5760, Loss: 0.024222731590270996\n",
      "Epoch: 26, Samples: 4000/5760, Loss: 0.011977165937423706\n",
      "Epoch: 26, Samples: 4032/5760, Loss: 0.03873179852962494\n",
      "Epoch: 26, Samples: 4064/5760, Loss: 0.014599055051803589\n",
      "Epoch: 26, Samples: 4096/5760, Loss: 0.01776304841041565\n",
      "Epoch: 26, Samples: 4128/5760, Loss: 0.020857512950897217\n",
      "Epoch: 26, Samples: 4160/5760, Loss: 0.020120322704315186\n",
      "Epoch: 26, Samples: 4192/5760, Loss: 0.024358004331588745\n",
      "Epoch: 26, Samples: 4224/5760, Loss: 0.016880542039871216\n",
      "Epoch: 26, Samples: 4256/5760, Loss: 0.02992802858352661\n",
      "Epoch: 26, Samples: 4288/5760, Loss: 0.011660963296890259\n",
      "Epoch: 26, Samples: 4320/5760, Loss: 0.036755919456481934\n",
      "Epoch: 26, Samples: 4352/5760, Loss: 0.016115009784698486\n",
      "Epoch: 26, Samples: 4384/5760, Loss: 0.013800323009490967\n",
      "Epoch: 26, Samples: 4416/5760, Loss: 0.014291197061538696\n",
      "Epoch: 26, Samples: 4448/5760, Loss: 0.02694264054298401\n",
      "Epoch: 26, Samples: 4480/5760, Loss: 0.016452759504318237\n",
      "Epoch: 26, Samples: 4512/5760, Loss: 0.020016640424728394\n",
      "Epoch: 26, Samples: 4544/5760, Loss: 0.02556285262107849\n",
      "Epoch: 26, Samples: 4576/5760, Loss: 0.01109236478805542\n",
      "Epoch: 26, Samples: 4608/5760, Loss: 0.02078041434288025\n",
      "Epoch: 26, Samples: 4640/5760, Loss: 0.014000117778778076\n",
      "Epoch: 26, Samples: 4672/5760, Loss: 0.01349368691444397\n",
      "Epoch: 26, Samples: 4704/5760, Loss: 0.013989418745040894\n",
      "Epoch: 26, Samples: 4736/5760, Loss: 0.0361044704914093\n",
      "Epoch: 26, Samples: 4768/5760, Loss: 0.016614317893981934\n",
      "Epoch: 26, Samples: 4800/5760, Loss: 0.015727490186691284\n",
      "Epoch: 26, Samples: 4832/5760, Loss: 0.01616528630256653\n",
      "Epoch: 26, Samples: 4864/5760, Loss: 0.02466598153114319\n",
      "Epoch: 26, Samples: 4896/5760, Loss: 0.010317236185073853\n",
      "Epoch: 26, Samples: 4928/5760, Loss: 0.022505730390548706\n",
      "Epoch: 26, Samples: 4960/5760, Loss: 0.032235801219940186\n",
      "Epoch: 26, Samples: 4992/5760, Loss: 0.022251486778259277\n",
      "Epoch: 26, Samples: 5024/5760, Loss: 0.022913962602615356\n",
      "Epoch: 26, Samples: 5056/5760, Loss: 0.028613761067390442\n",
      "Epoch: 26, Samples: 5088/5760, Loss: 0.035814255475997925\n",
      "Epoch: 26, Samples: 5120/5760, Loss: 0.027746260166168213\n",
      "Epoch: 26, Samples: 5152/5760, Loss: 0.0132293701171875\n",
      "Epoch: 26, Samples: 5184/5760, Loss: 0.034822821617126465\n",
      "Epoch: 26, Samples: 5216/5760, Loss: 0.01936022937297821\n",
      "Epoch: 26, Samples: 5248/5760, Loss: 0.038483500480651855\n",
      "Epoch: 26, Samples: 5280/5760, Loss: 0.01716768741607666\n",
      "Epoch: 26, Samples: 5312/5760, Loss: 0.01711222529411316\n",
      "Epoch: 26, Samples: 5344/5760, Loss: 0.03293953835964203\n",
      "Epoch: 26, Samples: 5376/5760, Loss: 0.02556431293487549\n",
      "Epoch: 26, Samples: 5408/5760, Loss: 0.02232608199119568\n",
      "Epoch: 26, Samples: 5440/5760, Loss: 0.01286172866821289\n",
      "Epoch: 26, Samples: 5472/5760, Loss: 0.018997132778167725\n",
      "Epoch: 26, Samples: 5504/5760, Loss: 0.016197234392166138\n",
      "Epoch: 26, Samples: 5536/5760, Loss: 0.021146193146705627\n",
      "Epoch: 26, Samples: 5568/5760, Loss: 0.023013144731521606\n",
      "Epoch: 26, Samples: 5600/5760, Loss: 0.01370614767074585\n",
      "Epoch: 26, Samples: 5632/5760, Loss: 0.026809722185134888\n",
      "Epoch: 26, Samples: 5664/5760, Loss: 0.02877478301525116\n",
      "Epoch: 26, Samples: 5696/5760, Loss: 0.030943065881729126\n",
      "Epoch: 26, Samples: 5728/5760, Loss: 0.9373223781585693\n",
      "\n",
      "Epoch: 26\n",
      "Training set: Average loss: 0.0279\n",
      "Validation set: Average loss: 0.3436, Accuracy: 747/818 (91%)\n",
      "Saving model (epoch 26) with lowest validation loss: 0.343558420928625\n",
      "Epoch: 27, Samples: 0/5760, Loss: 0.045814380049705505\n",
      "Epoch: 27, Samples: 32/5760, Loss: 0.0130043625831604\n",
      "Epoch: 27, Samples: 64/5760, Loss: 0.026848912239074707\n",
      "Epoch: 27, Samples: 96/5760, Loss: 0.019752532243728638\n",
      "Epoch: 27, Samples: 128/5760, Loss: 0.036961331963539124\n",
      "Epoch: 27, Samples: 160/5760, Loss: 0.018229037523269653\n",
      "Epoch: 27, Samples: 192/5760, Loss: 0.01282733678817749\n",
      "Epoch: 27, Samples: 224/5760, Loss: 0.02885514497756958\n",
      "Epoch: 27, Samples: 256/5760, Loss: 0.032848477363586426\n",
      "Epoch: 27, Samples: 288/5760, Loss: 0.020320028066635132\n",
      "Epoch: 27, Samples: 320/5760, Loss: 0.026910215616226196\n",
      "Epoch: 27, Samples: 352/5760, Loss: 0.021665602922439575\n",
      "Epoch: 27, Samples: 384/5760, Loss: 0.011272132396697998\n",
      "Epoch: 27, Samples: 416/5760, Loss: 0.013863295316696167\n",
      "Epoch: 27, Samples: 448/5760, Loss: 0.018046677112579346\n",
      "Epoch: 27, Samples: 480/5760, Loss: 0.014404535293579102\n",
      "Epoch: 27, Samples: 512/5760, Loss: 0.034885525703430176\n",
      "Epoch: 27, Samples: 544/5760, Loss: 0.027180522680282593\n",
      "Epoch: 27, Samples: 576/5760, Loss: 0.039324358105659485\n",
      "Epoch: 27, Samples: 608/5760, Loss: 0.02569866180419922\n",
      "Epoch: 27, Samples: 640/5760, Loss: 0.01593366265296936\n",
      "Epoch: 27, Samples: 672/5760, Loss: 0.02470412850379944\n",
      "Epoch: 27, Samples: 704/5760, Loss: 0.017049163579940796\n",
      "Epoch: 27, Samples: 736/5760, Loss: 0.013736873865127563\n",
      "Epoch: 27, Samples: 768/5760, Loss: 0.012883543968200684\n",
      "Epoch: 27, Samples: 800/5760, Loss: 0.012156844139099121\n",
      "Epoch: 27, Samples: 832/5760, Loss: 0.020685136318206787\n",
      "Epoch: 27, Samples: 864/5760, Loss: 0.020420432090759277\n",
      "Epoch: 27, Samples: 896/5760, Loss: 0.04132932424545288\n",
      "Epoch: 27, Samples: 928/5760, Loss: 0.017011195421218872\n",
      "Epoch: 27, Samples: 960/5760, Loss: 0.025229990482330322\n",
      "Epoch: 27, Samples: 992/5760, Loss: 0.022410959005355835\n",
      "Epoch: 27, Samples: 1024/5760, Loss: 0.016521960496902466\n",
      "Epoch: 27, Samples: 1056/5760, Loss: 0.016420960426330566\n",
      "Epoch: 27, Samples: 1088/5760, Loss: 0.024054527282714844\n",
      "Epoch: 27, Samples: 1120/5760, Loss: 0.025383412837982178\n",
      "Epoch: 27, Samples: 1152/5760, Loss: 0.05233709514141083\n",
      "Epoch: 27, Samples: 1184/5760, Loss: 0.02786991000175476\n",
      "Epoch: 27, Samples: 1216/5760, Loss: 0.05881023406982422\n",
      "Epoch: 27, Samples: 1248/5760, Loss: 0.017681747674942017\n",
      "Epoch: 27, Samples: 1280/5760, Loss: 0.02279600501060486\n",
      "Epoch: 27, Samples: 1312/5760, Loss: 0.04918554425239563\n",
      "Epoch: 27, Samples: 1344/5760, Loss: 0.016757190227508545\n",
      "Epoch: 27, Samples: 1376/5760, Loss: 0.01662588119506836\n",
      "Epoch: 27, Samples: 1408/5760, Loss: 0.018085896968841553\n",
      "Epoch: 27, Samples: 1440/5760, Loss: 0.01839601993560791\n",
      "Epoch: 27, Samples: 1472/5760, Loss: 0.012000858783721924\n",
      "Epoch: 27, Samples: 1504/5760, Loss: 0.06669355928897858\n",
      "Epoch: 27, Samples: 1536/5760, Loss: 0.011354178190231323\n",
      "Epoch: 27, Samples: 1568/5760, Loss: 0.0194532573223114\n",
      "Epoch: 27, Samples: 1600/5760, Loss: 0.012416362762451172\n",
      "Epoch: 27, Samples: 1632/5760, Loss: 0.015231162309646606\n",
      "Epoch: 27, Samples: 1664/5760, Loss: 0.01919993758201599\n",
      "Epoch: 27, Samples: 1696/5760, Loss: 0.02283167839050293\n",
      "Epoch: 27, Samples: 1728/5760, Loss: 0.020464450120925903\n",
      "Epoch: 27, Samples: 1760/5760, Loss: 0.02149665355682373\n",
      "Epoch: 27, Samples: 1792/5760, Loss: 0.013760238885879517\n",
      "Epoch: 27, Samples: 1824/5760, Loss: 0.032658785581588745\n",
      "Epoch: 27, Samples: 1856/5760, Loss: 0.015291363000869751\n",
      "Epoch: 27, Samples: 1888/5760, Loss: 0.01194828748703003\n",
      "Epoch: 27, Samples: 1920/5760, Loss: 0.022521719336509705\n",
      "Epoch: 27, Samples: 1952/5760, Loss: 0.022388190031051636\n",
      "Epoch: 27, Samples: 1984/5760, Loss: 0.014863073825836182\n",
      "Epoch: 27, Samples: 2016/5760, Loss: 0.01809677481651306\n",
      "Epoch: 27, Samples: 2048/5760, Loss: 0.02843882143497467\n",
      "Epoch: 27, Samples: 2080/5760, Loss: 0.014142334461212158\n",
      "Epoch: 27, Samples: 2112/5760, Loss: 0.030901119112968445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Samples: 2144/5760, Loss: 0.016757994890213013\n",
      "Epoch: 27, Samples: 2176/5760, Loss: 0.014956742525100708\n",
      "Epoch: 27, Samples: 2208/5760, Loss: 0.02271774411201477\n",
      "Epoch: 27, Samples: 2240/5760, Loss: 0.02214646339416504\n",
      "Epoch: 27, Samples: 2272/5760, Loss: 0.011755168437957764\n",
      "Epoch: 27, Samples: 2304/5760, Loss: 0.02498534321784973\n",
      "Epoch: 27, Samples: 2336/5760, Loss: 0.01718232035636902\n",
      "Epoch: 27, Samples: 2368/5760, Loss: 0.018114954233169556\n",
      "Epoch: 27, Samples: 2400/5760, Loss: 0.018265843391418457\n",
      "Epoch: 27, Samples: 2432/5760, Loss: 0.041543036699295044\n",
      "Epoch: 27, Samples: 2464/5760, Loss: 0.022929608821868896\n",
      "Epoch: 27, Samples: 2496/5760, Loss: 0.02557927370071411\n",
      "Epoch: 27, Samples: 2528/5760, Loss: 0.03033086657524109\n",
      "Epoch: 27, Samples: 2560/5760, Loss: 0.024796783924102783\n",
      "Epoch: 27, Samples: 2592/5760, Loss: 0.029279887676239014\n",
      "Epoch: 27, Samples: 2624/5760, Loss: 0.027142509818077087\n",
      "Epoch: 27, Samples: 2656/5760, Loss: 0.01738235354423523\n",
      "Epoch: 27, Samples: 2688/5760, Loss: 0.024069249629974365\n",
      "Epoch: 27, Samples: 2720/5760, Loss: 0.014855504035949707\n",
      "Epoch: 27, Samples: 2752/5760, Loss: 0.03208073973655701\n",
      "Epoch: 27, Samples: 2784/5760, Loss: 0.03397153317928314\n",
      "Epoch: 27, Samples: 2816/5760, Loss: 0.025648266077041626\n",
      "Epoch: 27, Samples: 2848/5760, Loss: 0.015426874160766602\n",
      "Epoch: 27, Samples: 2880/5760, Loss: 0.024894416332244873\n",
      "Epoch: 27, Samples: 2912/5760, Loss: 0.006778031587600708\n",
      "Epoch: 27, Samples: 2944/5760, Loss: 0.038304537534713745\n",
      "Epoch: 27, Samples: 2976/5760, Loss: 0.02441665530204773\n",
      "Epoch: 27, Samples: 3008/5760, Loss: 0.015376836061477661\n",
      "Epoch: 27, Samples: 3040/5760, Loss: 0.021707862615585327\n",
      "Epoch: 27, Samples: 3072/5760, Loss: 0.02077925205230713\n",
      "Epoch: 27, Samples: 3104/5760, Loss: 0.011108219623565674\n",
      "Epoch: 27, Samples: 3136/5760, Loss: 0.017197400331497192\n",
      "Epoch: 27, Samples: 3168/5760, Loss: 0.04261302947998047\n",
      "Epoch: 27, Samples: 3200/5760, Loss: 0.020831197500228882\n",
      "Epoch: 27, Samples: 3232/5760, Loss: 0.021406054496765137\n",
      "Epoch: 27, Samples: 3264/5760, Loss: 0.021198272705078125\n",
      "Epoch: 27, Samples: 3296/5760, Loss: 0.011713564395904541\n",
      "Epoch: 27, Samples: 3328/5760, Loss: 0.023628264665603638\n",
      "Epoch: 27, Samples: 3360/5760, Loss: 0.020036756992340088\n",
      "Epoch: 27, Samples: 3392/5760, Loss: 0.01755109429359436\n",
      "Epoch: 27, Samples: 3424/5760, Loss: 0.015865594148635864\n",
      "Epoch: 27, Samples: 3456/5760, Loss: 0.01952362060546875\n",
      "Epoch: 27, Samples: 3488/5760, Loss: 0.016202211380004883\n",
      "Epoch: 27, Samples: 3520/5760, Loss: 0.013096988201141357\n",
      "Epoch: 27, Samples: 3552/5760, Loss: 0.03799155354499817\n",
      "Epoch: 27, Samples: 3584/5760, Loss: 0.03486907482147217\n",
      "Epoch: 27, Samples: 3616/5760, Loss: 0.016701102256774902\n",
      "Epoch: 27, Samples: 3648/5760, Loss: 0.016633763909339905\n",
      "Epoch: 27, Samples: 3680/5760, Loss: 0.008801251649856567\n",
      "Epoch: 27, Samples: 3712/5760, Loss: 0.01973012089729309\n",
      "Epoch: 27, Samples: 3744/5760, Loss: 0.021916210651397705\n",
      "Epoch: 27, Samples: 3776/5760, Loss: 0.01572662591934204\n",
      "Epoch: 27, Samples: 3808/5760, Loss: 0.018632888793945312\n",
      "Epoch: 27, Samples: 3840/5760, Loss: 0.02446076273918152\n",
      "Epoch: 27, Samples: 3872/5760, Loss: 0.02937549352645874\n",
      "Epoch: 27, Samples: 3904/5760, Loss: 0.016253530979156494\n",
      "Epoch: 27, Samples: 3936/5760, Loss: 0.01891803741455078\n",
      "Epoch: 27, Samples: 3968/5760, Loss: 0.019913166761398315\n",
      "Epoch: 27, Samples: 4000/5760, Loss: 0.02397337555885315\n",
      "Epoch: 27, Samples: 4032/5760, Loss: 0.01728793978691101\n",
      "Epoch: 27, Samples: 4064/5760, Loss: 0.014282524585723877\n",
      "Epoch: 27, Samples: 4096/5760, Loss: 0.019364982843399048\n",
      "Epoch: 27, Samples: 4128/5760, Loss: 0.024725764989852905\n",
      "Epoch: 27, Samples: 4160/5760, Loss: 0.03379535675048828\n",
      "Epoch: 27, Samples: 4192/5760, Loss: 0.01619395613670349\n",
      "Epoch: 27, Samples: 4224/5760, Loss: 0.016664475202560425\n",
      "Epoch: 27, Samples: 4256/5760, Loss: 0.012312978506088257\n",
      "Epoch: 27, Samples: 4288/5760, Loss: 0.01086437702178955\n",
      "Epoch: 27, Samples: 4320/5760, Loss: 0.016848623752593994\n",
      "Epoch: 27, Samples: 4352/5760, Loss: 0.028108566999435425\n",
      "Epoch: 27, Samples: 4384/5760, Loss: 0.02878601849079132\n",
      "Epoch: 27, Samples: 4416/5760, Loss: 0.020248964428901672\n",
      "Epoch: 27, Samples: 4448/5760, Loss: 0.02600511908531189\n",
      "Epoch: 27, Samples: 4480/5760, Loss: 0.01847633719444275\n",
      "Epoch: 27, Samples: 4512/5760, Loss: 0.016771316528320312\n",
      "Epoch: 27, Samples: 4544/5760, Loss: 0.019899427890777588\n",
      "Epoch: 27, Samples: 4576/5760, Loss: 0.04119519889354706\n",
      "Epoch: 27, Samples: 4608/5760, Loss: 0.020715445280075073\n",
      "Epoch: 27, Samples: 4640/5760, Loss: 0.03105252981185913\n",
      "Epoch: 27, Samples: 4672/5760, Loss: 0.01246914267539978\n",
      "Epoch: 27, Samples: 4704/5760, Loss: 0.015913009643554688\n",
      "Epoch: 27, Samples: 4736/5760, Loss: 0.018373757600784302\n",
      "Epoch: 27, Samples: 4768/5760, Loss: 0.012955069541931152\n",
      "Epoch: 27, Samples: 4800/5760, Loss: 0.022830218076705933\n",
      "Epoch: 27, Samples: 4832/5760, Loss: 0.03270459175109863\n",
      "Epoch: 27, Samples: 4864/5760, Loss: 0.009604096412658691\n",
      "Epoch: 27, Samples: 4896/5760, Loss: 0.02666550874710083\n",
      "Epoch: 27, Samples: 4928/5760, Loss: 0.014406353235244751\n",
      "Epoch: 27, Samples: 4960/5760, Loss: 0.01649966835975647\n",
      "Epoch: 27, Samples: 4992/5760, Loss: 0.0388948917388916\n",
      "Epoch: 27, Samples: 5024/5760, Loss: 0.01936441659927368\n",
      "Epoch: 27, Samples: 5056/5760, Loss: 0.01651519536972046\n",
      "Epoch: 27, Samples: 5088/5760, Loss: 0.03269144892692566\n",
      "Epoch: 27, Samples: 5120/5760, Loss: 0.011361569166183472\n",
      "Epoch: 27, Samples: 5152/5760, Loss: 0.02507859468460083\n",
      "Epoch: 27, Samples: 5184/5760, Loss: 0.01562318205833435\n",
      "Epoch: 27, Samples: 5216/5760, Loss: 0.026939988136291504\n",
      "Epoch: 27, Samples: 5248/5760, Loss: 0.0210837721824646\n",
      "Epoch: 27, Samples: 5280/5760, Loss: 0.009647488594055176\n",
      "Epoch: 27, Samples: 5312/5760, Loss: 0.023752927780151367\n",
      "Epoch: 27, Samples: 5344/5760, Loss: 0.031577110290527344\n",
      "Epoch: 27, Samples: 5376/5760, Loss: 0.027619212865829468\n",
      "Epoch: 27, Samples: 5408/5760, Loss: 0.019910812377929688\n",
      "Epoch: 27, Samples: 5440/5760, Loss: 0.010013073682785034\n",
      "Epoch: 27, Samples: 5472/5760, Loss: 0.017352163791656494\n",
      "Epoch: 27, Samples: 5504/5760, Loss: 0.019943684339523315\n",
      "Epoch: 27, Samples: 5536/5760, Loss: 0.026149719953536987\n",
      "Epoch: 27, Samples: 5568/5760, Loss: 0.04922205209732056\n",
      "Epoch: 27, Samples: 5600/5760, Loss: 0.031913578510284424\n",
      "Epoch: 27, Samples: 5632/5760, Loss: 0.01829826831817627\n",
      "Epoch: 27, Samples: 5664/5760, Loss: 0.01771456003189087\n",
      "Epoch: 27, Samples: 5696/5760, Loss: 0.024652540683746338\n",
      "Epoch: 27, Samples: 5728/5760, Loss: 1.17853844165802\n",
      "\n",
      "Epoch: 27\n",
      "Training set: Average loss: 0.0287\n",
      "Validation set: Average loss: 0.3451, Accuracy: 752/818 (92%)\n",
      "Epoch: 28, Samples: 0/5760, Loss: 0.012641102075576782\n",
      "Epoch: 28, Samples: 32/5760, Loss: 0.03314012289047241\n",
      "Epoch: 28, Samples: 64/5760, Loss: 0.028225570917129517\n",
      "Epoch: 28, Samples: 96/5760, Loss: 0.07418122887611389\n",
      "Epoch: 28, Samples: 128/5760, Loss: 0.014544188976287842\n",
      "Epoch: 28, Samples: 160/5760, Loss: 0.010592252016067505\n",
      "Epoch: 28, Samples: 192/5760, Loss: 0.02568623423576355\n",
      "Epoch: 28, Samples: 224/5760, Loss: 0.011440873146057129\n",
      "Epoch: 28, Samples: 256/5760, Loss: 0.014992207288742065\n",
      "Epoch: 28, Samples: 288/5760, Loss: 0.02119782567024231\n",
      "Epoch: 28, Samples: 320/5760, Loss: 0.03815147280693054\n",
      "Epoch: 28, Samples: 352/5760, Loss: 0.015699684619903564\n",
      "Epoch: 28, Samples: 384/5760, Loss: 0.01817414164543152\n",
      "Epoch: 28, Samples: 416/5760, Loss: 0.01731342077255249\n",
      "Epoch: 28, Samples: 448/5760, Loss: 0.029312536120414734\n",
      "Epoch: 28, Samples: 480/5760, Loss: 0.022885531187057495\n",
      "Epoch: 28, Samples: 512/5760, Loss: 0.020556539297103882\n",
      "Epoch: 28, Samples: 544/5760, Loss: 0.02844908833503723\n",
      "Epoch: 28, Samples: 576/5760, Loss: 0.021859735250473022\n",
      "Epoch: 28, Samples: 608/5760, Loss: 0.02413877844810486\n",
      "Epoch: 28, Samples: 640/5760, Loss: 0.01953113079071045\n",
      "Epoch: 28, Samples: 672/5760, Loss: 0.02335914969444275\n",
      "Epoch: 28, Samples: 704/5760, Loss: 0.022156178951263428\n",
      "Epoch: 28, Samples: 736/5760, Loss: 0.018994778394699097\n",
      "Epoch: 28, Samples: 768/5760, Loss: 0.02180349826812744\n",
      "Epoch: 28, Samples: 800/5760, Loss: 0.028472304344177246\n",
      "Epoch: 28, Samples: 832/5760, Loss: 0.013589322566986084\n",
      "Epoch: 28, Samples: 864/5760, Loss: 0.035344868898391724\n",
      "Epoch: 28, Samples: 896/5760, Loss: 0.02156233787536621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Samples: 928/5760, Loss: 0.016012728214263916\n",
      "Epoch: 28, Samples: 960/5760, Loss: 0.012590020895004272\n",
      "Epoch: 28, Samples: 992/5760, Loss: 0.008936285972595215\n",
      "Epoch: 28, Samples: 1024/5760, Loss: 0.02280077338218689\n",
      "Epoch: 28, Samples: 1056/5760, Loss: 0.028162509202957153\n",
      "Epoch: 28, Samples: 1088/5760, Loss: 0.023860663175582886\n",
      "Epoch: 28, Samples: 1120/5760, Loss: 0.014024078845977783\n",
      "Epoch: 28, Samples: 1152/5760, Loss: 0.02312713861465454\n",
      "Epoch: 28, Samples: 1184/5760, Loss: 0.03101179003715515\n",
      "Epoch: 28, Samples: 1216/5760, Loss: 0.013218432664871216\n",
      "Epoch: 28, Samples: 1248/5760, Loss: 0.018772214651107788\n",
      "Epoch: 28, Samples: 1280/5760, Loss: 0.02387484908103943\n",
      "Epoch: 28, Samples: 1312/5760, Loss: 0.019869238138198853\n",
      "Epoch: 28, Samples: 1344/5760, Loss: 0.032164067029953\n",
      "Epoch: 28, Samples: 1376/5760, Loss: 0.013994872570037842\n",
      "Epoch: 28, Samples: 1408/5760, Loss: 0.009494900703430176\n",
      "Epoch: 28, Samples: 1440/5760, Loss: 0.01556423306465149\n",
      "Epoch: 28, Samples: 1472/5760, Loss: 0.029237821698188782\n",
      "Epoch: 28, Samples: 1504/5760, Loss: 0.049501076340675354\n",
      "Epoch: 28, Samples: 1536/5760, Loss: 0.014653831720352173\n",
      "Epoch: 28, Samples: 1568/5760, Loss: 0.01878148317337036\n",
      "Epoch: 28, Samples: 1600/5760, Loss: 0.041535377502441406\n",
      "Epoch: 28, Samples: 1632/5760, Loss: 0.015076041221618652\n",
      "Epoch: 28, Samples: 1664/5760, Loss: 0.028647243976593018\n",
      "Epoch: 28, Samples: 1696/5760, Loss: 0.02220630645751953\n",
      "Epoch: 28, Samples: 1728/5760, Loss: 0.04423210024833679\n",
      "Epoch: 28, Samples: 1760/5760, Loss: 0.01707199215888977\n",
      "Epoch: 28, Samples: 1792/5760, Loss: 0.0362430214881897\n",
      "Epoch: 28, Samples: 1824/5760, Loss: 0.02647128701210022\n",
      "Epoch: 28, Samples: 1856/5760, Loss: 0.020328909158706665\n",
      "Epoch: 28, Samples: 1888/5760, Loss: 0.017353922128677368\n",
      "Epoch: 28, Samples: 1920/5760, Loss: 0.031634747982025146\n",
      "Epoch: 28, Samples: 1952/5760, Loss: 0.014514431357383728\n",
      "Epoch: 28, Samples: 1984/5760, Loss: 0.034171849489212036\n",
      "Epoch: 28, Samples: 2016/5760, Loss: 0.03134426474571228\n",
      "Epoch: 28, Samples: 2048/5760, Loss: 0.015390872955322266\n",
      "Epoch: 28, Samples: 2080/5760, Loss: 0.01728963851928711\n",
      "Epoch: 28, Samples: 2112/5760, Loss: 0.035565972328186035\n",
      "Epoch: 28, Samples: 2144/5760, Loss: 0.019791483879089355\n",
      "Epoch: 28, Samples: 2176/5760, Loss: 0.042810410261154175\n",
      "Epoch: 28, Samples: 2208/5760, Loss: 0.018510878086090088\n",
      "Epoch: 28, Samples: 2240/5760, Loss: 0.019852250814437866\n",
      "Epoch: 28, Samples: 2272/5760, Loss: 0.034306615591049194\n",
      "Epoch: 28, Samples: 2304/5760, Loss: 0.01886773109436035\n",
      "Epoch: 28, Samples: 2336/5760, Loss: 0.027642875909805298\n",
      "Epoch: 28, Samples: 2368/5760, Loss: 0.03098757565021515\n",
      "Epoch: 28, Samples: 2400/5760, Loss: 0.01703605055809021\n",
      "Epoch: 28, Samples: 2432/5760, Loss: 0.012720733880996704\n",
      "Epoch: 28, Samples: 2464/5760, Loss: 0.010499894618988037\n",
      "Epoch: 28, Samples: 2496/5760, Loss: 0.014544516801834106\n",
      "Epoch: 28, Samples: 2528/5760, Loss: 0.017291635274887085\n",
      "Epoch: 28, Samples: 2560/5760, Loss: 0.012903392314910889\n",
      "Epoch: 28, Samples: 2592/5760, Loss: 0.019608676433563232\n",
      "Epoch: 28, Samples: 2624/5760, Loss: 0.029614657163619995\n",
      "Epoch: 28, Samples: 2656/5760, Loss: 0.018577784299850464\n",
      "Epoch: 28, Samples: 2688/5760, Loss: 0.018691062927246094\n",
      "Epoch: 28, Samples: 2720/5760, Loss: 0.033515095710754395\n",
      "Epoch: 28, Samples: 2752/5760, Loss: 0.03266608715057373\n",
      "Epoch: 28, Samples: 2784/5760, Loss: 0.01428106427192688\n",
      "Epoch: 28, Samples: 2816/5760, Loss: 0.025124937295913696\n",
      "Epoch: 28, Samples: 2848/5760, Loss: 0.022296547889709473\n",
      "Epoch: 28, Samples: 2880/5760, Loss: 0.012387990951538086\n",
      "Epoch: 28, Samples: 2912/5760, Loss: 0.01992848515510559\n",
      "Epoch: 28, Samples: 2944/5760, Loss: 0.013604223728179932\n",
      "Epoch: 28, Samples: 2976/5760, Loss: 0.01986992359161377\n",
      "Epoch: 28, Samples: 3008/5760, Loss: 0.014847517013549805\n",
      "Epoch: 28, Samples: 3040/5760, Loss: 0.024234473705291748\n",
      "Epoch: 28, Samples: 3072/5760, Loss: 0.027663886547088623\n",
      "Epoch: 28, Samples: 3104/5760, Loss: 0.03191179037094116\n",
      "Epoch: 28, Samples: 3136/5760, Loss: 0.017974406480789185\n",
      "Epoch: 28, Samples: 3168/5760, Loss: 0.017770320177078247\n",
      "Epoch: 28, Samples: 3200/5760, Loss: 0.029305845499038696\n",
      "Epoch: 28, Samples: 3232/5760, Loss: 0.017225712537765503\n",
      "Epoch: 28, Samples: 3264/5760, Loss: 0.01940092444419861\n",
      "Epoch: 28, Samples: 3296/5760, Loss: 0.019370615482330322\n",
      "Epoch: 28, Samples: 3328/5760, Loss: 0.023222029209136963\n",
      "Epoch: 28, Samples: 3360/5760, Loss: 0.013657450675964355\n",
      "Epoch: 28, Samples: 3392/5760, Loss: 0.037793442606925964\n",
      "Epoch: 28, Samples: 3424/5760, Loss: 0.030269861221313477\n",
      "Epoch: 28, Samples: 3456/5760, Loss: 0.018698841333389282\n",
      "Epoch: 28, Samples: 3488/5760, Loss: 0.010130584239959717\n",
      "Epoch: 28, Samples: 3520/5760, Loss: 0.03143849968910217\n",
      "Epoch: 28, Samples: 3552/5760, Loss: 0.019472241401672363\n",
      "Epoch: 28, Samples: 3584/5760, Loss: 0.025790631771087646\n",
      "Epoch: 28, Samples: 3616/5760, Loss: 0.021754056215286255\n",
      "Epoch: 28, Samples: 3648/5760, Loss: 0.016070634126663208\n",
      "Epoch: 28, Samples: 3680/5760, Loss: 0.0207747220993042\n",
      "Epoch: 28, Samples: 3712/5760, Loss: 0.025389671325683594\n",
      "Epoch: 28, Samples: 3744/5760, Loss: 0.009470224380493164\n",
      "Epoch: 28, Samples: 3776/5760, Loss: 0.01625695824623108\n",
      "Epoch: 28, Samples: 3808/5760, Loss: 0.019877910614013672\n",
      "Epoch: 28, Samples: 3840/5760, Loss: 0.01667863130569458\n",
      "Epoch: 28, Samples: 3872/5760, Loss: 0.01023036241531372\n",
      "Epoch: 28, Samples: 3904/5760, Loss: 0.03721892833709717\n",
      "Epoch: 28, Samples: 3936/5760, Loss: 0.017260074615478516\n",
      "Epoch: 28, Samples: 3968/5760, Loss: 0.013021886348724365\n",
      "Epoch: 28, Samples: 4000/5760, Loss: 0.01998773217201233\n",
      "Epoch: 28, Samples: 4032/5760, Loss: 0.01493331789970398\n",
      "Epoch: 28, Samples: 4064/5760, Loss: 0.039835214614868164\n",
      "Epoch: 28, Samples: 4096/5760, Loss: 0.015215277671813965\n",
      "Epoch: 28, Samples: 4128/5760, Loss: 0.023993879556655884\n",
      "Epoch: 28, Samples: 4160/5760, Loss: 0.01835748553276062\n",
      "Epoch: 28, Samples: 4192/5760, Loss: 0.01791664958000183\n",
      "Epoch: 28, Samples: 4224/5760, Loss: 0.02307601273059845\n",
      "Epoch: 28, Samples: 4256/5760, Loss: 0.014107763767242432\n",
      "Epoch: 28, Samples: 4288/5760, Loss: 0.02322453260421753\n",
      "Epoch: 28, Samples: 4320/5760, Loss: 0.027578800916671753\n",
      "Epoch: 28, Samples: 4352/5760, Loss: 0.020267575979232788\n",
      "Epoch: 28, Samples: 4384/5760, Loss: 0.012807071208953857\n",
      "Epoch: 28, Samples: 4416/5760, Loss: 0.013610243797302246\n",
      "Epoch: 28, Samples: 4448/5760, Loss: 0.012320458889007568\n",
      "Epoch: 28, Samples: 4480/5760, Loss: 0.015611737966537476\n",
      "Epoch: 28, Samples: 4512/5760, Loss: 0.020935744047164917\n",
      "Epoch: 28, Samples: 4544/5760, Loss: 0.029164433479309082\n",
      "Epoch: 28, Samples: 4576/5760, Loss: 0.010836988687515259\n",
      "Epoch: 28, Samples: 4608/5760, Loss: 0.04015710949897766\n",
      "Epoch: 28, Samples: 4640/5760, Loss: 0.02738797664642334\n",
      "Epoch: 28, Samples: 4672/5760, Loss: 0.02946867048740387\n",
      "Epoch: 28, Samples: 4704/5760, Loss: 0.009478867053985596\n",
      "Epoch: 28, Samples: 4736/5760, Loss: 0.022430360317230225\n",
      "Epoch: 28, Samples: 4768/5760, Loss: 0.015725910663604736\n",
      "Epoch: 28, Samples: 4800/5760, Loss: 0.007542848587036133\n",
      "Epoch: 28, Samples: 4832/5760, Loss: 0.02152228355407715\n",
      "Epoch: 28, Samples: 4864/5760, Loss: 0.018508046865463257\n",
      "Epoch: 28, Samples: 4896/5760, Loss: 0.04739581048488617\n",
      "Epoch: 28, Samples: 4928/5760, Loss: 0.02201327681541443\n",
      "Epoch: 28, Samples: 4960/5760, Loss: 0.008928537368774414\n",
      "Epoch: 28, Samples: 4992/5760, Loss: 0.019326716661453247\n",
      "Epoch: 28, Samples: 5024/5760, Loss: 0.014397621154785156\n",
      "Epoch: 28, Samples: 5056/5760, Loss: 0.032432228326797485\n",
      "Epoch: 28, Samples: 5088/5760, Loss: 0.0313444584608078\n",
      "Epoch: 28, Samples: 5120/5760, Loss: 0.01707404851913452\n",
      "Epoch: 28, Samples: 5152/5760, Loss: 0.017097413539886475\n",
      "Epoch: 28, Samples: 5184/5760, Loss: 0.015309661626815796\n",
      "Epoch: 28, Samples: 5216/5760, Loss: 0.013512730598449707\n",
      "Epoch: 28, Samples: 5248/5760, Loss: 0.022319525480270386\n",
      "Epoch: 28, Samples: 5280/5760, Loss: 0.027696192264556885\n",
      "Epoch: 28, Samples: 5312/5760, Loss: 0.01506662368774414\n",
      "Epoch: 28, Samples: 5344/5760, Loss: 0.01700422167778015\n",
      "Epoch: 28, Samples: 5376/5760, Loss: 0.029745370149612427\n",
      "Epoch: 28, Samples: 5408/5760, Loss: 0.01464688777923584\n",
      "Epoch: 28, Samples: 5440/5760, Loss: 0.02481570839881897\n",
      "Epoch: 28, Samples: 5472/5760, Loss: 0.017427116632461548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Samples: 5504/5760, Loss: 0.018975764513015747\n",
      "Epoch: 28, Samples: 5536/5760, Loss: 0.013733327388763428\n",
      "Epoch: 28, Samples: 5568/5760, Loss: 0.015713036060333252\n",
      "Epoch: 28, Samples: 5600/5760, Loss: 0.03526613116264343\n",
      "Epoch: 28, Samples: 5632/5760, Loss: 0.021706387400627136\n",
      "Epoch: 28, Samples: 5664/5760, Loss: 0.018443703651428223\n",
      "Epoch: 28, Samples: 5696/5760, Loss: 0.020573675632476807\n",
      "Epoch: 28, Samples: 5728/5760, Loss: 0.5890127420425415\n",
      "\n",
      "Epoch: 28\n",
      "Training set: Average loss: 0.0251\n",
      "Validation set: Average loss: 0.3571, Accuracy: 745/818 (91%)\n",
      "Epoch: 29, Samples: 0/5760, Loss: 0.01651132106781006\n",
      "Epoch: 29, Samples: 32/5760, Loss: 0.011760622262954712\n",
      "Epoch: 29, Samples: 64/5760, Loss: 0.015624135732650757\n",
      "Epoch: 29, Samples: 96/5760, Loss: 0.015499353408813477\n",
      "Epoch: 29, Samples: 128/5760, Loss: 0.010366857051849365\n",
      "Epoch: 29, Samples: 160/5760, Loss: 0.017531514167785645\n",
      "Epoch: 29, Samples: 192/5760, Loss: 0.015104740858078003\n",
      "Epoch: 29, Samples: 224/5760, Loss: 0.02794155478477478\n",
      "Epoch: 29, Samples: 256/5760, Loss: 0.020434021949768066\n",
      "Epoch: 29, Samples: 288/5760, Loss: 0.016268163919448853\n",
      "Epoch: 29, Samples: 320/5760, Loss: 0.015720456838607788\n",
      "Epoch: 29, Samples: 352/5760, Loss: 0.029078587889671326\n",
      "Epoch: 29, Samples: 384/5760, Loss: 0.019816935062408447\n",
      "Epoch: 29, Samples: 416/5760, Loss: 0.017318308353424072\n",
      "Epoch: 29, Samples: 448/5760, Loss: 0.021628737449645996\n",
      "Epoch: 29, Samples: 480/5760, Loss: 0.022828876972198486\n",
      "Epoch: 29, Samples: 512/5760, Loss: 0.011962711811065674\n",
      "Epoch: 29, Samples: 544/5760, Loss: 0.02398097515106201\n",
      "Epoch: 29, Samples: 576/5760, Loss: 0.012811720371246338\n",
      "Epoch: 29, Samples: 608/5760, Loss: 0.015354245901107788\n",
      "Epoch: 29, Samples: 640/5760, Loss: 0.026678621768951416\n",
      "Epoch: 29, Samples: 672/5760, Loss: 0.017585664987564087\n",
      "Epoch: 29, Samples: 704/5760, Loss: 0.014538943767547607\n",
      "Epoch: 29, Samples: 736/5760, Loss: 0.015234142541885376\n",
      "Epoch: 29, Samples: 768/5760, Loss: 0.015980273485183716\n",
      "Epoch: 29, Samples: 800/5760, Loss: 0.02813151478767395\n",
      "Epoch: 29, Samples: 832/5760, Loss: 0.018104851245880127\n",
      "Epoch: 29, Samples: 864/5760, Loss: 0.01566016674041748\n",
      "Epoch: 29, Samples: 896/5760, Loss: 0.0201987624168396\n",
      "Epoch: 29, Samples: 928/5760, Loss: 0.014187544584274292\n",
      "Epoch: 29, Samples: 960/5760, Loss: 0.009664475917816162\n",
      "Epoch: 29, Samples: 992/5760, Loss: 0.0175321102142334\n",
      "Epoch: 29, Samples: 1024/5760, Loss: 0.021715253591537476\n",
      "Epoch: 29, Samples: 1056/5760, Loss: 0.01406148076057434\n",
      "Epoch: 29, Samples: 1088/5760, Loss: 0.025743722915649414\n",
      "Epoch: 29, Samples: 1120/5760, Loss: 0.010934323072433472\n",
      "Epoch: 29, Samples: 1152/5760, Loss: 0.021095633506774902\n",
      "Epoch: 29, Samples: 1184/5760, Loss: 0.017767325043678284\n",
      "Epoch: 29, Samples: 1216/5760, Loss: 0.01976051926612854\n",
      "Epoch: 29, Samples: 1248/5760, Loss: 0.022018760442733765\n",
      "Epoch: 29, Samples: 1280/5760, Loss: 0.022815868258476257\n",
      "Epoch: 29, Samples: 1312/5760, Loss: 0.019875764846801758\n",
      "Epoch: 29, Samples: 1344/5760, Loss: 0.024968326091766357\n",
      "Epoch: 29, Samples: 1376/5760, Loss: 0.013070017099380493\n",
      "Epoch: 29, Samples: 1408/5760, Loss: 0.008464068174362183\n",
      "Epoch: 29, Samples: 1440/5760, Loss: 0.01961272954940796\n",
      "Epoch: 29, Samples: 1472/5760, Loss: 0.015665292739868164\n",
      "Epoch: 29, Samples: 1504/5760, Loss: 0.06062702834606171\n",
      "Epoch: 29, Samples: 1536/5760, Loss: 0.026444345712661743\n",
      "Epoch: 29, Samples: 1568/5760, Loss: 0.025448858737945557\n",
      "Epoch: 29, Samples: 1600/5760, Loss: 0.013946473598480225\n",
      "Epoch: 29, Samples: 1632/5760, Loss: 0.009189456701278687\n",
      "Epoch: 29, Samples: 1664/5760, Loss: 0.021835118532180786\n",
      "Epoch: 29, Samples: 1696/5760, Loss: 0.02145540714263916\n",
      "Epoch: 29, Samples: 1728/5760, Loss: 0.01432788372039795\n",
      "Epoch: 29, Samples: 1760/5760, Loss: 0.013077080249786377\n",
      "Epoch: 29, Samples: 1792/5760, Loss: 0.026404917240142822\n",
      "Epoch: 29, Samples: 1824/5760, Loss: 0.01691266894340515\n",
      "Epoch: 29, Samples: 1856/5760, Loss: 0.024333924055099487\n",
      "Epoch: 29, Samples: 1888/5760, Loss: 0.014871597290039062\n",
      "Epoch: 29, Samples: 1920/5760, Loss: 0.01761665940284729\n",
      "Epoch: 29, Samples: 1952/5760, Loss: 0.026771873235702515\n",
      "Epoch: 29, Samples: 1984/5760, Loss: 0.0218946635723114\n",
      "Epoch: 29, Samples: 2016/5760, Loss: 0.043779924511909485\n",
      "Epoch: 29, Samples: 2048/5760, Loss: 0.021536409854888916\n",
      "Epoch: 29, Samples: 2080/5760, Loss: 0.013996779918670654\n",
      "Epoch: 29, Samples: 2112/5760, Loss: 0.01019391417503357\n",
      "Epoch: 29, Samples: 2144/5760, Loss: 0.014214396476745605\n",
      "Epoch: 29, Samples: 2176/5760, Loss: 0.019968867301940918\n",
      "Epoch: 29, Samples: 2208/5760, Loss: 0.013513505458831787\n",
      "Epoch: 29, Samples: 2240/5760, Loss: 0.02010062336921692\n",
      "Epoch: 29, Samples: 2272/5760, Loss: 0.017849475145339966\n",
      "Epoch: 29, Samples: 2304/5760, Loss: 0.008789479732513428\n",
      "Epoch: 29, Samples: 2336/5760, Loss: 0.01971149444580078\n",
      "Epoch: 29, Samples: 2368/5760, Loss: 0.04298333823680878\n",
      "Epoch: 29, Samples: 2400/5760, Loss: 0.013402968645095825\n",
      "Epoch: 29, Samples: 2432/5760, Loss: 0.021661490201950073\n",
      "Epoch: 29, Samples: 2464/5760, Loss: 0.022090554237365723\n",
      "Epoch: 29, Samples: 2496/5760, Loss: 0.015809059143066406\n",
      "Epoch: 29, Samples: 2528/5760, Loss: 0.036558836698532104\n",
      "Epoch: 29, Samples: 2560/5760, Loss: 0.009821683168411255\n",
      "Epoch: 29, Samples: 2592/5760, Loss: 0.018110603094100952\n",
      "Epoch: 29, Samples: 2624/5760, Loss: 0.018319517374038696\n",
      "Epoch: 29, Samples: 2656/5760, Loss: 0.01631741225719452\n",
      "Epoch: 29, Samples: 2688/5760, Loss: 0.043659508228302\n",
      "Epoch: 29, Samples: 2720/5760, Loss: 0.011185109615325928\n",
      "Epoch: 29, Samples: 2752/5760, Loss: 0.01672649383544922\n",
      "Epoch: 29, Samples: 2784/5760, Loss: 0.01179969310760498\n",
      "Epoch: 29, Samples: 2816/5760, Loss: 0.020711570978164673\n",
      "Epoch: 29, Samples: 2848/5760, Loss: 0.02095124125480652\n",
      "Epoch: 29, Samples: 2880/5760, Loss: 0.012379229068756104\n",
      "Epoch: 29, Samples: 2912/5760, Loss: 0.024294614791870117\n",
      "Epoch: 29, Samples: 2944/5760, Loss: 0.011247068643569946\n",
      "Epoch: 29, Samples: 2976/5760, Loss: 0.020416244864463806\n",
      "Epoch: 29, Samples: 3008/5760, Loss: 0.024518385529518127\n",
      "Epoch: 29, Samples: 3040/5760, Loss: 0.011514782905578613\n",
      "Epoch: 29, Samples: 3072/5760, Loss: 0.020396173000335693\n",
      "Epoch: 29, Samples: 3104/5760, Loss: 0.01351138949394226\n",
      "Epoch: 29, Samples: 3136/5760, Loss: 0.013103783130645752\n",
      "Epoch: 29, Samples: 3168/5760, Loss: 0.051684558391571045\n",
      "Epoch: 29, Samples: 3200/5760, Loss: 0.02051284909248352\n",
      "Epoch: 29, Samples: 3232/5760, Loss: 0.016272425651550293\n",
      "Epoch: 29, Samples: 3264/5760, Loss: 0.02365472912788391\n",
      "Epoch: 29, Samples: 3296/5760, Loss: 0.017719537019729614\n",
      "Epoch: 29, Samples: 3328/5760, Loss: 0.015715837478637695\n",
      "Epoch: 29, Samples: 3360/5760, Loss: 0.02700817584991455\n",
      "Epoch: 29, Samples: 3392/5760, Loss: 0.0169086754322052\n",
      "Epoch: 29, Samples: 3424/5760, Loss: 0.01019895076751709\n",
      "Epoch: 29, Samples: 3456/5760, Loss: 0.012236177921295166\n",
      "Epoch: 29, Samples: 3488/5760, Loss: 0.015801936388015747\n",
      "Epoch: 29, Samples: 3520/5760, Loss: 0.016529768705368042\n",
      "Epoch: 29, Samples: 3552/5760, Loss: 0.016364693641662598\n",
      "Epoch: 29, Samples: 3584/5760, Loss: 0.03388705849647522\n",
      "Epoch: 29, Samples: 3616/5760, Loss: 0.013549715280532837\n",
      "Epoch: 29, Samples: 3648/5760, Loss: 0.014334410429000854\n",
      "Epoch: 29, Samples: 3680/5760, Loss: 0.014626890420913696\n",
      "Epoch: 29, Samples: 3712/5760, Loss: 0.009095311164855957\n",
      "Epoch: 29, Samples: 3744/5760, Loss: 0.017094284296035767\n",
      "Epoch: 29, Samples: 3776/5760, Loss: 0.019455909729003906\n",
      "Epoch: 29, Samples: 3808/5760, Loss: 0.03228408098220825\n",
      "Epoch: 29, Samples: 3840/5760, Loss: 0.019328385591506958\n",
      "Epoch: 29, Samples: 3872/5760, Loss: 0.026101112365722656\n",
      "Epoch: 29, Samples: 3904/5760, Loss: 0.016305625438690186\n",
      "Epoch: 29, Samples: 3936/5760, Loss: 0.020738720893859863\n",
      "Epoch: 29, Samples: 3968/5760, Loss: 0.017130732536315918\n",
      "Epoch: 29, Samples: 4000/5760, Loss: 0.009235411882400513\n",
      "Epoch: 29, Samples: 4032/5760, Loss: 0.0163593590259552\n",
      "Epoch: 29, Samples: 4064/5760, Loss: 0.02009683847427368\n",
      "Epoch: 29, Samples: 4096/5760, Loss: 0.01976191997528076\n",
      "Epoch: 29, Samples: 4128/5760, Loss: 0.01866862177848816\n",
      "Epoch: 29, Samples: 4160/5760, Loss: 0.024846524000167847\n",
      "Epoch: 29, Samples: 4192/5760, Loss: 0.010979264974594116\n",
      "Epoch: 29, Samples: 4224/5760, Loss: 0.019516736268997192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Samples: 4256/5760, Loss: 0.02589920163154602\n",
      "Epoch: 29, Samples: 4288/5760, Loss: 0.02527773380279541\n",
      "Epoch: 29, Samples: 4320/5760, Loss: 0.009940892457962036\n",
      "Epoch: 29, Samples: 4352/5760, Loss: 0.036473482847213745\n",
      "Epoch: 29, Samples: 4384/5760, Loss: 0.013015389442443848\n",
      "Epoch: 29, Samples: 4416/5760, Loss: 0.015117466449737549\n",
      "Epoch: 29, Samples: 4448/5760, Loss: 0.009230852127075195\n",
      "Epoch: 29, Samples: 4480/5760, Loss: 0.02443709969520569\n",
      "Epoch: 29, Samples: 4512/5760, Loss: 0.015880167484283447\n",
      "Epoch: 29, Samples: 4544/5760, Loss: 0.022671550512313843\n",
      "Epoch: 29, Samples: 4576/5760, Loss: 0.015601128339767456\n",
      "Epoch: 29, Samples: 4608/5760, Loss: 0.012880712747573853\n",
      "Epoch: 29, Samples: 4640/5760, Loss: 0.015538662672042847\n",
      "Epoch: 29, Samples: 4672/5760, Loss: 0.01706409454345703\n",
      "Epoch: 29, Samples: 4704/5760, Loss: 0.02230134606361389\n",
      "Epoch: 29, Samples: 4736/5760, Loss: 0.02426356077194214\n",
      "Epoch: 29, Samples: 4768/5760, Loss: 0.012996077537536621\n",
      "Epoch: 29, Samples: 4800/5760, Loss: 0.014733165502548218\n",
      "Epoch: 29, Samples: 4832/5760, Loss: 0.010414600372314453\n",
      "Epoch: 29, Samples: 4864/5760, Loss: 0.01995900273323059\n",
      "Epoch: 29, Samples: 4896/5760, Loss: 0.02348819375038147\n",
      "Epoch: 29, Samples: 4928/5760, Loss: 0.01673397421836853\n",
      "Epoch: 29, Samples: 4960/5760, Loss: 0.011514872312545776\n",
      "Epoch: 29, Samples: 4992/5760, Loss: 0.01400730013847351\n",
      "Epoch: 29, Samples: 5024/5760, Loss: 0.014312446117401123\n",
      "Epoch: 29, Samples: 5056/5760, Loss: 0.012181311845779419\n",
      "Epoch: 29, Samples: 5088/5760, Loss: 0.016035407781600952\n",
      "Epoch: 29, Samples: 5120/5760, Loss: 0.022470712661743164\n",
      "Epoch: 29, Samples: 5152/5760, Loss: 0.0097370445728302\n",
      "Epoch: 29, Samples: 5184/5760, Loss: 0.016729354858398438\n",
      "Epoch: 29, Samples: 5216/5760, Loss: 0.023303180932998657\n",
      "Epoch: 29, Samples: 5248/5760, Loss: 0.017035871744155884\n",
      "Epoch: 29, Samples: 5280/5760, Loss: 0.017621725797653198\n",
      "Epoch: 29, Samples: 5312/5760, Loss: 0.03400366008281708\n",
      "Epoch: 29, Samples: 5344/5760, Loss: 0.025334805250167847\n",
      "Epoch: 29, Samples: 5376/5760, Loss: 0.01895424723625183\n",
      "Epoch: 29, Samples: 5408/5760, Loss: 0.05420556664466858\n",
      "Epoch: 29, Samples: 5440/5760, Loss: 0.029363542795181274\n",
      "Epoch: 29, Samples: 5472/5760, Loss: 0.023601949214935303\n",
      "Epoch: 29, Samples: 5504/5760, Loss: 0.011809885501861572\n",
      "Epoch: 29, Samples: 5536/5760, Loss: 0.013073116540908813\n",
      "Epoch: 29, Samples: 5568/5760, Loss: 0.013553768396377563\n",
      "Epoch: 29, Samples: 5600/5760, Loss: 0.01235806941986084\n",
      "Epoch: 29, Samples: 5632/5760, Loss: 0.02700716257095337\n",
      "Epoch: 29, Samples: 5664/5760, Loss: 0.011313289403915405\n",
      "Epoch: 29, Samples: 5696/5760, Loss: 0.02007320523262024\n",
      "Epoch: 29, Samples: 5728/5760, Loss: 0.8218879699707031\n",
      "\n",
      "Epoch: 29\n",
      "Training set: Average loss: 0.0236\n",
      "Validation set: Average loss: 0.3470, Accuracy: 749/818 (92%)\n",
      "Epoch: 30, Samples: 0/5760, Loss: 0.012742280960083008\n",
      "Epoch: 30, Samples: 32/5760, Loss: 0.02116265892982483\n",
      "Epoch: 30, Samples: 64/5760, Loss: 0.012399017810821533\n",
      "Epoch: 30, Samples: 96/5760, Loss: 0.025859534740447998\n",
      "Epoch: 30, Samples: 128/5760, Loss: 0.010792970657348633\n",
      "Epoch: 30, Samples: 160/5760, Loss: 0.012088239192962646\n",
      "Epoch: 30, Samples: 192/5760, Loss: 0.012817293405532837\n",
      "Epoch: 30, Samples: 224/5760, Loss: 0.020831555128097534\n",
      "Epoch: 30, Samples: 256/5760, Loss: 0.01667758822441101\n",
      "Epoch: 30, Samples: 288/5760, Loss: 0.014210104942321777\n",
      "Epoch: 30, Samples: 320/5760, Loss: 0.01887357234954834\n",
      "Epoch: 30, Samples: 352/5760, Loss: 0.020402967929840088\n",
      "Epoch: 30, Samples: 384/5760, Loss: 0.016388416290283203\n",
      "Epoch: 30, Samples: 416/5760, Loss: 0.025839626789093018\n",
      "Epoch: 30, Samples: 448/5760, Loss: 0.0228804349899292\n",
      "Epoch: 30, Samples: 480/5760, Loss: 0.030300870537757874\n",
      "Epoch: 30, Samples: 512/5760, Loss: 0.026420459151268005\n",
      "Epoch: 30, Samples: 544/5760, Loss: 0.02086414396762848\n",
      "Epoch: 30, Samples: 576/5760, Loss: 0.023373007774353027\n",
      "Epoch: 30, Samples: 608/5760, Loss: 0.016313612461090088\n",
      "Epoch: 30, Samples: 640/5760, Loss: 0.01293674111366272\n",
      "Epoch: 30, Samples: 672/5760, Loss: 0.018546611070632935\n",
      "Epoch: 30, Samples: 704/5760, Loss: 0.013763874769210815\n",
      "Epoch: 30, Samples: 736/5760, Loss: 0.018409371376037598\n",
      "Epoch: 30, Samples: 768/5760, Loss: 0.009402185678482056\n",
      "Epoch: 30, Samples: 800/5760, Loss: 0.00904172658920288\n",
      "Epoch: 30, Samples: 832/5760, Loss: 0.019530385732650757\n",
      "Epoch: 30, Samples: 864/5760, Loss: 0.01716873049736023\n",
      "Epoch: 30, Samples: 896/5760, Loss: 0.02266111969947815\n",
      "Epoch: 30, Samples: 928/5760, Loss: 0.01357695460319519\n",
      "Epoch: 30, Samples: 960/5760, Loss: 0.013042718172073364\n",
      "Epoch: 30, Samples: 992/5760, Loss: 0.018063396215438843\n",
      "Epoch: 30, Samples: 1024/5760, Loss: 0.022354930639266968\n",
      "Epoch: 30, Samples: 1056/5760, Loss: 0.011946350336074829\n",
      "Epoch: 30, Samples: 1088/5760, Loss: 0.018744677305221558\n",
      "Epoch: 30, Samples: 1120/5760, Loss: 0.01577097177505493\n",
      "Epoch: 30, Samples: 1152/5760, Loss: 0.013845086097717285\n",
      "Epoch: 30, Samples: 1184/5760, Loss: 0.01998203992843628\n",
      "Epoch: 30, Samples: 1216/5760, Loss: 0.030489236116409302\n",
      "Epoch: 30, Samples: 1248/5760, Loss: 0.03665429353713989\n",
      "Epoch: 30, Samples: 1280/5760, Loss: 0.027860820293426514\n",
      "Epoch: 30, Samples: 1312/5760, Loss: 0.014866024255752563\n",
      "Epoch: 30, Samples: 1344/5760, Loss: 0.013801336288452148\n",
      "Epoch: 30, Samples: 1376/5760, Loss: 0.026517927646636963\n",
      "Epoch: 30, Samples: 1408/5760, Loss: 0.02051195502281189\n",
      "Epoch: 30, Samples: 1440/5760, Loss: 0.01779836416244507\n",
      "Epoch: 30, Samples: 1472/5760, Loss: 0.02011805772781372\n",
      "Epoch: 30, Samples: 1504/5760, Loss: 0.024373352527618408\n",
      "Epoch: 30, Samples: 1536/5760, Loss: 0.011812835931777954\n",
      "Epoch: 30, Samples: 1568/5760, Loss: 0.011564135551452637\n",
      "Epoch: 30, Samples: 1600/5760, Loss: 0.01200142502784729\n",
      "Epoch: 30, Samples: 1632/5760, Loss: 0.016832828521728516\n",
      "Epoch: 30, Samples: 1664/5760, Loss: 0.024326622486114502\n",
      "Epoch: 30, Samples: 1696/5760, Loss: 0.008551687002182007\n",
      "Epoch: 30, Samples: 1728/5760, Loss: 0.03695642948150635\n",
      "Epoch: 30, Samples: 1760/5760, Loss: 0.019955575466156006\n",
      "Epoch: 30, Samples: 1792/5760, Loss: 0.024166524410247803\n",
      "Epoch: 30, Samples: 1824/5760, Loss: 0.018346697092056274\n",
      "Epoch: 30, Samples: 1856/5760, Loss: 0.016937851905822754\n",
      "Epoch: 30, Samples: 1888/5760, Loss: 0.01854652166366577\n",
      "Epoch: 30, Samples: 1920/5760, Loss: 0.024355486035346985\n",
      "Epoch: 30, Samples: 1952/5760, Loss: 0.020385801792144775\n",
      "Epoch: 30, Samples: 1984/5760, Loss: 0.016930073499679565\n",
      "Epoch: 30, Samples: 2016/5760, Loss: 0.025764137506484985\n",
      "Epoch: 30, Samples: 2048/5760, Loss: 0.014572203159332275\n",
      "Epoch: 30, Samples: 2080/5760, Loss: 0.01658916473388672\n",
      "Epoch: 30, Samples: 2112/5760, Loss: 0.010840237140655518\n",
      "Epoch: 30, Samples: 2144/5760, Loss: 0.012008488178253174\n",
      "Epoch: 30, Samples: 2176/5760, Loss: 0.01770418882369995\n",
      "Epoch: 30, Samples: 2208/5760, Loss: 0.020268142223358154\n",
      "Epoch: 30, Samples: 2240/5760, Loss: 0.013899773359298706\n",
      "Epoch: 30, Samples: 2272/5760, Loss: 0.013214081525802612\n",
      "Epoch: 30, Samples: 2304/5760, Loss: 0.017754703760147095\n",
      "Epoch: 30, Samples: 2336/5760, Loss: 0.016984909772872925\n",
      "Epoch: 30, Samples: 2368/5760, Loss: 0.008373260498046875\n",
      "Epoch: 30, Samples: 2400/5760, Loss: 0.02497360110282898\n",
      "Epoch: 30, Samples: 2432/5760, Loss: 0.02907031774520874\n",
      "Epoch: 30, Samples: 2464/5760, Loss: 0.01644006371498108\n",
      "Epoch: 30, Samples: 2496/5760, Loss: 0.011357277631759644\n",
      "Epoch: 30, Samples: 2528/5760, Loss: 0.020074903964996338\n",
      "Epoch: 30, Samples: 2560/5760, Loss: 0.010744988918304443\n",
      "Epoch: 30, Samples: 2592/5760, Loss: 0.019403904676437378\n",
      "Epoch: 30, Samples: 2624/5760, Loss: 0.012100189924240112\n",
      "Epoch: 30, Samples: 2656/5760, Loss: 0.016823142766952515\n",
      "Epoch: 30, Samples: 2688/5760, Loss: 0.023803621530532837\n",
      "Epoch: 30, Samples: 2720/5760, Loss: 0.013412952423095703\n",
      "Epoch: 30, Samples: 2752/5760, Loss: 0.012172192335128784\n",
      "Epoch: 30, Samples: 2784/5760, Loss: 0.018072903156280518\n",
      "Epoch: 30, Samples: 2816/5760, Loss: 0.039226919412612915\n",
      "Epoch: 30, Samples: 2848/5760, Loss: 0.02033516764640808\n",
      "Epoch: 30, Samples: 2880/5760, Loss: 0.021727681159973145\n",
      "Epoch: 30, Samples: 2912/5760, Loss: 0.020032882690429688\n",
      "Epoch: 30, Samples: 2944/5760, Loss: 0.01830768585205078\n",
      "Epoch: 30, Samples: 2976/5760, Loss: 0.01613098382949829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Samples: 3008/5760, Loss: 0.01790452003479004\n",
      "Epoch: 30, Samples: 3040/5760, Loss: 0.012087374925613403\n",
      "Epoch: 30, Samples: 3072/5760, Loss: 0.015053033828735352\n",
      "Epoch: 30, Samples: 3104/5760, Loss: 0.011030256748199463\n",
      "Epoch: 30, Samples: 3136/5760, Loss: 0.019394367933273315\n",
      "Epoch: 30, Samples: 3168/5760, Loss: 0.014077484607696533\n",
      "Epoch: 30, Samples: 3200/5760, Loss: 0.01931534707546234\n",
      "Epoch: 30, Samples: 3232/5760, Loss: 0.023389533162117004\n",
      "Epoch: 30, Samples: 3264/5760, Loss: 0.0563642680644989\n",
      "Epoch: 30, Samples: 3296/5760, Loss: 0.011945903301239014\n",
      "Epoch: 30, Samples: 3328/5760, Loss: 0.03376007080078125\n",
      "Epoch: 30, Samples: 3360/5760, Loss: 0.021971017122268677\n",
      "Epoch: 30, Samples: 3392/5760, Loss: 0.016611456871032715\n",
      "Epoch: 30, Samples: 3424/5760, Loss: 0.008401244878768921\n",
      "Epoch: 30, Samples: 3456/5760, Loss: 0.0150068998336792\n",
      "Epoch: 30, Samples: 3488/5760, Loss: 0.014051258563995361\n",
      "Epoch: 30, Samples: 3520/5760, Loss: 0.023013725876808167\n",
      "Epoch: 30, Samples: 3552/5760, Loss: 0.019429266452789307\n",
      "Epoch: 30, Samples: 3584/5760, Loss: 0.016354680061340332\n",
      "Epoch: 30, Samples: 3616/5760, Loss: 0.010841846466064453\n",
      "Epoch: 30, Samples: 3648/5760, Loss: 0.011348545551300049\n",
      "Epoch: 30, Samples: 3680/5760, Loss: 0.013604700565338135\n",
      "Epoch: 30, Samples: 3712/5760, Loss: 0.027616649866104126\n",
      "Epoch: 30, Samples: 3744/5760, Loss: 0.030623525381088257\n",
      "Epoch: 30, Samples: 3776/5760, Loss: 0.028663575649261475\n",
      "Epoch: 30, Samples: 3808/5760, Loss: 0.013323664665222168\n",
      "Epoch: 30, Samples: 3840/5760, Loss: 0.017855286598205566\n",
      "Epoch: 30, Samples: 3872/5760, Loss: 0.013602674007415771\n",
      "Epoch: 30, Samples: 3904/5760, Loss: 0.015014886856079102\n",
      "Epoch: 30, Samples: 3936/5760, Loss: 0.008743494749069214\n",
      "Epoch: 30, Samples: 3968/5760, Loss: 0.012067586183547974\n",
      "Epoch: 30, Samples: 4000/5760, Loss: 0.024505257606506348\n",
      "Epoch: 30, Samples: 4032/5760, Loss: 0.018773257732391357\n",
      "Epoch: 30, Samples: 4064/5760, Loss: 0.010165750980377197\n",
      "Epoch: 30, Samples: 4096/5760, Loss: 0.013164490461349487\n",
      "Epoch: 30, Samples: 4128/5760, Loss: 0.016163259744644165\n",
      "Epoch: 30, Samples: 4160/5760, Loss: 0.019140958786010742\n",
      "Epoch: 30, Samples: 4192/5760, Loss: 0.015900403261184692\n",
      "Epoch: 30, Samples: 4224/5760, Loss: 0.0211983323097229\n",
      "Epoch: 30, Samples: 4256/5760, Loss: 0.015661031007766724\n",
      "Epoch: 30, Samples: 4288/5760, Loss: 0.008205324411392212\n",
      "Epoch: 30, Samples: 4320/5760, Loss: 0.011026233434677124\n",
      "Epoch: 30, Samples: 4352/5760, Loss: 0.050493478775024414\n",
      "Epoch: 30, Samples: 4384/5760, Loss: 0.008605331182479858\n",
      "Epoch: 30, Samples: 4416/5760, Loss: 0.010993599891662598\n",
      "Epoch: 30, Samples: 4448/5760, Loss: 0.019907891750335693\n",
      "Epoch: 30, Samples: 4480/5760, Loss: 0.0332084596157074\n",
      "Epoch: 30, Samples: 4512/5760, Loss: 0.015980303287506104\n",
      "Epoch: 30, Samples: 4544/5760, Loss: 0.014636337757110596\n",
      "Epoch: 30, Samples: 4576/5760, Loss: 0.01662147045135498\n",
      "Epoch: 30, Samples: 4608/5760, Loss: 0.012238085269927979\n",
      "Epoch: 30, Samples: 4640/5760, Loss: 0.01037532091140747\n",
      "Epoch: 30, Samples: 4672/5760, Loss: 0.01814308762550354\n",
      "Epoch: 30, Samples: 4704/5760, Loss: 0.010760128498077393\n",
      "Epoch: 30, Samples: 4736/5760, Loss: 0.011174619197845459\n",
      "Epoch: 30, Samples: 4768/5760, Loss: 0.025373265147209167\n",
      "Epoch: 30, Samples: 4800/5760, Loss: 0.014625132083892822\n",
      "Epoch: 30, Samples: 4832/5760, Loss: 0.011378377676010132\n",
      "Epoch: 30, Samples: 4864/5760, Loss: 0.02792850136756897\n",
      "Epoch: 30, Samples: 4896/5760, Loss: 0.026301980018615723\n",
      "Epoch: 30, Samples: 4928/5760, Loss: 0.024368494749069214\n",
      "Epoch: 30, Samples: 4960/5760, Loss: 0.015099495649337769\n",
      "Epoch: 30, Samples: 4992/5760, Loss: 0.024544090032577515\n",
      "Epoch: 30, Samples: 5024/5760, Loss: 0.02286754548549652\n",
      "Epoch: 30, Samples: 5056/5760, Loss: 0.014856070280075073\n",
      "Epoch: 30, Samples: 5088/5760, Loss: 0.013145536184310913\n",
      "Epoch: 30, Samples: 5120/5760, Loss: 0.015851914882659912\n",
      "Epoch: 30, Samples: 5152/5760, Loss: 0.02201603353023529\n",
      "Epoch: 30, Samples: 5184/5760, Loss: 0.016887247562408447\n",
      "Epoch: 30, Samples: 5216/5760, Loss: 0.011917352676391602\n",
      "Epoch: 30, Samples: 5248/5760, Loss: 0.01593935489654541\n",
      "Epoch: 30, Samples: 5280/5760, Loss: 0.01575523614883423\n",
      "Epoch: 30, Samples: 5312/5760, Loss: 0.012655258178710938\n",
      "Epoch: 30, Samples: 5344/5760, Loss: 0.018989115953445435\n",
      "Epoch: 30, Samples: 5376/5760, Loss: 0.01124832034111023\n",
      "Epoch: 30, Samples: 5408/5760, Loss: 0.017022013664245605\n",
      "Epoch: 30, Samples: 5440/5760, Loss: 0.021842360496520996\n",
      "Epoch: 30, Samples: 5472/5760, Loss: 0.028597861528396606\n",
      "Epoch: 30, Samples: 5504/5760, Loss: 0.017093747854232788\n",
      "Epoch: 30, Samples: 5536/5760, Loss: 0.04685083031654358\n",
      "Epoch: 30, Samples: 5568/5760, Loss: 0.026029974222183228\n",
      "Epoch: 30, Samples: 5600/5760, Loss: 0.009040534496307373\n",
      "Epoch: 30, Samples: 5632/5760, Loss: 0.019560575485229492\n",
      "Epoch: 30, Samples: 5664/5760, Loss: 0.009841114282608032\n",
      "Epoch: 30, Samples: 5696/5760, Loss: 0.014540523290634155\n",
      "Epoch: 30, Samples: 5728/5760, Loss: 0.19798076152801514\n",
      "\n",
      "Epoch: 30\n",
      "Training set: Average loss: 0.0194\n",
      "Validation set: Average loss: 0.3305, Accuracy: 744/818 (91%)\n",
      "Saving model (epoch 30) with lowest validation loss: 0.3305234903326401\n",
      "Epoch: 31, Samples: 0/5760, Loss: 0.027590394020080566\n",
      "Epoch: 31, Samples: 32/5760, Loss: 0.015464186668395996\n",
      "Epoch: 31, Samples: 64/5760, Loss: 0.012513548135757446\n",
      "Epoch: 31, Samples: 96/5760, Loss: 0.017279475927352905\n",
      "Epoch: 31, Samples: 128/5760, Loss: 0.016753971576690674\n",
      "Epoch: 31, Samples: 160/5760, Loss: 0.013357371091842651\n",
      "Epoch: 31, Samples: 192/5760, Loss: 0.016720205545425415\n",
      "Epoch: 31, Samples: 224/5760, Loss: 0.018756568431854248\n",
      "Epoch: 31, Samples: 256/5760, Loss: 0.020791292190551758\n",
      "Epoch: 31, Samples: 288/5760, Loss: 0.00912851095199585\n",
      "Epoch: 31, Samples: 320/5760, Loss: 0.015351623296737671\n",
      "Epoch: 31, Samples: 352/5760, Loss: 0.01683381199836731\n",
      "Epoch: 31, Samples: 384/5760, Loss: 0.019493818283081055\n",
      "Epoch: 31, Samples: 416/5760, Loss: 0.01346704363822937\n",
      "Epoch: 31, Samples: 448/5760, Loss: 0.01554688811302185\n",
      "Epoch: 31, Samples: 480/5760, Loss: 0.023523986339569092\n",
      "Epoch: 31, Samples: 512/5760, Loss: 0.012996017932891846\n",
      "Epoch: 31, Samples: 544/5760, Loss: 0.010030120611190796\n",
      "Epoch: 31, Samples: 576/5760, Loss: 0.010697543621063232\n",
      "Epoch: 31, Samples: 608/5760, Loss: 0.02072286605834961\n",
      "Epoch: 31, Samples: 640/5760, Loss: 0.009681671857833862\n",
      "Epoch: 31, Samples: 672/5760, Loss: 0.015404611825942993\n",
      "Epoch: 31, Samples: 704/5760, Loss: 0.012879043817520142\n",
      "Epoch: 31, Samples: 736/5760, Loss: 0.014661669731140137\n",
      "Epoch: 31, Samples: 768/5760, Loss: 0.00956752896308899\n",
      "Epoch: 31, Samples: 800/5760, Loss: 0.010524719953536987\n",
      "Epoch: 31, Samples: 832/5760, Loss: 0.04442407190799713\n",
      "Epoch: 31, Samples: 864/5760, Loss: 0.02771608531475067\n",
      "Epoch: 31, Samples: 896/5760, Loss: 0.022226810455322266\n",
      "Epoch: 31, Samples: 928/5760, Loss: 0.014320999383926392\n",
      "Epoch: 31, Samples: 960/5760, Loss: 0.02905973792076111\n",
      "Epoch: 31, Samples: 992/5760, Loss: 0.021414846181869507\n",
      "Epoch: 31, Samples: 1024/5760, Loss: 0.012044519186019897\n",
      "Epoch: 31, Samples: 1056/5760, Loss: 0.01230311393737793\n",
      "Epoch: 31, Samples: 1088/5760, Loss: 0.019749581813812256\n",
      "Epoch: 31, Samples: 1120/5760, Loss: 0.02900993824005127\n",
      "Epoch: 31, Samples: 1152/5760, Loss: 0.014349251985549927\n",
      "Epoch: 31, Samples: 1184/5760, Loss: 0.017515122890472412\n",
      "Epoch: 31, Samples: 1216/5760, Loss: 0.02439257502555847\n",
      "Epoch: 31, Samples: 1248/5760, Loss: 0.011860281229019165\n",
      "Epoch: 31, Samples: 1280/5760, Loss: 0.018754303455352783\n",
      "Epoch: 31, Samples: 1312/5760, Loss: 0.014436542987823486\n",
      "Epoch: 31, Samples: 1344/5760, Loss: 0.013332068920135498\n",
      "Epoch: 31, Samples: 1376/5760, Loss: 0.013402402400970459\n",
      "Epoch: 31, Samples: 1408/5760, Loss: 0.022634029388427734\n",
      "Epoch: 31, Samples: 1440/5760, Loss: 0.022788137197494507\n",
      "Epoch: 31, Samples: 1472/5760, Loss: 0.010033667087554932\n",
      "Epoch: 31, Samples: 1504/5760, Loss: 0.014805048704147339\n",
      "Epoch: 31, Samples: 1536/5760, Loss: 0.013251721858978271\n",
      "Epoch: 31, Samples: 1568/5760, Loss: 0.01684674620628357\n",
      "Epoch: 31, Samples: 1600/5760, Loss: 0.008146554231643677\n",
      "Epoch: 31, Samples: 1632/5760, Loss: 0.009769350290298462\n",
      "Epoch: 31, Samples: 1664/5760, Loss: 0.014992564916610718\n",
      "Epoch: 31, Samples: 1696/5760, Loss: 0.024981200695037842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31, Samples: 1728/5760, Loss: 0.014085590839385986\n",
      "Epoch: 31, Samples: 1760/5760, Loss: 0.025741398334503174\n",
      "Epoch: 31, Samples: 1792/5760, Loss: 0.019761323928833008\n",
      "Epoch: 31, Samples: 1824/5760, Loss: 0.008686244487762451\n",
      "Epoch: 31, Samples: 1856/5760, Loss: 0.011896371841430664\n",
      "Epoch: 31, Samples: 1888/5760, Loss: 0.011063307523727417\n",
      "Epoch: 31, Samples: 1920/5760, Loss: 0.012372434139251709\n",
      "Epoch: 31, Samples: 1952/5760, Loss: 0.016971677541732788\n",
      "Epoch: 31, Samples: 1984/5760, Loss: 0.010918796062469482\n",
      "Epoch: 31, Samples: 2016/5760, Loss: 0.00908079743385315\n",
      "Epoch: 31, Samples: 2048/5760, Loss: 0.01896733045578003\n",
      "Epoch: 31, Samples: 2080/5760, Loss: 0.020443469285964966\n",
      "Epoch: 31, Samples: 2112/5760, Loss: 0.02797597646713257\n",
      "Epoch: 31, Samples: 2144/5760, Loss: 0.020725220441818237\n",
      "Epoch: 31, Samples: 2176/5760, Loss: 0.01566988229751587\n",
      "Epoch: 31, Samples: 2208/5760, Loss: 0.01254051923751831\n",
      "Epoch: 31, Samples: 2240/5760, Loss: 0.014711946249008179\n",
      "Epoch: 31, Samples: 2272/5760, Loss: 0.01589679718017578\n",
      "Epoch: 31, Samples: 2304/5760, Loss: 0.009199172258377075\n",
      "Epoch: 31, Samples: 2336/5760, Loss: 0.014950692653656006\n",
      "Epoch: 31, Samples: 2368/5760, Loss: 0.018910378217697144\n",
      "Epoch: 31, Samples: 2400/5760, Loss: 0.013761073350906372\n",
      "Epoch: 31, Samples: 2432/5760, Loss: 0.015098035335540771\n",
      "Epoch: 31, Samples: 2464/5760, Loss: 0.010601550340652466\n",
      "Epoch: 31, Samples: 2496/5760, Loss: 0.012605398893356323\n",
      "Epoch: 31, Samples: 2528/5760, Loss: 0.01487058401107788\n",
      "Epoch: 31, Samples: 2560/5760, Loss: 0.022303849458694458\n",
      "Epoch: 31, Samples: 2592/5760, Loss: 0.021196097135543823\n",
      "Epoch: 31, Samples: 2624/5760, Loss: 0.020833700895309448\n",
      "Epoch: 31, Samples: 2656/5760, Loss: 0.02075129747390747\n",
      "Epoch: 31, Samples: 2688/5760, Loss: 0.013997465372085571\n",
      "Epoch: 31, Samples: 2720/5760, Loss: 0.015808671712875366\n",
      "Epoch: 31, Samples: 2752/5760, Loss: 0.01931971311569214\n",
      "Epoch: 31, Samples: 2784/5760, Loss: 0.010680228471755981\n",
      "Epoch: 31, Samples: 2816/5760, Loss: 0.017536818981170654\n",
      "Epoch: 31, Samples: 2848/5760, Loss: 0.0116158127784729\n",
      "Epoch: 31, Samples: 2880/5760, Loss: 0.010465413331985474\n",
      "Epoch: 31, Samples: 2912/5760, Loss: 0.01733940839767456\n",
      "Epoch: 31, Samples: 2944/5760, Loss: 0.009750127792358398\n",
      "Epoch: 31, Samples: 2976/5760, Loss: 0.013121604919433594\n",
      "Epoch: 31, Samples: 3008/5760, Loss: 0.024972140789031982\n",
      "Epoch: 31, Samples: 3040/5760, Loss: 0.018577009439468384\n",
      "Epoch: 31, Samples: 3072/5760, Loss: 0.020120441913604736\n",
      "Epoch: 31, Samples: 3104/5760, Loss: 0.01612195372581482\n",
      "Epoch: 31, Samples: 3136/5760, Loss: 0.01944059133529663\n",
      "Epoch: 31, Samples: 3168/5760, Loss: 0.010934323072433472\n",
      "Epoch: 31, Samples: 3200/5760, Loss: 0.02281084656715393\n",
      "Epoch: 31, Samples: 3232/5760, Loss: 0.01882210373878479\n",
      "Epoch: 31, Samples: 3264/5760, Loss: 0.018003225326538086\n",
      "Epoch: 31, Samples: 3296/5760, Loss: 0.01396113634109497\n",
      "Epoch: 31, Samples: 3328/5760, Loss: 0.01481395959854126\n",
      "Epoch: 31, Samples: 3360/5760, Loss: 0.02151849865913391\n",
      "Epoch: 31, Samples: 3392/5760, Loss: 0.008871853351593018\n",
      "Epoch: 31, Samples: 3424/5760, Loss: 0.03163596987724304\n",
      "Epoch: 31, Samples: 3456/5760, Loss: 0.014945745468139648\n",
      "Epoch: 31, Samples: 3488/5760, Loss: 0.019112706184387207\n",
      "Epoch: 31, Samples: 3520/5760, Loss: 0.010988593101501465\n",
      "Epoch: 31, Samples: 3552/5760, Loss: 0.020057648420333862\n",
      "Epoch: 31, Samples: 3584/5760, Loss: 0.01220276951789856\n",
      "Epoch: 31, Samples: 3616/5760, Loss: 0.050577715039253235\n",
      "Epoch: 31, Samples: 3648/5760, Loss: 0.01663842797279358\n",
      "Epoch: 31, Samples: 3680/5760, Loss: 0.023736566305160522\n",
      "Epoch: 31, Samples: 3712/5760, Loss: 0.017275482416152954\n",
      "Epoch: 31, Samples: 3744/5760, Loss: 0.01471179723739624\n",
      "Epoch: 31, Samples: 3776/5760, Loss: 0.023227035999298096\n",
      "Epoch: 31, Samples: 3808/5760, Loss: 0.02356088161468506\n",
      "Epoch: 31, Samples: 3840/5760, Loss: 0.013353079557418823\n",
      "Epoch: 31, Samples: 3872/5760, Loss: 0.01942107081413269\n",
      "Epoch: 31, Samples: 3904/5760, Loss: 0.01377221941947937\n",
      "Epoch: 31, Samples: 3936/5760, Loss: 0.019000202417373657\n",
      "Epoch: 31, Samples: 3968/5760, Loss: 0.017827779054641724\n",
      "Epoch: 31, Samples: 4000/5760, Loss: 0.014685302972793579\n",
      "Epoch: 31, Samples: 4032/5760, Loss: 0.012401461601257324\n",
      "Epoch: 31, Samples: 4064/5760, Loss: 0.026340514421463013\n",
      "Epoch: 31, Samples: 4096/5760, Loss: 0.021933019161224365\n",
      "Epoch: 31, Samples: 4128/5760, Loss: 0.011606782674789429\n",
      "Epoch: 31, Samples: 4160/5760, Loss: 0.01208430528640747\n",
      "Epoch: 31, Samples: 4192/5760, Loss: 0.016596734523773193\n",
      "Epoch: 31, Samples: 4224/5760, Loss: 0.025946438312530518\n",
      "Epoch: 31, Samples: 4256/5760, Loss: 0.027052372694015503\n",
      "Epoch: 31, Samples: 4288/5760, Loss: 0.024547994136810303\n",
      "Epoch: 31, Samples: 4320/5760, Loss: 0.028197377920150757\n",
      "Epoch: 31, Samples: 4352/5760, Loss: 0.008717328310012817\n",
      "Epoch: 31, Samples: 4384/5760, Loss: 0.02051207423210144\n",
      "Epoch: 31, Samples: 4416/5760, Loss: 0.013600319623947144\n",
      "Epoch: 31, Samples: 4448/5760, Loss: 0.01559951901435852\n",
      "Epoch: 31, Samples: 4480/5760, Loss: 0.016688883304595947\n",
      "Epoch: 31, Samples: 4512/5760, Loss: 0.009796470403671265\n",
      "Epoch: 31, Samples: 4544/5760, Loss: 0.010281562805175781\n",
      "Epoch: 31, Samples: 4576/5760, Loss: 0.02454724907875061\n",
      "Epoch: 31, Samples: 4608/5760, Loss: 0.014812111854553223\n",
      "Epoch: 31, Samples: 4640/5760, Loss: 0.014461934566497803\n",
      "Epoch: 31, Samples: 4672/5760, Loss: 0.01618131995201111\n",
      "Epoch: 31, Samples: 4704/5760, Loss: 0.014351606369018555\n",
      "Epoch: 31, Samples: 4736/5760, Loss: 0.02766430377960205\n",
      "Epoch: 31, Samples: 4768/5760, Loss: 0.017085224390029907\n",
      "Epoch: 31, Samples: 4800/5760, Loss: 0.02747933566570282\n",
      "Epoch: 31, Samples: 4832/5760, Loss: 0.02293732762336731\n",
      "Epoch: 31, Samples: 4864/5760, Loss: 0.02114969491958618\n",
      "Epoch: 31, Samples: 4896/5760, Loss: 0.01807469129562378\n",
      "Epoch: 31, Samples: 4928/5760, Loss: 0.039352238178253174\n",
      "Epoch: 31, Samples: 4960/5760, Loss: 0.013291001319885254\n",
      "Epoch: 31, Samples: 4992/5760, Loss: 0.01405644416809082\n",
      "Epoch: 31, Samples: 5024/5760, Loss: 0.017267227172851562\n",
      "Epoch: 31, Samples: 5056/5760, Loss: 0.011587530374526978\n",
      "Epoch: 31, Samples: 5088/5760, Loss: 0.010344594717025757\n",
      "Epoch: 31, Samples: 5120/5760, Loss: 0.030890583992004395\n",
      "Epoch: 31, Samples: 5152/5760, Loss: 0.016109377145767212\n",
      "Epoch: 31, Samples: 5184/5760, Loss: 0.01885545253753662\n",
      "Epoch: 31, Samples: 5216/5760, Loss: 0.025915414094924927\n",
      "Epoch: 31, Samples: 5248/5760, Loss: 0.011762529611587524\n",
      "Epoch: 31, Samples: 5280/5760, Loss: 0.019145280122756958\n",
      "Epoch: 31, Samples: 5312/5760, Loss: 0.013353526592254639\n",
      "Epoch: 31, Samples: 5344/5760, Loss: 0.011748194694519043\n",
      "Epoch: 31, Samples: 5376/5760, Loss: 0.009210944175720215\n",
      "Epoch: 31, Samples: 5408/5760, Loss: 0.01476353406906128\n",
      "Epoch: 31, Samples: 5440/5760, Loss: 0.010714590549468994\n",
      "Epoch: 31, Samples: 5472/5760, Loss: 0.008091181516647339\n",
      "Epoch: 31, Samples: 5504/5760, Loss: 0.014660477638244629\n",
      "Epoch: 31, Samples: 5536/5760, Loss: 0.02232426404953003\n",
      "Epoch: 31, Samples: 5568/5760, Loss: 0.023304343223571777\n",
      "Epoch: 31, Samples: 5600/5760, Loss: 0.03491811454296112\n",
      "Epoch: 31, Samples: 5632/5760, Loss: 0.030175015330314636\n",
      "Epoch: 31, Samples: 5664/5760, Loss: 0.012832075357437134\n",
      "Epoch: 31, Samples: 5696/5760, Loss: 0.011973917484283447\n",
      "Epoch: 31, Samples: 5728/5760, Loss: 0.9825286865234375\n",
      "\n",
      "Epoch: 31\n",
      "Training set: Average loss: 0.0228\n",
      "Validation set: Average loss: 0.3298, Accuracy: 750/818 (92%)\n",
      "Saving model (epoch 31) with lowest validation loss: 0.32982279990728086\n",
      "Epoch: 32, Samples: 0/5760, Loss: 0.02670520544052124\n",
      "Epoch: 32, Samples: 32/5760, Loss: 0.0162927508354187\n",
      "Epoch: 32, Samples: 64/5760, Loss: 0.011042892932891846\n",
      "Epoch: 32, Samples: 96/5760, Loss: 0.010075509548187256\n",
      "Epoch: 32, Samples: 128/5760, Loss: 0.02092111110687256\n",
      "Epoch: 32, Samples: 160/5760, Loss: 0.027925103902816772\n",
      "Epoch: 32, Samples: 192/5760, Loss: 0.007944673299789429\n",
      "Epoch: 32, Samples: 224/5760, Loss: 0.01639556884765625\n",
      "Epoch: 32, Samples: 256/5760, Loss: 0.026235833764076233\n",
      "Epoch: 32, Samples: 288/5760, Loss: 0.010386645793914795\n",
      "Epoch: 32, Samples: 320/5760, Loss: 0.009527534246444702\n",
      "Epoch: 32, Samples: 352/5760, Loss: 0.025124281644821167\n",
      "Epoch: 32, Samples: 384/5760, Loss: 0.04154789447784424\n",
      "Epoch: 32, Samples: 416/5760, Loss: 0.01630261540412903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Samples: 448/5760, Loss: 0.019306659698486328\n",
      "Epoch: 32, Samples: 480/5760, Loss: 0.01042678952217102\n",
      "Epoch: 32, Samples: 512/5760, Loss: 0.018144339323043823\n",
      "Epoch: 32, Samples: 544/5760, Loss: 0.027469336986541748\n",
      "Epoch: 32, Samples: 576/5760, Loss: 0.02783164381980896\n",
      "Epoch: 32, Samples: 608/5760, Loss: 0.00909385085105896\n",
      "Epoch: 32, Samples: 640/5760, Loss: 0.010749846696853638\n",
      "Epoch: 32, Samples: 672/5760, Loss: 0.022071748971939087\n",
      "Epoch: 32, Samples: 704/5760, Loss: 0.012432724237442017\n",
      "Epoch: 32, Samples: 736/5760, Loss: 0.009133100509643555\n",
      "Epoch: 32, Samples: 768/5760, Loss: 0.030370891094207764\n",
      "Epoch: 32, Samples: 800/5760, Loss: 0.033702343702316284\n",
      "Epoch: 32, Samples: 832/5760, Loss: 0.013592183589935303\n",
      "Epoch: 32, Samples: 864/5760, Loss: 0.015363246202468872\n",
      "Epoch: 32, Samples: 896/5760, Loss: 0.010311931371688843\n",
      "Epoch: 32, Samples: 928/5760, Loss: 0.007354676723480225\n",
      "Epoch: 32, Samples: 960/5760, Loss: 0.011167868971824646\n",
      "Epoch: 32, Samples: 992/5760, Loss: 0.010495930910110474\n",
      "Epoch: 32, Samples: 1024/5760, Loss: 0.012890398502349854\n",
      "Epoch: 32, Samples: 1056/5760, Loss: 0.010472267866134644\n",
      "Epoch: 32, Samples: 1088/5760, Loss: 0.01711207628250122\n",
      "Epoch: 32, Samples: 1120/5760, Loss: 0.0402836799621582\n",
      "Epoch: 32, Samples: 1152/5760, Loss: 0.017757147550582886\n",
      "Epoch: 32, Samples: 1184/5760, Loss: 0.012416660785675049\n",
      "Epoch: 32, Samples: 1216/5760, Loss: 0.018952488899230957\n",
      "Epoch: 32, Samples: 1248/5760, Loss: 0.018630564212799072\n",
      "Epoch: 32, Samples: 1280/5760, Loss: 0.02665695548057556\n",
      "Epoch: 32, Samples: 1312/5760, Loss: 0.01944848895072937\n",
      "Epoch: 32, Samples: 1344/5760, Loss: 0.022394657135009766\n",
      "Epoch: 32, Samples: 1376/5760, Loss: 0.030881404876708984\n",
      "Epoch: 32, Samples: 1408/5760, Loss: 0.013104885816574097\n",
      "Epoch: 32, Samples: 1440/5760, Loss: 0.018510282039642334\n",
      "Epoch: 32, Samples: 1472/5760, Loss: 0.006743729114532471\n",
      "Epoch: 32, Samples: 1504/5760, Loss: 0.011633336544036865\n",
      "Epoch: 32, Samples: 1536/5760, Loss: 0.049988582730293274\n",
      "Epoch: 32, Samples: 1568/5760, Loss: 0.020825445652008057\n",
      "Epoch: 32, Samples: 1600/5760, Loss: 0.013094991445541382\n",
      "Epoch: 32, Samples: 1632/5760, Loss: 0.018222182989120483\n",
      "Epoch: 32, Samples: 1664/5760, Loss: 0.013363420963287354\n",
      "Epoch: 32, Samples: 1696/5760, Loss: 0.008971035480499268\n",
      "Epoch: 32, Samples: 1728/5760, Loss: 0.024553120136260986\n",
      "Epoch: 32, Samples: 1760/5760, Loss: 0.02560645341873169\n",
      "Epoch: 32, Samples: 1792/5760, Loss: 0.006438791751861572\n",
      "Epoch: 32, Samples: 1824/5760, Loss: 0.02290967106819153\n",
      "Epoch: 32, Samples: 1856/5760, Loss: 0.011144191026687622\n",
      "Epoch: 32, Samples: 1888/5760, Loss: 0.021116584539413452\n",
      "Epoch: 32, Samples: 1920/5760, Loss: 0.03180122375488281\n",
      "Epoch: 32, Samples: 1952/5760, Loss: 0.021091341972351074\n",
      "Epoch: 32, Samples: 1984/5760, Loss: 0.006839245557785034\n",
      "Epoch: 32, Samples: 2016/5760, Loss: 0.022311344742774963\n",
      "Epoch: 32, Samples: 2048/5760, Loss: 0.008921533823013306\n",
      "Epoch: 32, Samples: 2080/5760, Loss: 0.048073768615722656\n",
      "Epoch: 32, Samples: 2112/5760, Loss: 0.017015188932418823\n",
      "Epoch: 32, Samples: 2144/5760, Loss: 0.016834408044815063\n",
      "Epoch: 32, Samples: 2176/5760, Loss: 0.018658757209777832\n",
      "Epoch: 32, Samples: 2208/5760, Loss: 0.01437610387802124\n",
      "Epoch: 32, Samples: 2240/5760, Loss: 0.015141308307647705\n",
      "Epoch: 32, Samples: 2272/5760, Loss: 0.009210973978042603\n",
      "Epoch: 32, Samples: 2304/5760, Loss: 0.03547850251197815\n",
      "Epoch: 32, Samples: 2336/5760, Loss: 0.007696926593780518\n",
      "Epoch: 32, Samples: 2368/5760, Loss: 0.016538500785827637\n",
      "Epoch: 32, Samples: 2400/5760, Loss: 0.018323183059692383\n",
      "Epoch: 32, Samples: 2432/5760, Loss: 0.015175193548202515\n",
      "Epoch: 32, Samples: 2464/5760, Loss: 0.008880078792572021\n",
      "Epoch: 32, Samples: 2496/5760, Loss: 0.013043910264968872\n",
      "Epoch: 32, Samples: 2528/5760, Loss: 0.01842641830444336\n",
      "Epoch: 32, Samples: 2560/5760, Loss: 0.010702550411224365\n",
      "Epoch: 32, Samples: 2592/5760, Loss: 0.017514407634735107\n",
      "Epoch: 32, Samples: 2624/5760, Loss: 0.016716480255126953\n",
      "Epoch: 32, Samples: 2656/5760, Loss: 0.01788356900215149\n",
      "Epoch: 32, Samples: 2688/5760, Loss: 0.01780170202255249\n",
      "Epoch: 32, Samples: 2720/5760, Loss: 0.010153114795684814\n",
      "Epoch: 32, Samples: 2752/5760, Loss: 0.011253386735916138\n",
      "Epoch: 32, Samples: 2784/5760, Loss: 0.00650671124458313\n",
      "Epoch: 32, Samples: 2816/5760, Loss: 0.01598799228668213\n",
      "Epoch: 32, Samples: 2848/5760, Loss: 0.016664594411849976\n",
      "Epoch: 32, Samples: 2880/5760, Loss: 0.01697474718093872\n",
      "Epoch: 32, Samples: 2912/5760, Loss: 0.01959472894668579\n",
      "Epoch: 32, Samples: 2944/5760, Loss: 0.006303220987319946\n",
      "Epoch: 32, Samples: 2976/5760, Loss: 0.021117568016052246\n",
      "Epoch: 32, Samples: 3008/5760, Loss: 0.012037336826324463\n",
      "Epoch: 32, Samples: 3040/5760, Loss: 0.009359091520309448\n",
      "Epoch: 32, Samples: 3072/5760, Loss: 0.02152431011199951\n",
      "Epoch: 32, Samples: 3104/5760, Loss: 0.009383231401443481\n",
      "Epoch: 32, Samples: 3136/5760, Loss: 0.03079277276992798\n",
      "Epoch: 32, Samples: 3168/5760, Loss: 0.02095705270767212\n",
      "Epoch: 32, Samples: 3200/5760, Loss: 0.01126021146774292\n",
      "Epoch: 32, Samples: 3232/5760, Loss: 0.021302849054336548\n",
      "Epoch: 32, Samples: 3264/5760, Loss: 0.010003507137298584\n",
      "Epoch: 32, Samples: 3296/5760, Loss: 0.016518235206604004\n",
      "Epoch: 32, Samples: 3328/5760, Loss: 0.02413913607597351\n",
      "Epoch: 32, Samples: 3360/5760, Loss: 0.01305726170539856\n",
      "Epoch: 32, Samples: 3392/5760, Loss: 0.021270334720611572\n",
      "Epoch: 32, Samples: 3424/5760, Loss: 0.011572867631912231\n",
      "Epoch: 32, Samples: 3456/5760, Loss: 0.019958525896072388\n",
      "Epoch: 32, Samples: 3488/5760, Loss: 0.010550051927566528\n",
      "Epoch: 32, Samples: 3520/5760, Loss: 0.007614076137542725\n",
      "Epoch: 32, Samples: 3552/5760, Loss: 0.10453413426876068\n",
      "Epoch: 32, Samples: 3584/5760, Loss: 0.011716395616531372\n",
      "Epoch: 32, Samples: 3616/5760, Loss: 0.010794341564178467\n",
      "Epoch: 32, Samples: 3648/5760, Loss: 0.021436333656311035\n",
      "Epoch: 32, Samples: 3680/5760, Loss: 0.010494887828826904\n",
      "Epoch: 32, Samples: 3712/5760, Loss: 0.02502518892288208\n",
      "Epoch: 32, Samples: 3744/5760, Loss: 0.014378100633621216\n",
      "Epoch: 32, Samples: 3776/5760, Loss: 0.010526120662689209\n",
      "Epoch: 32, Samples: 3808/5760, Loss: 0.008208423852920532\n",
      "Epoch: 32, Samples: 3840/5760, Loss: 0.015723973512649536\n",
      "Epoch: 32, Samples: 3872/5760, Loss: 0.022289037704467773\n",
      "Epoch: 32, Samples: 3904/5760, Loss: 0.012554764747619629\n",
      "Epoch: 32, Samples: 3936/5760, Loss: 0.01800006628036499\n",
      "Epoch: 32, Samples: 3968/5760, Loss: 0.01727890968322754\n",
      "Epoch: 32, Samples: 4000/5760, Loss: 0.015518814325332642\n",
      "Epoch: 32, Samples: 4032/5760, Loss: 0.022251635789871216\n",
      "Epoch: 32, Samples: 4064/5760, Loss: 0.013753682374954224\n",
      "Epoch: 32, Samples: 4096/5760, Loss: 0.027903690934181213\n",
      "Epoch: 32, Samples: 4128/5760, Loss: 0.013457059860229492\n",
      "Epoch: 32, Samples: 4160/5760, Loss: 0.012316077947616577\n",
      "Epoch: 32, Samples: 4192/5760, Loss: 0.015812009572982788\n",
      "Epoch: 32, Samples: 4224/5760, Loss: 0.007892102003097534\n",
      "Epoch: 32, Samples: 4256/5760, Loss: 0.010685741901397705\n",
      "Epoch: 32, Samples: 4288/5760, Loss: 0.012338846921920776\n",
      "Epoch: 32, Samples: 4320/5760, Loss: 0.011407822370529175\n",
      "Epoch: 32, Samples: 4352/5760, Loss: 0.010654419660568237\n",
      "Epoch: 32, Samples: 4384/5760, Loss: 0.020105063915252686\n",
      "Epoch: 32, Samples: 4416/5760, Loss: 0.02479773759841919\n",
      "Epoch: 32, Samples: 4448/5760, Loss: 0.016378194093704224\n",
      "Epoch: 32, Samples: 4480/5760, Loss: 0.042256057262420654\n",
      "Epoch: 32, Samples: 4512/5760, Loss: 0.02327093482017517\n",
      "Epoch: 32, Samples: 4544/5760, Loss: 0.010812371969223022\n",
      "Epoch: 32, Samples: 4576/5760, Loss: 0.019490480422973633\n",
      "Epoch: 32, Samples: 4608/5760, Loss: 0.017404288053512573\n",
      "Epoch: 32, Samples: 4640/5760, Loss: 0.022223398089408875\n",
      "Epoch: 32, Samples: 4672/5760, Loss: 0.024902313947677612\n",
      "Epoch: 32, Samples: 4704/5760, Loss: 0.01719924807548523\n",
      "Epoch: 32, Samples: 4736/5760, Loss: 0.00999516248703003\n",
      "Epoch: 32, Samples: 4768/5760, Loss: 0.019255459308624268\n",
      "Epoch: 32, Samples: 4800/5760, Loss: 0.01679190993309021\n",
      "Epoch: 32, Samples: 4832/5760, Loss: 0.013376176357269287\n",
      "Epoch: 32, Samples: 4864/5760, Loss: 0.019612133502960205\n",
      "Epoch: 32, Samples: 4896/5760, Loss: 0.013194054365158081\n",
      "Epoch: 32, Samples: 4928/5760, Loss: 0.009977847337722778\n",
      "Epoch: 32, Samples: 4960/5760, Loss: 0.013390719890594482\n",
      "Epoch: 32, Samples: 4992/5760, Loss: 0.018681615591049194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Samples: 5024/5760, Loss: 0.022292286157608032\n",
      "Epoch: 32, Samples: 5056/5760, Loss: 0.011328637599945068\n",
      "Epoch: 32, Samples: 5088/5760, Loss: 0.022584974765777588\n",
      "Epoch: 32, Samples: 5120/5760, Loss: 0.01953345537185669\n",
      "Epoch: 32, Samples: 5152/5760, Loss: 0.021537214517593384\n",
      "Epoch: 32, Samples: 5184/5760, Loss: 0.01969391107559204\n",
      "Epoch: 32, Samples: 5216/5760, Loss: 0.0180666446685791\n",
      "Epoch: 32, Samples: 5248/5760, Loss: 0.012847155332565308\n",
      "Epoch: 32, Samples: 5280/5760, Loss: 0.015117108821868896\n",
      "Epoch: 32, Samples: 5312/5760, Loss: 0.013497233390808105\n",
      "Epoch: 32, Samples: 5344/5760, Loss: 0.011180371046066284\n",
      "Epoch: 32, Samples: 5376/5760, Loss: 0.006939202547073364\n",
      "Epoch: 32, Samples: 5408/5760, Loss: 0.015932351350784302\n",
      "Epoch: 32, Samples: 5440/5760, Loss: 0.015182822942733765\n",
      "Epoch: 32, Samples: 5472/5760, Loss: 0.01588931679725647\n",
      "Epoch: 32, Samples: 5504/5760, Loss: 0.01122775673866272\n",
      "Epoch: 32, Samples: 5536/5760, Loss: 0.014389574527740479\n",
      "Epoch: 32, Samples: 5568/5760, Loss: 0.021601766347885132\n",
      "Epoch: 32, Samples: 5600/5760, Loss: 0.008405566215515137\n",
      "Epoch: 32, Samples: 5632/5760, Loss: 0.022806763648986816\n",
      "Epoch: 32, Samples: 5664/5760, Loss: 0.01560238003730774\n",
      "Epoch: 32, Samples: 5696/5760, Loss: 0.012151777744293213\n",
      "Epoch: 32, Samples: 5728/5760, Loss: 0.26370418071746826\n",
      "\n",
      "Epoch: 32\n",
      "Training set: Average loss: 0.0189\n",
      "Validation set: Average loss: 0.3409, Accuracy: 744/818 (91%)\n",
      "Epoch: 33, Samples: 0/5760, Loss: 0.02372869849205017\n",
      "Epoch: 33, Samples: 32/5760, Loss: 0.010095655918121338\n",
      "Epoch: 33, Samples: 64/5760, Loss: 0.006768673658370972\n",
      "Epoch: 33, Samples: 96/5760, Loss: 0.012151867151260376\n",
      "Epoch: 33, Samples: 128/5760, Loss: 0.02189144492149353\n",
      "Epoch: 33, Samples: 160/5760, Loss: 0.009779095649719238\n",
      "Epoch: 33, Samples: 192/5760, Loss: 0.010021030902862549\n",
      "Epoch: 33, Samples: 224/5760, Loss: 0.02164497971534729\n",
      "Epoch: 33, Samples: 256/5760, Loss: 0.01620769500732422\n",
      "Epoch: 33, Samples: 288/5760, Loss: 0.013577461242675781\n",
      "Epoch: 33, Samples: 320/5760, Loss: 0.021542459726333618\n",
      "Epoch: 33, Samples: 352/5760, Loss: 0.021632283926010132\n",
      "Epoch: 33, Samples: 384/5760, Loss: 0.020816564559936523\n",
      "Epoch: 33, Samples: 416/5760, Loss: 0.016437113285064697\n",
      "Epoch: 33, Samples: 448/5760, Loss: 0.01691707968711853\n",
      "Epoch: 33, Samples: 480/5760, Loss: 0.013038873672485352\n",
      "Epoch: 33, Samples: 512/5760, Loss: 0.012334823608398438\n",
      "Epoch: 33, Samples: 544/5760, Loss: 0.008823484182357788\n",
      "Epoch: 33, Samples: 576/5760, Loss: 0.007893234491348267\n",
      "Epoch: 33, Samples: 608/5760, Loss: 0.01928037405014038\n",
      "Epoch: 33, Samples: 640/5760, Loss: 0.0385918915271759\n",
      "Epoch: 33, Samples: 672/5760, Loss: 0.012148499488830566\n",
      "Epoch: 33, Samples: 704/5760, Loss: 0.014236658811569214\n",
      "Epoch: 33, Samples: 736/5760, Loss: 0.011854290962219238\n",
      "Epoch: 33, Samples: 768/5760, Loss: 0.023305147886276245\n",
      "Epoch: 33, Samples: 800/5760, Loss: 0.017415523529052734\n",
      "Epoch: 33, Samples: 832/5760, Loss: 0.010660916566848755\n",
      "Epoch: 33, Samples: 864/5760, Loss: 0.020716488361358643\n",
      "Epoch: 33, Samples: 896/5760, Loss: 0.011447787284851074\n",
      "Epoch: 33, Samples: 928/5760, Loss: 0.020622223615646362\n",
      "Epoch: 33, Samples: 960/5760, Loss: 0.005163401365280151\n",
      "Epoch: 33, Samples: 992/5760, Loss: 0.025036782026290894\n",
      "Epoch: 33, Samples: 1024/5760, Loss: 0.016344428062438965\n",
      "Epoch: 33, Samples: 1056/5760, Loss: 0.013599514961242676\n",
      "Epoch: 33, Samples: 1088/5760, Loss: 0.019006699323654175\n",
      "Epoch: 33, Samples: 1120/5760, Loss: 0.009939879179000854\n",
      "Epoch: 33, Samples: 1152/5760, Loss: 0.012775927782058716\n",
      "Epoch: 33, Samples: 1184/5760, Loss: 0.011234462261199951\n",
      "Epoch: 33, Samples: 1216/5760, Loss: 0.019681543111801147\n",
      "Epoch: 33, Samples: 1248/5760, Loss: 0.01454886794090271\n",
      "Epoch: 33, Samples: 1280/5760, Loss: 0.013492047786712646\n",
      "Epoch: 33, Samples: 1312/5760, Loss: 0.01691684126853943\n",
      "Epoch: 33, Samples: 1344/5760, Loss: 0.01244768500328064\n",
      "Epoch: 33, Samples: 1376/5760, Loss: 0.029623836278915405\n",
      "Epoch: 33, Samples: 1408/5760, Loss: 0.013397306203842163\n",
      "Epoch: 33, Samples: 1440/5760, Loss: 0.0119648277759552\n",
      "Epoch: 33, Samples: 1472/5760, Loss: 0.021245062351226807\n",
      "Epoch: 33, Samples: 1504/5760, Loss: 0.016119778156280518\n",
      "Epoch: 33, Samples: 1536/5760, Loss: 0.011042654514312744\n",
      "Epoch: 33, Samples: 1568/5760, Loss: 0.016374915838241577\n",
      "Epoch: 33, Samples: 1600/5760, Loss: 0.010796159505844116\n",
      "Epoch: 33, Samples: 1632/5760, Loss: 0.009357631206512451\n",
      "Epoch: 33, Samples: 1664/5760, Loss: 0.01611974835395813\n",
      "Epoch: 33, Samples: 1696/5760, Loss: 0.010982662439346313\n",
      "Epoch: 33, Samples: 1728/5760, Loss: 0.012338608503341675\n",
      "Epoch: 33, Samples: 1760/5760, Loss: 0.011512696743011475\n",
      "Epoch: 33, Samples: 1792/5760, Loss: 0.009753793478012085\n",
      "Epoch: 33, Samples: 1824/5760, Loss: 0.009693950414657593\n",
      "Epoch: 33, Samples: 1856/5760, Loss: 0.014794796705245972\n",
      "Epoch: 33, Samples: 1888/5760, Loss: 0.01016688346862793\n",
      "Epoch: 33, Samples: 1920/5760, Loss: 0.015774816274642944\n",
      "Epoch: 33, Samples: 1952/5760, Loss: 0.021305710077285767\n",
      "Epoch: 33, Samples: 1984/5760, Loss: 0.01264345645904541\n",
      "Epoch: 33, Samples: 2016/5760, Loss: 0.015495657920837402\n",
      "Epoch: 33, Samples: 2048/5760, Loss: 0.016103416681289673\n",
      "Epoch: 33, Samples: 2080/5760, Loss: 0.012484192848205566\n",
      "Epoch: 33, Samples: 2112/5760, Loss: 0.013713926076889038\n",
      "Epoch: 33, Samples: 2144/5760, Loss: 0.019623011350631714\n",
      "Epoch: 33, Samples: 2176/5760, Loss: 0.022992372512817383\n",
      "Epoch: 33, Samples: 2208/5760, Loss: 0.01611429452896118\n",
      "Epoch: 33, Samples: 2240/5760, Loss: 0.03563162684440613\n",
      "Epoch: 33, Samples: 2272/5760, Loss: 0.013723582029342651\n",
      "Epoch: 33, Samples: 2304/5760, Loss: 0.022381484508514404\n",
      "Epoch: 33, Samples: 2336/5760, Loss: 0.01675465703010559\n",
      "Epoch: 33, Samples: 2368/5760, Loss: 0.012316405773162842\n",
      "Epoch: 33, Samples: 2400/5760, Loss: 0.01767289638519287\n",
      "Epoch: 33, Samples: 2432/5760, Loss: 0.014906913042068481\n",
      "Epoch: 33, Samples: 2464/5760, Loss: 0.008805453777313232\n",
      "Epoch: 33, Samples: 2496/5760, Loss: 0.015358984470367432\n",
      "Epoch: 33, Samples: 2528/5760, Loss: 0.016687721014022827\n",
      "Epoch: 33, Samples: 2560/5760, Loss: 0.01879522204399109\n",
      "Epoch: 33, Samples: 2592/5760, Loss: 0.013850122690200806\n",
      "Epoch: 33, Samples: 2624/5760, Loss: 0.011160939931869507\n",
      "Epoch: 33, Samples: 2656/5760, Loss: 0.016089260578155518\n",
      "Epoch: 33, Samples: 2688/5760, Loss: 0.009915858507156372\n",
      "Epoch: 33, Samples: 2720/5760, Loss: 0.008675158023834229\n",
      "Epoch: 33, Samples: 2752/5760, Loss: 0.008175700902938843\n",
      "Epoch: 33, Samples: 2784/5760, Loss: 0.010530561208724976\n",
      "Epoch: 33, Samples: 2816/5760, Loss: 0.019268840551376343\n",
      "Epoch: 33, Samples: 2848/5760, Loss: 0.009512573480606079\n",
      "Epoch: 33, Samples: 2880/5760, Loss: 0.014825701713562012\n",
      "Epoch: 33, Samples: 2912/5760, Loss: 0.012084484100341797\n",
      "Epoch: 33, Samples: 2944/5760, Loss: 0.019341588020324707\n",
      "Epoch: 33, Samples: 2976/5760, Loss: 0.010267049074172974\n",
      "Epoch: 33, Samples: 3008/5760, Loss: 0.018568485975265503\n",
      "Epoch: 33, Samples: 3040/5760, Loss: 0.008758097887039185\n",
      "Epoch: 33, Samples: 3072/5760, Loss: 0.021450906991958618\n",
      "Epoch: 33, Samples: 3104/5760, Loss: 0.0176297128200531\n",
      "Epoch: 33, Samples: 3136/5760, Loss: 0.015715956687927246\n",
      "Epoch: 33, Samples: 3168/5760, Loss: 0.011843979358673096\n",
      "Epoch: 33, Samples: 3200/5760, Loss: 0.017800480127334595\n",
      "Epoch: 33, Samples: 3232/5760, Loss: 0.01455715298652649\n",
      "Epoch: 33, Samples: 3264/5760, Loss: 0.011452794075012207\n",
      "Epoch: 33, Samples: 3296/5760, Loss: 0.012734442949295044\n",
      "Epoch: 33, Samples: 3328/5760, Loss: 0.009790688753128052\n",
      "Epoch: 33, Samples: 3360/5760, Loss: 0.009801119565963745\n",
      "Epoch: 33, Samples: 3392/5760, Loss: 0.011220276355743408\n",
      "Epoch: 33, Samples: 3424/5760, Loss: 0.02678927779197693\n",
      "Epoch: 33, Samples: 3456/5760, Loss: 0.015059739351272583\n",
      "Epoch: 33, Samples: 3488/5760, Loss: 0.007209450006484985\n",
      "Epoch: 33, Samples: 3520/5760, Loss: 0.013105332851409912\n",
      "Epoch: 33, Samples: 3552/5760, Loss: 0.011207640171051025\n",
      "Epoch: 33, Samples: 3584/5760, Loss: 0.018591225147247314\n",
      "Epoch: 33, Samples: 3616/5760, Loss: 0.010253340005874634\n",
      "Epoch: 33, Samples: 3648/5760, Loss: 0.007452219724655151\n",
      "Epoch: 33, Samples: 3680/5760, Loss: 0.014364898204803467\n",
      "Epoch: 33, Samples: 3712/5760, Loss: 0.010252892971038818\n",
      "Epoch: 33, Samples: 3744/5760, Loss: 0.01687130331993103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33, Samples: 3776/5760, Loss: 0.013382524251937866\n",
      "Epoch: 33, Samples: 3808/5760, Loss: 0.010000944137573242\n",
      "Epoch: 33, Samples: 3840/5760, Loss: 0.01940193772315979\n",
      "Epoch: 33, Samples: 3872/5760, Loss: 0.015382468700408936\n",
      "Epoch: 33, Samples: 3904/5760, Loss: 0.019982248544692993\n",
      "Epoch: 33, Samples: 3936/5760, Loss: 0.007617741823196411\n",
      "Epoch: 33, Samples: 3968/5760, Loss: 0.0096454918384552\n",
      "Epoch: 33, Samples: 4000/5760, Loss: 0.012921839952468872\n",
      "Epoch: 33, Samples: 4032/5760, Loss: 0.011721312999725342\n",
      "Epoch: 33, Samples: 4064/5760, Loss: 0.0134810209274292\n",
      "Epoch: 33, Samples: 4096/5760, Loss: 0.013248920440673828\n",
      "Epoch: 33, Samples: 4128/5760, Loss: 0.022493302822113037\n",
      "Epoch: 33, Samples: 4160/5760, Loss: 0.010216623544692993\n",
      "Epoch: 33, Samples: 4192/5760, Loss: 0.03340825438499451\n",
      "Epoch: 33, Samples: 4224/5760, Loss: 0.019981026649475098\n",
      "Epoch: 33, Samples: 4256/5760, Loss: 0.010745912790298462\n",
      "Epoch: 33, Samples: 4288/5760, Loss: 0.014109596610069275\n",
      "Epoch: 33, Samples: 4320/5760, Loss: 0.030612319707870483\n",
      "Epoch: 33, Samples: 4352/5760, Loss: 0.013715773820877075\n",
      "Epoch: 33, Samples: 4384/5760, Loss: 0.01584756374359131\n",
      "Epoch: 33, Samples: 4416/5760, Loss: 0.021624445915222168\n",
      "Epoch: 33, Samples: 4448/5760, Loss: 0.01687544584274292\n",
      "Epoch: 33, Samples: 4480/5760, Loss: 0.0166623592376709\n",
      "Epoch: 33, Samples: 4512/5760, Loss: 0.017116695642471313\n",
      "Epoch: 33, Samples: 4544/5760, Loss: 0.006817340850830078\n",
      "Epoch: 33, Samples: 4576/5760, Loss: 0.021634399890899658\n",
      "Epoch: 33, Samples: 4608/5760, Loss: 0.0255814790725708\n",
      "Epoch: 33, Samples: 4640/5760, Loss: 0.010348021984100342\n",
      "Epoch: 33, Samples: 4672/5760, Loss: 0.018275976181030273\n",
      "Epoch: 33, Samples: 4704/5760, Loss: 0.011024266481399536\n",
      "Epoch: 33, Samples: 4736/5760, Loss: 0.022160083055496216\n",
      "Epoch: 33, Samples: 4768/5760, Loss: 0.01429283618927002\n",
      "Epoch: 33, Samples: 4800/5760, Loss: 0.02123066782951355\n",
      "Epoch: 33, Samples: 4832/5760, Loss: 0.012468189001083374\n",
      "Epoch: 33, Samples: 4864/5760, Loss: 0.01764760911464691\n",
      "Epoch: 33, Samples: 4896/5760, Loss: 0.01889607310295105\n",
      "Epoch: 33, Samples: 4928/5760, Loss: 0.007263213396072388\n",
      "Epoch: 33, Samples: 4960/5760, Loss: 0.029090821743011475\n",
      "Epoch: 33, Samples: 4992/5760, Loss: 0.032101765275001526\n",
      "Epoch: 33, Samples: 5024/5760, Loss: 0.012464582920074463\n",
      "Epoch: 33, Samples: 5056/5760, Loss: 0.016873836517333984\n",
      "Epoch: 33, Samples: 5088/5760, Loss: 0.014488816261291504\n",
      "Epoch: 33, Samples: 5120/5760, Loss: 0.024935588240623474\n",
      "Epoch: 33, Samples: 5152/5760, Loss: 0.0191098153591156\n",
      "Epoch: 33, Samples: 5184/5760, Loss: 0.012811392545700073\n",
      "Epoch: 33, Samples: 5216/5760, Loss: 0.011551648378372192\n",
      "Epoch: 33, Samples: 5248/5760, Loss: 0.0103415846824646\n",
      "Epoch: 33, Samples: 5280/5760, Loss: 0.018350660800933838\n",
      "Epoch: 33, Samples: 5312/5760, Loss: 0.012521296739578247\n",
      "Epoch: 33, Samples: 5344/5760, Loss: 0.014621704816818237\n",
      "Epoch: 33, Samples: 5376/5760, Loss: 0.015724748373031616\n",
      "Epoch: 33, Samples: 5408/5760, Loss: 0.021021336317062378\n",
      "Epoch: 33, Samples: 5440/5760, Loss: 0.010773509740829468\n",
      "Epoch: 33, Samples: 5472/5760, Loss: 0.01530119776725769\n",
      "Epoch: 33, Samples: 5504/5760, Loss: 0.026846259832382202\n",
      "Epoch: 33, Samples: 5536/5760, Loss: 0.01887252926826477\n",
      "Epoch: 33, Samples: 5568/5760, Loss: 0.006070762872695923\n",
      "Epoch: 33, Samples: 5600/5760, Loss: 0.016594886779785156\n",
      "Epoch: 33, Samples: 5632/5760, Loss: 0.014106601476669312\n",
      "Epoch: 33, Samples: 5664/5760, Loss: 0.00695878267288208\n",
      "Epoch: 33, Samples: 5696/5760, Loss: 0.017791956663131714\n",
      "Epoch: 33, Samples: 5728/5760, Loss: 1.3841323852539062\n",
      "\n",
      "Epoch: 33\n",
      "Training set: Average loss: 0.0230\n",
      "Validation set: Average loss: 0.3381, Accuracy: 748/818 (91%)\n",
      "Epoch: 34, Samples: 0/5760, Loss: 0.020695477724075317\n",
      "Epoch: 34, Samples: 32/5760, Loss: 0.011641174554824829\n",
      "Epoch: 34, Samples: 64/5760, Loss: 0.01576879620552063\n",
      "Epoch: 34, Samples: 96/5760, Loss: 0.011018097400665283\n",
      "Epoch: 34, Samples: 128/5760, Loss: 0.02134615182876587\n",
      "Epoch: 34, Samples: 160/5760, Loss: 0.013193309307098389\n",
      "Epoch: 34, Samples: 192/5760, Loss: 0.006880760192871094\n",
      "Epoch: 34, Samples: 224/5760, Loss: 0.010629534721374512\n",
      "Epoch: 34, Samples: 256/5760, Loss: 0.015835046768188477\n",
      "Epoch: 34, Samples: 288/5760, Loss: 0.01383829116821289\n",
      "Epoch: 34, Samples: 320/5760, Loss: 0.03929978609085083\n",
      "Epoch: 34, Samples: 352/5760, Loss: 0.013349384069442749\n",
      "Epoch: 34, Samples: 384/5760, Loss: 0.014364391565322876\n",
      "Epoch: 34, Samples: 416/5760, Loss: 0.012954890727996826\n",
      "Epoch: 34, Samples: 448/5760, Loss: 0.01462787389755249\n",
      "Epoch: 34, Samples: 480/5760, Loss: 0.012276828289031982\n",
      "Epoch: 34, Samples: 512/5760, Loss: 0.010221362113952637\n",
      "Epoch: 34, Samples: 544/5760, Loss: 0.011721909046173096\n",
      "Epoch: 34, Samples: 576/5760, Loss: 0.009166717529296875\n",
      "Epoch: 34, Samples: 608/5760, Loss: 0.012093603610992432\n",
      "Epoch: 34, Samples: 640/5760, Loss: 0.014218300580978394\n",
      "Epoch: 34, Samples: 672/5760, Loss: 0.020432382822036743\n",
      "Epoch: 34, Samples: 704/5760, Loss: 0.011433243751525879\n",
      "Epoch: 34, Samples: 736/5760, Loss: 0.023247718811035156\n",
      "Epoch: 34, Samples: 768/5760, Loss: 0.012984305620193481\n",
      "Epoch: 34, Samples: 800/5760, Loss: 0.022135287523269653\n",
      "Epoch: 34, Samples: 832/5760, Loss: 0.014559656381607056\n",
      "Epoch: 34, Samples: 864/5760, Loss: 0.019319474697113037\n",
      "Epoch: 34, Samples: 896/5760, Loss: 0.01538228988647461\n",
      "Epoch: 34, Samples: 928/5760, Loss: 0.010722041130065918\n",
      "Epoch: 34, Samples: 960/5760, Loss: 0.011804968118667603\n",
      "Epoch: 34, Samples: 992/5760, Loss: 0.013464629650115967\n",
      "Epoch: 34, Samples: 1024/5760, Loss: 0.021092712879180908\n",
      "Epoch: 34, Samples: 1056/5760, Loss: 0.011217713356018066\n",
      "Epoch: 34, Samples: 1088/5760, Loss: 0.016722500324249268\n",
      "Epoch: 34, Samples: 1120/5760, Loss: 0.012472152709960938\n",
      "Epoch: 34, Samples: 1152/5760, Loss: 0.014822840690612793\n",
      "Epoch: 34, Samples: 1184/5760, Loss: 0.024301141500473022\n",
      "Epoch: 34, Samples: 1216/5760, Loss: 0.013286024332046509\n",
      "Epoch: 34, Samples: 1248/5760, Loss: 0.017343103885650635\n",
      "Epoch: 34, Samples: 1280/5760, Loss: 0.012316733598709106\n",
      "Epoch: 34, Samples: 1312/5760, Loss: 0.010080963373184204\n",
      "Epoch: 34, Samples: 1344/5760, Loss: 0.037318527698516846\n",
      "Epoch: 34, Samples: 1376/5760, Loss: 0.006767958402633667\n",
      "Epoch: 34, Samples: 1408/5760, Loss: 0.010515004396438599\n",
      "Epoch: 34, Samples: 1440/5760, Loss: 0.016019701957702637\n",
      "Epoch: 34, Samples: 1472/5760, Loss: 0.0249406099319458\n",
      "Epoch: 34, Samples: 1504/5760, Loss: 0.010959535837173462\n",
      "Epoch: 34, Samples: 1536/5760, Loss: 0.024074405431747437\n",
      "Epoch: 34, Samples: 1568/5760, Loss: 0.02008262276649475\n",
      "Epoch: 34, Samples: 1600/5760, Loss: 0.009105801582336426\n",
      "Epoch: 34, Samples: 1632/5760, Loss: 0.014347732067108154\n",
      "Epoch: 34, Samples: 1664/5760, Loss: 0.01547124981880188\n",
      "Epoch: 34, Samples: 1696/5760, Loss: 0.01714453101158142\n",
      "Epoch: 34, Samples: 1728/5760, Loss: 0.020472794771194458\n",
      "Epoch: 34, Samples: 1760/5760, Loss: 0.02620542049407959\n",
      "Epoch: 34, Samples: 1792/5760, Loss: 0.01407364010810852\n",
      "Epoch: 34, Samples: 1824/5760, Loss: 0.008216530084609985\n",
      "Epoch: 34, Samples: 1856/5760, Loss: 0.0109386146068573\n",
      "Epoch: 34, Samples: 1888/5760, Loss: 0.008764773607254028\n",
      "Epoch: 34, Samples: 1920/5760, Loss: 0.013154417276382446\n",
      "Epoch: 34, Samples: 1952/5760, Loss: 0.021674245595932007\n",
      "Epoch: 34, Samples: 1984/5760, Loss: 0.017291367053985596\n",
      "Epoch: 34, Samples: 2016/5760, Loss: 0.02282014489173889\n",
      "Epoch: 34, Samples: 2048/5760, Loss: 0.022003591060638428\n",
      "Epoch: 34, Samples: 2080/5760, Loss: 0.010655373334884644\n",
      "Epoch: 34, Samples: 2112/5760, Loss: 0.025988638401031494\n",
      "Epoch: 34, Samples: 2144/5760, Loss: 0.012318521738052368\n",
      "Epoch: 34, Samples: 2176/5760, Loss: 0.021525442600250244\n",
      "Epoch: 34, Samples: 2208/5760, Loss: 0.04595702886581421\n",
      "Epoch: 34, Samples: 2240/5760, Loss: 0.021913260221481323\n",
      "Epoch: 34, Samples: 2272/5760, Loss: 0.009211122989654541\n",
      "Epoch: 34, Samples: 2304/5760, Loss: 0.015101969242095947\n",
      "Epoch: 34, Samples: 2336/5760, Loss: 0.016284555196762085\n",
      "Epoch: 34, Samples: 2368/5760, Loss: 0.0180436372756958\n",
      "Epoch: 34, Samples: 2400/5760, Loss: 0.021717488765716553\n",
      "Epoch: 34, Samples: 2432/5760, Loss: 0.009249359369277954\n",
      "Epoch: 34, Samples: 2464/5760, Loss: 0.010883718729019165\n",
      "Epoch: 34, Samples: 2496/5760, Loss: 0.009358316659927368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Samples: 2528/5760, Loss: 0.01481163501739502\n",
      "Epoch: 34, Samples: 2560/5760, Loss: 0.014632582664489746\n",
      "Epoch: 34, Samples: 2592/5760, Loss: 0.013903915882110596\n",
      "Epoch: 34, Samples: 2624/5760, Loss: 0.01014062762260437\n",
      "Epoch: 34, Samples: 2656/5760, Loss: 0.012912511825561523\n",
      "Epoch: 34, Samples: 2688/5760, Loss: 0.02824905514717102\n",
      "Epoch: 34, Samples: 2720/5760, Loss: 0.012184768915176392\n",
      "Epoch: 34, Samples: 2752/5760, Loss: 0.008307188749313354\n",
      "Epoch: 34, Samples: 2784/5760, Loss: 0.01121896505355835\n",
      "Epoch: 34, Samples: 2816/5760, Loss: 0.013360083103179932\n",
      "Epoch: 34, Samples: 2848/5760, Loss: 0.009716689586639404\n",
      "Epoch: 34, Samples: 2880/5760, Loss: 0.010176897048950195\n",
      "Epoch: 34, Samples: 2912/5760, Loss: 0.015461832284927368\n",
      "Epoch: 34, Samples: 2944/5760, Loss: 0.010196179151535034\n",
      "Epoch: 34, Samples: 2976/5760, Loss: 0.01354306936264038\n",
      "Epoch: 34, Samples: 3008/5760, Loss: 0.0109386146068573\n",
      "Epoch: 34, Samples: 3040/5760, Loss: 0.014626890420913696\n",
      "Epoch: 34, Samples: 3072/5760, Loss: 0.014414966106414795\n",
      "Epoch: 34, Samples: 3104/5760, Loss: 0.008031457662582397\n",
      "Epoch: 34, Samples: 3136/5760, Loss: 0.02108544111251831\n",
      "Epoch: 34, Samples: 3168/5760, Loss: 0.01307860016822815\n",
      "Epoch: 34, Samples: 3200/5760, Loss: 0.020034730434417725\n",
      "Epoch: 34, Samples: 3232/5760, Loss: 0.02282610535621643\n",
      "Epoch: 34, Samples: 3264/5760, Loss: 0.011681318283081055\n",
      "Epoch: 34, Samples: 3296/5760, Loss: 0.015514552593231201\n",
      "Epoch: 34, Samples: 3328/5760, Loss: 0.018532872200012207\n",
      "Epoch: 34, Samples: 3360/5760, Loss: 0.00883936882019043\n",
      "Epoch: 34, Samples: 3392/5760, Loss: 0.014507889747619629\n",
      "Epoch: 34, Samples: 3424/5760, Loss: 0.027047187089920044\n",
      "Epoch: 34, Samples: 3456/5760, Loss: 0.01893863081932068\n",
      "Epoch: 34, Samples: 3488/5760, Loss: 0.0145263671875\n",
      "Epoch: 34, Samples: 3520/5760, Loss: 0.00907278060913086\n",
      "Epoch: 34, Samples: 3552/5760, Loss: 0.01918904483318329\n",
      "Epoch: 34, Samples: 3584/5760, Loss: 0.01396268606185913\n",
      "Epoch: 34, Samples: 3616/5760, Loss: 0.0143299400806427\n",
      "Epoch: 34, Samples: 3648/5760, Loss: 0.018069565296173096\n",
      "Epoch: 34, Samples: 3680/5760, Loss: 0.015093773603439331\n",
      "Epoch: 34, Samples: 3712/5760, Loss: 0.008847713470458984\n",
      "Epoch: 34, Samples: 3744/5760, Loss: 0.017519116401672363\n",
      "Epoch: 34, Samples: 3776/5760, Loss: 0.01071774959564209\n",
      "Epoch: 34, Samples: 3808/5760, Loss: 0.02683815360069275\n",
      "Epoch: 34, Samples: 3840/5760, Loss: 0.015498876571655273\n",
      "Epoch: 34, Samples: 3872/5760, Loss: 0.012554794549942017\n",
      "Epoch: 34, Samples: 3904/5760, Loss: 0.008370786905288696\n",
      "Epoch: 34, Samples: 3936/5760, Loss: 0.014693289995193481\n",
      "Epoch: 34, Samples: 3968/5760, Loss: 0.013800472021102905\n",
      "Epoch: 34, Samples: 4000/5760, Loss: 0.01820051670074463\n",
      "Epoch: 34, Samples: 4032/5760, Loss: 0.011990666389465332\n",
      "Epoch: 34, Samples: 4064/5760, Loss: 0.014059394598007202\n",
      "Epoch: 34, Samples: 4096/5760, Loss: 0.015947431325912476\n",
      "Epoch: 34, Samples: 4128/5760, Loss: 0.022247612476348877\n",
      "Epoch: 34, Samples: 4160/5760, Loss: 0.01669982075691223\n",
      "Epoch: 34, Samples: 4192/5760, Loss: 0.009079903364181519\n",
      "Epoch: 34, Samples: 4224/5760, Loss: 0.011602848768234253\n",
      "Epoch: 34, Samples: 4256/5760, Loss: 0.01660400629043579\n",
      "Epoch: 34, Samples: 4288/5760, Loss: 0.013532757759094238\n",
      "Epoch: 34, Samples: 4320/5760, Loss: 0.013851583003997803\n",
      "Epoch: 34, Samples: 4352/5760, Loss: 0.011778116226196289\n",
      "Epoch: 34, Samples: 4384/5760, Loss: 0.016105979681015015\n",
      "Epoch: 34, Samples: 4416/5760, Loss: 0.015011698007583618\n",
      "Epoch: 34, Samples: 4448/5760, Loss: 0.017469406127929688\n",
      "Epoch: 34, Samples: 4480/5760, Loss: 0.01151043176651001\n",
      "Epoch: 34, Samples: 4512/5760, Loss: 0.014797985553741455\n",
      "Epoch: 34, Samples: 4544/5760, Loss: 0.011968910694122314\n",
      "Epoch: 34, Samples: 4576/5760, Loss: 0.00946950912475586\n",
      "Epoch: 34, Samples: 4608/5760, Loss: 0.01482120156288147\n",
      "Epoch: 34, Samples: 4640/5760, Loss: 0.02994436025619507\n",
      "Epoch: 34, Samples: 4672/5760, Loss: 0.01212167739868164\n",
      "Epoch: 34, Samples: 4704/5760, Loss: 0.013483613729476929\n",
      "Epoch: 34, Samples: 4736/5760, Loss: 0.023187488317489624\n",
      "Epoch: 34, Samples: 4768/5760, Loss: 0.015822917222976685\n",
      "Epoch: 34, Samples: 4800/5760, Loss: 0.01542675495147705\n",
      "Epoch: 34, Samples: 4832/5760, Loss: 0.023295611143112183\n",
      "Epoch: 34, Samples: 4864/5760, Loss: 0.031898051500320435\n",
      "Epoch: 34, Samples: 4896/5760, Loss: 0.011798292398452759\n",
      "Epoch: 34, Samples: 4928/5760, Loss: 0.014129579067230225\n",
      "Epoch: 34, Samples: 4960/5760, Loss: 0.012031465768814087\n",
      "Epoch: 34, Samples: 4992/5760, Loss: 0.006567239761352539\n",
      "Epoch: 34, Samples: 5024/5760, Loss: 0.013890773057937622\n",
      "Epoch: 34, Samples: 5056/5760, Loss: 0.013088464736938477\n",
      "Epoch: 34, Samples: 5088/5760, Loss: 0.028082609176635742\n",
      "Epoch: 34, Samples: 5120/5760, Loss: 0.021925777196884155\n",
      "Epoch: 34, Samples: 5152/5760, Loss: 0.008921891450881958\n",
      "Epoch: 34, Samples: 5184/5760, Loss: 0.02469390630722046\n",
      "Epoch: 34, Samples: 5216/5760, Loss: 0.014512181282043457\n",
      "Epoch: 34, Samples: 5248/5760, Loss: 0.00691455602645874\n",
      "Epoch: 34, Samples: 5280/5760, Loss: 0.016072064638137817\n",
      "Epoch: 34, Samples: 5312/5760, Loss: 0.012082189321517944\n",
      "Epoch: 34, Samples: 5344/5760, Loss: 0.025725066661834717\n",
      "Epoch: 34, Samples: 5376/5760, Loss: 0.016234368085861206\n",
      "Epoch: 34, Samples: 5408/5760, Loss: 0.009910494089126587\n",
      "Epoch: 34, Samples: 5440/5760, Loss: 0.009125471115112305\n",
      "Epoch: 34, Samples: 5472/5760, Loss: 0.01280885934829712\n",
      "Epoch: 34, Samples: 5504/5760, Loss: 0.008863061666488647\n",
      "Epoch: 34, Samples: 5536/5760, Loss: 0.013269484043121338\n",
      "Epoch: 34, Samples: 5568/5760, Loss: 0.011098027229309082\n",
      "Epoch: 34, Samples: 5600/5760, Loss: 0.010871052742004395\n",
      "Epoch: 34, Samples: 5632/5760, Loss: 0.01609286665916443\n",
      "Epoch: 34, Samples: 5664/5760, Loss: 0.01118507981300354\n",
      "Epoch: 34, Samples: 5696/5760, Loss: 0.014501124620437622\n",
      "Epoch: 34, Samples: 5728/5760, Loss: 1.9926351308822632\n",
      "\n",
      "Epoch: 34\n",
      "Training set: Average loss: 0.0264\n",
      "Validation set: Average loss: 0.3320, Accuracy: 745/818 (91%)\n",
      "Epoch: 35, Samples: 0/5760, Loss: 0.012996792793273926\n",
      "Epoch: 35, Samples: 32/5760, Loss: 0.012456774711608887\n",
      "Epoch: 35, Samples: 64/5760, Loss: 0.015456318855285645\n",
      "Epoch: 35, Samples: 96/5760, Loss: 0.013390809297561646\n",
      "Epoch: 35, Samples: 128/5760, Loss: 0.0141848623752594\n",
      "Epoch: 35, Samples: 160/5760, Loss: 0.01433447003364563\n",
      "Epoch: 35, Samples: 192/5760, Loss: 0.013461917638778687\n",
      "Epoch: 35, Samples: 224/5760, Loss: 0.01718074083328247\n",
      "Epoch: 35, Samples: 256/5760, Loss: 0.007352858781814575\n",
      "Epoch: 35, Samples: 288/5760, Loss: 0.01028558611869812\n",
      "Epoch: 35, Samples: 320/5760, Loss: 0.01669985055923462\n",
      "Epoch: 35, Samples: 352/5760, Loss: 0.012152403593063354\n",
      "Epoch: 35, Samples: 384/5760, Loss: 0.007090121507644653\n",
      "Epoch: 35, Samples: 416/5760, Loss: 0.017127692699432373\n",
      "Epoch: 35, Samples: 448/5760, Loss: 0.01100689172744751\n",
      "Epoch: 35, Samples: 480/5760, Loss: 0.016542047262191772\n",
      "Epoch: 35, Samples: 512/5760, Loss: 0.01559978723526001\n",
      "Epoch: 35, Samples: 544/5760, Loss: 0.01939663290977478\n",
      "Epoch: 35, Samples: 576/5760, Loss: 0.014932602643966675\n",
      "Epoch: 35, Samples: 608/5760, Loss: 0.011652916669845581\n",
      "Epoch: 35, Samples: 640/5760, Loss: 0.014424830675125122\n",
      "Epoch: 35, Samples: 672/5760, Loss: 0.017881155014038086\n",
      "Epoch: 35, Samples: 704/5760, Loss: 0.015986651182174683\n",
      "Epoch: 35, Samples: 736/5760, Loss: 0.0175132155418396\n",
      "Epoch: 35, Samples: 768/5760, Loss: 0.0046690404415130615\n",
      "Epoch: 35, Samples: 800/5760, Loss: 0.016801059246063232\n",
      "Epoch: 35, Samples: 832/5760, Loss: 0.019497007131576538\n",
      "Epoch: 35, Samples: 864/5760, Loss: 0.020187795162200928\n",
      "Epoch: 35, Samples: 896/5760, Loss: 0.01378786563873291\n",
      "Epoch: 35, Samples: 928/5760, Loss: 0.012973129749298096\n",
      "Epoch: 35, Samples: 960/5760, Loss: 0.010278642177581787\n",
      "Epoch: 35, Samples: 992/5760, Loss: 0.019884109497070312\n",
      "Epoch: 35, Samples: 1024/5760, Loss: 0.02846243977546692\n",
      "Epoch: 35, Samples: 1056/5760, Loss: 0.01405683159828186\n",
      "Epoch: 35, Samples: 1088/5760, Loss: 0.011770039796829224\n",
      "Epoch: 35, Samples: 1120/5760, Loss: 0.012669265270233154\n",
      "Epoch: 35, Samples: 1152/5760, Loss: 0.010932773351669312\n",
      "Epoch: 35, Samples: 1184/5760, Loss: 0.02566555142402649\n",
      "Epoch: 35, Samples: 1216/5760, Loss: 0.01224362850189209\n",
      "Epoch: 35, Samples: 1248/5760, Loss: 0.01812228560447693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, Samples: 1280/5760, Loss: 0.01477116346359253\n",
      "Epoch: 35, Samples: 1312/5760, Loss: 0.01791548728942871\n",
      "Epoch: 35, Samples: 1344/5760, Loss: 0.0100460946559906\n",
      "Epoch: 35, Samples: 1376/5760, Loss: 0.013408541679382324\n",
      "Epoch: 35, Samples: 1408/5760, Loss: 0.013894438743591309\n",
      "Epoch: 35, Samples: 1440/5760, Loss: 0.0329795777797699\n",
      "Epoch: 35, Samples: 1472/5760, Loss: 0.010822057723999023\n",
      "Epoch: 35, Samples: 1504/5760, Loss: 0.008426517248153687\n",
      "Epoch: 35, Samples: 1536/5760, Loss: 0.009595096111297607\n",
      "Epoch: 35, Samples: 1568/5760, Loss: 0.012001961469650269\n",
      "Epoch: 35, Samples: 1600/5760, Loss: 0.013014644384384155\n",
      "Epoch: 35, Samples: 1632/5760, Loss: 0.0691843181848526\n",
      "Epoch: 35, Samples: 1664/5760, Loss: 0.00963902473449707\n",
      "Epoch: 35, Samples: 1696/5760, Loss: 0.013124972581863403\n",
      "Epoch: 35, Samples: 1728/5760, Loss: 0.012741178274154663\n",
      "Epoch: 35, Samples: 1760/5760, Loss: 0.010468393564224243\n",
      "Epoch: 35, Samples: 1792/5760, Loss: 0.00918048620223999\n",
      "Epoch: 35, Samples: 1824/5760, Loss: 0.01712566614151001\n",
      "Epoch: 35, Samples: 1856/5760, Loss: 0.013610273599624634\n",
      "Epoch: 35, Samples: 1888/5760, Loss: 0.02182886004447937\n",
      "Epoch: 35, Samples: 1920/5760, Loss: 0.009015649557113647\n",
      "Epoch: 35, Samples: 1952/5760, Loss: 0.011318117380142212\n",
      "Epoch: 35, Samples: 1984/5760, Loss: 0.009803175926208496\n",
      "Epoch: 35, Samples: 2016/5760, Loss: 0.010821878910064697\n",
      "Epoch: 35, Samples: 2048/5760, Loss: 0.010976850986480713\n",
      "Epoch: 35, Samples: 2080/5760, Loss: 0.01968654990196228\n",
      "Epoch: 35, Samples: 2112/5760, Loss: 0.01169469952583313\n",
      "Epoch: 35, Samples: 2144/5760, Loss: 0.01412975788116455\n",
      "Epoch: 35, Samples: 2176/5760, Loss: 0.019550055265426636\n",
      "Epoch: 35, Samples: 2208/5760, Loss: 0.009195446968078613\n",
      "Epoch: 35, Samples: 2240/5760, Loss: 0.01391950249671936\n",
      "Epoch: 35, Samples: 2272/5760, Loss: 0.026166826486587524\n",
      "Epoch: 35, Samples: 2304/5760, Loss: 0.015354424715042114\n",
      "Epoch: 35, Samples: 2336/5760, Loss: 0.012349456548690796\n",
      "Epoch: 35, Samples: 2368/5760, Loss: 0.01963016390800476\n",
      "Epoch: 35, Samples: 2400/5760, Loss: 0.018554508686065674\n",
      "Epoch: 35, Samples: 2432/5760, Loss: 0.008956313133239746\n",
      "Epoch: 35, Samples: 2464/5760, Loss: 0.016975730657577515\n",
      "Epoch: 35, Samples: 2496/5760, Loss: 0.00905659794807434\n",
      "Epoch: 35, Samples: 2528/5760, Loss: 0.020711004734039307\n",
      "Epoch: 35, Samples: 2560/5760, Loss: 0.025757327675819397\n",
      "Epoch: 35, Samples: 2592/5760, Loss: 0.016360878944396973\n",
      "Epoch: 35, Samples: 2624/5760, Loss: 0.008815258741378784\n",
      "Epoch: 35, Samples: 2656/5760, Loss: 0.010621517896652222\n",
      "Epoch: 35, Samples: 2688/5760, Loss: 0.022821784019470215\n",
      "Epoch: 35, Samples: 2720/5760, Loss: 0.025978833436965942\n",
      "Epoch: 35, Samples: 2752/5760, Loss: 0.008423477411270142\n",
      "Epoch: 35, Samples: 2784/5760, Loss: 0.0123690664768219\n",
      "Epoch: 35, Samples: 2816/5760, Loss: 0.006976872682571411\n",
      "Epoch: 35, Samples: 2848/5760, Loss: 0.013358652591705322\n",
      "Epoch: 35, Samples: 2880/5760, Loss: 0.02396261692047119\n",
      "Epoch: 35, Samples: 2912/5760, Loss: 0.010990142822265625\n",
      "Epoch: 35, Samples: 2944/5760, Loss: 0.010379761457443237\n",
      "Epoch: 35, Samples: 2976/5760, Loss: 0.007194042205810547\n",
      "Epoch: 35, Samples: 3008/5760, Loss: 0.02018129825592041\n",
      "Epoch: 35, Samples: 3040/5760, Loss: 0.012472271919250488\n",
      "Epoch: 35, Samples: 3072/5760, Loss: 0.01338222622871399\n",
      "Epoch: 35, Samples: 3104/5760, Loss: 0.021968752145767212\n",
      "Epoch: 35, Samples: 3136/5760, Loss: 0.01209983229637146\n",
      "Epoch: 35, Samples: 3168/5760, Loss: 0.013038843870162964\n",
      "Epoch: 35, Samples: 3200/5760, Loss: 0.014276683330535889\n",
      "Epoch: 35, Samples: 3232/5760, Loss: 0.030489861965179443\n",
      "Epoch: 35, Samples: 3264/5760, Loss: 0.013153284788131714\n",
      "Epoch: 35, Samples: 3296/5760, Loss: 0.013274610042572021\n",
      "Epoch: 35, Samples: 3328/5760, Loss: 0.012929022312164307\n",
      "Epoch: 35, Samples: 3360/5760, Loss: 0.012747466564178467\n",
      "Epoch: 35, Samples: 3392/5760, Loss: 0.017530232667922974\n",
      "Epoch: 35, Samples: 3424/5760, Loss: 0.026637613773345947\n",
      "Epoch: 35, Samples: 3456/5760, Loss: 0.009753644466400146\n",
      "Epoch: 35, Samples: 3488/5760, Loss: 0.0071450769901275635\n",
      "Epoch: 35, Samples: 3520/5760, Loss: 0.026748746633529663\n",
      "Epoch: 35, Samples: 3552/5760, Loss: 0.022607922554016113\n",
      "Epoch: 35, Samples: 3584/5760, Loss: 0.011366456747055054\n",
      "Epoch: 35, Samples: 3616/5760, Loss: 0.007262825965881348\n",
      "Epoch: 35, Samples: 3648/5760, Loss: 0.01919540762901306\n",
      "Epoch: 35, Samples: 3680/5760, Loss: 0.011081159114837646\n",
      "Epoch: 35, Samples: 3712/5760, Loss: 0.01149982213973999\n",
      "Epoch: 35, Samples: 3744/5760, Loss: 0.01055482029914856\n",
      "Epoch: 35, Samples: 3776/5760, Loss: 0.010788261890411377\n",
      "Epoch: 35, Samples: 3808/5760, Loss: 0.017782628536224365\n",
      "Epoch: 35, Samples: 3840/5760, Loss: 0.008966386318206787\n",
      "Epoch: 35, Samples: 3872/5760, Loss: 0.01863524317741394\n",
      "Epoch: 35, Samples: 3904/5760, Loss: 0.01784336566925049\n",
      "Epoch: 35, Samples: 3936/5760, Loss: 0.012298822402954102\n",
      "Epoch: 35, Samples: 3968/5760, Loss: 0.009065300226211548\n",
      "Epoch: 35, Samples: 4000/5760, Loss: 0.03201708197593689\n",
      "Epoch: 35, Samples: 4032/5760, Loss: 0.006825685501098633\n",
      "Epoch: 35, Samples: 4064/5760, Loss: 0.010439008474349976\n",
      "Epoch: 35, Samples: 4096/5760, Loss: 0.012072503566741943\n",
      "Epoch: 35, Samples: 4128/5760, Loss: 0.015055984258651733\n",
      "Epoch: 35, Samples: 4160/5760, Loss: 0.013829350471496582\n",
      "Epoch: 35, Samples: 4192/5760, Loss: 0.020093709230422974\n",
      "Epoch: 35, Samples: 4224/5760, Loss: 0.016339242458343506\n",
      "Epoch: 35, Samples: 4256/5760, Loss: 0.017053931951522827\n",
      "Epoch: 35, Samples: 4288/5760, Loss: 0.009372860193252563\n",
      "Epoch: 35, Samples: 4320/5760, Loss: 0.018185675144195557\n",
      "Epoch: 35, Samples: 4352/5760, Loss: 0.03056679666042328\n",
      "Epoch: 35, Samples: 4384/5760, Loss: 0.013192564249038696\n",
      "Epoch: 35, Samples: 4416/5760, Loss: 0.017475992441177368\n",
      "Epoch: 35, Samples: 4448/5760, Loss: 0.011130869388580322\n",
      "Epoch: 35, Samples: 4480/5760, Loss: 0.01679292321205139\n",
      "Epoch: 35, Samples: 4512/5760, Loss: 0.016919761896133423\n",
      "Epoch: 35, Samples: 4544/5760, Loss: 0.024568110704421997\n",
      "Epoch: 35, Samples: 4576/5760, Loss: 0.026887565851211548\n",
      "Epoch: 35, Samples: 4608/5760, Loss: 0.007503896951675415\n",
      "Epoch: 35, Samples: 4640/5760, Loss: 0.014963358640670776\n",
      "Epoch: 35, Samples: 4672/5760, Loss: 0.019740909337997437\n",
      "Epoch: 35, Samples: 4704/5760, Loss: 0.02114313840866089\n",
      "Epoch: 35, Samples: 4736/5760, Loss: 0.010240405797958374\n",
      "Epoch: 35, Samples: 4768/5760, Loss: 0.011873602867126465\n",
      "Epoch: 35, Samples: 4800/5760, Loss: 0.028458327054977417\n",
      "Epoch: 35, Samples: 4832/5760, Loss: 0.008069455623626709\n",
      "Epoch: 35, Samples: 4864/5760, Loss: 0.012716829776763916\n",
      "Epoch: 35, Samples: 4896/5760, Loss: 0.012056052684783936\n",
      "Epoch: 35, Samples: 4928/5760, Loss: 0.013806968927383423\n",
      "Epoch: 35, Samples: 4960/5760, Loss: 0.017108827829360962\n",
      "Epoch: 35, Samples: 4992/5760, Loss: 0.010824233293533325\n",
      "Epoch: 35, Samples: 5024/5760, Loss: 0.016803160309791565\n",
      "Epoch: 35, Samples: 5056/5760, Loss: 0.017858684062957764\n",
      "Epoch: 35, Samples: 5088/5760, Loss: 0.016978442668914795\n",
      "Epoch: 35, Samples: 5120/5760, Loss: 0.016118347644805908\n",
      "Epoch: 35, Samples: 5152/5760, Loss: 0.013382703065872192\n",
      "Epoch: 35, Samples: 5184/5760, Loss: 0.00833919644355774\n",
      "Epoch: 35, Samples: 5216/5760, Loss: 0.015533804893493652\n",
      "Epoch: 35, Samples: 5248/5760, Loss: 0.015383690595626831\n",
      "Epoch: 35, Samples: 5280/5760, Loss: 0.009339332580566406\n",
      "Epoch: 35, Samples: 5312/5760, Loss: 0.010231226682662964\n",
      "Epoch: 35, Samples: 5344/5760, Loss: 0.012155979871749878\n",
      "Epoch: 35, Samples: 5376/5760, Loss: 0.015528857707977295\n",
      "Epoch: 35, Samples: 5408/5760, Loss: 0.009783059358596802\n",
      "Epoch: 35, Samples: 5440/5760, Loss: 0.018508225679397583\n",
      "Epoch: 35, Samples: 5472/5760, Loss: 0.006095021963119507\n",
      "Epoch: 35, Samples: 5504/5760, Loss: 0.026015132665634155\n",
      "Epoch: 35, Samples: 5536/5760, Loss: 0.008066624402999878\n",
      "Epoch: 35, Samples: 5568/5760, Loss: 0.018766969442367554\n",
      "Epoch: 35, Samples: 5600/5760, Loss: 0.0146636962890625\n",
      "Epoch: 35, Samples: 5632/5760, Loss: 0.009865760803222656\n",
      "Epoch: 35, Samples: 5664/5760, Loss: 0.01587510108947754\n",
      "Epoch: 35, Samples: 5696/5760, Loss: 0.012652486562728882\n",
      "Epoch: 35, Samples: 5728/5760, Loss: 1.063546895980835\n",
      "\n",
      "Epoch: 35\n",
      "Training set: Average loss: 0.0210\n",
      "Validation set: Average loss: 0.3434, Accuracy: 743/818 (91%)\n",
      "Epoch: 36, Samples: 0/5760, Loss: 0.014014840126037598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Samples: 32/5760, Loss: 0.011537045240402222\n",
      "Epoch: 36, Samples: 64/5760, Loss: 0.016598135232925415\n",
      "Epoch: 36, Samples: 96/5760, Loss: 0.006555497646331787\n",
      "Epoch: 36, Samples: 128/5760, Loss: 0.008317232131958008\n",
      "Epoch: 36, Samples: 160/5760, Loss: 0.009199023246765137\n",
      "Epoch: 36, Samples: 192/5760, Loss: 0.014899998903274536\n",
      "Epoch: 36, Samples: 224/5760, Loss: 0.017903178930282593\n",
      "Epoch: 36, Samples: 256/5760, Loss: 0.005057424306869507\n",
      "Epoch: 36, Samples: 288/5760, Loss: 0.00931328535079956\n",
      "Epoch: 36, Samples: 320/5760, Loss: 0.013228774070739746\n",
      "Epoch: 36, Samples: 352/5760, Loss: 0.00954464077949524\n",
      "Epoch: 36, Samples: 384/5760, Loss: 0.014191865921020508\n",
      "Epoch: 36, Samples: 416/5760, Loss: 0.01017335057258606\n",
      "Epoch: 36, Samples: 448/5760, Loss: 0.010151594877243042\n",
      "Epoch: 36, Samples: 480/5760, Loss: 0.007933169603347778\n",
      "Epoch: 36, Samples: 512/5760, Loss: 0.01431286334991455\n",
      "Epoch: 36, Samples: 544/5760, Loss: 0.012421607971191406\n",
      "Epoch: 36, Samples: 576/5760, Loss: 0.008906006813049316\n",
      "Epoch: 36, Samples: 608/5760, Loss: 0.012035459280014038\n",
      "Epoch: 36, Samples: 640/5760, Loss: 0.015190541744232178\n",
      "Epoch: 36, Samples: 672/5760, Loss: 0.011941760778427124\n",
      "Epoch: 36, Samples: 704/5760, Loss: 0.010700315237045288\n",
      "Epoch: 36, Samples: 736/5760, Loss: 0.019640237092971802\n",
      "Epoch: 36, Samples: 768/5760, Loss: 0.01659923791885376\n",
      "Epoch: 36, Samples: 800/5760, Loss: 0.016682296991348267\n",
      "Epoch: 36, Samples: 832/5760, Loss: 0.00771293044090271\n",
      "Epoch: 36, Samples: 864/5760, Loss: 0.010066717863082886\n",
      "Epoch: 36, Samples: 896/5760, Loss: 0.01319471001625061\n",
      "Epoch: 36, Samples: 928/5760, Loss: 0.01724010705947876\n",
      "Epoch: 36, Samples: 960/5760, Loss: 0.010924100875854492\n",
      "Epoch: 36, Samples: 992/5760, Loss: 0.017573297023773193\n",
      "Epoch: 36, Samples: 1024/5760, Loss: 0.010357916355133057\n",
      "Epoch: 36, Samples: 1056/5760, Loss: 0.014350146055221558\n",
      "Epoch: 36, Samples: 1088/5760, Loss: 0.0078043341636657715\n",
      "Epoch: 36, Samples: 1120/5760, Loss: 0.008449375629425049\n",
      "Epoch: 36, Samples: 1152/5760, Loss: 0.016700953245162964\n",
      "Epoch: 36, Samples: 1184/5760, Loss: 0.017486751079559326\n",
      "Epoch: 36, Samples: 1216/5760, Loss: 0.013269096612930298\n",
      "Epoch: 36, Samples: 1248/5760, Loss: 0.013588190078735352\n",
      "Epoch: 36, Samples: 1280/5760, Loss: 0.019175857305526733\n",
      "Epoch: 36, Samples: 1312/5760, Loss: 0.011539936065673828\n",
      "Epoch: 36, Samples: 1344/5760, Loss: 0.02055501937866211\n",
      "Epoch: 36, Samples: 1376/5760, Loss: 0.025278881192207336\n",
      "Epoch: 36, Samples: 1408/5760, Loss: 0.013921916484832764\n",
      "Epoch: 36, Samples: 1440/5760, Loss: 0.013549566268920898\n",
      "Epoch: 36, Samples: 1472/5760, Loss: 0.013652801513671875\n",
      "Epoch: 36, Samples: 1504/5760, Loss: 0.012234658002853394\n",
      "Epoch: 36, Samples: 1536/5760, Loss: 0.011459499597549438\n",
      "Epoch: 36, Samples: 1568/5760, Loss: 0.011346966028213501\n",
      "Epoch: 36, Samples: 1600/5760, Loss: 0.01180693507194519\n",
      "Epoch: 36, Samples: 1632/5760, Loss: 0.01118612289428711\n",
      "Epoch: 36, Samples: 1664/5760, Loss: 0.008994519710540771\n",
      "Epoch: 36, Samples: 1696/5760, Loss: 0.015568047761917114\n",
      "Epoch: 36, Samples: 1728/5760, Loss: 0.023940742015838623\n",
      "Epoch: 36, Samples: 1760/5760, Loss: 0.009411454200744629\n",
      "Epoch: 36, Samples: 1792/5760, Loss: 0.04254347085952759\n",
      "Epoch: 36, Samples: 1824/5760, Loss: 0.010351419448852539\n",
      "Epoch: 36, Samples: 1856/5760, Loss: 0.023843616247177124\n",
      "Epoch: 36, Samples: 1888/5760, Loss: 0.011678814888000488\n",
      "Epoch: 36, Samples: 1920/5760, Loss: 0.007941991090774536\n",
      "Epoch: 36, Samples: 1952/5760, Loss: 0.012496083974838257\n",
      "Epoch: 36, Samples: 1984/5760, Loss: 0.022956013679504395\n",
      "Epoch: 36, Samples: 2016/5760, Loss: 0.006541699171066284\n",
      "Epoch: 36, Samples: 2048/5760, Loss: 0.011905938386917114\n",
      "Epoch: 36, Samples: 2080/5760, Loss: 0.010913461446762085\n",
      "Epoch: 36, Samples: 2112/5760, Loss: 0.012572497129440308\n",
      "Epoch: 36, Samples: 2144/5760, Loss: 0.01857161521911621\n",
      "Epoch: 36, Samples: 2176/5760, Loss: 0.020964711904525757\n",
      "Epoch: 36, Samples: 2208/5760, Loss: 0.011206656694412231\n",
      "Epoch: 36, Samples: 2240/5760, Loss: 0.012187063694000244\n",
      "Epoch: 36, Samples: 2272/5760, Loss: 0.010258853435516357\n",
      "Epoch: 36, Samples: 2304/5760, Loss: 0.010453075170516968\n",
      "Epoch: 36, Samples: 2336/5760, Loss: 0.008449435234069824\n",
      "Epoch: 36, Samples: 2368/5760, Loss: 0.01902291178703308\n",
      "Epoch: 36, Samples: 2400/5760, Loss: 0.010855704545974731\n",
      "Epoch: 36, Samples: 2432/5760, Loss: 0.013965606689453125\n",
      "Epoch: 36, Samples: 2464/5760, Loss: 0.012920171022415161\n",
      "Epoch: 36, Samples: 2496/5760, Loss: 0.013885557651519775\n",
      "Epoch: 36, Samples: 2528/5760, Loss: 0.010390669107437134\n",
      "Epoch: 36, Samples: 2560/5760, Loss: 0.021040350198745728\n",
      "Epoch: 36, Samples: 2592/5760, Loss: 0.013675004243850708\n",
      "Epoch: 36, Samples: 2624/5760, Loss: 0.009654730558395386\n",
      "Epoch: 36, Samples: 2656/5760, Loss: 0.012221068143844604\n",
      "Epoch: 36, Samples: 2688/5760, Loss: 0.00901535153388977\n",
      "Epoch: 36, Samples: 2720/5760, Loss: 0.014176547527313232\n",
      "Epoch: 36, Samples: 2752/5760, Loss: 0.012524604797363281\n",
      "Epoch: 36, Samples: 2784/5760, Loss: 0.015750020742416382\n",
      "Epoch: 36, Samples: 2816/5760, Loss: 0.011142700910568237\n",
      "Epoch: 36, Samples: 2848/5760, Loss: 0.016211241483688354\n",
      "Epoch: 36, Samples: 2880/5760, Loss: 0.008079051971435547\n",
      "Epoch: 36, Samples: 2912/5760, Loss: 0.01732078194618225\n",
      "Epoch: 36, Samples: 2944/5760, Loss: 0.014181941747665405\n",
      "Epoch: 36, Samples: 2976/5760, Loss: 0.021986335515975952\n",
      "Epoch: 36, Samples: 3008/5760, Loss: 0.007141411304473877\n",
      "Epoch: 36, Samples: 3040/5760, Loss: 0.016950160264968872\n",
      "Epoch: 36, Samples: 3072/5760, Loss: 0.012538552284240723\n",
      "Epoch: 36, Samples: 3104/5760, Loss: 0.017070919275283813\n",
      "Epoch: 36, Samples: 3136/5760, Loss: 0.011183559894561768\n",
      "Epoch: 36, Samples: 3168/5760, Loss: 0.012856334447860718\n",
      "Epoch: 36, Samples: 3200/5760, Loss: 0.03430500626564026\n",
      "Epoch: 36, Samples: 3232/5760, Loss: 0.012278199195861816\n",
      "Epoch: 36, Samples: 3264/5760, Loss: 0.006361812353134155\n",
      "Epoch: 36, Samples: 3296/5760, Loss: 0.04049976170063019\n",
      "Epoch: 36, Samples: 3328/5760, Loss: 0.012894690036773682\n",
      "Epoch: 36, Samples: 3360/5760, Loss: 0.020053192973136902\n",
      "Epoch: 36, Samples: 3392/5760, Loss: 0.00833362340927124\n",
      "Epoch: 36, Samples: 3424/5760, Loss: 0.009627044200897217\n",
      "Epoch: 36, Samples: 3456/5760, Loss: 0.011636167764663696\n",
      "Epoch: 36, Samples: 3488/5760, Loss: 0.012980282306671143\n",
      "Epoch: 36, Samples: 3520/5760, Loss: 0.01357218623161316\n",
      "Epoch: 36, Samples: 3552/5760, Loss: 0.011744886636734009\n",
      "Epoch: 36, Samples: 3584/5760, Loss: 0.023161083459854126\n",
      "Epoch: 36, Samples: 3616/5760, Loss: 0.024573087692260742\n",
      "Epoch: 36, Samples: 3648/5760, Loss: 0.015459656715393066\n",
      "Epoch: 36, Samples: 3680/5760, Loss: 0.008063554763793945\n",
      "Epoch: 36, Samples: 3712/5760, Loss: 0.0076313018798828125\n",
      "Epoch: 36, Samples: 3744/5760, Loss: 0.011465013027191162\n",
      "Epoch: 36, Samples: 3776/5760, Loss: 0.01161038875579834\n",
      "Epoch: 36, Samples: 3808/5760, Loss: 0.012941330671310425\n",
      "Epoch: 36, Samples: 3840/5760, Loss: 0.006580322980880737\n",
      "Epoch: 36, Samples: 3872/5760, Loss: 0.004864752292633057\n",
      "Epoch: 36, Samples: 3904/5760, Loss: 0.015501320362091064\n",
      "Epoch: 36, Samples: 3936/5760, Loss: 0.022172898054122925\n",
      "Epoch: 36, Samples: 3968/5760, Loss: 0.021548688411712646\n",
      "Epoch: 36, Samples: 4000/5760, Loss: 0.006949901580810547\n",
      "Epoch: 36, Samples: 4032/5760, Loss: 0.01929524540901184\n",
      "Epoch: 36, Samples: 4064/5760, Loss: 0.013015925884246826\n",
      "Epoch: 36, Samples: 4096/5760, Loss: 0.009876519441604614\n",
      "Epoch: 36, Samples: 4128/5760, Loss: 0.010495960712432861\n",
      "Epoch: 36, Samples: 4160/5760, Loss: 0.016039639711380005\n",
      "Epoch: 36, Samples: 4192/5760, Loss: 0.016142666339874268\n",
      "Epoch: 36, Samples: 4224/5760, Loss: 0.0087374746799469\n",
      "Epoch: 36, Samples: 4256/5760, Loss: 0.01890796422958374\n",
      "Epoch: 36, Samples: 4288/5760, Loss: 0.007529318332672119\n",
      "Epoch: 36, Samples: 4320/5760, Loss: 0.009030967950820923\n",
      "Epoch: 36, Samples: 4352/5760, Loss: 0.009396195411682129\n",
      "Epoch: 36, Samples: 4384/5760, Loss: 0.02155652642250061\n",
      "Epoch: 36, Samples: 4416/5760, Loss: 0.015656262636184692\n",
      "Epoch: 36, Samples: 4448/5760, Loss: 0.01815962791442871\n",
      "Epoch: 36, Samples: 4480/5760, Loss: 0.008292943239212036\n",
      "Epoch: 36, Samples: 4512/5760, Loss: 0.031142666935920715\n",
      "Epoch: 36, Samples: 4544/5760, Loss: 0.011821240186691284\n",
      "Epoch: 36, Samples: 4576/5760, Loss: 0.013362765312194824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Samples: 4608/5760, Loss: 0.006521314382553101\n",
      "Epoch: 36, Samples: 4640/5760, Loss: 0.010896354913711548\n",
      "Epoch: 36, Samples: 4672/5760, Loss: 0.016266852617263794\n",
      "Epoch: 36, Samples: 4704/5760, Loss: 0.0202905535697937\n",
      "Epoch: 36, Samples: 4736/5760, Loss: 0.013714849948883057\n",
      "Epoch: 36, Samples: 4768/5760, Loss: 0.011987447738647461\n",
      "Epoch: 36, Samples: 4800/5760, Loss: 0.03250129520893097\n",
      "Epoch: 36, Samples: 4832/5760, Loss: 0.005751222372055054\n",
      "Epoch: 36, Samples: 4864/5760, Loss: 0.011336982250213623\n",
      "Epoch: 36, Samples: 4896/5760, Loss: 0.040911346673965454\n",
      "Epoch: 36, Samples: 4928/5760, Loss: 0.009568393230438232\n",
      "Epoch: 36, Samples: 4960/5760, Loss: 0.02466866374015808\n",
      "Epoch: 36, Samples: 4992/5760, Loss: 0.011072725057601929\n",
      "Epoch: 36, Samples: 5024/5760, Loss: 0.02193659543991089\n",
      "Epoch: 36, Samples: 5056/5760, Loss: 0.016878902912139893\n",
      "Epoch: 36, Samples: 5088/5760, Loss: 0.007997274398803711\n",
      "Epoch: 36, Samples: 5120/5760, Loss: 0.012377321720123291\n",
      "Epoch: 36, Samples: 5152/5760, Loss: 0.023617655038833618\n",
      "Epoch: 36, Samples: 5184/5760, Loss: 0.008308470249176025\n",
      "Epoch: 36, Samples: 5216/5760, Loss: 0.010365515947341919\n",
      "Epoch: 36, Samples: 5248/5760, Loss: 0.010995090007781982\n",
      "Epoch: 36, Samples: 5280/5760, Loss: 0.0111674964427948\n",
      "Epoch: 36, Samples: 5312/5760, Loss: 0.022744476795196533\n",
      "Epoch: 36, Samples: 5344/5760, Loss: 0.010165423154830933\n",
      "Epoch: 36, Samples: 5376/5760, Loss: 0.024177491664886475\n",
      "Epoch: 36, Samples: 5408/5760, Loss: 0.01555556058883667\n",
      "Epoch: 36, Samples: 5440/5760, Loss: 0.029854804277420044\n",
      "Epoch: 36, Samples: 5472/5760, Loss: 0.007288902997970581\n",
      "Epoch: 36, Samples: 5504/5760, Loss: 0.010769516229629517\n",
      "Epoch: 36, Samples: 5536/5760, Loss: 0.018359750509262085\n",
      "Epoch: 36, Samples: 5568/5760, Loss: 0.016249537467956543\n",
      "Epoch: 36, Samples: 5600/5760, Loss: 0.01274600625038147\n",
      "Epoch: 36, Samples: 5632/5760, Loss: 0.016340017318725586\n",
      "Epoch: 36, Samples: 5664/5760, Loss: 0.012271225452423096\n",
      "Epoch: 36, Samples: 5696/5760, Loss: 0.01649227738380432\n",
      "Epoch: 36, Samples: 5728/5760, Loss: 0.5348018407821655\n",
      "\n",
      "Epoch: 36\n",
      "Training set: Average loss: 0.0171\n",
      "Validation set: Average loss: 0.3366, Accuracy: 754/818 (92%)\n",
      "Epoch: 37, Samples: 0/5760, Loss: 0.025402307510375977\n",
      "Epoch: 37, Samples: 32/5760, Loss: 0.01060253381729126\n",
      "Epoch: 37, Samples: 64/5760, Loss: 0.009907424449920654\n",
      "Epoch: 37, Samples: 96/5760, Loss: 0.012650907039642334\n",
      "Epoch: 37, Samples: 128/5760, Loss: 0.008037358522415161\n",
      "Epoch: 37, Samples: 160/5760, Loss: 0.014159977436065674\n",
      "Epoch: 37, Samples: 192/5760, Loss: 0.014739662408828735\n",
      "Epoch: 37, Samples: 224/5760, Loss: 0.011351972818374634\n",
      "Epoch: 37, Samples: 256/5760, Loss: 0.009509742259979248\n",
      "Epoch: 37, Samples: 288/5760, Loss: 0.010774284601211548\n",
      "Epoch: 37, Samples: 320/5760, Loss: 0.03334909677505493\n",
      "Epoch: 37, Samples: 352/5760, Loss: 0.010929495096206665\n",
      "Epoch: 37, Samples: 384/5760, Loss: 0.008552104234695435\n",
      "Epoch: 37, Samples: 416/5760, Loss: 0.014225363731384277\n",
      "Epoch: 37, Samples: 448/5760, Loss: 0.015747100114822388\n",
      "Epoch: 37, Samples: 480/5760, Loss: 0.014647185802459717\n",
      "Epoch: 37, Samples: 512/5760, Loss: 0.02620682120323181\n",
      "Epoch: 37, Samples: 544/5760, Loss: 0.010922551155090332\n",
      "Epoch: 37, Samples: 576/5760, Loss: 0.020645007491111755\n",
      "Epoch: 37, Samples: 608/5760, Loss: 0.015714287757873535\n",
      "Epoch: 37, Samples: 640/5760, Loss: 0.012726396322250366\n",
      "Epoch: 37, Samples: 672/5760, Loss: 0.016490191221237183\n",
      "Epoch: 37, Samples: 704/5760, Loss: 0.0110853910446167\n",
      "Epoch: 37, Samples: 736/5760, Loss: 0.01165163516998291\n",
      "Epoch: 37, Samples: 768/5760, Loss: 0.009368330240249634\n",
      "Epoch: 37, Samples: 800/5760, Loss: 0.00960281491279602\n",
      "Epoch: 37, Samples: 832/5760, Loss: 0.009690046310424805\n",
      "Epoch: 37, Samples: 864/5760, Loss: 0.03205861151218414\n",
      "Epoch: 37, Samples: 896/5760, Loss: 0.008394747972488403\n",
      "Epoch: 37, Samples: 928/5760, Loss: 0.01214069128036499\n",
      "Epoch: 37, Samples: 960/5760, Loss: 0.014798283576965332\n",
      "Epoch: 37, Samples: 992/5760, Loss: 0.010487794876098633\n",
      "Epoch: 37, Samples: 1024/5760, Loss: 0.013543158769607544\n",
      "Epoch: 37, Samples: 1056/5760, Loss: 0.010925918817520142\n",
      "Epoch: 37, Samples: 1088/5760, Loss: 0.01285591721534729\n",
      "Epoch: 37, Samples: 1120/5760, Loss: 0.017094910144805908\n",
      "Epoch: 37, Samples: 1152/5760, Loss: 0.012329280376434326\n",
      "Epoch: 37, Samples: 1184/5760, Loss: 0.02577364444732666\n",
      "Epoch: 37, Samples: 1216/5760, Loss: 0.012630164623260498\n",
      "Epoch: 37, Samples: 1248/5760, Loss: 0.009715020656585693\n",
      "Epoch: 37, Samples: 1280/5760, Loss: 0.016502946615219116\n",
      "Epoch: 37, Samples: 1312/5760, Loss: 0.007170379161834717\n",
      "Epoch: 37, Samples: 1344/5760, Loss: 0.006407380104064941\n",
      "Epoch: 37, Samples: 1376/5760, Loss: 0.01905384659767151\n",
      "Epoch: 37, Samples: 1408/5760, Loss: 0.010378777980804443\n",
      "Epoch: 37, Samples: 1440/5760, Loss: 0.005686372518539429\n",
      "Epoch: 37, Samples: 1472/5760, Loss: 0.010217875242233276\n",
      "Epoch: 37, Samples: 1504/5760, Loss: 0.00901678204536438\n",
      "Epoch: 37, Samples: 1536/5760, Loss: 0.020567789673805237\n",
      "Epoch: 37, Samples: 1568/5760, Loss: 0.013932615518569946\n",
      "Epoch: 37, Samples: 1600/5760, Loss: 0.011019289493560791\n",
      "Epoch: 37, Samples: 1632/5760, Loss: 0.013479173183441162\n",
      "Epoch: 37, Samples: 1664/5760, Loss: 0.015074938535690308\n",
      "Epoch: 37, Samples: 1696/5760, Loss: 0.011483490467071533\n",
      "Epoch: 37, Samples: 1728/5760, Loss: 0.007655531167984009\n",
      "Epoch: 37, Samples: 1760/5760, Loss: 0.009867221117019653\n",
      "Epoch: 37, Samples: 1792/5760, Loss: 0.00827595591545105\n",
      "Epoch: 37, Samples: 1824/5760, Loss: 0.008710771799087524\n",
      "Epoch: 37, Samples: 1856/5760, Loss: 0.009597092866897583\n",
      "Epoch: 37, Samples: 1888/5760, Loss: 0.012802362442016602\n",
      "Epoch: 37, Samples: 1920/5760, Loss: 0.012016624212265015\n",
      "Epoch: 37, Samples: 1952/5760, Loss: 0.014276117086410522\n",
      "Epoch: 37, Samples: 1984/5760, Loss: 0.011322438716888428\n",
      "Epoch: 37, Samples: 2016/5760, Loss: 0.007949203252792358\n",
      "Epoch: 37, Samples: 2048/5760, Loss: 0.015353500843048096\n",
      "Epoch: 37, Samples: 2080/5760, Loss: 0.014793097972869873\n",
      "Epoch: 37, Samples: 2112/5760, Loss: 0.012259751558303833\n",
      "Epoch: 37, Samples: 2144/5760, Loss: 0.010936170816421509\n",
      "Epoch: 37, Samples: 2176/5760, Loss: 0.019832134246826172\n",
      "Epoch: 37, Samples: 2208/5760, Loss: 0.00978192687034607\n",
      "Epoch: 37, Samples: 2240/5760, Loss: 0.021723687648773193\n",
      "Epoch: 37, Samples: 2272/5760, Loss: 0.02548345923423767\n",
      "Epoch: 37, Samples: 2304/5760, Loss: 0.04339367151260376\n",
      "Epoch: 37, Samples: 2336/5760, Loss: 0.01567956805229187\n",
      "Epoch: 37, Samples: 2368/5760, Loss: 0.009886711835861206\n",
      "Epoch: 37, Samples: 2400/5760, Loss: 0.010672241449356079\n",
      "Epoch: 37, Samples: 2432/5760, Loss: 0.011315345764160156\n",
      "Epoch: 37, Samples: 2464/5760, Loss: 0.030109882354736328\n",
      "Epoch: 37, Samples: 2496/5760, Loss: 0.011728078126907349\n",
      "Epoch: 37, Samples: 2528/5760, Loss: 0.008710354566574097\n",
      "Epoch: 37, Samples: 2560/5760, Loss: 0.01070261001586914\n",
      "Epoch: 37, Samples: 2592/5760, Loss: 0.008377939462661743\n",
      "Epoch: 37, Samples: 2624/5760, Loss: 0.010435551404953003\n",
      "Epoch: 37, Samples: 2656/5760, Loss: 0.01777064800262451\n",
      "Epoch: 37, Samples: 2688/5760, Loss: 0.013167113065719604\n",
      "Epoch: 37, Samples: 2720/5760, Loss: 0.01303863525390625\n",
      "Epoch: 37, Samples: 2752/5760, Loss: 0.011126935482025146\n",
      "Epoch: 37, Samples: 2784/5760, Loss: 0.010011911392211914\n",
      "Epoch: 37, Samples: 2816/5760, Loss: 0.013705998659133911\n",
      "Epoch: 37, Samples: 2848/5760, Loss: 0.009518593549728394\n",
      "Epoch: 37, Samples: 2880/5760, Loss: 0.010678976774215698\n",
      "Epoch: 37, Samples: 2912/5760, Loss: 0.010297566652297974\n",
      "Epoch: 37, Samples: 2944/5760, Loss: 0.014288723468780518\n",
      "Epoch: 37, Samples: 2976/5760, Loss: 0.013074874877929688\n",
      "Epoch: 37, Samples: 3008/5760, Loss: 0.03119605779647827\n",
      "Epoch: 37, Samples: 3040/5760, Loss: 0.011361628770828247\n",
      "Epoch: 37, Samples: 3072/5760, Loss: 0.013411402702331543\n",
      "Epoch: 37, Samples: 3104/5760, Loss: 0.008656501770019531\n",
      "Epoch: 37, Samples: 3136/5760, Loss: 0.01366877555847168\n",
      "Epoch: 37, Samples: 3168/5760, Loss: 0.007597625255584717\n",
      "Epoch: 37, Samples: 3200/5760, Loss: 0.010067403316497803\n",
      "Epoch: 37, Samples: 3232/5760, Loss: 0.01264449954032898\n",
      "Epoch: 37, Samples: 3264/5760, Loss: 0.0069724321365356445\n",
      "Epoch: 37, Samples: 3296/5760, Loss: 0.013673931360244751\n",
      "Epoch: 37, Samples: 3328/5760, Loss: 0.016470402479171753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37, Samples: 3360/5760, Loss: 0.011898130178451538\n",
      "Epoch: 37, Samples: 3392/5760, Loss: 0.01187160611152649\n",
      "Epoch: 37, Samples: 3424/5760, Loss: 0.008055001497268677\n",
      "Epoch: 37, Samples: 3456/5760, Loss: 0.01705065369606018\n",
      "Epoch: 37, Samples: 3488/5760, Loss: 0.020523816347122192\n",
      "Epoch: 37, Samples: 3520/5760, Loss: 0.008727580308914185\n",
      "Epoch: 37, Samples: 3552/5760, Loss: 0.011511474847793579\n",
      "Epoch: 37, Samples: 3584/5760, Loss: 0.010452061891555786\n",
      "Epoch: 37, Samples: 3616/5760, Loss: 0.013422399759292603\n",
      "Epoch: 37, Samples: 3648/5760, Loss: 0.006282120943069458\n",
      "Epoch: 37, Samples: 3680/5760, Loss: 0.01159214973449707\n",
      "Epoch: 37, Samples: 3712/5760, Loss: 0.022983521223068237\n",
      "Epoch: 37, Samples: 3744/5760, Loss: 0.01699492335319519\n",
      "Epoch: 37, Samples: 3776/5760, Loss: 0.025185048580169678\n",
      "Epoch: 37, Samples: 3808/5760, Loss: 0.04596327245235443\n",
      "Epoch: 37, Samples: 3840/5760, Loss: 0.012648820877075195\n",
      "Epoch: 37, Samples: 3872/5760, Loss: 0.009285926818847656\n",
      "Epoch: 37, Samples: 3904/5760, Loss: 0.012815147638320923\n",
      "Epoch: 37, Samples: 3936/5760, Loss: 0.008336395025253296\n",
      "Epoch: 37, Samples: 3968/5760, Loss: 0.00909307599067688\n",
      "Epoch: 37, Samples: 4000/5760, Loss: 0.012392431497573853\n",
      "Epoch: 37, Samples: 4032/5760, Loss: 0.014899671077728271\n",
      "Epoch: 37, Samples: 4064/5760, Loss: 0.021543532609939575\n",
      "Epoch: 37, Samples: 4096/5760, Loss: 0.018914461135864258\n",
      "Epoch: 37, Samples: 4128/5760, Loss: 0.01802125573158264\n",
      "Epoch: 37, Samples: 4160/5760, Loss: 0.008831322193145752\n",
      "Epoch: 37, Samples: 4192/5760, Loss: 0.019991755485534668\n",
      "Epoch: 37, Samples: 4224/5760, Loss: 0.014349967241287231\n",
      "Epoch: 37, Samples: 4256/5760, Loss: 0.005960255861282349\n",
      "Epoch: 37, Samples: 4288/5760, Loss: 0.010343074798583984\n",
      "Epoch: 37, Samples: 4320/5760, Loss: 0.015173107385635376\n",
      "Epoch: 37, Samples: 4352/5760, Loss: 0.014759927988052368\n",
      "Epoch: 37, Samples: 4384/5760, Loss: 0.015093386173248291\n",
      "Epoch: 37, Samples: 4416/5760, Loss: 0.012759119272232056\n",
      "Epoch: 37, Samples: 4448/5760, Loss: 0.011254280805587769\n",
      "Epoch: 37, Samples: 4480/5760, Loss: 0.010732561349868774\n",
      "Epoch: 37, Samples: 4512/5760, Loss: 0.010410100221633911\n",
      "Epoch: 37, Samples: 4544/5760, Loss: 0.01560935378074646\n",
      "Epoch: 37, Samples: 4576/5760, Loss: 0.006796926259994507\n",
      "Epoch: 37, Samples: 4608/5760, Loss: 0.015844374895095825\n",
      "Epoch: 37, Samples: 4640/5760, Loss: 0.007757633924484253\n",
      "Epoch: 37, Samples: 4672/5760, Loss: 0.01020011305809021\n",
      "Epoch: 37, Samples: 4704/5760, Loss: 0.020677432417869568\n",
      "Epoch: 37, Samples: 4736/5760, Loss: 0.014637768268585205\n",
      "Epoch: 37, Samples: 4768/5760, Loss: 0.016668617725372314\n",
      "Epoch: 37, Samples: 4800/5760, Loss: 0.013896197080612183\n",
      "Epoch: 37, Samples: 4832/5760, Loss: 0.007435411214828491\n",
      "Epoch: 37, Samples: 4864/5760, Loss: 0.015216290950775146\n",
      "Epoch: 37, Samples: 4896/5760, Loss: 0.01575109362602234\n",
      "Epoch: 37, Samples: 4928/5760, Loss: 0.01684659719467163\n",
      "Epoch: 37, Samples: 4960/5760, Loss: 0.013473749160766602\n",
      "Epoch: 37, Samples: 4992/5760, Loss: 0.017590224742889404\n",
      "Epoch: 37, Samples: 5024/5760, Loss: 0.010188549757003784\n",
      "Epoch: 37, Samples: 5056/5760, Loss: 0.015567988157272339\n",
      "Epoch: 37, Samples: 5088/5760, Loss: 0.0126018226146698\n",
      "Epoch: 37, Samples: 5120/5760, Loss: 0.012927055358886719\n",
      "Epoch: 37, Samples: 5152/5760, Loss: 0.019917696714401245\n",
      "Epoch: 37, Samples: 5184/5760, Loss: 0.010795384645462036\n",
      "Epoch: 37, Samples: 5216/5760, Loss: 0.01260480284690857\n",
      "Epoch: 37, Samples: 5248/5760, Loss: 0.008954077959060669\n",
      "Epoch: 37, Samples: 5280/5760, Loss: 0.019958198070526123\n",
      "Epoch: 37, Samples: 5312/5760, Loss: 0.009136736392974854\n",
      "Epoch: 37, Samples: 5344/5760, Loss: 0.016446828842163086\n",
      "Epoch: 37, Samples: 5376/5760, Loss: 0.012214094400405884\n",
      "Epoch: 37, Samples: 5408/5760, Loss: 0.014477699995040894\n",
      "Epoch: 37, Samples: 5440/5760, Loss: 0.014313757419586182\n",
      "Epoch: 37, Samples: 5472/5760, Loss: 0.013358831405639648\n",
      "Epoch: 37, Samples: 5504/5760, Loss: 0.008002817630767822\n",
      "Epoch: 37, Samples: 5536/5760, Loss: 0.025614798069000244\n",
      "Epoch: 37, Samples: 5568/5760, Loss: 0.016298502683639526\n",
      "Epoch: 37, Samples: 5600/5760, Loss: 0.007038801908493042\n",
      "Epoch: 37, Samples: 5632/5760, Loss: 0.021009981632232666\n",
      "Epoch: 37, Samples: 5664/5760, Loss: 0.012432783842086792\n",
      "Epoch: 37, Samples: 5696/5760, Loss: 0.01568010449409485\n",
      "Epoch: 37, Samples: 5728/5760, Loss: 0.488348126411438\n",
      "\n",
      "Epoch: 37\n",
      "Training set: Average loss: 0.0165\n",
      "Validation set: Average loss: 0.3432, Accuracy: 747/818 (91%)\n",
      "Epoch: 38, Samples: 0/5760, Loss: 0.010355234146118164\n",
      "Epoch: 38, Samples: 32/5760, Loss: 0.013931185007095337\n",
      "Epoch: 38, Samples: 64/5760, Loss: 0.01245260238647461\n",
      "Epoch: 38, Samples: 96/5760, Loss: 0.02605500817298889\n",
      "Epoch: 38, Samples: 128/5760, Loss: 0.014342963695526123\n",
      "Epoch: 38, Samples: 160/5760, Loss: 0.011413812637329102\n",
      "Epoch: 38, Samples: 192/5760, Loss: 0.04357759654521942\n",
      "Epoch: 38, Samples: 224/5760, Loss: 0.01318061351776123\n",
      "Epoch: 38, Samples: 256/5760, Loss: 0.009431660175323486\n",
      "Epoch: 38, Samples: 288/5760, Loss: 0.011529773473739624\n",
      "Epoch: 38, Samples: 320/5760, Loss: 0.009020030498504639\n",
      "Epoch: 38, Samples: 352/5760, Loss: 0.014545589685440063\n",
      "Epoch: 38, Samples: 384/5760, Loss: 0.012925654649734497\n",
      "Epoch: 38, Samples: 416/5760, Loss: 0.008223563432693481\n",
      "Epoch: 38, Samples: 448/5760, Loss: 0.018088459968566895\n",
      "Epoch: 38, Samples: 480/5760, Loss: 0.007075667381286621\n",
      "Epoch: 38, Samples: 512/5760, Loss: 0.01789030432701111\n",
      "Epoch: 38, Samples: 544/5760, Loss: 0.0158078670501709\n",
      "Epoch: 38, Samples: 576/5760, Loss: 0.00888282060623169\n",
      "Epoch: 38, Samples: 608/5760, Loss: 0.010325998067855835\n",
      "Epoch: 38, Samples: 640/5760, Loss: 0.016847878694534302\n",
      "Epoch: 38, Samples: 672/5760, Loss: 0.012790977954864502\n",
      "Epoch: 38, Samples: 704/5760, Loss: 0.00783228874206543\n",
      "Epoch: 38, Samples: 736/5760, Loss: 0.007707536220550537\n",
      "Epoch: 38, Samples: 768/5760, Loss: 0.013438284397125244\n",
      "Epoch: 38, Samples: 800/5760, Loss: 0.014504671096801758\n",
      "Epoch: 38, Samples: 832/5760, Loss: 0.019257068634033203\n",
      "Epoch: 38, Samples: 864/5760, Loss: 0.02298879623413086\n",
      "Epoch: 38, Samples: 896/5760, Loss: 0.011882781982421875\n",
      "Epoch: 38, Samples: 928/5760, Loss: 0.012203037738800049\n",
      "Epoch: 38, Samples: 960/5760, Loss: 0.013319313526153564\n",
      "Epoch: 38, Samples: 992/5760, Loss: 0.008269518613815308\n",
      "Epoch: 38, Samples: 1024/5760, Loss: 0.007387071847915649\n",
      "Epoch: 38, Samples: 1056/5760, Loss: 0.027617931365966797\n",
      "Epoch: 38, Samples: 1088/5760, Loss: 0.01458674669265747\n",
      "Epoch: 38, Samples: 1120/5760, Loss: 0.010223358869552612\n",
      "Epoch: 38, Samples: 1152/5760, Loss: 0.009051293134689331\n",
      "Epoch: 38, Samples: 1184/5760, Loss: 0.02001611888408661\n",
      "Epoch: 38, Samples: 1216/5760, Loss: 0.01826038956642151\n",
      "Epoch: 38, Samples: 1248/5760, Loss: 0.010754644870758057\n",
      "Epoch: 38, Samples: 1280/5760, Loss: 0.01867002248764038\n",
      "Epoch: 38, Samples: 1312/5760, Loss: 0.00789746642112732\n",
      "Epoch: 38, Samples: 1344/5760, Loss: 0.015041321516036987\n",
      "Epoch: 38, Samples: 1376/5760, Loss: 0.017680078744888306\n",
      "Epoch: 38, Samples: 1408/5760, Loss: 0.010429471731185913\n",
      "Epoch: 38, Samples: 1440/5760, Loss: 0.0168725848197937\n",
      "Epoch: 38, Samples: 1472/5760, Loss: 0.012855350971221924\n",
      "Epoch: 38, Samples: 1504/5760, Loss: 0.016247421503067017\n",
      "Epoch: 38, Samples: 1536/5760, Loss: 0.01422882080078125\n",
      "Epoch: 38, Samples: 1568/5760, Loss: 0.015477567911148071\n",
      "Epoch: 38, Samples: 1600/5760, Loss: 0.010369747877120972\n",
      "Epoch: 38, Samples: 1632/5760, Loss: 0.00866246223449707\n",
      "Epoch: 38, Samples: 1664/5760, Loss: 0.013063132762908936\n",
      "Epoch: 38, Samples: 1696/5760, Loss: 0.012176454067230225\n",
      "Epoch: 38, Samples: 1728/5760, Loss: 0.014535874128341675\n",
      "Epoch: 38, Samples: 1760/5760, Loss: 0.013810455799102783\n",
      "Epoch: 38, Samples: 1792/5760, Loss: 0.008960455656051636\n",
      "Epoch: 38, Samples: 1824/5760, Loss: 0.011541545391082764\n",
      "Epoch: 38, Samples: 1856/5760, Loss: 0.01323544979095459\n",
      "Epoch: 38, Samples: 1888/5760, Loss: 0.010875970125198364\n",
      "Epoch: 38, Samples: 1920/5760, Loss: 0.013730853796005249\n",
      "Epoch: 38, Samples: 1952/5760, Loss: 0.016000717878341675\n",
      "Epoch: 38, Samples: 1984/5760, Loss: 0.009823471307754517\n",
      "Epoch: 38, Samples: 2016/5760, Loss: 0.007936835289001465\n",
      "Epoch: 38, Samples: 2048/5760, Loss: 0.010396778583526611\n",
      "Epoch: 38, Samples: 2080/5760, Loss: 0.01907426118850708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, Samples: 2112/5760, Loss: 0.009296923875808716\n",
      "Epoch: 38, Samples: 2144/5760, Loss: 0.01266014575958252\n",
      "Epoch: 38, Samples: 2176/5760, Loss: 0.01635652780532837\n",
      "Epoch: 38, Samples: 2208/5760, Loss: 0.0181502103805542\n",
      "Epoch: 38, Samples: 2240/5760, Loss: 0.014406710863113403\n",
      "Epoch: 38, Samples: 2272/5760, Loss: 0.0068333446979522705\n",
      "Epoch: 38, Samples: 2304/5760, Loss: 0.025874197483062744\n",
      "Epoch: 38, Samples: 2336/5760, Loss: 0.016238927841186523\n",
      "Epoch: 38, Samples: 2368/5760, Loss: 0.02435550093650818\n",
      "Epoch: 38, Samples: 2400/5760, Loss: 0.01424756646156311\n",
      "Epoch: 38, Samples: 2432/5760, Loss: 0.009630769491195679\n",
      "Epoch: 38, Samples: 2464/5760, Loss: 0.005727797746658325\n",
      "Epoch: 38, Samples: 2496/5760, Loss: 0.014052361249923706\n",
      "Epoch: 38, Samples: 2528/5760, Loss: 0.012665867805480957\n",
      "Epoch: 38, Samples: 2560/5760, Loss: 0.009496152400970459\n",
      "Epoch: 38, Samples: 2592/5760, Loss: 0.013760864734649658\n",
      "Epoch: 38, Samples: 2624/5760, Loss: 0.028394490480422974\n",
      "Epoch: 38, Samples: 2656/5760, Loss: 0.023770123720169067\n",
      "Epoch: 38, Samples: 2688/5760, Loss: 0.008725076913833618\n",
      "Epoch: 38, Samples: 2720/5760, Loss: 0.012560397386550903\n",
      "Epoch: 38, Samples: 2752/5760, Loss: 0.013441085815429688\n",
      "Epoch: 38, Samples: 2784/5760, Loss: 0.014061719179153442\n",
      "Epoch: 38, Samples: 2816/5760, Loss: 0.025173485279083252\n",
      "Epoch: 38, Samples: 2848/5760, Loss: 0.018995970487594604\n",
      "Epoch: 38, Samples: 2880/5760, Loss: 0.01673603057861328\n",
      "Epoch: 38, Samples: 2912/5760, Loss: 0.00937807559967041\n",
      "Epoch: 38, Samples: 2944/5760, Loss: 0.02084556221961975\n",
      "Epoch: 38, Samples: 2976/5760, Loss: 0.008317947387695312\n",
      "Epoch: 38, Samples: 3008/5760, Loss: 0.010250747203826904\n",
      "Epoch: 38, Samples: 3040/5760, Loss: 0.015853196382522583\n",
      "Epoch: 38, Samples: 3072/5760, Loss: 0.015622437000274658\n",
      "Epoch: 38, Samples: 3104/5760, Loss: 0.011053740978240967\n",
      "Epoch: 38, Samples: 3136/5760, Loss: 0.017038077116012573\n",
      "Epoch: 38, Samples: 3168/5760, Loss: 0.022603482007980347\n",
      "Epoch: 38, Samples: 3200/5760, Loss: 0.012273281812667847\n",
      "Epoch: 38, Samples: 3232/5760, Loss: 0.007272392511367798\n",
      "Epoch: 38, Samples: 3264/5760, Loss: 0.009288668632507324\n",
      "Epoch: 38, Samples: 3296/5760, Loss: 0.014601320028305054\n",
      "Epoch: 38, Samples: 3328/5760, Loss: 0.007187604904174805\n",
      "Epoch: 38, Samples: 3360/5760, Loss: 0.008173942565917969\n",
      "Epoch: 38, Samples: 3392/5760, Loss: 0.01874956488609314\n",
      "Epoch: 38, Samples: 3424/5760, Loss: 0.008839935064315796\n",
      "Epoch: 38, Samples: 3456/5760, Loss: 0.01325225830078125\n",
      "Epoch: 38, Samples: 3488/5760, Loss: 0.014369785785675049\n",
      "Epoch: 38, Samples: 3520/5760, Loss: 0.007071971893310547\n",
      "Epoch: 38, Samples: 3552/5760, Loss: 0.012830227613449097\n",
      "Epoch: 38, Samples: 3584/5760, Loss: 0.010330736637115479\n",
      "Epoch: 38, Samples: 3616/5760, Loss: 0.029843896627426147\n",
      "Epoch: 38, Samples: 3648/5760, Loss: 0.01919800043106079\n",
      "Epoch: 38, Samples: 3680/5760, Loss: 0.007861316204071045\n",
      "Epoch: 38, Samples: 3712/5760, Loss: 0.01993468403816223\n",
      "Epoch: 38, Samples: 3744/5760, Loss: 0.024913549423217773\n",
      "Epoch: 38, Samples: 3776/5760, Loss: 0.015659749507904053\n",
      "Epoch: 38, Samples: 3808/5760, Loss: 0.009022951126098633\n",
      "Epoch: 38, Samples: 3840/5760, Loss: 0.012991249561309814\n",
      "Epoch: 38, Samples: 3872/5760, Loss: 0.016296863555908203\n",
      "Epoch: 38, Samples: 3904/5760, Loss: 0.010458499193191528\n",
      "Epoch: 38, Samples: 3936/5760, Loss: 0.011612385511398315\n",
      "Epoch: 38, Samples: 3968/5760, Loss: 0.010020583868026733\n",
      "Epoch: 38, Samples: 4000/5760, Loss: 0.008856624364852905\n",
      "Epoch: 38, Samples: 4032/5760, Loss: 0.02125796675682068\n",
      "Epoch: 38, Samples: 4064/5760, Loss: 0.00827491283416748\n",
      "Epoch: 38, Samples: 4096/5760, Loss: 0.00787442922592163\n",
      "Epoch: 38, Samples: 4128/5760, Loss: 0.012779176235198975\n",
      "Epoch: 38, Samples: 4160/5760, Loss: 0.015016824007034302\n",
      "Epoch: 38, Samples: 4192/5760, Loss: 0.010948210954666138\n",
      "Epoch: 38, Samples: 4224/5760, Loss: 0.02206796407699585\n",
      "Epoch: 38, Samples: 4256/5760, Loss: 0.014772385358810425\n",
      "Epoch: 38, Samples: 4288/5760, Loss: 0.02486950159072876\n",
      "Epoch: 38, Samples: 4320/5760, Loss: 0.017505496740341187\n",
      "Epoch: 38, Samples: 4352/5760, Loss: 0.010488510131835938\n",
      "Epoch: 38, Samples: 4384/5760, Loss: 0.0302584171295166\n",
      "Epoch: 38, Samples: 4416/5760, Loss: 0.012088239192962646\n",
      "Epoch: 38, Samples: 4448/5760, Loss: 0.016930371522903442\n",
      "Epoch: 38, Samples: 4480/5760, Loss: 0.013414174318313599\n",
      "Epoch: 38, Samples: 4512/5760, Loss: 0.0122222900390625\n",
      "Epoch: 38, Samples: 4544/5760, Loss: 0.01162448525428772\n",
      "Epoch: 38, Samples: 4576/5760, Loss: 0.008600115776062012\n",
      "Epoch: 38, Samples: 4608/5760, Loss: 0.030089348554611206\n",
      "Epoch: 38, Samples: 4640/5760, Loss: 0.011969804763793945\n",
      "Epoch: 38, Samples: 4672/5760, Loss: 0.008316725492477417\n",
      "Epoch: 38, Samples: 4704/5760, Loss: 0.01745814085006714\n",
      "Epoch: 38, Samples: 4736/5760, Loss: 0.010254710912704468\n",
      "Epoch: 38, Samples: 4768/5760, Loss: 0.010943382978439331\n",
      "Epoch: 38, Samples: 4800/5760, Loss: 0.02683134377002716\n",
      "Epoch: 38, Samples: 4832/5760, Loss: 0.020362436771392822\n",
      "Epoch: 38, Samples: 4864/5760, Loss: 0.012206673622131348\n",
      "Epoch: 38, Samples: 4896/5760, Loss: 0.01770228147506714\n",
      "Epoch: 38, Samples: 4928/5760, Loss: 0.013798832893371582\n",
      "Epoch: 38, Samples: 4960/5760, Loss: 0.023576602339744568\n",
      "Epoch: 38, Samples: 4992/5760, Loss: 0.024201303720474243\n",
      "Epoch: 38, Samples: 5024/5760, Loss: 0.015476465225219727\n",
      "Epoch: 38, Samples: 5056/5760, Loss: 0.01072189211845398\n",
      "Epoch: 38, Samples: 5088/5760, Loss: 0.015169501304626465\n",
      "Epoch: 38, Samples: 5120/5760, Loss: 0.01603493094444275\n",
      "Epoch: 38, Samples: 5152/5760, Loss: 0.014838635921478271\n",
      "Epoch: 38, Samples: 5184/5760, Loss: 0.007554054260253906\n",
      "Epoch: 38, Samples: 5216/5760, Loss: 0.010146498680114746\n",
      "Epoch: 38, Samples: 5248/5760, Loss: 0.010525524616241455\n",
      "Epoch: 38, Samples: 5280/5760, Loss: 0.009749829769134521\n",
      "Epoch: 38, Samples: 5312/5760, Loss: 0.012300223112106323\n",
      "Epoch: 38, Samples: 5344/5760, Loss: 0.01999199390411377\n",
      "Epoch: 38, Samples: 5376/5760, Loss: 0.007417738437652588\n",
      "Epoch: 38, Samples: 5408/5760, Loss: 0.008804470300674438\n",
      "Epoch: 38, Samples: 5440/5760, Loss: 0.015437155961990356\n",
      "Epoch: 38, Samples: 5472/5760, Loss: 0.011751025915145874\n",
      "Epoch: 38, Samples: 5504/5760, Loss: 0.0058554112911224365\n",
      "Epoch: 38, Samples: 5536/5760, Loss: 0.013929545879364014\n",
      "Epoch: 38, Samples: 5568/5760, Loss: 0.015537232160568237\n",
      "Epoch: 38, Samples: 5600/5760, Loss: 0.009940266609191895\n",
      "Epoch: 38, Samples: 5632/5760, Loss: 0.00627291202545166\n",
      "Epoch: 38, Samples: 5664/5760, Loss: 0.01362881064414978\n",
      "Epoch: 38, Samples: 5696/5760, Loss: 0.008076906204223633\n",
      "Epoch: 38, Samples: 5728/5760, Loss: 0.55164635181427\n",
      "\n",
      "Epoch: 38\n",
      "Training set: Average loss: 0.0171\n",
      "Validation set: Average loss: 0.3366, Accuracy: 751/818 (92%)\n",
      "Epoch: 39, Samples: 0/5760, Loss: 0.021451890468597412\n",
      "Epoch: 39, Samples: 32/5760, Loss: 0.024271875619888306\n",
      "Epoch: 39, Samples: 64/5760, Loss: 0.008822858333587646\n",
      "Epoch: 39, Samples: 96/5760, Loss: 0.012025445699691772\n",
      "Epoch: 39, Samples: 128/5760, Loss: 0.010151565074920654\n",
      "Epoch: 39, Samples: 160/5760, Loss: 0.016759544610977173\n",
      "Epoch: 39, Samples: 192/5760, Loss: 0.04474388062953949\n",
      "Epoch: 39, Samples: 224/5760, Loss: 0.01711249351501465\n",
      "Epoch: 39, Samples: 256/5760, Loss: 0.008171230554580688\n",
      "Epoch: 39, Samples: 288/5760, Loss: 0.008191406726837158\n",
      "Epoch: 39, Samples: 320/5760, Loss: 0.011587947607040405\n",
      "Epoch: 39, Samples: 352/5760, Loss: 0.009938210248947144\n",
      "Epoch: 39, Samples: 384/5760, Loss: 0.028808847069740295\n",
      "Epoch: 39, Samples: 416/5760, Loss: 0.01262173056602478\n",
      "Epoch: 39, Samples: 448/5760, Loss: 0.014439821243286133\n",
      "Epoch: 39, Samples: 480/5760, Loss: 0.012397795915603638\n",
      "Epoch: 39, Samples: 512/5760, Loss: 0.006959974765777588\n",
      "Epoch: 39, Samples: 544/5760, Loss: 0.009969621896743774\n",
      "Epoch: 39, Samples: 576/5760, Loss: 0.011676222085952759\n",
      "Epoch: 39, Samples: 608/5760, Loss: 0.007647335529327393\n",
      "Epoch: 39, Samples: 640/5760, Loss: 0.010757267475128174\n",
      "Epoch: 39, Samples: 672/5760, Loss: 0.02045866847038269\n",
      "Epoch: 39, Samples: 704/5760, Loss: 0.009025126695632935\n",
      "Epoch: 39, Samples: 736/5760, Loss: 0.031289488077163696\n",
      "Epoch: 39, Samples: 768/5760, Loss: 0.007068812847137451\n",
      "Epoch: 39, Samples: 800/5760, Loss: 0.007073938846588135\n",
      "Epoch: 39, Samples: 832/5760, Loss: 0.012900680303573608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, Samples: 864/5760, Loss: 0.01111370325088501\n",
      "Epoch: 39, Samples: 896/5760, Loss: 0.0093003511428833\n",
      "Epoch: 39, Samples: 928/5760, Loss: 0.021150588989257812\n",
      "Epoch: 39, Samples: 960/5760, Loss: 0.015160083770751953\n",
      "Epoch: 39, Samples: 992/5760, Loss: 0.015731215476989746\n",
      "Epoch: 39, Samples: 1024/5760, Loss: 0.01072189211845398\n",
      "Epoch: 39, Samples: 1056/5760, Loss: 0.0134505033493042\n",
      "Epoch: 39, Samples: 1088/5760, Loss: 0.02043136954307556\n",
      "Epoch: 39, Samples: 1120/5760, Loss: 0.00887840986251831\n",
      "Epoch: 39, Samples: 1152/5760, Loss: 0.006212830543518066\n",
      "Epoch: 39, Samples: 1184/5760, Loss: 0.013715535402297974\n",
      "Epoch: 39, Samples: 1216/5760, Loss: 0.010569840669631958\n",
      "Epoch: 39, Samples: 1248/5760, Loss: 0.01485288143157959\n",
      "Epoch: 39, Samples: 1280/5760, Loss: 0.034729570150375366\n",
      "Epoch: 39, Samples: 1312/5760, Loss: 0.021931186318397522\n",
      "Epoch: 39, Samples: 1344/5760, Loss: 0.009541571140289307\n",
      "Epoch: 39, Samples: 1376/5760, Loss: 0.0073080360889434814\n",
      "Epoch: 39, Samples: 1408/5760, Loss: 0.010118931531906128\n",
      "Epoch: 39, Samples: 1440/5760, Loss: 0.015809684991836548\n",
      "Epoch: 39, Samples: 1472/5760, Loss: 0.020384520292282104\n",
      "Epoch: 39, Samples: 1504/5760, Loss: 0.013593167066574097\n",
      "Epoch: 39, Samples: 1536/5760, Loss: 0.008254379034042358\n",
      "Epoch: 39, Samples: 1568/5760, Loss: 0.005233347415924072\n",
      "Epoch: 39, Samples: 1600/5760, Loss: 0.012009888887405396\n",
      "Epoch: 39, Samples: 1632/5760, Loss: 0.013557881116867065\n",
      "Epoch: 39, Samples: 1664/5760, Loss: 0.008462131023406982\n",
      "Epoch: 39, Samples: 1696/5760, Loss: 0.017311453819274902\n",
      "Epoch: 39, Samples: 1728/5760, Loss: 0.017599880695343018\n",
      "Epoch: 39, Samples: 1760/5760, Loss: 0.006666034460067749\n",
      "Epoch: 39, Samples: 1792/5760, Loss: 0.015268951654434204\n",
      "Epoch: 39, Samples: 1824/5760, Loss: 0.02010074257850647\n",
      "Epoch: 39, Samples: 1856/5760, Loss: 0.017999768257141113\n",
      "Epoch: 39, Samples: 1888/5760, Loss: 0.010964184999465942\n",
      "Epoch: 39, Samples: 1920/5760, Loss: 0.011842399835586548\n",
      "Epoch: 39, Samples: 1952/5760, Loss: 0.01677793264389038\n",
      "Epoch: 39, Samples: 1984/5760, Loss: 0.012393176555633545\n",
      "Epoch: 39, Samples: 2016/5760, Loss: 0.022932663559913635\n",
      "Epoch: 39, Samples: 2048/5760, Loss: 0.008157312870025635\n",
      "Epoch: 39, Samples: 2080/5760, Loss: 0.008384674787521362\n",
      "Epoch: 39, Samples: 2112/5760, Loss: 0.007437318563461304\n",
      "Epoch: 39, Samples: 2144/5760, Loss: 0.007597595453262329\n",
      "Epoch: 39, Samples: 2176/5760, Loss: 0.005602478981018066\n",
      "Epoch: 39, Samples: 2208/5760, Loss: 0.011581242084503174\n",
      "Epoch: 39, Samples: 2240/5760, Loss: 0.014492183923721313\n",
      "Epoch: 39, Samples: 2272/5760, Loss: 0.006053447723388672\n",
      "Epoch: 39, Samples: 2304/5760, Loss: 0.016573041677474976\n",
      "Epoch: 39, Samples: 2336/5760, Loss: 0.014588505029678345\n",
      "Epoch: 39, Samples: 2368/5760, Loss: 0.005430698394775391\n",
      "Epoch: 39, Samples: 2400/5760, Loss: 0.0064283013343811035\n",
      "Epoch: 39, Samples: 2432/5760, Loss: 0.020685851573944092\n",
      "Epoch: 39, Samples: 2464/5760, Loss: 0.04371337592601776\n",
      "Epoch: 39, Samples: 2496/5760, Loss: 0.00765642523765564\n",
      "Epoch: 39, Samples: 2528/5760, Loss: 0.0049784183502197266\n",
      "Epoch: 39, Samples: 2560/5760, Loss: 0.01478627324104309\n",
      "Epoch: 39, Samples: 2592/5760, Loss: 0.01183655858039856\n",
      "Epoch: 39, Samples: 2624/5760, Loss: 0.01030394434928894\n",
      "Epoch: 39, Samples: 2656/5760, Loss: 0.006323665380477905\n",
      "Epoch: 39, Samples: 2688/5760, Loss: 0.011447161436080933\n",
      "Epoch: 39, Samples: 2720/5760, Loss: 0.018202364444732666\n",
      "Epoch: 39, Samples: 2752/5760, Loss: 0.018848925828933716\n",
      "Epoch: 39, Samples: 2784/5760, Loss: 0.01308467984199524\n",
      "Epoch: 39, Samples: 2816/5760, Loss: 0.013155847787857056\n",
      "Epoch: 39, Samples: 2848/5760, Loss: 0.01753827929496765\n",
      "Epoch: 39, Samples: 2880/5760, Loss: 0.012693732976913452\n",
      "Epoch: 39, Samples: 2912/5760, Loss: 0.008101940155029297\n",
      "Epoch: 39, Samples: 2944/5760, Loss: 0.009876042604446411\n",
      "Epoch: 39, Samples: 2976/5760, Loss: 0.00827866792678833\n",
      "Epoch: 39, Samples: 3008/5760, Loss: 0.006880730390548706\n",
      "Epoch: 39, Samples: 3040/5760, Loss: 0.012850344181060791\n",
      "Epoch: 39, Samples: 3072/5760, Loss: 0.013704448938369751\n",
      "Epoch: 39, Samples: 3104/5760, Loss: 0.009584158658981323\n",
      "Epoch: 39, Samples: 3136/5760, Loss: 0.011572659015655518\n",
      "Epoch: 39, Samples: 3168/5760, Loss: 0.005920380353927612\n",
      "Epoch: 39, Samples: 3200/5760, Loss: 0.0116213858127594\n",
      "Epoch: 39, Samples: 3232/5760, Loss: 0.01399211585521698\n",
      "Epoch: 39, Samples: 3264/5760, Loss: 0.009819984436035156\n",
      "Epoch: 39, Samples: 3296/5760, Loss: 0.011428087949752808\n",
      "Epoch: 39, Samples: 3328/5760, Loss: 0.010050952434539795\n",
      "Epoch: 39, Samples: 3360/5760, Loss: 0.010367155075073242\n",
      "Epoch: 39, Samples: 3392/5760, Loss: 0.008473247289657593\n",
      "Epoch: 39, Samples: 3424/5760, Loss: 0.013342201709747314\n",
      "Epoch: 39, Samples: 3456/5760, Loss: 0.025867879390716553\n",
      "Epoch: 39, Samples: 3488/5760, Loss: 0.016276240348815918\n",
      "Epoch: 39, Samples: 3520/5760, Loss: 0.009194612503051758\n",
      "Epoch: 39, Samples: 3552/5760, Loss: 0.010349243879318237\n",
      "Epoch: 39, Samples: 3584/5760, Loss: 0.019596487283706665\n",
      "Epoch: 39, Samples: 3616/5760, Loss: 0.021950870752334595\n",
      "Epoch: 39, Samples: 3648/5760, Loss: 0.006870537996292114\n",
      "Epoch: 39, Samples: 3680/5760, Loss: 0.015194118022918701\n",
      "Epoch: 39, Samples: 3712/5760, Loss: 0.012569516897201538\n",
      "Epoch: 39, Samples: 3744/5760, Loss: 0.010486960411071777\n",
      "Epoch: 39, Samples: 3776/5760, Loss: 0.008899539709091187\n",
      "Epoch: 39, Samples: 3808/5760, Loss: 0.018248528242111206\n",
      "Epoch: 39, Samples: 3840/5760, Loss: 0.018251806497573853\n",
      "Epoch: 39, Samples: 3872/5760, Loss: 0.013966739177703857\n",
      "Epoch: 39, Samples: 3904/5760, Loss: 0.010459423065185547\n",
      "Epoch: 39, Samples: 3936/5760, Loss: 0.00940755009651184\n",
      "Epoch: 39, Samples: 3968/5760, Loss: 0.010766088962554932\n",
      "Epoch: 39, Samples: 4000/5760, Loss: 0.010042279958724976\n",
      "Epoch: 39, Samples: 4032/5760, Loss: 0.019420772790908813\n",
      "Epoch: 39, Samples: 4064/5760, Loss: 0.02069428563117981\n",
      "Epoch: 39, Samples: 4096/5760, Loss: 0.011897742748260498\n",
      "Epoch: 39, Samples: 4128/5760, Loss: 0.00977364182472229\n",
      "Epoch: 39, Samples: 4160/5760, Loss: 0.013989388942718506\n",
      "Epoch: 39, Samples: 4192/5760, Loss: 0.009937703609466553\n",
      "Epoch: 39, Samples: 4224/5760, Loss: 0.010854750871658325\n",
      "Epoch: 39, Samples: 4256/5760, Loss: 0.009360849857330322\n",
      "Epoch: 39, Samples: 4288/5760, Loss: 0.013980090618133545\n",
      "Epoch: 39, Samples: 4320/5760, Loss: 0.009573906660079956\n",
      "Epoch: 39, Samples: 4352/5760, Loss: 0.016941845417022705\n",
      "Epoch: 39, Samples: 4384/5760, Loss: 0.029442399740219116\n",
      "Epoch: 39, Samples: 4416/5760, Loss: 0.013218075037002563\n",
      "Epoch: 39, Samples: 4448/5760, Loss: 0.010737031698226929\n",
      "Epoch: 39, Samples: 4480/5760, Loss: 0.015032827854156494\n",
      "Epoch: 39, Samples: 4512/5760, Loss: 0.01468360424041748\n",
      "Epoch: 39, Samples: 4544/5760, Loss: 0.0062846839427948\n",
      "Epoch: 39, Samples: 4576/5760, Loss: 0.01941579580307007\n",
      "Epoch: 39, Samples: 4608/5760, Loss: 0.015353083610534668\n",
      "Epoch: 39, Samples: 4640/5760, Loss: 0.014492571353912354\n",
      "Epoch: 39, Samples: 4672/5760, Loss: 0.01695343852043152\n",
      "Epoch: 39, Samples: 4704/5760, Loss: 0.020450204610824585\n",
      "Epoch: 39, Samples: 4736/5760, Loss: 0.0065066516399383545\n",
      "Epoch: 39, Samples: 4768/5760, Loss: 0.025506794452667236\n",
      "Epoch: 39, Samples: 4800/5760, Loss: 0.00890854001045227\n",
      "Epoch: 39, Samples: 4832/5760, Loss: 0.013485461473464966\n",
      "Epoch: 39, Samples: 4864/5760, Loss: 0.01151186227798462\n",
      "Epoch: 39, Samples: 4896/5760, Loss: 0.008741885423660278\n",
      "Epoch: 39, Samples: 4928/5760, Loss: 0.0077537596225738525\n",
      "Epoch: 39, Samples: 4960/5760, Loss: 0.008603990077972412\n",
      "Epoch: 39, Samples: 4992/5760, Loss: 0.02510949969291687\n",
      "Epoch: 39, Samples: 5024/5760, Loss: 0.01240307092666626\n",
      "Epoch: 39, Samples: 5056/5760, Loss: 0.017242401838302612\n",
      "Epoch: 39, Samples: 5088/5760, Loss: 0.01682332158088684\n",
      "Epoch: 39, Samples: 5120/5760, Loss: 0.00976020097732544\n",
      "Epoch: 39, Samples: 5152/5760, Loss: 0.014944136142730713\n",
      "Epoch: 39, Samples: 5184/5760, Loss: 0.015587270259857178\n",
      "Epoch: 39, Samples: 5216/5760, Loss: 0.01098853349685669\n",
      "Epoch: 39, Samples: 5248/5760, Loss: 0.00780034065246582\n",
      "Epoch: 39, Samples: 5280/5760, Loss: 0.011382341384887695\n",
      "Epoch: 39, Samples: 5312/5760, Loss: 0.02030119299888611\n",
      "Epoch: 39, Samples: 5344/5760, Loss: 0.009933024644851685\n",
      "Epoch: 39, Samples: 5376/5760, Loss: 0.007759004831314087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, Samples: 5408/5760, Loss: 0.023109644651412964\n",
      "Epoch: 39, Samples: 5440/5760, Loss: 0.009837627410888672\n",
      "Epoch: 39, Samples: 5472/5760, Loss: 0.01598450541496277\n",
      "Epoch: 39, Samples: 5504/5760, Loss: 0.015286654233932495\n",
      "Epoch: 39, Samples: 5536/5760, Loss: 0.021609872579574585\n",
      "Epoch: 39, Samples: 5568/5760, Loss: 0.004074782133102417\n",
      "Epoch: 39, Samples: 5600/5760, Loss: 0.014474481344223022\n",
      "Epoch: 39, Samples: 5632/5760, Loss: 0.009519696235656738\n",
      "Epoch: 39, Samples: 5664/5760, Loss: 0.014634788036346436\n",
      "Epoch: 39, Samples: 5696/5760, Loss: 0.006804108619689941\n",
      "Epoch: 39, Samples: 5728/5760, Loss: 0.19255602359771729\n",
      "\n",
      "Epoch: 39\n",
      "Training set: Average loss: 0.0145\n",
      "Validation set: Average loss: 0.3323, Accuracy: 747/818 (91%)\n",
      "Epoch: 40, Samples: 0/5760, Loss: 0.009458482265472412\n",
      "Epoch: 40, Samples: 32/5760, Loss: 0.011114895343780518\n",
      "Epoch: 40, Samples: 64/5760, Loss: 0.01090395450592041\n",
      "Epoch: 40, Samples: 96/5760, Loss: 0.012338876724243164\n",
      "Epoch: 40, Samples: 128/5760, Loss: 0.025408506393432617\n",
      "Epoch: 40, Samples: 160/5760, Loss: 0.01018756628036499\n",
      "Epoch: 40, Samples: 192/5760, Loss: 0.007742136716842651\n",
      "Epoch: 40, Samples: 224/5760, Loss: 0.015515923500061035\n",
      "Epoch: 40, Samples: 256/5760, Loss: 0.02284911274909973\n",
      "Epoch: 40, Samples: 288/5760, Loss: 0.010720402002334595\n",
      "Epoch: 40, Samples: 320/5760, Loss: 0.009675353765487671\n",
      "Epoch: 40, Samples: 352/5760, Loss: 0.010875672101974487\n",
      "Epoch: 40, Samples: 384/5760, Loss: 0.010982900857925415\n",
      "Epoch: 40, Samples: 416/5760, Loss: 0.00785335898399353\n",
      "Epoch: 40, Samples: 448/5760, Loss: 0.008796721696853638\n",
      "Epoch: 40, Samples: 480/5760, Loss: 0.016463041305541992\n",
      "Epoch: 40, Samples: 512/5760, Loss: 0.02852940559387207\n",
      "Epoch: 40, Samples: 544/5760, Loss: 0.004085361957550049\n",
      "Epoch: 40, Samples: 576/5760, Loss: 0.01652267575263977\n",
      "Epoch: 40, Samples: 608/5760, Loss: 0.0082167387008667\n",
      "Epoch: 40, Samples: 640/5760, Loss: 0.010335355997085571\n",
      "Epoch: 40, Samples: 672/5760, Loss: 0.016757279634475708\n",
      "Epoch: 40, Samples: 704/5760, Loss: 0.009277015924453735\n",
      "Epoch: 40, Samples: 736/5760, Loss: 0.008423388004302979\n",
      "Epoch: 40, Samples: 768/5760, Loss: 0.0066710710525512695\n",
      "Epoch: 40, Samples: 800/5760, Loss: 0.014375656843185425\n",
      "Epoch: 40, Samples: 832/5760, Loss: 0.01622997224330902\n",
      "Epoch: 40, Samples: 864/5760, Loss: 0.01313692331314087\n",
      "Epoch: 40, Samples: 896/5760, Loss: 0.03341619670391083\n",
      "Epoch: 40, Samples: 928/5760, Loss: 0.009512573480606079\n",
      "Epoch: 40, Samples: 960/5760, Loss: 0.006968110799789429\n",
      "Epoch: 40, Samples: 992/5760, Loss: 0.015337169170379639\n",
      "Epoch: 40, Samples: 1024/5760, Loss: 0.02488294243812561\n",
      "Epoch: 40, Samples: 1056/5760, Loss: 0.011542677879333496\n",
      "Epoch: 40, Samples: 1088/5760, Loss: 0.014592528343200684\n",
      "Epoch: 40, Samples: 1120/5760, Loss: 0.013822436332702637\n",
      "Epoch: 40, Samples: 1152/5760, Loss: 0.011718779802322388\n",
      "Epoch: 40, Samples: 1184/5760, Loss: 0.00967949628829956\n",
      "Epoch: 40, Samples: 1216/5760, Loss: 0.012097001075744629\n",
      "Epoch: 40, Samples: 1248/5760, Loss: 0.015970289707183838\n",
      "Epoch: 40, Samples: 1280/5760, Loss: 0.011485546827316284\n",
      "Epoch: 40, Samples: 1312/5760, Loss: 0.008418470621109009\n",
      "Epoch: 40, Samples: 1344/5760, Loss: 0.012712836265563965\n",
      "Epoch: 40, Samples: 1376/5760, Loss: 0.016597092151641846\n",
      "Epoch: 40, Samples: 1408/5760, Loss: 0.013032406568527222\n",
      "Epoch: 40, Samples: 1440/5760, Loss: 0.00929713249206543\n",
      "Epoch: 40, Samples: 1472/5760, Loss: 0.013691633939743042\n",
      "Epoch: 40, Samples: 1504/5760, Loss: 0.014619797468185425\n",
      "Epoch: 40, Samples: 1536/5760, Loss: 0.011462628841400146\n",
      "Epoch: 40, Samples: 1568/5760, Loss: 0.011107444763183594\n",
      "Epoch: 40, Samples: 1600/5760, Loss: 0.008942514657974243\n",
      "Epoch: 40, Samples: 1632/5760, Loss: 0.006040990352630615\n",
      "Epoch: 40, Samples: 1664/5760, Loss: 0.011478245258331299\n",
      "Epoch: 40, Samples: 1696/5760, Loss: 0.009398043155670166\n",
      "Epoch: 40, Samples: 1728/5760, Loss: 0.006174296140670776\n",
      "Epoch: 40, Samples: 1760/5760, Loss: 0.01101568341255188\n",
      "Epoch: 40, Samples: 1792/5760, Loss: 0.009716004133224487\n",
      "Epoch: 40, Samples: 1824/5760, Loss: 0.016550421714782715\n",
      "Epoch: 40, Samples: 1856/5760, Loss: 0.012509942054748535\n",
      "Epoch: 40, Samples: 1888/5760, Loss: 0.00959402322769165\n",
      "Epoch: 40, Samples: 1920/5760, Loss: 0.0116654634475708\n",
      "Epoch: 40, Samples: 1952/5760, Loss: 0.009555190801620483\n",
      "Epoch: 40, Samples: 1984/5760, Loss: 0.01334264874458313\n",
      "Epoch: 40, Samples: 2016/5760, Loss: 0.015604466199874878\n",
      "Epoch: 40, Samples: 2048/5760, Loss: 0.010086268186569214\n",
      "Epoch: 40, Samples: 2080/5760, Loss: 0.01724645495414734\n",
      "Epoch: 40, Samples: 2112/5760, Loss: 0.013398736715316772\n",
      "Epoch: 40, Samples: 2144/5760, Loss: 0.01641753315925598\n",
      "Epoch: 40, Samples: 2176/5760, Loss: 0.013535022735595703\n",
      "Epoch: 40, Samples: 2208/5760, Loss: 0.018274307250976562\n",
      "Epoch: 40, Samples: 2240/5760, Loss: 0.008292824029922485\n",
      "Epoch: 40, Samples: 2272/5760, Loss: 0.007187783718109131\n",
      "Epoch: 40, Samples: 2304/5760, Loss: 0.019259870052337646\n",
      "Epoch: 40, Samples: 2336/5760, Loss: 0.011531800031661987\n",
      "Epoch: 40, Samples: 2368/5760, Loss: 0.013651043176651001\n",
      "Epoch: 40, Samples: 2400/5760, Loss: 0.010571569204330444\n",
      "Epoch: 40, Samples: 2432/5760, Loss: 0.010154157876968384\n",
      "Epoch: 40, Samples: 2464/5760, Loss: 0.01703202724456787\n",
      "Epoch: 40, Samples: 2496/5760, Loss: 0.01284801959991455\n",
      "Epoch: 40, Samples: 2528/5760, Loss: 0.008189111948013306\n",
      "Epoch: 40, Samples: 2560/5760, Loss: 0.013821035623550415\n",
      "Epoch: 40, Samples: 2592/5760, Loss: 0.0096721351146698\n",
      "Epoch: 40, Samples: 2624/5760, Loss: 0.015789061784744263\n",
      "Epoch: 40, Samples: 2656/5760, Loss: 0.009596854448318481\n",
      "Epoch: 40, Samples: 2688/5760, Loss: 0.007282167673110962\n",
      "Epoch: 40, Samples: 2720/5760, Loss: 0.008156895637512207\n",
      "Epoch: 40, Samples: 2752/5760, Loss: 0.007712751626968384\n",
      "Epoch: 40, Samples: 2784/5760, Loss: 0.00790587067604065\n",
      "Epoch: 40, Samples: 2816/5760, Loss: 0.010438472032546997\n",
      "Epoch: 40, Samples: 2848/5760, Loss: 0.010774314403533936\n",
      "Epoch: 40, Samples: 2880/5760, Loss: 0.010716438293457031\n",
      "Epoch: 40, Samples: 2912/5760, Loss: 0.01426553726196289\n",
      "Epoch: 40, Samples: 2944/5760, Loss: 0.012228250503540039\n",
      "Epoch: 40, Samples: 2976/5760, Loss: 0.015110790729522705\n",
      "Epoch: 40, Samples: 3008/5760, Loss: 0.014256864786148071\n",
      "Epoch: 40, Samples: 3040/5760, Loss: 0.004707634449005127\n",
      "Epoch: 40, Samples: 3072/5760, Loss: 0.01146194338798523\n",
      "Epoch: 40, Samples: 3104/5760, Loss: 0.016219139099121094\n",
      "Epoch: 40, Samples: 3136/5760, Loss: 0.015983492136001587\n",
      "Epoch: 40, Samples: 3168/5760, Loss: 0.006525486707687378\n",
      "Epoch: 40, Samples: 3200/5760, Loss: 0.009485810995101929\n",
      "Epoch: 40, Samples: 3232/5760, Loss: 0.011365026235580444\n",
      "Epoch: 40, Samples: 3264/5760, Loss: 0.005915135145187378\n",
      "Epoch: 40, Samples: 3296/5760, Loss: 0.008428245782852173\n",
      "Epoch: 40, Samples: 3328/5760, Loss: 0.015596598386764526\n",
      "Epoch: 40, Samples: 3360/5760, Loss: 0.040230900049209595\n",
      "Epoch: 40, Samples: 3392/5760, Loss: 0.008283108472824097\n",
      "Epoch: 40, Samples: 3424/5760, Loss: 0.006675392389297485\n",
      "Epoch: 40, Samples: 3456/5760, Loss: 0.00993916392326355\n",
      "Epoch: 40, Samples: 3488/5760, Loss: 0.00855553150177002\n",
      "Epoch: 40, Samples: 3520/5760, Loss: 0.010116368532180786\n",
      "Epoch: 40, Samples: 3552/5760, Loss: 0.013853281736373901\n",
      "Epoch: 40, Samples: 3584/5760, Loss: 0.010823547840118408\n",
      "Epoch: 40, Samples: 3616/5760, Loss: 0.009274929761886597\n",
      "Epoch: 40, Samples: 3648/5760, Loss: 0.007050424814224243\n",
      "Epoch: 40, Samples: 3680/5760, Loss: 0.012236177921295166\n",
      "Epoch: 40, Samples: 3712/5760, Loss: 0.01130586862564087\n",
      "Epoch: 40, Samples: 3744/5760, Loss: 0.008127748966217041\n",
      "Epoch: 40, Samples: 3776/5760, Loss: 0.020288735628128052\n",
      "Epoch: 40, Samples: 3808/5760, Loss: 0.011425495147705078\n",
      "Epoch: 40, Samples: 3840/5760, Loss: 0.012394100427627563\n",
      "Epoch: 40, Samples: 3872/5760, Loss: 0.005261361598968506\n",
      "Epoch: 40, Samples: 3904/5760, Loss: 0.006216555833816528\n",
      "Epoch: 40, Samples: 3936/5760, Loss: 0.015149533748626709\n",
      "Epoch: 40, Samples: 3968/5760, Loss: 0.008770912885665894\n",
      "Epoch: 40, Samples: 4000/5760, Loss: 0.01125529408454895\n",
      "Epoch: 40, Samples: 4032/5760, Loss: 0.00797206163406372\n",
      "Epoch: 40, Samples: 4064/5760, Loss: 0.011191904544830322\n",
      "Epoch: 40, Samples: 4096/5760, Loss: 0.023845583200454712\n",
      "Epoch: 40, Samples: 4128/5760, Loss: 0.010902255773544312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Samples: 4160/5760, Loss: 0.009255677461624146\n",
      "Epoch: 40, Samples: 4192/5760, Loss: 0.008927613496780396\n",
      "Epoch: 40, Samples: 4224/5760, Loss: 0.010351479053497314\n",
      "Epoch: 40, Samples: 4256/5760, Loss: 0.011178761720657349\n",
      "Epoch: 40, Samples: 4288/5760, Loss: 0.013051420450210571\n",
      "Epoch: 40, Samples: 4320/5760, Loss: 0.006807655096054077\n",
      "Epoch: 40, Samples: 4352/5760, Loss: 0.009043216705322266\n",
      "Epoch: 40, Samples: 4384/5760, Loss: 0.01620011031627655\n",
      "Epoch: 40, Samples: 4416/5760, Loss: 0.008702099323272705\n",
      "Epoch: 40, Samples: 4448/5760, Loss: 0.004730015993118286\n",
      "Epoch: 40, Samples: 4480/5760, Loss: 0.01546981930732727\n",
      "Epoch: 40, Samples: 4512/5760, Loss: 0.012266397476196289\n",
      "Epoch: 40, Samples: 4544/5760, Loss: 0.0076906681060791016\n",
      "Epoch: 40, Samples: 4576/5760, Loss: 0.012430757284164429\n",
      "Epoch: 40, Samples: 4608/5760, Loss: 0.007741719484329224\n",
      "Epoch: 40, Samples: 4640/5760, Loss: 0.015294492244720459\n",
      "Epoch: 40, Samples: 4672/5760, Loss: 0.007891088724136353\n",
      "Epoch: 40, Samples: 4704/5760, Loss: 0.004992961883544922\n",
      "Epoch: 40, Samples: 4736/5760, Loss: 0.010839402675628662\n",
      "Epoch: 40, Samples: 4768/5760, Loss: 0.01551125943660736\n",
      "Epoch: 40, Samples: 4800/5760, Loss: 0.015548616647720337\n",
      "Epoch: 40, Samples: 4832/5760, Loss: 0.00836104154586792\n",
      "Epoch: 40, Samples: 4864/5760, Loss: 0.007554590702056885\n",
      "Epoch: 40, Samples: 4896/5760, Loss: 0.011608093976974487\n",
      "Epoch: 40, Samples: 4928/5760, Loss: 0.013514041900634766\n",
      "Epoch: 40, Samples: 4960/5760, Loss: 0.019332438707351685\n",
      "Epoch: 40, Samples: 4992/5760, Loss: 0.018825262784957886\n",
      "Epoch: 40, Samples: 5024/5760, Loss: 0.01678982377052307\n",
      "Epoch: 40, Samples: 5056/5760, Loss: 0.006911605596542358\n",
      "Epoch: 40, Samples: 5088/5760, Loss: 0.01406775414943695\n",
      "Epoch: 40, Samples: 5120/5760, Loss: 0.009083449840545654\n",
      "Epoch: 40, Samples: 5152/5760, Loss: 0.017070800065994263\n",
      "Epoch: 40, Samples: 5184/5760, Loss: 0.006454020738601685\n",
      "Epoch: 40, Samples: 5216/5760, Loss: 0.01922696828842163\n",
      "Epoch: 40, Samples: 5248/5760, Loss: 0.009506970643997192\n",
      "Epoch: 40, Samples: 5280/5760, Loss: 0.008061468601226807\n",
      "Epoch: 40, Samples: 5312/5760, Loss: 0.01644366979598999\n",
      "Epoch: 40, Samples: 5344/5760, Loss: 0.009409904479980469\n",
      "Epoch: 40, Samples: 5376/5760, Loss: 0.009503036737442017\n",
      "Epoch: 40, Samples: 5408/5760, Loss: 0.009517401456832886\n",
      "Epoch: 40, Samples: 5440/5760, Loss: 0.006259739398956299\n",
      "Epoch: 40, Samples: 5472/5760, Loss: 0.012695938348770142\n",
      "Epoch: 40, Samples: 5504/5760, Loss: 0.011084675788879395\n",
      "Epoch: 40, Samples: 5536/5760, Loss: 0.013165771961212158\n",
      "Epoch: 40, Samples: 5568/5760, Loss: 0.017798423767089844\n",
      "Epoch: 40, Samples: 5600/5760, Loss: 0.0059622228145599365\n",
      "Epoch: 40, Samples: 5632/5760, Loss: 0.014217734336853027\n",
      "Epoch: 40, Samples: 5664/5760, Loss: 0.02100604772567749\n",
      "Epoch: 40, Samples: 5696/5760, Loss: 0.006317734718322754\n",
      "Epoch: 40, Samples: 5728/5760, Loss: 0.7039144039154053\n",
      "\n",
      "Epoch: 40\n",
      "Training set: Average loss: 0.0159\n",
      "Validation set: Average loss: 0.3369, Accuracy: 750/818 (92%)\n",
      "Epoch: 41, Samples: 0/5760, Loss: 0.006668061017990112\n",
      "Epoch: 41, Samples: 32/5760, Loss: 0.007989585399627686\n",
      "Epoch: 41, Samples: 64/5760, Loss: 0.006530731916427612\n",
      "Epoch: 41, Samples: 96/5760, Loss: 0.005859047174453735\n",
      "Epoch: 41, Samples: 128/5760, Loss: 0.009101539850234985\n",
      "Epoch: 41, Samples: 160/5760, Loss: 0.03607484698295593\n",
      "Epoch: 41, Samples: 192/5760, Loss: 0.0065964460372924805\n",
      "Epoch: 41, Samples: 224/5760, Loss: 0.012028098106384277\n",
      "Epoch: 41, Samples: 256/5760, Loss: 0.00872918963432312\n",
      "Epoch: 41, Samples: 288/5760, Loss: 0.02216830849647522\n",
      "Epoch: 41, Samples: 320/5760, Loss: 0.015642255544662476\n",
      "Epoch: 41, Samples: 352/5760, Loss: 0.019854635000228882\n",
      "Epoch: 41, Samples: 384/5760, Loss: 0.01062127947807312\n",
      "Epoch: 41, Samples: 416/5760, Loss: 0.018551796674728394\n",
      "Epoch: 41, Samples: 448/5760, Loss: 0.011378675699234009\n",
      "Epoch: 41, Samples: 480/5760, Loss: 0.01046741008758545\n",
      "Epoch: 41, Samples: 512/5760, Loss: 0.012960225343704224\n",
      "Epoch: 41, Samples: 544/5760, Loss: 0.019890382885932922\n",
      "Epoch: 41, Samples: 576/5760, Loss: 0.011140435934066772\n",
      "Epoch: 41, Samples: 608/5760, Loss: 0.00880056619644165\n",
      "Epoch: 41, Samples: 640/5760, Loss: 0.012177497148513794\n",
      "Epoch: 41, Samples: 672/5760, Loss: 0.01991894841194153\n",
      "Epoch: 41, Samples: 704/5760, Loss: 0.008497923612594604\n",
      "Epoch: 41, Samples: 736/5760, Loss: 0.008353054523468018\n",
      "Epoch: 41, Samples: 768/5760, Loss: 0.00923183560371399\n",
      "Epoch: 41, Samples: 800/5760, Loss: 0.010777294635772705\n",
      "Epoch: 41, Samples: 832/5760, Loss: 0.012256890535354614\n",
      "Epoch: 41, Samples: 864/5760, Loss: 0.01312151551246643\n",
      "Epoch: 41, Samples: 896/5760, Loss: 0.007622390985488892\n",
      "Epoch: 41, Samples: 928/5760, Loss: 0.013598710298538208\n",
      "Epoch: 41, Samples: 960/5760, Loss: 0.007040858268737793\n",
      "Epoch: 41, Samples: 992/5760, Loss: 0.01063045859336853\n",
      "Epoch: 41, Samples: 1024/5760, Loss: 0.010879993438720703\n",
      "Epoch: 41, Samples: 1056/5760, Loss: 0.010385245084762573\n",
      "Epoch: 41, Samples: 1088/5760, Loss: 0.008572012186050415\n",
      "Epoch: 41, Samples: 1120/5760, Loss: 0.006899356842041016\n",
      "Epoch: 41, Samples: 1152/5760, Loss: 0.014519542455673218\n",
      "Epoch: 41, Samples: 1184/5760, Loss: 0.010884463787078857\n",
      "Epoch: 41, Samples: 1216/5760, Loss: 0.014220431447029114\n",
      "Epoch: 41, Samples: 1248/5760, Loss: 0.012226134538650513\n",
      "Epoch: 41, Samples: 1280/5760, Loss: 0.009450912475585938\n",
      "Epoch: 41, Samples: 1312/5760, Loss: 0.010217815637588501\n",
      "Epoch: 41, Samples: 1344/5760, Loss: 0.007761478424072266\n",
      "Epoch: 41, Samples: 1376/5760, Loss: 0.007982462644577026\n",
      "Epoch: 41, Samples: 1408/5760, Loss: 0.011128485202789307\n",
      "Epoch: 41, Samples: 1440/5760, Loss: 0.005888849496841431\n",
      "Epoch: 41, Samples: 1472/5760, Loss: 0.013921976089477539\n",
      "Epoch: 41, Samples: 1504/5760, Loss: 0.010702967643737793\n",
      "Epoch: 41, Samples: 1536/5760, Loss: 0.009114116430282593\n",
      "Epoch: 41, Samples: 1568/5760, Loss: 0.010906785726547241\n",
      "Epoch: 41, Samples: 1600/5760, Loss: 0.012732714414596558\n",
      "Epoch: 41, Samples: 1632/5760, Loss: 0.008929550647735596\n",
      "Epoch: 41, Samples: 1664/5760, Loss: 0.01581639051437378\n",
      "Epoch: 41, Samples: 1696/5760, Loss: 0.008348315954208374\n",
      "Epoch: 41, Samples: 1728/5760, Loss: 0.011487007141113281\n",
      "Epoch: 41, Samples: 1760/5760, Loss: 0.017322391271591187\n",
      "Epoch: 41, Samples: 1792/5760, Loss: 0.01582154631614685\n",
      "Epoch: 41, Samples: 1824/5760, Loss: 0.016647547483444214\n",
      "Epoch: 41, Samples: 1856/5760, Loss: 0.007506847381591797\n",
      "Epoch: 41, Samples: 1888/5760, Loss: 0.015291988849639893\n",
      "Epoch: 41, Samples: 1920/5760, Loss: 0.019479304552078247\n",
      "Epoch: 41, Samples: 1952/5760, Loss: 0.019271880388259888\n",
      "Epoch: 41, Samples: 1984/5760, Loss: 0.006692737340927124\n",
      "Epoch: 41, Samples: 2016/5760, Loss: 0.014908909797668457\n",
      "Epoch: 41, Samples: 2048/5760, Loss: 0.016174495220184326\n",
      "Epoch: 41, Samples: 2080/5760, Loss: 0.007300466299057007\n",
      "Epoch: 41, Samples: 2112/5760, Loss: 0.009286046028137207\n",
      "Epoch: 41, Samples: 2144/5760, Loss: 0.013799339532852173\n",
      "Epoch: 41, Samples: 2176/5760, Loss: 0.012364894151687622\n",
      "Epoch: 41, Samples: 2208/5760, Loss: 0.009998947381973267\n",
      "Epoch: 41, Samples: 2240/5760, Loss: 0.009656637907028198\n",
      "Epoch: 41, Samples: 2272/5760, Loss: 0.008701682090759277\n",
      "Epoch: 41, Samples: 2304/5760, Loss: 0.00773957371711731\n",
      "Epoch: 41, Samples: 2336/5760, Loss: 0.012141823768615723\n",
      "Epoch: 41, Samples: 2368/5760, Loss: 0.012878507375717163\n",
      "Epoch: 41, Samples: 2400/5760, Loss: 0.011154502630233765\n",
      "Epoch: 41, Samples: 2432/5760, Loss: 0.017576992511749268\n",
      "Epoch: 41, Samples: 2464/5760, Loss: 0.011443495750427246\n",
      "Epoch: 41, Samples: 2496/5760, Loss: 0.019858092069625854\n",
      "Epoch: 41, Samples: 2528/5760, Loss: 0.011581122875213623\n",
      "Epoch: 41, Samples: 2560/5760, Loss: 0.007110297679901123\n",
      "Epoch: 41, Samples: 2592/5760, Loss: 0.01043081283569336\n",
      "Epoch: 41, Samples: 2624/5760, Loss: 0.00869220495223999\n",
      "Epoch: 41, Samples: 2656/5760, Loss: 0.013782739639282227\n",
      "Epoch: 41, Samples: 2688/5760, Loss: 0.011727511882781982\n",
      "Epoch: 41, Samples: 2720/5760, Loss: 0.007561773061752319\n",
      "Epoch: 41, Samples: 2752/5760, Loss: 0.0069380998611450195\n",
      "Epoch: 41, Samples: 2784/5760, Loss: 0.018857479095458984\n",
      "Epoch: 41, Samples: 2816/5760, Loss: 0.009035974740982056\n",
      "Epoch: 41, Samples: 2848/5760, Loss: 0.0086250901222229\n",
      "Epoch: 41, Samples: 2880/5760, Loss: 0.006163835525512695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Samples: 2912/5760, Loss: 0.009028196334838867\n",
      "Epoch: 41, Samples: 2944/5760, Loss: 0.008766263723373413\n",
      "Epoch: 41, Samples: 2976/5760, Loss: 0.006917744874954224\n",
      "Epoch: 41, Samples: 3008/5760, Loss: 0.015354841947555542\n",
      "Epoch: 41, Samples: 3040/5760, Loss: 0.012199044227600098\n",
      "Epoch: 41, Samples: 3072/5760, Loss: 0.009527981281280518\n",
      "Epoch: 41, Samples: 3104/5760, Loss: 0.012903839349746704\n",
      "Epoch: 41, Samples: 3136/5760, Loss: 0.0156155526638031\n",
      "Epoch: 41, Samples: 3168/5760, Loss: 0.008312791585922241\n",
      "Epoch: 41, Samples: 3200/5760, Loss: 0.012354493141174316\n",
      "Epoch: 41, Samples: 3232/5760, Loss: 0.012515455484390259\n",
      "Epoch: 41, Samples: 3264/5760, Loss: 0.009495139122009277\n",
      "Epoch: 41, Samples: 3296/5760, Loss: 0.005181372165679932\n",
      "Epoch: 41, Samples: 3328/5760, Loss: 0.009702086448669434\n",
      "Epoch: 41, Samples: 3360/5760, Loss: 0.0064668357372283936\n",
      "Epoch: 41, Samples: 3392/5760, Loss: 0.011009842157363892\n",
      "Epoch: 41, Samples: 3424/5760, Loss: 0.010302722454071045\n",
      "Epoch: 41, Samples: 3456/5760, Loss: 0.012917250394821167\n",
      "Epoch: 41, Samples: 3488/5760, Loss: 0.007613182067871094\n",
      "Epoch: 41, Samples: 3520/5760, Loss: 0.006569862365722656\n",
      "Epoch: 41, Samples: 3552/5760, Loss: 0.01207035779953003\n",
      "Epoch: 41, Samples: 3584/5760, Loss: 0.008613944053649902\n",
      "Epoch: 41, Samples: 3616/5760, Loss: 0.007777303457260132\n",
      "Epoch: 41, Samples: 3648/5760, Loss: 0.011482059955596924\n",
      "Epoch: 41, Samples: 3680/5760, Loss: 0.012196838855743408\n",
      "Epoch: 41, Samples: 3712/5760, Loss: 0.016537964344024658\n",
      "Epoch: 41, Samples: 3744/5760, Loss: 0.018458575010299683\n",
      "Epoch: 41, Samples: 3776/5760, Loss: 0.007965326309204102\n",
      "Epoch: 41, Samples: 3808/5760, Loss: 0.012141019105911255\n",
      "Epoch: 41, Samples: 3840/5760, Loss: 0.012584447860717773\n",
      "Epoch: 41, Samples: 3872/5760, Loss: 0.0066716670989990234\n",
      "Epoch: 41, Samples: 3904/5760, Loss: 0.009078174829483032\n",
      "Epoch: 41, Samples: 3936/5760, Loss: 0.008929193019866943\n",
      "Epoch: 41, Samples: 3968/5760, Loss: 0.008798390626907349\n",
      "Epoch: 41, Samples: 4000/5760, Loss: 0.01339489221572876\n",
      "Epoch: 41, Samples: 4032/5760, Loss: 0.011483699083328247\n",
      "Epoch: 41, Samples: 4064/5760, Loss: 0.005840927362442017\n",
      "Epoch: 41, Samples: 4096/5760, Loss: 0.0063110291957855225\n",
      "Epoch: 41, Samples: 4128/5760, Loss: 0.008296102285385132\n",
      "Epoch: 41, Samples: 4160/5760, Loss: 0.013558536767959595\n",
      "Epoch: 41, Samples: 4192/5760, Loss: 0.011138170957565308\n",
      "Epoch: 41, Samples: 4224/5760, Loss: 0.028804585337638855\n",
      "Epoch: 41, Samples: 4256/5760, Loss: 0.007532775402069092\n",
      "Epoch: 41, Samples: 4288/5760, Loss: 0.007968991994857788\n",
      "Epoch: 41, Samples: 4320/5760, Loss: 0.01023942232131958\n",
      "Epoch: 41, Samples: 4352/5760, Loss: 0.012635976076126099\n",
      "Epoch: 41, Samples: 4384/5760, Loss: 0.005945771932601929\n",
      "Epoch: 41, Samples: 4416/5760, Loss: 0.017201662063598633\n",
      "Epoch: 41, Samples: 4448/5760, Loss: 0.016336411237716675\n",
      "Epoch: 41, Samples: 4480/5760, Loss: 0.009137570858001709\n",
      "Epoch: 41, Samples: 4512/5760, Loss: 0.015320509672164917\n",
      "Epoch: 41, Samples: 4544/5760, Loss: 0.013883143663406372\n",
      "Epoch: 41, Samples: 4576/5760, Loss: 0.014352023601531982\n",
      "Epoch: 41, Samples: 4608/5760, Loss: 0.012621253728866577\n",
      "Epoch: 41, Samples: 4640/5760, Loss: 0.006187111139297485\n",
      "Epoch: 41, Samples: 4672/5760, Loss: 0.011306047439575195\n",
      "Epoch: 41, Samples: 4704/5760, Loss: 0.009051710367202759\n",
      "Epoch: 41, Samples: 4736/5760, Loss: 0.014929443597793579\n",
      "Epoch: 41, Samples: 4768/5760, Loss: 0.012129515409469604\n",
      "Epoch: 41, Samples: 4800/5760, Loss: 0.015293389558792114\n",
      "Epoch: 41, Samples: 4832/5760, Loss: 0.007295519113540649\n",
      "Epoch: 41, Samples: 4864/5760, Loss: 0.008888185024261475\n",
      "Epoch: 41, Samples: 4896/5760, Loss: 0.005971789360046387\n",
      "Epoch: 41, Samples: 4928/5760, Loss: 0.008694827556610107\n",
      "Epoch: 41, Samples: 4960/5760, Loss: 0.007691025733947754\n",
      "Epoch: 41, Samples: 4992/5760, Loss: 0.008230984210968018\n",
      "Epoch: 41, Samples: 5024/5760, Loss: 0.01617145538330078\n",
      "Epoch: 41, Samples: 5056/5760, Loss: 0.010193347930908203\n",
      "Epoch: 41, Samples: 5088/5760, Loss: 0.006010890007019043\n",
      "Epoch: 41, Samples: 5120/5760, Loss: 0.018807053565979004\n",
      "Epoch: 41, Samples: 5152/5760, Loss: 0.007959485054016113\n",
      "Epoch: 41, Samples: 5184/5760, Loss: 0.014458835124969482\n",
      "Epoch: 41, Samples: 5216/5760, Loss: 0.013566374778747559\n",
      "Epoch: 41, Samples: 5248/5760, Loss: 0.015483260154724121\n",
      "Epoch: 41, Samples: 5280/5760, Loss: 0.012053608894348145\n",
      "Epoch: 41, Samples: 5312/5760, Loss: 0.008717715740203857\n",
      "Epoch: 41, Samples: 5344/5760, Loss: 0.007030099630355835\n",
      "Epoch: 41, Samples: 5376/5760, Loss: 0.01713430881500244\n",
      "Epoch: 41, Samples: 5408/5760, Loss: 0.009686172008514404\n",
      "Epoch: 41, Samples: 5440/5760, Loss: 0.010137230157852173\n",
      "Epoch: 41, Samples: 5472/5760, Loss: 0.008593380451202393\n",
      "Epoch: 41, Samples: 5504/5760, Loss: 0.007331669330596924\n",
      "Epoch: 41, Samples: 5536/5760, Loss: 0.010046154260635376\n",
      "Epoch: 41, Samples: 5568/5760, Loss: 0.01367679238319397\n",
      "Epoch: 41, Samples: 5600/5760, Loss: 0.008066684007644653\n",
      "Epoch: 41, Samples: 5632/5760, Loss: 0.008801162242889404\n",
      "Epoch: 41, Samples: 5664/5760, Loss: 0.012120962142944336\n",
      "Epoch: 41, Samples: 5696/5760, Loss: 0.010544449090957642\n",
      "Epoch: 41, Samples: 5728/5760, Loss: 0.9339903593063354\n",
      "\n",
      "Epoch: 41\n",
      "Training set: Average loss: 0.0165\n",
      "Validation set: Average loss: 0.3455, Accuracy: 748/818 (91%)\n",
      "Epoch: 42, Samples: 0/5760, Loss: 0.011174052953720093\n",
      "Epoch: 42, Samples: 32/5760, Loss: 0.015115499496459961\n",
      "Epoch: 42, Samples: 64/5760, Loss: 0.011713653802871704\n",
      "Epoch: 42, Samples: 96/5760, Loss: 0.015150368213653564\n",
      "Epoch: 42, Samples: 128/5760, Loss: 0.004344642162322998\n",
      "Epoch: 42, Samples: 160/5760, Loss: 0.01500287652015686\n",
      "Epoch: 42, Samples: 192/5760, Loss: 0.009204953908920288\n",
      "Epoch: 42, Samples: 224/5760, Loss: 0.009728699922561646\n",
      "Epoch: 42, Samples: 256/5760, Loss: 0.010522127151489258\n",
      "Epoch: 42, Samples: 288/5760, Loss: 0.01124909520149231\n",
      "Epoch: 42, Samples: 320/5760, Loss: 0.007201105356216431\n",
      "Epoch: 42, Samples: 352/5760, Loss: 0.007922649383544922\n",
      "Epoch: 42, Samples: 384/5760, Loss: 0.00723537802696228\n",
      "Epoch: 42, Samples: 416/5760, Loss: 0.011587291955947876\n",
      "Epoch: 42, Samples: 448/5760, Loss: 0.006535887718200684\n",
      "Epoch: 42, Samples: 480/5760, Loss: 0.010322332382202148\n",
      "Epoch: 42, Samples: 512/5760, Loss: 0.014239132404327393\n",
      "Epoch: 42, Samples: 544/5760, Loss: 0.009907305240631104\n",
      "Epoch: 42, Samples: 576/5760, Loss: 0.011788606643676758\n",
      "Epoch: 42, Samples: 608/5760, Loss: 0.00671878457069397\n",
      "Epoch: 42, Samples: 640/5760, Loss: 0.006047964096069336\n",
      "Epoch: 42, Samples: 672/5760, Loss: 0.015074402093887329\n",
      "Epoch: 42, Samples: 704/5760, Loss: 0.009941816329956055\n",
      "Epoch: 42, Samples: 736/5760, Loss: 0.0075206458568573\n",
      "Epoch: 42, Samples: 768/5760, Loss: 0.008885711431503296\n",
      "Epoch: 42, Samples: 800/5760, Loss: 0.017828375101089478\n",
      "Epoch: 42, Samples: 832/5760, Loss: 0.008133351802825928\n",
      "Epoch: 42, Samples: 864/5760, Loss: 0.014512509107589722\n",
      "Epoch: 42, Samples: 896/5760, Loss: 0.009254902601242065\n",
      "Epoch: 42, Samples: 928/5760, Loss: 0.04119732975959778\n",
      "Epoch: 42, Samples: 960/5760, Loss: 0.008759558200836182\n",
      "Epoch: 42, Samples: 992/5760, Loss: 0.01763024926185608\n",
      "Epoch: 42, Samples: 1024/5760, Loss: 0.008624613285064697\n",
      "Epoch: 42, Samples: 1056/5760, Loss: 0.037664204835891724\n",
      "Epoch: 42, Samples: 1088/5760, Loss: 0.00987088680267334\n",
      "Epoch: 42, Samples: 1120/5760, Loss: 0.012709230184555054\n",
      "Epoch: 42, Samples: 1152/5760, Loss: 0.012370944023132324\n",
      "Epoch: 42, Samples: 1184/5760, Loss: 0.01219642162322998\n",
      "Epoch: 42, Samples: 1216/5760, Loss: 0.009616643190383911\n",
      "Epoch: 42, Samples: 1248/5760, Loss: 0.007920920848846436\n",
      "Epoch: 42, Samples: 1280/5760, Loss: 0.008862733840942383\n",
      "Epoch: 42, Samples: 1312/5760, Loss: 0.013471364974975586\n",
      "Epoch: 42, Samples: 1344/5760, Loss: 0.008282274007797241\n",
      "Epoch: 42, Samples: 1376/5760, Loss: 0.006035029888153076\n",
      "Epoch: 42, Samples: 1408/5760, Loss: 0.0075355470180511475\n",
      "Epoch: 42, Samples: 1440/5760, Loss: 0.012479245662689209\n",
      "Epoch: 42, Samples: 1472/5760, Loss: 0.0067303478717803955\n",
      "Epoch: 42, Samples: 1504/5760, Loss: 0.005297034978866577\n",
      "Epoch: 42, Samples: 1536/5760, Loss: 0.008726298809051514\n",
      "Epoch: 42, Samples: 1568/5760, Loss: 0.015268325805664062\n",
      "Epoch: 42, Samples: 1600/5760, Loss: 0.009139299392700195\n",
      "Epoch: 42, Samples: 1632/5760, Loss: 0.011511176824569702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, Samples: 1664/5760, Loss: 0.01131710410118103\n",
      "Epoch: 42, Samples: 1696/5760, Loss: 0.007616698741912842\n",
      "Epoch: 42, Samples: 1728/5760, Loss: 0.011266738176345825\n",
      "Epoch: 42, Samples: 1760/5760, Loss: 0.007150053977966309\n",
      "Epoch: 42, Samples: 1792/5760, Loss: 0.025743544101715088\n",
      "Epoch: 42, Samples: 1824/5760, Loss: 0.00985109806060791\n",
      "Epoch: 42, Samples: 1856/5760, Loss: 0.01652306318283081\n",
      "Epoch: 42, Samples: 1888/5760, Loss: 0.01427432894706726\n",
      "Epoch: 42, Samples: 1920/5760, Loss: 0.006602823734283447\n",
      "Epoch: 42, Samples: 1952/5760, Loss: 0.013857543468475342\n",
      "Epoch: 42, Samples: 1984/5760, Loss: 0.006391257047653198\n",
      "Epoch: 42, Samples: 2016/5760, Loss: 0.011297851800918579\n",
      "Epoch: 42, Samples: 2048/5760, Loss: 0.003294706344604492\n",
      "Epoch: 42, Samples: 2080/5760, Loss: 0.0076455771923065186\n",
      "Epoch: 42, Samples: 2112/5760, Loss: 0.03085547685623169\n",
      "Epoch: 42, Samples: 2144/5760, Loss: 0.008794009685516357\n",
      "Epoch: 42, Samples: 2176/5760, Loss: 0.030874088406562805\n",
      "Epoch: 42, Samples: 2208/5760, Loss: 0.03403832018375397\n",
      "Epoch: 42, Samples: 2240/5760, Loss: 0.006120026111602783\n",
      "Epoch: 42, Samples: 2272/5760, Loss: 0.007033020257949829\n",
      "Epoch: 42, Samples: 2304/5760, Loss: 0.005771934986114502\n",
      "Epoch: 42, Samples: 2336/5760, Loss: 0.01387554407119751\n",
      "Epoch: 42, Samples: 2368/5760, Loss: 0.010935813188552856\n",
      "Epoch: 42, Samples: 2400/5760, Loss: 0.010729372501373291\n",
      "Epoch: 42, Samples: 2432/5760, Loss: 0.019601136445999146\n",
      "Epoch: 42, Samples: 2464/5760, Loss: 0.017346203327178955\n",
      "Epoch: 42, Samples: 2496/5760, Loss: 0.01276502013206482\n",
      "Epoch: 42, Samples: 2528/5760, Loss: 0.01440650224685669\n",
      "Epoch: 42, Samples: 2560/5760, Loss: 0.017284303903579712\n",
      "Epoch: 42, Samples: 2592/5760, Loss: 0.009065806865692139\n",
      "Epoch: 42, Samples: 2624/5760, Loss: 0.009016036987304688\n",
      "Epoch: 42, Samples: 2656/5760, Loss: 0.012600138783454895\n",
      "Epoch: 42, Samples: 2688/5760, Loss: 0.012890547513961792\n",
      "Epoch: 42, Samples: 2720/5760, Loss: 0.012466222047805786\n",
      "Epoch: 42, Samples: 2752/5760, Loss: 0.012641370296478271\n",
      "Epoch: 42, Samples: 2784/5760, Loss: 0.009709417819976807\n",
      "Epoch: 42, Samples: 2816/5760, Loss: 0.00921720266342163\n",
      "Epoch: 42, Samples: 2848/5760, Loss: 0.01046687364578247\n",
      "Epoch: 42, Samples: 2880/5760, Loss: 0.006623655557632446\n",
      "Epoch: 42, Samples: 2912/5760, Loss: 0.014011889696121216\n",
      "Epoch: 42, Samples: 2944/5760, Loss: 0.010295778512954712\n",
      "Epoch: 42, Samples: 2976/5760, Loss: 0.009993553161621094\n",
      "Epoch: 42, Samples: 3008/5760, Loss: 0.007830828428268433\n",
      "Epoch: 42, Samples: 3040/5760, Loss: 0.005431383848190308\n",
      "Epoch: 42, Samples: 3072/5760, Loss: 0.013317734003067017\n",
      "Epoch: 42, Samples: 3104/5760, Loss: 0.014793574810028076\n",
      "Epoch: 42, Samples: 3136/5760, Loss: 0.021288156509399414\n",
      "Epoch: 42, Samples: 3168/5760, Loss: 0.008261620998382568\n",
      "Epoch: 42, Samples: 3200/5760, Loss: 0.007209271192550659\n",
      "Epoch: 42, Samples: 3232/5760, Loss: 0.019486308097839355\n",
      "Epoch: 42, Samples: 3264/5760, Loss: 0.012032866477966309\n",
      "Epoch: 42, Samples: 3296/5760, Loss: 0.011106908321380615\n",
      "Epoch: 42, Samples: 3328/5760, Loss: 0.013588875532150269\n",
      "Epoch: 42, Samples: 3360/5760, Loss: 0.006682366132736206\n",
      "Epoch: 42, Samples: 3392/5760, Loss: 0.014454185962677002\n",
      "Epoch: 42, Samples: 3424/5760, Loss: 0.009027481079101562\n",
      "Epoch: 42, Samples: 3456/5760, Loss: 0.006420642137527466\n",
      "Epoch: 42, Samples: 3488/5760, Loss: 0.010637760162353516\n",
      "Epoch: 42, Samples: 3520/5760, Loss: 0.015926271677017212\n",
      "Epoch: 42, Samples: 3552/5760, Loss: 0.0051236748695373535\n",
      "Epoch: 42, Samples: 3584/5760, Loss: 0.009234100580215454\n",
      "Epoch: 42, Samples: 3616/5760, Loss: 0.009830653667449951\n",
      "Epoch: 42, Samples: 3648/5760, Loss: 0.006511420011520386\n",
      "Epoch: 42, Samples: 3680/5760, Loss: 0.010154813528060913\n",
      "Epoch: 42, Samples: 3712/5760, Loss: 0.010788708925247192\n",
      "Epoch: 42, Samples: 3744/5760, Loss: 0.007481783628463745\n",
      "Epoch: 42, Samples: 3776/5760, Loss: 0.01594465970993042\n",
      "Epoch: 42, Samples: 3808/5760, Loss: 0.008271485567092896\n",
      "Epoch: 42, Samples: 3840/5760, Loss: 0.005908846855163574\n",
      "Epoch: 42, Samples: 3872/5760, Loss: 0.005974411964416504\n",
      "Epoch: 42, Samples: 3904/5760, Loss: 0.020157277584075928\n",
      "Epoch: 42, Samples: 3936/5760, Loss: 0.022850394248962402\n",
      "Epoch: 42, Samples: 3968/5760, Loss: 0.006283670663833618\n",
      "Epoch: 42, Samples: 4000/5760, Loss: 0.011434048414230347\n",
      "Epoch: 42, Samples: 4032/5760, Loss: 0.007898032665252686\n",
      "Epoch: 42, Samples: 4064/5760, Loss: 0.00848272442817688\n",
      "Epoch: 42, Samples: 4096/5760, Loss: 0.013481944799423218\n",
      "Epoch: 42, Samples: 4128/5760, Loss: 0.02413293719291687\n",
      "Epoch: 42, Samples: 4160/5760, Loss: 0.006676733493804932\n",
      "Epoch: 42, Samples: 4192/5760, Loss: 0.012138158082962036\n",
      "Epoch: 42, Samples: 4224/5760, Loss: 0.011507153511047363\n",
      "Epoch: 42, Samples: 4256/5760, Loss: 0.04058977961540222\n",
      "Epoch: 42, Samples: 4288/5760, Loss: 0.016667544841766357\n",
      "Epoch: 42, Samples: 4320/5760, Loss: 0.012254714965820312\n",
      "Epoch: 42, Samples: 4352/5760, Loss: 0.013021886348724365\n",
      "Epoch: 42, Samples: 4384/5760, Loss: 0.006525129079818726\n",
      "Epoch: 42, Samples: 4416/5760, Loss: 0.017893224954605103\n",
      "Epoch: 42, Samples: 4448/5760, Loss: 0.009728848934173584\n",
      "Epoch: 42, Samples: 4480/5760, Loss: 0.009847193956375122\n",
      "Epoch: 42, Samples: 4512/5760, Loss: 0.015675842761993408\n",
      "Epoch: 42, Samples: 4544/5760, Loss: 0.01243048906326294\n",
      "Epoch: 42, Samples: 4576/5760, Loss: 0.014301657676696777\n",
      "Epoch: 42, Samples: 4608/5760, Loss: 0.008775502443313599\n",
      "Epoch: 42, Samples: 4640/5760, Loss: 0.0058016180992126465\n",
      "Epoch: 42, Samples: 4672/5760, Loss: 0.006887972354888916\n",
      "Epoch: 42, Samples: 4704/5760, Loss: 0.008870691061019897\n",
      "Epoch: 42, Samples: 4736/5760, Loss: 0.011308521032333374\n",
      "Epoch: 42, Samples: 4768/5760, Loss: 0.009300768375396729\n",
      "Epoch: 42, Samples: 4800/5760, Loss: 0.012161314487457275\n",
      "Epoch: 42, Samples: 4832/5760, Loss: 0.028999239206314087\n",
      "Epoch: 42, Samples: 4864/5760, Loss: 0.010070860385894775\n",
      "Epoch: 42, Samples: 4896/5760, Loss: 0.00591963529586792\n",
      "Epoch: 42, Samples: 4928/5760, Loss: 0.008092433214187622\n",
      "Epoch: 42, Samples: 4960/5760, Loss: 0.01101529598236084\n",
      "Epoch: 42, Samples: 4992/5760, Loss: 0.009617090225219727\n",
      "Epoch: 42, Samples: 5024/5760, Loss: 0.0259668231010437\n",
      "Epoch: 42, Samples: 5056/5760, Loss: 0.007975667715072632\n",
      "Epoch: 42, Samples: 5088/5760, Loss: 0.009067028760910034\n",
      "Epoch: 42, Samples: 5120/5760, Loss: 0.011505752801895142\n",
      "Epoch: 42, Samples: 5152/5760, Loss: 0.012092679738998413\n",
      "Epoch: 42, Samples: 5184/5760, Loss: 0.007941126823425293\n",
      "Epoch: 42, Samples: 5216/5760, Loss: 0.009356319904327393\n",
      "Epoch: 42, Samples: 5248/5760, Loss: 0.014432907104492188\n",
      "Epoch: 42, Samples: 5280/5760, Loss: 0.019234180450439453\n",
      "Epoch: 42, Samples: 5312/5760, Loss: 0.00814715027809143\n",
      "Epoch: 42, Samples: 5344/5760, Loss: 0.013494998216629028\n",
      "Epoch: 42, Samples: 5376/5760, Loss: 0.009057551622390747\n",
      "Epoch: 42, Samples: 5408/5760, Loss: 0.00824826955795288\n",
      "Epoch: 42, Samples: 5440/5760, Loss: 0.010466396808624268\n",
      "Epoch: 42, Samples: 5472/5760, Loss: 0.008637696504592896\n",
      "Epoch: 42, Samples: 5504/5760, Loss: 0.017968207597732544\n",
      "Epoch: 42, Samples: 5536/5760, Loss: 0.012659251689910889\n",
      "Epoch: 42, Samples: 5568/5760, Loss: 0.009927570819854736\n",
      "Epoch: 42, Samples: 5600/5760, Loss: 0.013725131750106812\n",
      "Epoch: 42, Samples: 5632/5760, Loss: 0.014119923114776611\n",
      "Epoch: 42, Samples: 5664/5760, Loss: 0.007101923227310181\n",
      "Epoch: 42, Samples: 5696/5760, Loss: 0.009574025869369507\n",
      "Epoch: 42, Samples: 5728/5760, Loss: 0.7781583070755005\n",
      "\n",
      "Epoch: 42\n",
      "Training set: Average loss: 0.0162\n",
      "Validation set: Average loss: 0.3372, Accuracy: 748/818 (91%)\n",
      "Epoch: 43, Samples: 0/5760, Loss: 0.006493091583251953\n",
      "Epoch: 43, Samples: 32/5760, Loss: 0.009169071912765503\n",
      "Epoch: 43, Samples: 64/5760, Loss: 0.010112911462783813\n",
      "Epoch: 43, Samples: 96/5760, Loss: 0.025019004940986633\n",
      "Epoch: 43, Samples: 128/5760, Loss: 0.014284133911132812\n",
      "Epoch: 43, Samples: 160/5760, Loss: 0.019420266151428223\n",
      "Epoch: 43, Samples: 192/5760, Loss: 0.012643992900848389\n",
      "Epoch: 43, Samples: 224/5760, Loss: 0.008487462997436523\n",
      "Epoch: 43, Samples: 256/5760, Loss: 0.011023223400115967\n",
      "Epoch: 43, Samples: 288/5760, Loss: 0.014966577291488647\n",
      "Epoch: 43, Samples: 320/5760, Loss: 0.02413061261177063\n",
      "Epoch: 43, Samples: 352/5760, Loss: 0.0071402788162231445\n",
      "Epoch: 43, Samples: 384/5760, Loss: 0.005316376686096191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43, Samples: 416/5760, Loss: 0.010492444038391113\n",
      "Epoch: 43, Samples: 448/5760, Loss: 0.009901165962219238\n",
      "Epoch: 43, Samples: 480/5760, Loss: 0.02124878764152527\n",
      "Epoch: 43, Samples: 512/5760, Loss: 0.01069679856300354\n",
      "Epoch: 43, Samples: 544/5760, Loss: 0.0063259899616241455\n",
      "Epoch: 43, Samples: 576/5760, Loss: 0.024524539709091187\n",
      "Epoch: 43, Samples: 608/5760, Loss: 0.012559175491333008\n",
      "Epoch: 43, Samples: 640/5760, Loss: 0.008269041776657104\n",
      "Epoch: 43, Samples: 672/5760, Loss: 0.01128658652305603\n",
      "Epoch: 43, Samples: 704/5760, Loss: 0.012143939733505249\n",
      "Epoch: 43, Samples: 736/5760, Loss: 0.007981598377227783\n",
      "Epoch: 43, Samples: 768/5760, Loss: 0.021183878183364868\n",
      "Epoch: 43, Samples: 800/5760, Loss: 0.009473264217376709\n",
      "Epoch: 43, Samples: 832/5760, Loss: 0.008440375328063965\n",
      "Epoch: 43, Samples: 864/5760, Loss: 0.020565658807754517\n",
      "Epoch: 43, Samples: 896/5760, Loss: 0.014488041400909424\n",
      "Epoch: 43, Samples: 928/5760, Loss: 0.007971316576004028\n",
      "Epoch: 43, Samples: 960/5760, Loss: 0.019140899181365967\n",
      "Epoch: 43, Samples: 992/5760, Loss: 0.008890897035598755\n",
      "Epoch: 43, Samples: 1024/5760, Loss: 0.008688569068908691\n",
      "Epoch: 43, Samples: 1056/5760, Loss: 0.01094818115234375\n",
      "Epoch: 43, Samples: 1088/5760, Loss: 0.01869228482246399\n",
      "Epoch: 43, Samples: 1120/5760, Loss: 0.00897359848022461\n",
      "Epoch: 43, Samples: 1152/5760, Loss: 0.025995641946792603\n",
      "Epoch: 43, Samples: 1184/5760, Loss: 0.00987255573272705\n",
      "Epoch: 43, Samples: 1216/5760, Loss: 0.012467831373214722\n",
      "Epoch: 43, Samples: 1248/5760, Loss: 0.010669827461242676\n",
      "Epoch: 43, Samples: 1280/5760, Loss: 0.010325729846954346\n",
      "Epoch: 43, Samples: 1312/5760, Loss: 0.010297417640686035\n",
      "Epoch: 43, Samples: 1344/5760, Loss: 0.012864947319030762\n",
      "Epoch: 43, Samples: 1376/5760, Loss: 0.010553240776062012\n",
      "Epoch: 43, Samples: 1408/5760, Loss: 0.012972205877304077\n",
      "Epoch: 43, Samples: 1440/5760, Loss: 0.01673942804336548\n",
      "Epoch: 43, Samples: 1472/5760, Loss: 0.029340773820877075\n",
      "Epoch: 43, Samples: 1504/5760, Loss: 0.0143393874168396\n",
      "Epoch: 43, Samples: 1536/5760, Loss: 0.008007943630218506\n",
      "Epoch: 43, Samples: 1568/5760, Loss: 0.006932765245437622\n",
      "Epoch: 43, Samples: 1600/5760, Loss: 0.009573966264724731\n",
      "Epoch: 43, Samples: 1632/5760, Loss: 0.03125643730163574\n",
      "Epoch: 43, Samples: 1664/5760, Loss: 0.010786473751068115\n",
      "Epoch: 43, Samples: 1696/5760, Loss: 0.00733533501625061\n",
      "Epoch: 43, Samples: 1728/5760, Loss: 0.01765701174736023\n",
      "Epoch: 43, Samples: 1760/5760, Loss: 0.008813560009002686\n",
      "Epoch: 43, Samples: 1792/5760, Loss: 0.008506029844284058\n",
      "Epoch: 43, Samples: 1824/5760, Loss: 0.016086488962173462\n",
      "Epoch: 43, Samples: 1856/5760, Loss: 0.011419564485549927\n",
      "Epoch: 43, Samples: 1888/5760, Loss: 0.020650863647460938\n",
      "Epoch: 43, Samples: 1920/5760, Loss: 0.010477572679519653\n",
      "Epoch: 43, Samples: 1952/5760, Loss: 0.013433575630187988\n",
      "Epoch: 43, Samples: 1984/5760, Loss: 0.00975152850151062\n",
      "Epoch: 43, Samples: 2016/5760, Loss: 0.007587790489196777\n",
      "Epoch: 43, Samples: 2048/5760, Loss: 0.013421952724456787\n",
      "Epoch: 43, Samples: 2080/5760, Loss: 0.010831624269485474\n",
      "Epoch: 43, Samples: 2112/5760, Loss: 0.013924896717071533\n",
      "Epoch: 43, Samples: 2144/5760, Loss: 0.015723824501037598\n",
      "Epoch: 43, Samples: 2176/5760, Loss: 0.01414799690246582\n",
      "Epoch: 43, Samples: 2208/5760, Loss: 0.006427884101867676\n",
      "Epoch: 43, Samples: 2240/5760, Loss: 0.015004992485046387\n",
      "Epoch: 43, Samples: 2272/5760, Loss: 0.007842957973480225\n",
      "Epoch: 43, Samples: 2304/5760, Loss: 0.023168236017227173\n",
      "Epoch: 43, Samples: 2336/5760, Loss: 0.008551061153411865\n",
      "Epoch: 43, Samples: 2368/5760, Loss: 0.010897457599639893\n",
      "Epoch: 43, Samples: 2400/5760, Loss: 0.008491098880767822\n",
      "Epoch: 43, Samples: 2432/5760, Loss: 0.011417150497436523\n",
      "Epoch: 43, Samples: 2464/5760, Loss: 0.020588189363479614\n",
      "Epoch: 43, Samples: 2496/5760, Loss: 0.009809136390686035\n",
      "Epoch: 43, Samples: 2528/5760, Loss: 0.005085378885269165\n",
      "Epoch: 43, Samples: 2560/5760, Loss: 0.010489225387573242\n",
      "Epoch: 43, Samples: 2592/5760, Loss: 0.005963712930679321\n",
      "Epoch: 43, Samples: 2624/5760, Loss: 0.02081012725830078\n",
      "Epoch: 43, Samples: 2656/5760, Loss: 0.00826120376586914\n",
      "Epoch: 43, Samples: 2688/5760, Loss: 0.00818401575088501\n",
      "Epoch: 43, Samples: 2720/5760, Loss: 0.011748909950256348\n",
      "Epoch: 43, Samples: 2752/5760, Loss: 0.006729006767272949\n",
      "Epoch: 43, Samples: 2784/5760, Loss: 0.009755998849868774\n",
      "Epoch: 43, Samples: 2816/5760, Loss: 0.012345314025878906\n",
      "Epoch: 43, Samples: 2848/5760, Loss: 0.01403382420539856\n",
      "Epoch: 43, Samples: 2880/5760, Loss: 0.007118433713912964\n",
      "Epoch: 43, Samples: 2912/5760, Loss: 0.004626035690307617\n",
      "Epoch: 43, Samples: 2944/5760, Loss: 0.017194926738739014\n",
      "Epoch: 43, Samples: 2976/5760, Loss: 0.010880887508392334\n",
      "Epoch: 43, Samples: 3008/5760, Loss: 0.01153072714805603\n",
      "Epoch: 43, Samples: 3040/5760, Loss: 0.010690540075302124\n",
      "Epoch: 43, Samples: 3072/5760, Loss: 0.00875478982925415\n",
      "Epoch: 43, Samples: 3104/5760, Loss: 0.023503541946411133\n",
      "Epoch: 43, Samples: 3136/5760, Loss: 0.011585325002670288\n",
      "Epoch: 43, Samples: 3168/5760, Loss: 0.012965410947799683\n",
      "Epoch: 43, Samples: 3200/5760, Loss: 0.00816604495048523\n",
      "Epoch: 43, Samples: 3232/5760, Loss: 0.014790892601013184\n",
      "Epoch: 43, Samples: 3264/5760, Loss: 0.010869711637496948\n",
      "Epoch: 43, Samples: 3296/5760, Loss: 0.012256443500518799\n",
      "Epoch: 43, Samples: 3328/5760, Loss: 0.02074974775314331\n",
      "Epoch: 43, Samples: 3360/5760, Loss: 0.006941854953765869\n",
      "Epoch: 43, Samples: 3392/5760, Loss: 0.0075623393058776855\n",
      "Epoch: 43, Samples: 3424/5760, Loss: 0.006045103073120117\n",
      "Epoch: 43, Samples: 3456/5760, Loss: 0.013460934162139893\n",
      "Epoch: 43, Samples: 3488/5760, Loss: 0.0092792809009552\n",
      "Epoch: 43, Samples: 3520/5760, Loss: 0.018240034580230713\n",
      "Epoch: 43, Samples: 3552/5760, Loss: 0.012643784284591675\n",
      "Epoch: 43, Samples: 3584/5760, Loss: 0.005522400140762329\n",
      "Epoch: 43, Samples: 3616/5760, Loss: 0.010443925857543945\n",
      "Epoch: 43, Samples: 3648/5760, Loss: 0.009091436862945557\n",
      "Epoch: 43, Samples: 3680/5760, Loss: 0.014918297529220581\n",
      "Epoch: 43, Samples: 3712/5760, Loss: 0.008885115385055542\n",
      "Epoch: 43, Samples: 3744/5760, Loss: 0.009651124477386475\n",
      "Epoch: 43, Samples: 3776/5760, Loss: 0.005993068218231201\n",
      "Epoch: 43, Samples: 3808/5760, Loss: 0.007891327142715454\n",
      "Epoch: 43, Samples: 3840/5760, Loss: 0.012258201837539673\n",
      "Epoch: 43, Samples: 3872/5760, Loss: 0.01049429178237915\n",
      "Epoch: 43, Samples: 3904/5760, Loss: 0.003947943449020386\n",
      "Epoch: 43, Samples: 3936/5760, Loss: 0.01184391975402832\n",
      "Epoch: 43, Samples: 3968/5760, Loss: 0.013173043727874756\n",
      "Epoch: 43, Samples: 4000/5760, Loss: 0.014295786619186401\n",
      "Epoch: 43, Samples: 4032/5760, Loss: 0.022782325744628906\n",
      "Epoch: 43, Samples: 4064/5760, Loss: 0.012522369623184204\n",
      "Epoch: 43, Samples: 4096/5760, Loss: 0.02576470375061035\n",
      "Epoch: 43, Samples: 4128/5760, Loss: 0.005521029233932495\n",
      "Epoch: 43, Samples: 4160/5760, Loss: 0.011374801397323608\n",
      "Epoch: 43, Samples: 4192/5760, Loss: 0.009948194026947021\n",
      "Epoch: 43, Samples: 4224/5760, Loss: 0.011977970600128174\n",
      "Epoch: 43, Samples: 4256/5760, Loss: 0.009911179542541504\n",
      "Epoch: 43, Samples: 4288/5760, Loss: 0.010704129934310913\n",
      "Epoch: 43, Samples: 4320/5760, Loss: 0.009474754333496094\n",
      "Epoch: 43, Samples: 4352/5760, Loss: 0.018201708793640137\n",
      "Epoch: 43, Samples: 4384/5760, Loss: 0.005623906850814819\n",
      "Epoch: 43, Samples: 4416/5760, Loss: 0.01327696442604065\n",
      "Epoch: 43, Samples: 4448/5760, Loss: 0.015030205249786377\n",
      "Epoch: 43, Samples: 4480/5760, Loss: 0.008354753255844116\n",
      "Epoch: 43, Samples: 4512/5760, Loss: 0.009445637464523315\n",
      "Epoch: 43, Samples: 4544/5760, Loss: 0.011551111936569214\n",
      "Epoch: 43, Samples: 4576/5760, Loss: 0.01123228669166565\n",
      "Epoch: 43, Samples: 4608/5760, Loss: 0.012208372354507446\n",
      "Epoch: 43, Samples: 4640/5760, Loss: 0.0384577214717865\n",
      "Epoch: 43, Samples: 4672/5760, Loss: 0.014736384153366089\n",
      "Epoch: 43, Samples: 4704/5760, Loss: 0.010574549436569214\n",
      "Epoch: 43, Samples: 4736/5760, Loss: 0.008526444435119629\n",
      "Epoch: 43, Samples: 4768/5760, Loss: 0.011152803897857666\n",
      "Epoch: 43, Samples: 4800/5760, Loss: 0.011606723070144653\n",
      "Epoch: 43, Samples: 4832/5760, Loss: 0.005765736103057861\n",
      "Epoch: 43, Samples: 4864/5760, Loss: 0.018826782703399658\n",
      "Epoch: 43, Samples: 4896/5760, Loss: 0.010177969932556152\n",
      "Epoch: 43, Samples: 4928/5760, Loss: 0.011372417211532593\n",
      "Epoch: 43, Samples: 4960/5760, Loss: 0.01471826434135437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43, Samples: 4992/5760, Loss: 0.007611781358718872\n",
      "Epoch: 43, Samples: 5024/5760, Loss: 0.005949497222900391\n",
      "Epoch: 43, Samples: 5056/5760, Loss: 0.008633673191070557\n",
      "Epoch: 43, Samples: 5088/5760, Loss: 0.005283355712890625\n",
      "Epoch: 43, Samples: 5120/5760, Loss: 0.0171109139919281\n",
      "Epoch: 43, Samples: 5152/5760, Loss: 0.009578287601470947\n",
      "Epoch: 43, Samples: 5184/5760, Loss: 0.008605241775512695\n",
      "Epoch: 43, Samples: 5216/5760, Loss: 0.009634286165237427\n",
      "Epoch: 43, Samples: 5248/5760, Loss: 0.00848168134689331\n",
      "Epoch: 43, Samples: 5280/5760, Loss: 0.01358833909034729\n",
      "Epoch: 43, Samples: 5312/5760, Loss: 0.0061415135860443115\n",
      "Epoch: 43, Samples: 5344/5760, Loss: 0.0113430917263031\n",
      "Epoch: 43, Samples: 5376/5760, Loss: 0.006002336740493774\n",
      "Epoch: 43, Samples: 5408/5760, Loss: 0.02961432933807373\n",
      "Epoch: 43, Samples: 5440/5760, Loss: 0.010146886110305786\n",
      "Epoch: 43, Samples: 5472/5760, Loss: 0.007822036743164062\n",
      "Epoch: 43, Samples: 5504/5760, Loss: 0.009762197732925415\n",
      "Epoch: 43, Samples: 5536/5760, Loss: 0.013524174690246582\n",
      "Epoch: 43, Samples: 5568/5760, Loss: 0.010454773902893066\n",
      "Epoch: 43, Samples: 5600/5760, Loss: 0.01204991340637207\n",
      "Epoch: 43, Samples: 5632/5760, Loss: 0.0057392418384552\n",
      "Epoch: 43, Samples: 5664/5760, Loss: 0.008348733186721802\n",
      "Epoch: 43, Samples: 5696/5760, Loss: 0.009885072708129883\n",
      "Epoch: 43, Samples: 5728/5760, Loss: 0.313602089881897\n",
      "\n",
      "Epoch: 43\n",
      "Training set: Average loss: 0.0138\n",
      "Validation set: Average loss: 0.3448, Accuracy: 742/818 (91%)\n",
      "Epoch: 44, Samples: 0/5760, Loss: 0.007104605436325073\n",
      "Epoch: 44, Samples: 32/5760, Loss: 0.004767507314682007\n",
      "Epoch: 44, Samples: 64/5760, Loss: 0.007422715425491333\n",
      "Epoch: 44, Samples: 96/5760, Loss: 0.008197486400604248\n",
      "Epoch: 44, Samples: 128/5760, Loss: 0.01406008005142212\n",
      "Epoch: 44, Samples: 160/5760, Loss: 0.007001161575317383\n",
      "Epoch: 44, Samples: 192/5760, Loss: 0.008112728595733643\n",
      "Epoch: 44, Samples: 224/5760, Loss: 0.007690101861953735\n",
      "Epoch: 44, Samples: 256/5760, Loss: 0.006848722696304321\n",
      "Epoch: 44, Samples: 288/5760, Loss: 0.01048809289932251\n",
      "Epoch: 44, Samples: 320/5760, Loss: 0.013598501682281494\n",
      "Epoch: 44, Samples: 352/5760, Loss: 0.007174015045166016\n",
      "Epoch: 44, Samples: 384/5760, Loss: 0.01423671841621399\n",
      "Epoch: 44, Samples: 416/5760, Loss: 0.01416546106338501\n",
      "Epoch: 44, Samples: 448/5760, Loss: 0.00813254714012146\n",
      "Epoch: 44, Samples: 480/5760, Loss: 0.008781909942626953\n",
      "Epoch: 44, Samples: 512/5760, Loss: 0.017633169889450073\n",
      "Epoch: 44, Samples: 544/5760, Loss: 0.026011750102043152\n",
      "Epoch: 44, Samples: 576/5760, Loss: 0.011833816766738892\n",
      "Epoch: 44, Samples: 608/5760, Loss: 0.014443248510360718\n",
      "Epoch: 44, Samples: 640/5760, Loss: 0.009566009044647217\n",
      "Epoch: 44, Samples: 672/5760, Loss: 0.01142042875289917\n",
      "Epoch: 44, Samples: 704/5760, Loss: 0.007661014795303345\n",
      "Epoch: 44, Samples: 736/5760, Loss: 0.028107404708862305\n",
      "Epoch: 44, Samples: 768/5760, Loss: 0.009765475988388062\n",
      "Epoch: 44, Samples: 800/5760, Loss: 0.009441345930099487\n",
      "Epoch: 44, Samples: 832/5760, Loss: 0.009031742811203003\n",
      "Epoch: 44, Samples: 864/5760, Loss: 0.024827241897583008\n",
      "Epoch: 44, Samples: 896/5760, Loss: 0.009120911359786987\n",
      "Epoch: 44, Samples: 928/5760, Loss: 0.01656380295753479\n",
      "Epoch: 44, Samples: 960/5760, Loss: 0.0069734156131744385\n",
      "Epoch: 44, Samples: 992/5760, Loss: 0.012458354234695435\n",
      "Epoch: 44, Samples: 1024/5760, Loss: 0.009927600622177124\n",
      "Epoch: 44, Samples: 1056/5760, Loss: 0.00391504168510437\n",
      "Epoch: 44, Samples: 1088/5760, Loss: 0.008223742246627808\n",
      "Epoch: 44, Samples: 1120/5760, Loss: 0.01573920249938965\n",
      "Epoch: 44, Samples: 1152/5760, Loss: 0.007997274398803711\n",
      "Epoch: 44, Samples: 1184/5760, Loss: 0.009007126092910767\n",
      "Epoch: 44, Samples: 1216/5760, Loss: 0.007168322801589966\n",
      "Epoch: 44, Samples: 1248/5760, Loss: 0.01464417576789856\n",
      "Epoch: 44, Samples: 1280/5760, Loss: 0.016988277435302734\n",
      "Epoch: 44, Samples: 1312/5760, Loss: 0.006714314222335815\n",
      "Epoch: 44, Samples: 1344/5760, Loss: 0.010008901357650757\n",
      "Epoch: 44, Samples: 1376/5760, Loss: 0.011426419019699097\n",
      "Epoch: 44, Samples: 1408/5760, Loss: 0.009128302335739136\n",
      "Epoch: 44, Samples: 1440/5760, Loss: 0.0072675347328186035\n",
      "Epoch: 44, Samples: 1472/5760, Loss: 0.0161190927028656\n",
      "Epoch: 44, Samples: 1504/5760, Loss: 0.005773991346359253\n",
      "Epoch: 44, Samples: 1536/5760, Loss: 0.004708379507064819\n",
      "Epoch: 44, Samples: 1568/5760, Loss: 0.012665808200836182\n",
      "Epoch: 44, Samples: 1600/5760, Loss: 0.008387088775634766\n",
      "Epoch: 44, Samples: 1632/5760, Loss: 0.012128829956054688\n",
      "Epoch: 44, Samples: 1664/5760, Loss: 0.010634958744049072\n",
      "Epoch: 44, Samples: 1696/5760, Loss: 0.008410423994064331\n",
      "Epoch: 44, Samples: 1728/5760, Loss: 0.008393555879592896\n",
      "Epoch: 44, Samples: 1760/5760, Loss: 0.006119728088378906\n",
      "Epoch: 44, Samples: 1792/5760, Loss: 0.013741403818130493\n",
      "Epoch: 44, Samples: 1824/5760, Loss: 0.020366936922073364\n",
      "Epoch: 44, Samples: 1856/5760, Loss: 0.008008480072021484\n",
      "Epoch: 44, Samples: 1888/5760, Loss: 0.004329323768615723\n",
      "Epoch: 44, Samples: 1920/5760, Loss: 0.010689318180084229\n",
      "Epoch: 44, Samples: 1952/5760, Loss: 0.013598978519439697\n",
      "Epoch: 44, Samples: 1984/5760, Loss: 0.020915687084197998\n",
      "Epoch: 44, Samples: 2016/5760, Loss: 0.005157172679901123\n",
      "Epoch: 44, Samples: 2048/5760, Loss: 0.010670691728591919\n",
      "Epoch: 44, Samples: 2080/5760, Loss: 0.01929253339767456\n",
      "Epoch: 44, Samples: 2112/5760, Loss: 0.01396128535270691\n",
      "Epoch: 44, Samples: 2144/5760, Loss: 0.007640630006790161\n",
      "Epoch: 44, Samples: 2176/5760, Loss: 0.018927842378616333\n",
      "Epoch: 44, Samples: 2208/5760, Loss: 0.01208534836769104\n",
      "Epoch: 44, Samples: 2240/5760, Loss: 0.008770525455474854\n",
      "Epoch: 44, Samples: 2272/5760, Loss: 0.010011643171310425\n",
      "Epoch: 44, Samples: 2304/5760, Loss: 0.009008079767227173\n",
      "Epoch: 44, Samples: 2336/5760, Loss: 0.01154714822769165\n",
      "Epoch: 44, Samples: 2368/5760, Loss: 0.005543351173400879\n",
      "Epoch: 44, Samples: 2400/5760, Loss: 0.008805990219116211\n",
      "Epoch: 44, Samples: 2432/5760, Loss: 0.02161318063735962\n",
      "Epoch: 44, Samples: 2464/5760, Loss: 0.004421234130859375\n",
      "Epoch: 44, Samples: 2496/5760, Loss: 0.011525571346282959\n",
      "Epoch: 44, Samples: 2528/5760, Loss: 0.015025109052658081\n",
      "Epoch: 44, Samples: 2560/5760, Loss: 0.006876170635223389\n",
      "Epoch: 44, Samples: 2592/5760, Loss: 0.010091066360473633\n",
      "Epoch: 44, Samples: 2624/5760, Loss: 0.014241516590118408\n",
      "Epoch: 44, Samples: 2656/5760, Loss: 0.010094165802001953\n",
      "Epoch: 44, Samples: 2688/5760, Loss: 0.009241044521331787\n",
      "Epoch: 44, Samples: 2720/5760, Loss: 0.005348384380340576\n",
      "Epoch: 44, Samples: 2752/5760, Loss: 0.006320267915725708\n",
      "Epoch: 44, Samples: 2784/5760, Loss: 0.01334407925605774\n",
      "Epoch: 44, Samples: 2816/5760, Loss: 0.008725553750991821\n",
      "Epoch: 44, Samples: 2848/5760, Loss: 0.013358891010284424\n",
      "Epoch: 44, Samples: 2880/5760, Loss: 0.006409198045730591\n",
      "Epoch: 44, Samples: 2912/5760, Loss: 0.013289153575897217\n",
      "Epoch: 44, Samples: 2944/5760, Loss: 0.008615493774414062\n",
      "Epoch: 44, Samples: 2976/5760, Loss: 0.008871287107467651\n",
      "Epoch: 44, Samples: 3008/5760, Loss: 0.009229332208633423\n",
      "Epoch: 44, Samples: 3040/5760, Loss: 0.006361961364746094\n",
      "Epoch: 44, Samples: 3072/5760, Loss: 0.010417550802230835\n",
      "Epoch: 44, Samples: 3104/5760, Loss: 0.011309117078781128\n",
      "Epoch: 44, Samples: 3136/5760, Loss: 0.005662709474563599\n",
      "Epoch: 44, Samples: 3168/5760, Loss: 0.019675344228744507\n",
      "Epoch: 44, Samples: 3200/5760, Loss: 0.008833616971969604\n",
      "Epoch: 44, Samples: 3232/5760, Loss: 0.010360300540924072\n",
      "Epoch: 44, Samples: 3264/5760, Loss: 0.013766258955001831\n",
      "Epoch: 44, Samples: 3296/5760, Loss: 0.019508063793182373\n",
      "Epoch: 44, Samples: 3328/5760, Loss: 0.011982500553131104\n",
      "Epoch: 44, Samples: 3360/5760, Loss: 0.007652401924133301\n",
      "Epoch: 44, Samples: 3392/5760, Loss: 0.008415758609771729\n",
      "Epoch: 44, Samples: 3424/5760, Loss: 0.005891203880310059\n",
      "Epoch: 44, Samples: 3456/5760, Loss: 0.010522782802581787\n",
      "Epoch: 44, Samples: 3488/5760, Loss: 0.005858510732650757\n",
      "Epoch: 44, Samples: 3520/5760, Loss: 0.013555407524108887\n",
      "Epoch: 44, Samples: 3552/5760, Loss: 0.005684494972229004\n",
      "Epoch: 44, Samples: 3584/5760, Loss: 0.011378437280654907\n",
      "Epoch: 44, Samples: 3616/5760, Loss: 0.01127612590789795\n",
      "Epoch: 44, Samples: 3648/5760, Loss: 0.013116776943206787\n",
      "Epoch: 44, Samples: 3680/5760, Loss: 0.013423383235931396\n",
      "Epoch: 44, Samples: 3712/5760, Loss: 0.009681135416030884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Samples: 3744/5760, Loss: 0.007752835750579834\n",
      "Epoch: 44, Samples: 3776/5760, Loss: 0.005915701389312744\n",
      "Epoch: 44, Samples: 3808/5760, Loss: 0.010630190372467041\n",
      "Epoch: 44, Samples: 3840/5760, Loss: 0.009998083114624023\n",
      "Epoch: 44, Samples: 3872/5760, Loss: 0.006621748208999634\n",
      "Epoch: 44, Samples: 3904/5760, Loss: 0.006863385438919067\n",
      "Epoch: 44, Samples: 3936/5760, Loss: 0.007639139890670776\n",
      "Epoch: 44, Samples: 3968/5760, Loss: 0.012335360050201416\n",
      "Epoch: 44, Samples: 4000/5760, Loss: 0.019397318363189697\n",
      "Epoch: 44, Samples: 4032/5760, Loss: 0.011289745569229126\n",
      "Epoch: 44, Samples: 4064/5760, Loss: 0.011745065450668335\n",
      "Epoch: 44, Samples: 4096/5760, Loss: 0.006762802600860596\n",
      "Epoch: 44, Samples: 4128/5760, Loss: 0.005703777074813843\n",
      "Epoch: 44, Samples: 4160/5760, Loss: 0.014147520065307617\n",
      "Epoch: 44, Samples: 4192/5760, Loss: 0.008552640676498413\n",
      "Epoch: 44, Samples: 4224/5760, Loss: 0.005747348070144653\n",
      "Epoch: 44, Samples: 4256/5760, Loss: 0.0077638328075408936\n",
      "Epoch: 44, Samples: 4288/5760, Loss: 0.012583106756210327\n",
      "Epoch: 44, Samples: 4320/5760, Loss: 0.010593205690383911\n",
      "Epoch: 44, Samples: 4352/5760, Loss: 0.016397610306739807\n",
      "Epoch: 44, Samples: 4384/5760, Loss: 0.0072889626026153564\n",
      "Epoch: 44, Samples: 4416/5760, Loss: 0.005096435546875\n",
      "Epoch: 44, Samples: 4448/5760, Loss: 0.008167386054992676\n",
      "Epoch: 44, Samples: 4480/5760, Loss: 0.017600029706954956\n",
      "Epoch: 44, Samples: 4512/5760, Loss: 0.0075190067291259766\n",
      "Epoch: 44, Samples: 4544/5760, Loss: 0.019675999879837036\n",
      "Epoch: 44, Samples: 4576/5760, Loss: 0.004132121801376343\n",
      "Epoch: 44, Samples: 4608/5760, Loss: 0.010946124792098999\n",
      "Epoch: 44, Samples: 4640/5760, Loss: 0.009416759014129639\n",
      "Epoch: 44, Samples: 4672/5760, Loss: 0.009605258703231812\n",
      "Epoch: 44, Samples: 4704/5760, Loss: 0.006726682186126709\n",
      "Epoch: 44, Samples: 4736/5760, Loss: 0.009325563907623291\n",
      "Epoch: 44, Samples: 4768/5760, Loss: 0.005385100841522217\n",
      "Epoch: 44, Samples: 4800/5760, Loss: 0.007426023483276367\n",
      "Epoch: 44, Samples: 4832/5760, Loss: 0.006750345230102539\n",
      "Epoch: 44, Samples: 4864/5760, Loss: 0.01604977250099182\n",
      "Epoch: 44, Samples: 4896/5760, Loss: 0.006695806980133057\n",
      "Epoch: 44, Samples: 4928/5760, Loss: 0.011740356683731079\n",
      "Epoch: 44, Samples: 4960/5760, Loss: 0.01032787561416626\n",
      "Epoch: 44, Samples: 4992/5760, Loss: 0.009211242198944092\n",
      "Epoch: 44, Samples: 5024/5760, Loss: 0.014312982559204102\n",
      "Epoch: 44, Samples: 5056/5760, Loss: 0.005806297063827515\n",
      "Epoch: 44, Samples: 5088/5760, Loss: 0.010146617889404297\n",
      "Epoch: 44, Samples: 5120/5760, Loss: 0.006211429834365845\n",
      "Epoch: 44, Samples: 5152/5760, Loss: 0.007286429405212402\n",
      "Epoch: 44, Samples: 5184/5760, Loss: 0.006932944059371948\n",
      "Epoch: 44, Samples: 5216/5760, Loss: 0.010547727346420288\n",
      "Epoch: 44, Samples: 5248/5760, Loss: 0.01493149995803833\n",
      "Epoch: 44, Samples: 5280/5760, Loss: 0.016144007444381714\n",
      "Epoch: 44, Samples: 5312/5760, Loss: 0.004709422588348389\n",
      "Epoch: 44, Samples: 5344/5760, Loss: 0.006686091423034668\n",
      "Epoch: 44, Samples: 5376/5760, Loss: 0.01025804877281189\n",
      "Epoch: 44, Samples: 5408/5760, Loss: 0.011416345834732056\n",
      "Epoch: 44, Samples: 5440/5760, Loss: 0.01588648557662964\n",
      "Epoch: 44, Samples: 5472/5760, Loss: 0.011759579181671143\n",
      "Epoch: 44, Samples: 5504/5760, Loss: 0.012594848871231079\n",
      "Epoch: 44, Samples: 5536/5760, Loss: 0.009024888277053833\n",
      "Epoch: 44, Samples: 5568/5760, Loss: 0.008267402648925781\n",
      "Epoch: 44, Samples: 5600/5760, Loss: 0.01594865322113037\n",
      "Epoch: 44, Samples: 5632/5760, Loss: 0.0062593817710876465\n",
      "Epoch: 44, Samples: 5664/5760, Loss: 0.008916079998016357\n",
      "Epoch: 44, Samples: 5696/5760, Loss: 0.013436585664749146\n",
      "Epoch: 44, Samples: 5728/5760, Loss: 0.43705952167510986\n",
      "\n",
      "Epoch: 44\n",
      "Training set: Average loss: 0.0129\n",
      "Validation set: Average loss: 0.3413, Accuracy: 745/818 (91%)\n",
      "Epoch: 45, Samples: 0/5760, Loss: 0.009082704782485962\n",
      "Epoch: 45, Samples: 32/5760, Loss: 0.006692409515380859\n",
      "Epoch: 45, Samples: 64/5760, Loss: 0.011649608612060547\n",
      "Epoch: 45, Samples: 96/5760, Loss: 0.009676814079284668\n",
      "Epoch: 45, Samples: 128/5760, Loss: 0.03018203377723694\n",
      "Epoch: 45, Samples: 160/5760, Loss: 0.007364094257354736\n",
      "Epoch: 45, Samples: 192/5760, Loss: 0.00826418399810791\n",
      "Epoch: 45, Samples: 224/5760, Loss: 0.004754215478897095\n",
      "Epoch: 45, Samples: 256/5760, Loss: 0.011347949504852295\n",
      "Epoch: 45, Samples: 288/5760, Loss: 0.00747305154800415\n",
      "Epoch: 45, Samples: 320/5760, Loss: 0.00747343897819519\n",
      "Epoch: 45, Samples: 352/5760, Loss: 0.008913278579711914\n",
      "Epoch: 45, Samples: 384/5760, Loss: 0.007120341062545776\n",
      "Epoch: 45, Samples: 416/5760, Loss: 0.02136293053627014\n",
      "Epoch: 45, Samples: 448/5760, Loss: 0.014428257942199707\n",
      "Epoch: 45, Samples: 480/5760, Loss: 0.011795073747634888\n",
      "Epoch: 45, Samples: 512/5760, Loss: 0.010246515274047852\n",
      "Epoch: 45, Samples: 544/5760, Loss: 0.015484124422073364\n",
      "Epoch: 45, Samples: 576/5760, Loss: 0.0193614661693573\n",
      "Epoch: 45, Samples: 608/5760, Loss: 0.013887792825698853\n",
      "Epoch: 45, Samples: 640/5760, Loss: 0.008452951908111572\n",
      "Epoch: 45, Samples: 672/5760, Loss: 0.008646160364151001\n",
      "Epoch: 45, Samples: 704/5760, Loss: 0.006612032651901245\n",
      "Epoch: 45, Samples: 736/5760, Loss: 0.00887879729270935\n",
      "Epoch: 45, Samples: 768/5760, Loss: 0.012765347957611084\n",
      "Epoch: 45, Samples: 800/5760, Loss: 0.015014111995697021\n",
      "Epoch: 45, Samples: 832/5760, Loss: 0.024056732654571533\n",
      "Epoch: 45, Samples: 864/5760, Loss: 0.007021039724349976\n",
      "Epoch: 45, Samples: 896/5760, Loss: 0.0067521631717681885\n",
      "Epoch: 45, Samples: 928/5760, Loss: 0.018110066652297974\n",
      "Epoch: 45, Samples: 960/5760, Loss: 0.01957741379737854\n",
      "Epoch: 45, Samples: 992/5760, Loss: 0.006498187780380249\n",
      "Epoch: 45, Samples: 1024/5760, Loss: 0.01598486304283142\n",
      "Epoch: 45, Samples: 1056/5760, Loss: 0.014350950717926025\n",
      "Epoch: 45, Samples: 1088/5760, Loss: 0.021817684173583984\n",
      "Epoch: 45, Samples: 1120/5760, Loss: 0.00770685076713562\n",
      "Epoch: 45, Samples: 1152/5760, Loss: 0.009068876504898071\n",
      "Epoch: 45, Samples: 1184/5760, Loss: 0.007587701082229614\n",
      "Epoch: 45, Samples: 1216/5760, Loss: 0.0093231201171875\n",
      "Epoch: 45, Samples: 1248/5760, Loss: 0.009886205196380615\n",
      "Epoch: 45, Samples: 1280/5760, Loss: 0.008220463991165161\n",
      "Epoch: 45, Samples: 1312/5760, Loss: 0.005500227212905884\n",
      "Epoch: 45, Samples: 1344/5760, Loss: 0.009840607643127441\n",
      "Epoch: 45, Samples: 1376/5760, Loss: 0.005978614091873169\n",
      "Epoch: 45, Samples: 1408/5760, Loss: 0.00807499885559082\n",
      "Epoch: 45, Samples: 1440/5760, Loss: 0.006803035736083984\n",
      "Epoch: 45, Samples: 1472/5760, Loss: 0.020501822233200073\n",
      "Epoch: 45, Samples: 1504/5760, Loss: 0.009844213724136353\n",
      "Epoch: 45, Samples: 1536/5760, Loss: 0.01081693172454834\n",
      "Epoch: 45, Samples: 1568/5760, Loss: 0.008258402347564697\n",
      "Epoch: 45, Samples: 1600/5760, Loss: 0.01010480523109436\n",
      "Epoch: 45, Samples: 1632/5760, Loss: 0.014766395092010498\n",
      "Epoch: 45, Samples: 1664/5760, Loss: 0.015551269054412842\n",
      "Epoch: 45, Samples: 1696/5760, Loss: 0.005792438983917236\n",
      "Epoch: 45, Samples: 1728/5760, Loss: 0.0074145495891571045\n",
      "Epoch: 45, Samples: 1760/5760, Loss: 0.006540805101394653\n",
      "Epoch: 45, Samples: 1792/5760, Loss: 0.00889095664024353\n",
      "Epoch: 45, Samples: 1824/5760, Loss: 0.010041981935501099\n",
      "Epoch: 45, Samples: 1856/5760, Loss: 0.00562208890914917\n",
      "Epoch: 45, Samples: 1888/5760, Loss: 0.0070518553256988525\n",
      "Epoch: 45, Samples: 1920/5760, Loss: 0.010419905185699463\n",
      "Epoch: 45, Samples: 1952/5760, Loss: 0.005536526441574097\n",
      "Epoch: 45, Samples: 1984/5760, Loss: 0.010537683963775635\n",
      "Epoch: 45, Samples: 2016/5760, Loss: 0.010033965110778809\n",
      "Epoch: 45, Samples: 2048/5760, Loss: 0.01700231432914734\n",
      "Epoch: 45, Samples: 2080/5760, Loss: 0.007357925176620483\n",
      "Epoch: 45, Samples: 2112/5760, Loss: 0.0060691237449646\n",
      "Epoch: 45, Samples: 2144/5760, Loss: 0.00554230809211731\n",
      "Epoch: 45, Samples: 2176/5760, Loss: 0.007675737142562866\n",
      "Epoch: 45, Samples: 2208/5760, Loss: 0.0066800713539123535\n",
      "Epoch: 45, Samples: 2240/5760, Loss: 0.01597413420677185\n",
      "Epoch: 45, Samples: 2272/5760, Loss: 0.012201309204101562\n",
      "Epoch: 45, Samples: 2304/5760, Loss: 0.009311586618423462\n",
      "Epoch: 45, Samples: 2336/5760, Loss: 0.013235807418823242\n",
      "Epoch: 45, Samples: 2368/5760, Loss: 0.010645240545272827\n",
      "Epoch: 45, Samples: 2400/5760, Loss: 0.01102498173713684\n",
      "Epoch: 45, Samples: 2432/5760, Loss: 0.009092628955841064\n",
      "Epoch: 45, Samples: 2464/5760, Loss: 0.005532324314117432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45, Samples: 2496/5760, Loss: 0.007015585899353027\n",
      "Epoch: 45, Samples: 2528/5760, Loss: 0.021564483642578125\n",
      "Epoch: 45, Samples: 2560/5760, Loss: 0.008944064378738403\n",
      "Epoch: 45, Samples: 2592/5760, Loss: 0.012343078851699829\n",
      "Epoch: 45, Samples: 2624/5760, Loss: 0.010653138160705566\n",
      "Epoch: 45, Samples: 2656/5760, Loss: 0.014688640832901001\n",
      "Epoch: 45, Samples: 2688/5760, Loss: 0.0086536705493927\n",
      "Epoch: 45, Samples: 2720/5760, Loss: 0.011757224798202515\n",
      "Epoch: 45, Samples: 2752/5760, Loss: 0.0075608789920806885\n",
      "Epoch: 45, Samples: 2784/5760, Loss: 0.01019597053527832\n",
      "Epoch: 45, Samples: 2816/5760, Loss: 0.007518351078033447\n",
      "Epoch: 45, Samples: 2848/5760, Loss: 0.009277880191802979\n",
      "Epoch: 45, Samples: 2880/5760, Loss: 0.008031278848648071\n",
      "Epoch: 45, Samples: 2912/5760, Loss: 0.005962878465652466\n",
      "Epoch: 45, Samples: 2944/5760, Loss: 0.007684677839279175\n",
      "Epoch: 45, Samples: 2976/5760, Loss: 0.008395940065383911\n",
      "Epoch: 45, Samples: 3008/5760, Loss: 0.007908463478088379\n",
      "Epoch: 45, Samples: 3040/5760, Loss: 0.007705956697463989\n",
      "Epoch: 45, Samples: 3072/5760, Loss: 0.008504629135131836\n",
      "Epoch: 45, Samples: 3104/5760, Loss: 0.00464823842048645\n",
      "Epoch: 45, Samples: 3136/5760, Loss: 0.005500882863998413\n",
      "Epoch: 45, Samples: 3168/5760, Loss: 0.01408722996711731\n",
      "Epoch: 45, Samples: 3200/5760, Loss: 0.011025696992874146\n",
      "Epoch: 45, Samples: 3232/5760, Loss: 0.009281128644943237\n",
      "Epoch: 45, Samples: 3264/5760, Loss: 0.01152530312538147\n",
      "Epoch: 45, Samples: 3296/5760, Loss: 0.0076920390129089355\n",
      "Epoch: 45, Samples: 3328/5760, Loss: 0.010499030351638794\n",
      "Epoch: 45, Samples: 3360/5760, Loss: 0.0074103474617004395\n",
      "Epoch: 45, Samples: 3392/5760, Loss: 0.00977364182472229\n",
      "Epoch: 45, Samples: 3424/5760, Loss: 0.012452781200408936\n",
      "Epoch: 45, Samples: 3456/5760, Loss: 0.019347965717315674\n",
      "Epoch: 45, Samples: 3488/5760, Loss: 0.027666956186294556\n",
      "Epoch: 45, Samples: 3520/5760, Loss: 0.007064908742904663\n",
      "Epoch: 45, Samples: 3552/5760, Loss: 0.01795700192451477\n",
      "Epoch: 45, Samples: 3584/5760, Loss: 0.0069210827350616455\n",
      "Epoch: 45, Samples: 3616/5760, Loss: 0.006875097751617432\n",
      "Epoch: 45, Samples: 3648/5760, Loss: 0.008453458547592163\n",
      "Epoch: 45, Samples: 3680/5760, Loss: 0.00861409306526184\n",
      "Epoch: 45, Samples: 3712/5760, Loss: 0.010243028402328491\n",
      "Epoch: 45, Samples: 3744/5760, Loss: 0.012986361980438232\n",
      "Epoch: 45, Samples: 3776/5760, Loss: 0.007268726825714111\n",
      "Epoch: 45, Samples: 3808/5760, Loss: 0.014504760503768921\n",
      "Epoch: 45, Samples: 3840/5760, Loss: 0.006036162376403809\n",
      "Epoch: 45, Samples: 3872/5760, Loss: 0.01209491491317749\n",
      "Epoch: 45, Samples: 3904/5760, Loss: 0.010451704263687134\n",
      "Epoch: 45, Samples: 3936/5760, Loss: 0.005621016025543213\n",
      "Epoch: 45, Samples: 3968/5760, Loss: 0.011463135480880737\n",
      "Epoch: 45, Samples: 4000/5760, Loss: 0.011262089014053345\n",
      "Epoch: 45, Samples: 4032/5760, Loss: 0.013909026980400085\n",
      "Epoch: 45, Samples: 4064/5760, Loss: 0.011124789714813232\n",
      "Epoch: 45, Samples: 4096/5760, Loss: 0.006421893835067749\n",
      "Epoch: 45, Samples: 4128/5760, Loss: 0.00938907265663147\n",
      "Epoch: 45, Samples: 4160/5760, Loss: 0.009863227605819702\n",
      "Epoch: 45, Samples: 4192/5760, Loss: 0.005395084619522095\n",
      "Epoch: 45, Samples: 4224/5760, Loss: 0.01664435863494873\n",
      "Epoch: 45, Samples: 4256/5760, Loss: 0.00767168402671814\n",
      "Epoch: 45, Samples: 4288/5760, Loss: 0.0085543692111969\n",
      "Epoch: 45, Samples: 4320/5760, Loss: 0.010606199502944946\n",
      "Epoch: 45, Samples: 4352/5760, Loss: 0.011157572269439697\n",
      "Epoch: 45, Samples: 4384/5760, Loss: 0.0033878684043884277\n",
      "Epoch: 45, Samples: 4416/5760, Loss: 0.007064849138259888\n",
      "Epoch: 45, Samples: 4448/5760, Loss: 0.011041909456253052\n",
      "Epoch: 45, Samples: 4480/5760, Loss: 0.0036517083644866943\n",
      "Epoch: 45, Samples: 4512/5760, Loss: 0.008600682020187378\n",
      "Epoch: 45, Samples: 4544/5760, Loss: 0.008792519569396973\n",
      "Epoch: 45, Samples: 4576/5760, Loss: 0.005682945251464844\n",
      "Epoch: 45, Samples: 4608/5760, Loss: 0.003874540328979492\n",
      "Epoch: 45, Samples: 4640/5760, Loss: 0.012711822986602783\n",
      "Epoch: 45, Samples: 4672/5760, Loss: 0.018118858337402344\n",
      "Epoch: 45, Samples: 4704/5760, Loss: 0.008408695459365845\n",
      "Epoch: 45, Samples: 4736/5760, Loss: 0.00851207971572876\n",
      "Epoch: 45, Samples: 4768/5760, Loss: 0.010704874992370605\n",
      "Epoch: 45, Samples: 4800/5760, Loss: 0.02004009485244751\n",
      "Epoch: 45, Samples: 4832/5760, Loss: 0.012971252202987671\n",
      "Epoch: 45, Samples: 4864/5760, Loss: 0.007725805044174194\n",
      "Epoch: 45, Samples: 4896/5760, Loss: 0.011748164892196655\n",
      "Epoch: 45, Samples: 4928/5760, Loss: 0.02212071418762207\n",
      "Epoch: 45, Samples: 4960/5760, Loss: 0.008482128381729126\n",
      "Epoch: 45, Samples: 4992/5760, Loss: 0.008189558982849121\n",
      "Epoch: 45, Samples: 5024/5760, Loss: 0.011989086866378784\n",
      "Epoch: 45, Samples: 5056/5760, Loss: 0.006836295127868652\n",
      "Epoch: 45, Samples: 5088/5760, Loss: 0.016772329807281494\n",
      "Epoch: 45, Samples: 5120/5760, Loss: 0.013471841812133789\n",
      "Epoch: 45, Samples: 5152/5760, Loss: 0.019053339958190918\n",
      "Epoch: 45, Samples: 5184/5760, Loss: 0.01306229829788208\n",
      "Epoch: 45, Samples: 5216/5760, Loss: 0.01379057765007019\n",
      "Epoch: 45, Samples: 5248/5760, Loss: 0.006500333547592163\n",
      "Epoch: 45, Samples: 5280/5760, Loss: 0.013806164264678955\n",
      "Epoch: 45, Samples: 5312/5760, Loss: 0.009540975093841553\n",
      "Epoch: 45, Samples: 5344/5760, Loss: 0.021141216158866882\n",
      "Epoch: 45, Samples: 5376/5760, Loss: 0.012576431035995483\n",
      "Epoch: 45, Samples: 5408/5760, Loss: 0.01663804054260254\n",
      "Epoch: 45, Samples: 5440/5760, Loss: 0.008082985877990723\n",
      "Epoch: 45, Samples: 5472/5760, Loss: 0.006453901529312134\n",
      "Epoch: 45, Samples: 5504/5760, Loss: 0.019807934761047363\n",
      "Epoch: 45, Samples: 5536/5760, Loss: 0.009186714887619019\n",
      "Epoch: 45, Samples: 5568/5760, Loss: 0.013282984495162964\n",
      "Epoch: 45, Samples: 5600/5760, Loss: 0.006369411945343018\n",
      "Epoch: 45, Samples: 5632/5760, Loss: 0.007221966981887817\n",
      "Epoch: 45, Samples: 5664/5760, Loss: 0.00665128231048584\n",
      "Epoch: 45, Samples: 5696/5760, Loss: 0.006159991025924683\n",
      "Epoch: 45, Samples: 5728/5760, Loss: 0.20721065998077393\n",
      "\n",
      "Epoch: 45\n",
      "Training set: Average loss: 0.0117\n",
      "Validation set: Average loss: 0.3322, Accuracy: 750/818 (92%)\n",
      "Epoch: 46, Samples: 0/5760, Loss: 0.017716646194458008\n",
      "Epoch: 46, Samples: 32/5760, Loss: 0.01080310344696045\n",
      "Epoch: 46, Samples: 64/5760, Loss: 0.005942791700363159\n",
      "Epoch: 46, Samples: 96/5760, Loss: 0.016662582755088806\n",
      "Epoch: 46, Samples: 128/5760, Loss: 0.012041598558425903\n",
      "Epoch: 46, Samples: 160/5760, Loss: 0.013054430484771729\n",
      "Epoch: 46, Samples: 192/5760, Loss: 0.007498621940612793\n",
      "Epoch: 46, Samples: 224/5760, Loss: 0.02325648069381714\n",
      "Epoch: 46, Samples: 256/5760, Loss: 0.007066994905471802\n",
      "Epoch: 46, Samples: 288/5760, Loss: 0.009258568286895752\n",
      "Epoch: 46, Samples: 320/5760, Loss: 0.02225854992866516\n",
      "Epoch: 46, Samples: 352/5760, Loss: 0.008474975824356079\n",
      "Epoch: 46, Samples: 384/5760, Loss: 0.009428471326828003\n",
      "Epoch: 46, Samples: 416/5760, Loss: 0.006725519895553589\n",
      "Epoch: 46, Samples: 448/5760, Loss: 0.008320003747940063\n",
      "Epoch: 46, Samples: 480/5760, Loss: 0.012830257415771484\n",
      "Epoch: 46, Samples: 512/5760, Loss: 0.006392240524291992\n",
      "Epoch: 46, Samples: 544/5760, Loss: 0.00789952278137207\n",
      "Epoch: 46, Samples: 576/5760, Loss: 0.010601848363876343\n",
      "Epoch: 46, Samples: 608/5760, Loss: 0.007564812898635864\n",
      "Epoch: 46, Samples: 640/5760, Loss: 0.007450073957443237\n",
      "Epoch: 46, Samples: 672/5760, Loss: 0.01195189356803894\n",
      "Epoch: 46, Samples: 704/5760, Loss: 0.014652401208877563\n",
      "Epoch: 46, Samples: 736/5760, Loss: 0.014293074607849121\n",
      "Epoch: 46, Samples: 768/5760, Loss: 0.007287204265594482\n",
      "Epoch: 46, Samples: 800/5760, Loss: 0.00809529423713684\n",
      "Epoch: 46, Samples: 832/5760, Loss: 0.01445189118385315\n",
      "Epoch: 46, Samples: 864/5760, Loss: 0.007051140069961548\n",
      "Epoch: 46, Samples: 896/5760, Loss: 0.011777758598327637\n",
      "Epoch: 46, Samples: 928/5760, Loss: 0.005672812461853027\n",
      "Epoch: 46, Samples: 960/5760, Loss: 0.00667300820350647\n",
      "Epoch: 46, Samples: 992/5760, Loss: 0.008387058973312378\n",
      "Epoch: 46, Samples: 1024/5760, Loss: 0.009241670370101929\n",
      "Epoch: 46, Samples: 1056/5760, Loss: 0.005728542804718018\n",
      "Epoch: 46, Samples: 1088/5760, Loss: 0.00880846381187439\n",
      "Epoch: 46, Samples: 1120/5760, Loss: 0.011380136013031006\n",
      "Epoch: 46, Samples: 1152/5760, Loss: 0.007705479860305786\n",
      "Epoch: 46, Samples: 1184/5760, Loss: 0.004766374826431274\n",
      "Epoch: 46, Samples: 1216/5760, Loss: 0.008564263582229614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Samples: 1248/5760, Loss: 0.005948066711425781\n",
      "Epoch: 46, Samples: 1280/5760, Loss: 0.0040589869022369385\n",
      "Epoch: 46, Samples: 1312/5760, Loss: 0.012230873107910156\n",
      "Epoch: 46, Samples: 1344/5760, Loss: 0.007919460535049438\n",
      "Epoch: 46, Samples: 1376/5760, Loss: 0.008267849683761597\n",
      "Epoch: 46, Samples: 1408/5760, Loss: 0.006869882345199585\n",
      "Epoch: 46, Samples: 1440/5760, Loss: 0.005080461502075195\n",
      "Epoch: 46, Samples: 1472/5760, Loss: 0.00803595781326294\n",
      "Epoch: 46, Samples: 1504/5760, Loss: 0.009611845016479492\n",
      "Epoch: 46, Samples: 1536/5760, Loss: 0.0106315016746521\n",
      "Epoch: 46, Samples: 1568/5760, Loss: 0.009696334600448608\n",
      "Epoch: 46, Samples: 1600/5760, Loss: 0.015861809253692627\n",
      "Epoch: 46, Samples: 1632/5760, Loss: 0.01238405704498291\n",
      "Epoch: 46, Samples: 1664/5760, Loss: 0.006764888763427734\n",
      "Epoch: 46, Samples: 1696/5760, Loss: 0.009655535221099854\n",
      "Epoch: 46, Samples: 1728/5760, Loss: 0.0058100223541259766\n",
      "Epoch: 46, Samples: 1760/5760, Loss: 0.009314507246017456\n",
      "Epoch: 46, Samples: 1792/5760, Loss: 0.01357334852218628\n",
      "Epoch: 46, Samples: 1824/5760, Loss: 0.0066048502922058105\n",
      "Epoch: 46, Samples: 1856/5760, Loss: 0.012125879526138306\n",
      "Epoch: 46, Samples: 1888/5760, Loss: 0.013015061616897583\n",
      "Epoch: 46, Samples: 1920/5760, Loss: 0.00880509614944458\n",
      "Epoch: 46, Samples: 1952/5760, Loss: 0.007140398025512695\n",
      "Epoch: 46, Samples: 1984/5760, Loss: 0.006767183542251587\n",
      "Epoch: 46, Samples: 2016/5760, Loss: 0.0095444917678833\n",
      "Epoch: 46, Samples: 2048/5760, Loss: 0.007481664419174194\n",
      "Epoch: 46, Samples: 2080/5760, Loss: 0.012746930122375488\n",
      "Epoch: 46, Samples: 2112/5760, Loss: 0.010451644659042358\n",
      "Epoch: 46, Samples: 2144/5760, Loss: 0.004976540803909302\n",
      "Epoch: 46, Samples: 2176/5760, Loss: 0.010727107524871826\n",
      "Epoch: 46, Samples: 2208/5760, Loss: 0.01114422082901001\n",
      "Epoch: 46, Samples: 2240/5760, Loss: 0.01342061161994934\n",
      "Epoch: 46, Samples: 2272/5760, Loss: 0.006143838167190552\n",
      "Epoch: 46, Samples: 2304/5760, Loss: 0.005188405513763428\n",
      "Epoch: 46, Samples: 2336/5760, Loss: 0.006161540746688843\n",
      "Epoch: 46, Samples: 2368/5760, Loss: 0.0035200119018554688\n",
      "Epoch: 46, Samples: 2400/5760, Loss: 0.006444364786148071\n",
      "Epoch: 46, Samples: 2432/5760, Loss: 0.009817332029342651\n",
      "Epoch: 46, Samples: 2464/5760, Loss: 0.0131894052028656\n",
      "Epoch: 46, Samples: 2496/5760, Loss: 0.008318454027175903\n",
      "Epoch: 46, Samples: 2528/5760, Loss: 0.012959867715835571\n",
      "Epoch: 46, Samples: 2560/5760, Loss: 0.010182768106460571\n",
      "Epoch: 46, Samples: 2592/5760, Loss: 0.009607791900634766\n",
      "Epoch: 46, Samples: 2624/5760, Loss: 0.006499797105789185\n",
      "Epoch: 46, Samples: 2656/5760, Loss: 0.01623690128326416\n",
      "Epoch: 46, Samples: 2688/5760, Loss: 0.008777439594268799\n",
      "Epoch: 46, Samples: 2720/5760, Loss: 0.010710299015045166\n",
      "Epoch: 46, Samples: 2752/5760, Loss: 0.012101024389266968\n",
      "Epoch: 46, Samples: 2784/5760, Loss: 0.009092241525650024\n",
      "Epoch: 46, Samples: 2816/5760, Loss: 0.010547906160354614\n",
      "Epoch: 46, Samples: 2848/5760, Loss: 0.010932832956314087\n",
      "Epoch: 46, Samples: 2880/5760, Loss: 0.0195024311542511\n",
      "Epoch: 46, Samples: 2912/5760, Loss: 0.007072180509567261\n",
      "Epoch: 46, Samples: 2944/5760, Loss: 0.006783992052078247\n",
      "Epoch: 46, Samples: 2976/5760, Loss: 0.005419343709945679\n",
      "Epoch: 46, Samples: 3008/5760, Loss: 0.006605476140975952\n",
      "Epoch: 46, Samples: 3040/5760, Loss: 0.010886520147323608\n",
      "Epoch: 46, Samples: 3072/5760, Loss: 0.006334275007247925\n",
      "Epoch: 46, Samples: 3104/5760, Loss: 0.01180875301361084\n",
      "Epoch: 46, Samples: 3136/5760, Loss: 0.006277978420257568\n",
      "Epoch: 46, Samples: 3168/5760, Loss: 0.006522238254547119\n",
      "Epoch: 46, Samples: 3200/5760, Loss: 0.014767348766326904\n",
      "Epoch: 46, Samples: 3232/5760, Loss: 0.012748301029205322\n",
      "Epoch: 46, Samples: 3264/5760, Loss: 0.007037311792373657\n",
      "Epoch: 46, Samples: 3296/5760, Loss: 0.021268188953399658\n",
      "Epoch: 46, Samples: 3328/5760, Loss: 0.006567925214767456\n",
      "Epoch: 46, Samples: 3360/5760, Loss: 0.009463638067245483\n",
      "Epoch: 46, Samples: 3392/5760, Loss: 0.008325904607772827\n",
      "Epoch: 46, Samples: 3424/5760, Loss: 0.005141079425811768\n",
      "Epoch: 46, Samples: 3456/5760, Loss: 0.005471557378768921\n",
      "Epoch: 46, Samples: 3488/5760, Loss: 0.009256213903427124\n",
      "Epoch: 46, Samples: 3520/5760, Loss: 0.006353825330734253\n",
      "Epoch: 46, Samples: 3552/5760, Loss: 0.010961264371871948\n",
      "Epoch: 46, Samples: 3584/5760, Loss: 0.007956773042678833\n",
      "Epoch: 46, Samples: 3616/5760, Loss: 0.011452585458755493\n",
      "Epoch: 46, Samples: 3648/5760, Loss: 0.011103004217147827\n",
      "Epoch: 46, Samples: 3680/5760, Loss: 0.010974258184432983\n",
      "Epoch: 46, Samples: 3712/5760, Loss: 0.004730015993118286\n",
      "Epoch: 46, Samples: 3744/5760, Loss: 0.011796295642852783\n",
      "Epoch: 46, Samples: 3776/5760, Loss: 0.01855248212814331\n",
      "Epoch: 46, Samples: 3808/5760, Loss: 0.0072227418422698975\n",
      "Epoch: 46, Samples: 3840/5760, Loss: 0.008601158857345581\n",
      "Epoch: 46, Samples: 3872/5760, Loss: 0.008181214332580566\n",
      "Epoch: 46, Samples: 3904/5760, Loss: 0.007202267646789551\n",
      "Epoch: 46, Samples: 3936/5760, Loss: 0.005481451749801636\n",
      "Epoch: 46, Samples: 3968/5760, Loss: 0.00954592227935791\n",
      "Epoch: 46, Samples: 4000/5760, Loss: 0.0101584792137146\n",
      "Epoch: 46, Samples: 4032/5760, Loss: 0.0069596171379089355\n",
      "Epoch: 46, Samples: 4064/5760, Loss: 0.005042940378189087\n",
      "Epoch: 46, Samples: 4096/5760, Loss: 0.012640714645385742\n",
      "Epoch: 46, Samples: 4128/5760, Loss: 0.013270556926727295\n",
      "Epoch: 46, Samples: 4160/5760, Loss: 0.01316744089126587\n",
      "Epoch: 46, Samples: 4192/5760, Loss: 0.014327675104141235\n",
      "Epoch: 46, Samples: 4224/5760, Loss: 0.009613215923309326\n",
      "Epoch: 46, Samples: 4256/5760, Loss: 0.004771322011947632\n",
      "Epoch: 46, Samples: 4288/5760, Loss: 0.009580224752426147\n",
      "Epoch: 46, Samples: 4320/5760, Loss: 0.008207321166992188\n",
      "Epoch: 46, Samples: 4352/5760, Loss: 0.010317593812942505\n",
      "Epoch: 46, Samples: 4384/5760, Loss: 0.018765747547149658\n",
      "Epoch: 46, Samples: 4416/5760, Loss: 0.008547842502593994\n",
      "Epoch: 46, Samples: 4448/5760, Loss: 0.01930016279220581\n",
      "Epoch: 46, Samples: 4480/5760, Loss: 0.0076926350593566895\n",
      "Epoch: 46, Samples: 4512/5760, Loss: 0.008456528186798096\n",
      "Epoch: 46, Samples: 4544/5760, Loss: 0.01161348819732666\n",
      "Epoch: 46, Samples: 4576/5760, Loss: 0.01524418592453003\n",
      "Epoch: 46, Samples: 4608/5760, Loss: 0.009380608797073364\n",
      "Epoch: 46, Samples: 4640/5760, Loss: 0.005970954895019531\n",
      "Epoch: 46, Samples: 4672/5760, Loss: 0.012669265270233154\n",
      "Epoch: 46, Samples: 4704/5760, Loss: 0.010727554559707642\n",
      "Epoch: 46, Samples: 4736/5760, Loss: 0.009074926376342773\n",
      "Epoch: 46, Samples: 4768/5760, Loss: 0.005253851413726807\n",
      "Epoch: 46, Samples: 4800/5760, Loss: 0.00556105375289917\n",
      "Epoch: 46, Samples: 4832/5760, Loss: 0.0063728392124176025\n",
      "Epoch: 46, Samples: 4864/5760, Loss: 0.010047614574432373\n",
      "Epoch: 46, Samples: 4896/5760, Loss: 0.00892987847328186\n",
      "Epoch: 46, Samples: 4928/5760, Loss: 0.007414340972900391\n",
      "Epoch: 46, Samples: 4960/5760, Loss: 0.012147024273872375\n",
      "Epoch: 46, Samples: 4992/5760, Loss: 0.007465362548828125\n",
      "Epoch: 46, Samples: 5024/5760, Loss: 0.011472851037979126\n",
      "Epoch: 46, Samples: 5056/5760, Loss: 0.008182346820831299\n",
      "Epoch: 46, Samples: 5088/5760, Loss: 0.006144315004348755\n",
      "Epoch: 46, Samples: 5120/5760, Loss: 0.0096377432346344\n",
      "Epoch: 46, Samples: 5152/5760, Loss: 0.010904550552368164\n",
      "Epoch: 46, Samples: 5184/5760, Loss: 0.01193961501121521\n",
      "Epoch: 46, Samples: 5216/5760, Loss: 0.012563318014144897\n",
      "Epoch: 46, Samples: 5248/5760, Loss: 0.0035993456840515137\n",
      "Epoch: 46, Samples: 5280/5760, Loss: 0.011062711477279663\n",
      "Epoch: 46, Samples: 5312/5760, Loss: 0.007682770490646362\n",
      "Epoch: 46, Samples: 5344/5760, Loss: 0.006126433610916138\n",
      "Epoch: 46, Samples: 5376/5760, Loss: 0.019413888454437256\n",
      "Epoch: 46, Samples: 5408/5760, Loss: 0.010879695415496826\n",
      "Epoch: 46, Samples: 5440/5760, Loss: 0.0051653385162353516\n",
      "Epoch: 46, Samples: 5472/5760, Loss: 0.006245076656341553\n",
      "Epoch: 46, Samples: 5504/5760, Loss: 0.007378488779067993\n",
      "Epoch: 46, Samples: 5536/5760, Loss: 0.012091338634490967\n",
      "Epoch: 46, Samples: 5568/5760, Loss: 0.012366145849227905\n",
      "Epoch: 46, Samples: 5600/5760, Loss: 0.015441089868545532\n",
      "Epoch: 46, Samples: 5632/5760, Loss: 0.005480438470840454\n",
      "Epoch: 46, Samples: 5664/5760, Loss: 0.006617218255996704\n",
      "Epoch: 46, Samples: 5696/5760, Loss: 0.008153706789016724\n",
      "Epoch: 46, Samples: 5728/5760, Loss: 0.21667003631591797\n",
      "\n",
      "Epoch: 46\n",
      "Training set: Average loss: 0.0108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss: 0.3259, Accuracy: 751/818 (92%)\n",
      "Saving model (epoch 46) with lowest validation loss: 0.32594968435856014\n",
      "Epoch: 47, Samples: 0/5760, Loss: 0.007136911153793335\n",
      "Epoch: 47, Samples: 32/5760, Loss: 0.012286543846130371\n",
      "Epoch: 47, Samples: 64/5760, Loss: 0.003948777914047241\n",
      "Epoch: 47, Samples: 96/5760, Loss: 0.007755666971206665\n",
      "Epoch: 47, Samples: 128/5760, Loss: 0.008120715618133545\n",
      "Epoch: 47, Samples: 160/5760, Loss: 0.00634273886680603\n",
      "Epoch: 47, Samples: 192/5760, Loss: 0.00691106915473938\n",
      "Epoch: 47, Samples: 224/5760, Loss: 0.006502896547317505\n",
      "Epoch: 47, Samples: 256/5760, Loss: 0.005880892276763916\n",
      "Epoch: 47, Samples: 288/5760, Loss: 0.010006129741668701\n",
      "Epoch: 47, Samples: 320/5760, Loss: 0.011111795902252197\n",
      "Epoch: 47, Samples: 352/5760, Loss: 0.008767783641815186\n",
      "Epoch: 47, Samples: 384/5760, Loss: 0.00886523723602295\n",
      "Epoch: 47, Samples: 416/5760, Loss: 0.011189281940460205\n",
      "Epoch: 47, Samples: 448/5760, Loss: 0.004808157682418823\n",
      "Epoch: 47, Samples: 480/5760, Loss: 0.006635546684265137\n",
      "Epoch: 47, Samples: 512/5760, Loss: 0.007503330707550049\n",
      "Epoch: 47, Samples: 544/5760, Loss: 0.005283623933792114\n",
      "Epoch: 47, Samples: 576/5760, Loss: 0.008194804191589355\n",
      "Epoch: 47, Samples: 608/5760, Loss: 0.004732340574264526\n",
      "Epoch: 47, Samples: 640/5760, Loss: 0.011167585849761963\n",
      "Epoch: 47, Samples: 672/5760, Loss: 0.0054323673248291016\n",
      "Epoch: 47, Samples: 704/5760, Loss: 0.006824105978012085\n",
      "Epoch: 47, Samples: 736/5760, Loss: 0.013627171516418457\n",
      "Epoch: 47, Samples: 768/5760, Loss: 0.01420852541923523\n",
      "Epoch: 47, Samples: 800/5760, Loss: 0.010568857192993164\n",
      "Epoch: 47, Samples: 832/5760, Loss: 0.005400031805038452\n",
      "Epoch: 47, Samples: 864/5760, Loss: 0.0043516457080841064\n",
      "Epoch: 47, Samples: 896/5760, Loss: 0.00539398193359375\n",
      "Epoch: 47, Samples: 928/5760, Loss: 0.004250258207321167\n",
      "Epoch: 47, Samples: 960/5760, Loss: 0.011314600706100464\n",
      "Epoch: 47, Samples: 992/5760, Loss: 0.015200585126876831\n",
      "Epoch: 47, Samples: 1024/5760, Loss: 0.012332499027252197\n",
      "Epoch: 47, Samples: 1056/5760, Loss: 0.012569308280944824\n",
      "Epoch: 47, Samples: 1088/5760, Loss: 0.005516767501831055\n",
      "Epoch: 47, Samples: 1120/5760, Loss: 0.011631220579147339\n",
      "Epoch: 47, Samples: 1152/5760, Loss: 0.008501708507537842\n",
      "Epoch: 47, Samples: 1184/5760, Loss: 0.012601017951965332\n",
      "Epoch: 47, Samples: 1216/5760, Loss: 0.008576124906539917\n",
      "Epoch: 47, Samples: 1248/5760, Loss: 0.004464328289031982\n",
      "Epoch: 47, Samples: 1280/5760, Loss: 0.007508277893066406\n",
      "Epoch: 47, Samples: 1312/5760, Loss: 0.009974151849746704\n",
      "Epoch: 47, Samples: 1344/5760, Loss: 0.005683958530426025\n",
      "Epoch: 47, Samples: 1376/5760, Loss: 0.010273277759552002\n",
      "Epoch: 47, Samples: 1408/5760, Loss: 0.010925918817520142\n",
      "Epoch: 47, Samples: 1440/5760, Loss: 0.008130550384521484\n",
      "Epoch: 47, Samples: 1472/5760, Loss: 0.005605936050415039\n",
      "Epoch: 47, Samples: 1504/5760, Loss: 0.006161868572235107\n",
      "Epoch: 47, Samples: 1536/5760, Loss: 0.0065126121044158936\n",
      "Epoch: 47, Samples: 1568/5760, Loss: 0.011732995510101318\n",
      "Epoch: 47, Samples: 1600/5760, Loss: 0.007373213768005371\n",
      "Epoch: 47, Samples: 1632/5760, Loss: 0.009363561868667603\n",
      "Epoch: 47, Samples: 1664/5760, Loss: 0.011438518762588501\n",
      "Epoch: 47, Samples: 1696/5760, Loss: 0.0072037577629089355\n",
      "Epoch: 47, Samples: 1728/5760, Loss: 0.010603755712509155\n",
      "Epoch: 47, Samples: 1760/5760, Loss: 0.007066905498504639\n",
      "Epoch: 47, Samples: 1792/5760, Loss: 0.010550141334533691\n",
      "Epoch: 47, Samples: 1824/5760, Loss: 0.00816500186920166\n",
      "Epoch: 47, Samples: 1856/5760, Loss: 0.008811712265014648\n",
      "Epoch: 47, Samples: 1888/5760, Loss: 0.019041836261749268\n",
      "Epoch: 47, Samples: 1920/5760, Loss: 0.00921720266342163\n",
      "Epoch: 47, Samples: 1952/5760, Loss: 0.010123640298843384\n",
      "Epoch: 47, Samples: 1984/5760, Loss: 0.00865250825881958\n",
      "Epoch: 47, Samples: 2016/5760, Loss: 0.0070760250091552734\n",
      "Epoch: 47, Samples: 2048/5760, Loss: 0.010584741830825806\n",
      "Epoch: 47, Samples: 2080/5760, Loss: 0.014261573553085327\n",
      "Epoch: 47, Samples: 2112/5760, Loss: 0.014188885688781738\n",
      "Epoch: 47, Samples: 2144/5760, Loss: 0.01003795862197876\n",
      "Epoch: 47, Samples: 2176/5760, Loss: 0.008171647787094116\n",
      "Epoch: 47, Samples: 2208/5760, Loss: 0.012297958135604858\n",
      "Epoch: 47, Samples: 2240/5760, Loss: 0.009732723236083984\n",
      "Epoch: 47, Samples: 2272/5760, Loss: 0.014784008264541626\n",
      "Epoch: 47, Samples: 2304/5760, Loss: 0.010300815105438232\n",
      "Epoch: 47, Samples: 2336/5760, Loss: 0.010110646486282349\n",
      "Epoch: 47, Samples: 2368/5760, Loss: 0.011779636144638062\n",
      "Epoch: 47, Samples: 2400/5760, Loss: 0.01267862319946289\n",
      "Epoch: 47, Samples: 2432/5760, Loss: 0.009512841701507568\n",
      "Epoch: 47, Samples: 2464/5760, Loss: 0.005353838205337524\n",
      "Epoch: 47, Samples: 2496/5760, Loss: 0.008675724267959595\n",
      "Epoch: 47, Samples: 2528/5760, Loss: 0.017281264066696167\n",
      "Epoch: 47, Samples: 2560/5760, Loss: 0.00941312313079834\n",
      "Epoch: 47, Samples: 2592/5760, Loss: 0.0043100714683532715\n",
      "Epoch: 47, Samples: 2624/5760, Loss: 0.008182108402252197\n",
      "Epoch: 47, Samples: 2656/5760, Loss: 0.010862141847610474\n",
      "Epoch: 47, Samples: 2688/5760, Loss: 0.005702674388885498\n",
      "Epoch: 47, Samples: 2720/5760, Loss: 0.005682438611984253\n",
      "Epoch: 47, Samples: 2752/5760, Loss: 0.00467565655708313\n",
      "Epoch: 47, Samples: 2784/5760, Loss: 0.005259960889816284\n",
      "Epoch: 47, Samples: 2816/5760, Loss: 0.005270153284072876\n",
      "Epoch: 47, Samples: 2848/5760, Loss: 0.00618782639503479\n",
      "Epoch: 47, Samples: 2880/5760, Loss: 0.010232746601104736\n",
      "Epoch: 47, Samples: 2912/5760, Loss: 0.007153421640396118\n",
      "Epoch: 47, Samples: 2944/5760, Loss: 0.01755353808403015\n",
      "Epoch: 47, Samples: 2976/5760, Loss: 0.009034276008605957\n",
      "Epoch: 47, Samples: 3008/5760, Loss: 0.012500137090682983\n",
      "Epoch: 47, Samples: 3040/5760, Loss: 0.007181614637374878\n",
      "Epoch: 47, Samples: 3072/5760, Loss: 0.019607335329055786\n",
      "Epoch: 47, Samples: 3104/5760, Loss: 0.014715641736984253\n",
      "Epoch: 47, Samples: 3136/5760, Loss: 0.01613941788673401\n",
      "Epoch: 47, Samples: 3168/5760, Loss: 0.00781753659248352\n",
      "Epoch: 47, Samples: 3200/5760, Loss: 0.009074002504348755\n",
      "Epoch: 47, Samples: 3232/5760, Loss: 0.004303663969039917\n",
      "Epoch: 47, Samples: 3264/5760, Loss: 0.010477930307388306\n",
      "Epoch: 47, Samples: 3296/5760, Loss: 0.016399085521697998\n",
      "Epoch: 47, Samples: 3328/5760, Loss: 0.009044617414474487\n",
      "Epoch: 47, Samples: 3360/5760, Loss: 0.010392606258392334\n",
      "Epoch: 47, Samples: 3392/5760, Loss: 0.012870639562606812\n",
      "Epoch: 47, Samples: 3424/5760, Loss: 0.005326598882675171\n",
      "Epoch: 47, Samples: 3456/5760, Loss: 0.006242156028747559\n",
      "Epoch: 47, Samples: 3488/5760, Loss: 0.010363399982452393\n",
      "Epoch: 47, Samples: 3520/5760, Loss: 0.017458856105804443\n",
      "Epoch: 47, Samples: 3552/5760, Loss: 0.011709511280059814\n",
      "Epoch: 47, Samples: 3584/5760, Loss: 0.011770814657211304\n",
      "Epoch: 47, Samples: 3616/5760, Loss: 0.01116025447845459\n",
      "Epoch: 47, Samples: 3648/5760, Loss: 0.011360198259353638\n",
      "Epoch: 47, Samples: 3680/5760, Loss: 0.013890355825424194\n",
      "Epoch: 47, Samples: 3712/5760, Loss: 0.005575984716415405\n",
      "Epoch: 47, Samples: 3744/5760, Loss: 0.01627051830291748\n",
      "Epoch: 47, Samples: 3776/5760, Loss: 0.012167751789093018\n",
      "Epoch: 47, Samples: 3808/5760, Loss: 0.006925642490386963\n",
      "Epoch: 47, Samples: 3840/5760, Loss: 0.013181835412979126\n",
      "Epoch: 47, Samples: 3872/5760, Loss: 0.004301249980926514\n",
      "Epoch: 47, Samples: 3904/5760, Loss: 0.006883591413497925\n",
      "Epoch: 47, Samples: 3936/5760, Loss: 0.01902356743812561\n",
      "Epoch: 47, Samples: 3968/5760, Loss: 0.017389684915542603\n",
      "Epoch: 47, Samples: 4000/5760, Loss: 0.010900050401687622\n",
      "Epoch: 47, Samples: 4032/5760, Loss: 0.0031587183475494385\n",
      "Epoch: 47, Samples: 4064/5760, Loss: 0.015354275703430176\n",
      "Epoch: 47, Samples: 4096/5760, Loss: 0.00890997052192688\n",
      "Epoch: 47, Samples: 4128/5760, Loss: 0.0070209503173828125\n",
      "Epoch: 47, Samples: 4160/5760, Loss: 0.008845865726470947\n",
      "Epoch: 47, Samples: 4192/5760, Loss: 0.008698374032974243\n",
      "Epoch: 47, Samples: 4224/5760, Loss: 0.016014516353607178\n",
      "Epoch: 47, Samples: 4256/5760, Loss: 0.0061675310134887695\n",
      "Epoch: 47, Samples: 4288/5760, Loss: 0.010145485401153564\n",
      "Epoch: 47, Samples: 4320/5760, Loss: 0.00884920358657837\n",
      "Epoch: 47, Samples: 4352/5760, Loss: 0.006894886493682861\n",
      "Epoch: 47, Samples: 4384/5760, Loss: 0.00777927041053772\n",
      "Epoch: 47, Samples: 4416/5760, Loss: 0.011109709739685059\n",
      "Epoch: 47, Samples: 4448/5760, Loss: 0.008897155523300171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Samples: 4480/5760, Loss: 0.005606889724731445\n",
      "Epoch: 47, Samples: 4512/5760, Loss: 0.00587993860244751\n",
      "Epoch: 47, Samples: 4544/5760, Loss: 0.014791399240493774\n",
      "Epoch: 47, Samples: 4576/5760, Loss: 0.011939376592636108\n",
      "Epoch: 47, Samples: 4608/5760, Loss: 0.01596096158027649\n",
      "Epoch: 47, Samples: 4640/5760, Loss: 0.010197281837463379\n",
      "Epoch: 47, Samples: 4672/5760, Loss: 0.004083096981048584\n",
      "Epoch: 47, Samples: 4704/5760, Loss: 0.008379369974136353\n",
      "Epoch: 47, Samples: 4736/5760, Loss: 0.004507273435592651\n",
      "Epoch: 47, Samples: 4768/5760, Loss: 0.008018970489501953\n",
      "Epoch: 47, Samples: 4800/5760, Loss: 0.012430131435394287\n",
      "Epoch: 47, Samples: 4832/5760, Loss: 0.0071144700050354\n",
      "Epoch: 47, Samples: 4864/5760, Loss: 0.004157334566116333\n",
      "Epoch: 47, Samples: 4896/5760, Loss: 0.007173299789428711\n",
      "Epoch: 47, Samples: 4928/5760, Loss: 0.009181320667266846\n",
      "Epoch: 47, Samples: 4960/5760, Loss: 0.00848233699798584\n",
      "Epoch: 47, Samples: 4992/5760, Loss: 0.005824685096740723\n",
      "Epoch: 47, Samples: 5024/5760, Loss: 0.022588133811950684\n",
      "Epoch: 47, Samples: 5056/5760, Loss: 0.0074586570262908936\n",
      "Epoch: 47, Samples: 5088/5760, Loss: 0.005180627107620239\n",
      "Epoch: 47, Samples: 5120/5760, Loss: 0.010972380638122559\n",
      "Epoch: 47, Samples: 5152/5760, Loss: 0.012252867221832275\n",
      "Epoch: 47, Samples: 5184/5760, Loss: 0.005271404981613159\n",
      "Epoch: 47, Samples: 5216/5760, Loss: 0.009539186954498291\n",
      "Epoch: 47, Samples: 5248/5760, Loss: 0.0070536136627197266\n",
      "Epoch: 47, Samples: 5280/5760, Loss: 0.010452896356582642\n",
      "Epoch: 47, Samples: 5312/5760, Loss: 0.005704909563064575\n",
      "Epoch: 47, Samples: 5344/5760, Loss: 0.017764270305633545\n",
      "Epoch: 47, Samples: 5376/5760, Loss: 0.011468619108200073\n",
      "Epoch: 47, Samples: 5408/5760, Loss: 0.007776886224746704\n",
      "Epoch: 47, Samples: 5440/5760, Loss: 0.013175874948501587\n",
      "Epoch: 47, Samples: 5472/5760, Loss: 0.013194739818572998\n",
      "Epoch: 47, Samples: 5504/5760, Loss: 0.010298728942871094\n",
      "Epoch: 47, Samples: 5536/5760, Loss: 0.007849723100662231\n",
      "Epoch: 47, Samples: 5568/5760, Loss: 0.007848352193832397\n",
      "Epoch: 47, Samples: 5600/5760, Loss: 0.010634511709213257\n",
      "Epoch: 47, Samples: 5632/5760, Loss: 0.00819861888885498\n",
      "Epoch: 47, Samples: 5664/5760, Loss: 0.006053924560546875\n",
      "Epoch: 47, Samples: 5696/5760, Loss: 0.008807957172393799\n",
      "Epoch: 47, Samples: 5728/5760, Loss: 0.33463263511657715\n",
      "\n",
      "Epoch: 47\n",
      "Training set: Average loss: 0.0113\n",
      "Validation set: Average loss: 0.3415, Accuracy: 748/818 (91%)\n",
      "Epoch: 48, Samples: 0/5760, Loss: 0.004028111696243286\n",
      "Epoch: 48, Samples: 32/5760, Loss: 0.006903707981109619\n",
      "Epoch: 48, Samples: 64/5760, Loss: 0.008234858512878418\n",
      "Epoch: 48, Samples: 96/5760, Loss: 0.007551670074462891\n",
      "Epoch: 48, Samples: 128/5760, Loss: 0.008813261985778809\n",
      "Epoch: 48, Samples: 160/5760, Loss: 0.006247282028198242\n",
      "Epoch: 48, Samples: 192/5760, Loss: 0.009925007820129395\n",
      "Epoch: 48, Samples: 224/5760, Loss: 0.007260352373123169\n",
      "Epoch: 48, Samples: 256/5760, Loss: 0.0115165114402771\n",
      "Epoch: 48, Samples: 288/5760, Loss: 0.00828564167022705\n",
      "Epoch: 48, Samples: 320/5760, Loss: 0.002319842576980591\n",
      "Epoch: 48, Samples: 352/5760, Loss: 0.010235518217086792\n",
      "Epoch: 48, Samples: 384/5760, Loss: 0.006314665079116821\n",
      "Epoch: 48, Samples: 416/5760, Loss: 0.008506804704666138\n",
      "Epoch: 48, Samples: 448/5760, Loss: 0.007017999887466431\n",
      "Epoch: 48, Samples: 480/5760, Loss: 0.009377837181091309\n",
      "Epoch: 48, Samples: 512/5760, Loss: 0.010206997394561768\n",
      "Epoch: 48, Samples: 544/5760, Loss: 0.006756007671356201\n",
      "Epoch: 48, Samples: 576/5760, Loss: 0.008515387773513794\n",
      "Epoch: 48, Samples: 608/5760, Loss: 0.007013380527496338\n",
      "Epoch: 48, Samples: 640/5760, Loss: 0.015492647886276245\n",
      "Epoch: 48, Samples: 672/5760, Loss: 0.014640390872955322\n",
      "Epoch: 48, Samples: 704/5760, Loss: 0.01942373812198639\n",
      "Epoch: 48, Samples: 736/5760, Loss: 0.01538512110710144\n",
      "Epoch: 48, Samples: 768/5760, Loss: 0.006624996662139893\n",
      "Epoch: 48, Samples: 800/5760, Loss: 0.005621433258056641\n",
      "Epoch: 48, Samples: 832/5760, Loss: 0.004056006669998169\n",
      "Epoch: 48, Samples: 864/5760, Loss: 0.011309206485748291\n",
      "Epoch: 48, Samples: 896/5760, Loss: 0.0097484290599823\n",
      "Epoch: 48, Samples: 928/5760, Loss: 0.0247022807598114\n",
      "Epoch: 48, Samples: 960/5760, Loss: 0.009240329265594482\n",
      "Epoch: 48, Samples: 992/5760, Loss: 0.008782774209976196\n",
      "Epoch: 48, Samples: 1024/5760, Loss: 0.007478773593902588\n",
      "Epoch: 48, Samples: 1056/5760, Loss: 0.009028404951095581\n",
      "Epoch: 48, Samples: 1088/5760, Loss: 0.008933663368225098\n",
      "Epoch: 48, Samples: 1120/5760, Loss: 0.005857646465301514\n",
      "Epoch: 48, Samples: 1152/5760, Loss: 0.02967187762260437\n",
      "Epoch: 48, Samples: 1184/5760, Loss: 0.01075965166091919\n",
      "Epoch: 48, Samples: 1216/5760, Loss: 0.0098094642162323\n",
      "Epoch: 48, Samples: 1248/5760, Loss: 0.010767132043838501\n",
      "Epoch: 48, Samples: 1280/5760, Loss: 0.009848028421401978\n",
      "Epoch: 48, Samples: 1312/5760, Loss: 0.0060509443283081055\n",
      "Epoch: 48, Samples: 1344/5760, Loss: 0.0166960209608078\n",
      "Epoch: 48, Samples: 1376/5760, Loss: 0.00877833366394043\n",
      "Epoch: 48, Samples: 1408/5760, Loss: 0.009150832891464233\n",
      "Epoch: 48, Samples: 1440/5760, Loss: 0.007735073566436768\n",
      "Epoch: 48, Samples: 1472/5760, Loss: 0.006161212921142578\n",
      "Epoch: 48, Samples: 1504/5760, Loss: 0.00555795431137085\n",
      "Epoch: 48, Samples: 1536/5760, Loss: 0.011276811361312866\n",
      "Epoch: 48, Samples: 1568/5760, Loss: 0.004705935716629028\n",
      "Epoch: 48, Samples: 1600/5760, Loss: 0.013603359460830688\n",
      "Epoch: 48, Samples: 1632/5760, Loss: 0.0067708492279052734\n",
      "Epoch: 48, Samples: 1664/5760, Loss: 0.01355627179145813\n",
      "Epoch: 48, Samples: 1696/5760, Loss: 0.004738211631774902\n",
      "Epoch: 48, Samples: 1728/5760, Loss: 0.006377100944519043\n",
      "Epoch: 48, Samples: 1760/5760, Loss: 0.01322925090789795\n",
      "Epoch: 48, Samples: 1792/5760, Loss: 0.007468819618225098\n",
      "Epoch: 48, Samples: 1824/5760, Loss: 0.009199947118759155\n",
      "Epoch: 48, Samples: 1856/5760, Loss: 0.01712477207183838\n",
      "Epoch: 48, Samples: 1888/5760, Loss: 0.009669065475463867\n",
      "Epoch: 48, Samples: 1920/5760, Loss: 0.01053658127784729\n",
      "Epoch: 48, Samples: 1952/5760, Loss: 0.003516584634780884\n",
      "Epoch: 48, Samples: 1984/5760, Loss: 0.013689130544662476\n",
      "Epoch: 48, Samples: 2016/5760, Loss: 0.014187783002853394\n",
      "Epoch: 48, Samples: 2048/5760, Loss: 0.005473464727401733\n",
      "Epoch: 48, Samples: 2080/5760, Loss: 0.007945030927658081\n",
      "Epoch: 48, Samples: 2112/5760, Loss: 0.01190975308418274\n",
      "Epoch: 48, Samples: 2144/5760, Loss: 0.006488323211669922\n",
      "Epoch: 48, Samples: 2176/5760, Loss: 0.01767304539680481\n",
      "Epoch: 48, Samples: 2208/5760, Loss: 0.008842378854751587\n",
      "Epoch: 48, Samples: 2240/5760, Loss: 0.008481591939926147\n",
      "Epoch: 48, Samples: 2272/5760, Loss: 0.006951093673706055\n",
      "Epoch: 48, Samples: 2304/5760, Loss: 0.003298848867416382\n",
      "Epoch: 48, Samples: 2336/5760, Loss: 0.00854453444480896\n",
      "Epoch: 48, Samples: 2368/5760, Loss: 0.007188975811004639\n",
      "Epoch: 48, Samples: 2400/5760, Loss: 0.006445020437240601\n",
      "Epoch: 48, Samples: 2432/5760, Loss: 0.012199103832244873\n",
      "Epoch: 48, Samples: 2464/5760, Loss: 0.0037521719932556152\n",
      "Epoch: 48, Samples: 2496/5760, Loss: 0.008273005485534668\n",
      "Epoch: 48, Samples: 2528/5760, Loss: 0.004205137491226196\n",
      "Epoch: 48, Samples: 2560/5760, Loss: 0.005374908447265625\n",
      "Epoch: 48, Samples: 2592/5760, Loss: 0.006873577833175659\n",
      "Epoch: 48, Samples: 2624/5760, Loss: 0.02544693648815155\n",
      "Epoch: 48, Samples: 2656/5760, Loss: 0.007168948650360107\n",
      "Epoch: 48, Samples: 2688/5760, Loss: 0.013641804456710815\n",
      "Epoch: 48, Samples: 2720/5760, Loss: 0.010200589895248413\n",
      "Epoch: 48, Samples: 2752/5760, Loss: 0.004583269357681274\n",
      "Epoch: 48, Samples: 2784/5760, Loss: 0.006379663944244385\n",
      "Epoch: 48, Samples: 2816/5760, Loss: 0.008438974618911743\n",
      "Epoch: 48, Samples: 2848/5760, Loss: 0.004256933927536011\n",
      "Epoch: 48, Samples: 2880/5760, Loss: 0.0065003931522369385\n",
      "Epoch: 48, Samples: 2912/5760, Loss: 0.004800170660018921\n",
      "Epoch: 48, Samples: 2944/5760, Loss: 0.008065760135650635\n",
      "Epoch: 48, Samples: 2976/5760, Loss: 0.013303190469741821\n",
      "Epoch: 48, Samples: 3008/5760, Loss: 0.004556030035018921\n",
      "Epoch: 48, Samples: 3040/5760, Loss: 0.008827269077301025\n",
      "Epoch: 48, Samples: 3072/5760, Loss: 0.007086247205734253\n",
      "Epoch: 48, Samples: 3104/5760, Loss: 0.009465664625167847\n",
      "Epoch: 48, Samples: 3136/5760, Loss: 0.0061279237270355225\n",
      "Epoch: 48, Samples: 3168/5760, Loss: 0.004400521516799927\n",
      "Epoch: 48, Samples: 3200/5760, Loss: 0.0074600279331207275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48, Samples: 3232/5760, Loss: 0.013112753629684448\n",
      "Epoch: 48, Samples: 3264/5760, Loss: 0.006318002939224243\n",
      "Epoch: 48, Samples: 3296/5760, Loss: 0.00819927453994751\n",
      "Epoch: 48, Samples: 3328/5760, Loss: 0.011329621076583862\n",
      "Epoch: 48, Samples: 3360/5760, Loss: 0.006364405155181885\n",
      "Epoch: 48, Samples: 3392/5760, Loss: 0.004953235387802124\n",
      "Epoch: 48, Samples: 3424/5760, Loss: 0.008685410022735596\n",
      "Epoch: 48, Samples: 3456/5760, Loss: 0.012883782386779785\n",
      "Epoch: 48, Samples: 3488/5760, Loss: 0.02392953634262085\n",
      "Epoch: 48, Samples: 3520/5760, Loss: 0.008068293333053589\n",
      "Epoch: 48, Samples: 3552/5760, Loss: 0.008302301168441772\n",
      "Epoch: 48, Samples: 3584/5760, Loss: 0.0065024495124816895\n",
      "Epoch: 48, Samples: 3616/5760, Loss: 0.009308189153671265\n",
      "Epoch: 48, Samples: 3648/5760, Loss: 0.006617933511734009\n",
      "Epoch: 48, Samples: 3680/5760, Loss: 0.010849148035049438\n",
      "Epoch: 48, Samples: 3712/5760, Loss: 0.005703568458557129\n",
      "Epoch: 48, Samples: 3744/5760, Loss: 0.00785115361213684\n",
      "Epoch: 48, Samples: 3776/5760, Loss: 0.006844639778137207\n",
      "Epoch: 48, Samples: 3808/5760, Loss: 0.01200026273727417\n",
      "Epoch: 48, Samples: 3840/5760, Loss: 0.006987273693084717\n",
      "Epoch: 48, Samples: 3872/5760, Loss: 0.004526048898696899\n",
      "Epoch: 48, Samples: 3904/5760, Loss: 0.006943643093109131\n",
      "Epoch: 48, Samples: 3936/5760, Loss: 0.0070095062255859375\n",
      "Epoch: 48, Samples: 3968/5760, Loss: 0.02440643310546875\n",
      "Epoch: 48, Samples: 4000/5760, Loss: 0.011105954647064209\n",
      "Epoch: 48, Samples: 4032/5760, Loss: 0.004881739616394043\n",
      "Epoch: 48, Samples: 4064/5760, Loss: 0.01114317774772644\n",
      "Epoch: 48, Samples: 4096/5760, Loss: 0.00505027174949646\n",
      "Epoch: 48, Samples: 4128/5760, Loss: 0.004575520753860474\n",
      "Epoch: 48, Samples: 4160/5760, Loss: 0.005275815725326538\n",
      "Epoch: 48, Samples: 4192/5760, Loss: 0.005322813987731934\n",
      "Epoch: 48, Samples: 4224/5760, Loss: 0.007848769426345825\n",
      "Epoch: 48, Samples: 4256/5760, Loss: 0.010147958993911743\n",
      "Epoch: 48, Samples: 4288/5760, Loss: 0.017066389322280884\n",
      "Epoch: 48, Samples: 4320/5760, Loss: 0.013392031192779541\n",
      "Epoch: 48, Samples: 4352/5760, Loss: 0.00905066728591919\n",
      "Epoch: 48, Samples: 4384/5760, Loss: 0.005001038312911987\n",
      "Epoch: 48, Samples: 4416/5760, Loss: 0.0112551748752594\n",
      "Epoch: 48, Samples: 4448/5760, Loss: 0.004554927349090576\n",
      "Epoch: 48, Samples: 4480/5760, Loss: 0.0030675530433654785\n",
      "Epoch: 48, Samples: 4512/5760, Loss: 0.010701984167098999\n",
      "Epoch: 48, Samples: 4544/5760, Loss: 0.013257414102554321\n",
      "Epoch: 48, Samples: 4576/5760, Loss: 0.0056629180908203125\n",
      "Epoch: 48, Samples: 4608/5760, Loss: 0.009816735982894897\n",
      "Epoch: 48, Samples: 4640/5760, Loss: 0.006836801767349243\n",
      "Epoch: 48, Samples: 4672/5760, Loss: 0.008537828922271729\n",
      "Epoch: 48, Samples: 4704/5760, Loss: 0.011834383010864258\n",
      "Epoch: 48, Samples: 4736/5760, Loss: 0.007939130067825317\n",
      "Epoch: 48, Samples: 4768/5760, Loss: 0.01669764518737793\n",
      "Epoch: 48, Samples: 4800/5760, Loss: 0.006218045949935913\n",
      "Epoch: 48, Samples: 4832/5760, Loss: 0.01129564642906189\n",
      "Epoch: 48, Samples: 4864/5760, Loss: 0.00641968846321106\n",
      "Epoch: 48, Samples: 4896/5760, Loss: 0.013328582048416138\n",
      "Epoch: 48, Samples: 4928/5760, Loss: 0.008384078741073608\n",
      "Epoch: 48, Samples: 4960/5760, Loss: 0.008120685815811157\n",
      "Epoch: 48, Samples: 4992/5760, Loss: 0.006314367055892944\n",
      "Epoch: 48, Samples: 5024/5760, Loss: 0.012396633625030518\n",
      "Epoch: 48, Samples: 5056/5760, Loss: 0.005270779132843018\n",
      "Epoch: 48, Samples: 5088/5760, Loss: 0.012811005115509033\n",
      "Epoch: 48, Samples: 5120/5760, Loss: 0.00898006558418274\n",
      "Epoch: 48, Samples: 5152/5760, Loss: 0.006339907646179199\n",
      "Epoch: 48, Samples: 5184/5760, Loss: 0.007853031158447266\n",
      "Epoch: 48, Samples: 5216/5760, Loss: 0.009138345718383789\n",
      "Epoch: 48, Samples: 5248/5760, Loss: 0.011542528867721558\n",
      "Epoch: 48, Samples: 5280/5760, Loss: 0.00780835747718811\n",
      "Epoch: 48, Samples: 5312/5760, Loss: 0.037443339824676514\n",
      "Epoch: 48, Samples: 5344/5760, Loss: 0.008685678243637085\n",
      "Epoch: 48, Samples: 5376/5760, Loss: 0.022300690412521362\n",
      "Epoch: 48, Samples: 5408/5760, Loss: 0.0103873610496521\n",
      "Epoch: 48, Samples: 5440/5760, Loss: 0.00871613621711731\n",
      "Epoch: 48, Samples: 5472/5760, Loss: 0.017637193202972412\n",
      "Epoch: 48, Samples: 5504/5760, Loss: 0.008082717657089233\n",
      "Epoch: 48, Samples: 5536/5760, Loss: 0.011892139911651611\n",
      "Epoch: 48, Samples: 5568/5760, Loss: 0.0077288150787353516\n",
      "Epoch: 48, Samples: 5600/5760, Loss: 0.012992918491363525\n",
      "Epoch: 48, Samples: 5632/5760, Loss: 0.00702279806137085\n",
      "Epoch: 48, Samples: 5664/5760, Loss: 0.009766072034835815\n",
      "Epoch: 48, Samples: 5696/5760, Loss: 0.005077451467514038\n",
      "Epoch: 48, Samples: 5728/5760, Loss: 0.27911436557769775\n",
      "\n",
      "Epoch: 48\n",
      "Training set: Average loss: 0.0109\n",
      "Validation set: Average loss: 0.3337, Accuracy: 748/818 (91%)\n",
      "Epoch: 49, Samples: 0/5760, Loss: 0.01798558235168457\n",
      "Epoch: 49, Samples: 32/5760, Loss: 0.006945013999938965\n",
      "Epoch: 49, Samples: 64/5760, Loss: 0.009486287832260132\n",
      "Epoch: 49, Samples: 96/5760, Loss: 0.005058735609054565\n",
      "Epoch: 49, Samples: 128/5760, Loss: 0.011536657810211182\n",
      "Epoch: 49, Samples: 160/5760, Loss: 0.006645947694778442\n",
      "Epoch: 49, Samples: 192/5760, Loss: 0.005293309688568115\n",
      "Epoch: 49, Samples: 224/5760, Loss: 0.0055761635303497314\n",
      "Epoch: 49, Samples: 256/5760, Loss: 0.007104605436325073\n",
      "Epoch: 49, Samples: 288/5760, Loss: 0.0071749091148376465\n",
      "Epoch: 49, Samples: 320/5760, Loss: 0.0038290023803710938\n",
      "Epoch: 49, Samples: 352/5760, Loss: 0.0040323734283447266\n",
      "Epoch: 49, Samples: 384/5760, Loss: 0.008977621793746948\n",
      "Epoch: 49, Samples: 416/5760, Loss: 0.010815978050231934\n",
      "Epoch: 49, Samples: 448/5760, Loss: 0.007729440927505493\n",
      "Epoch: 49, Samples: 480/5760, Loss: 0.005356431007385254\n",
      "Epoch: 49, Samples: 512/5760, Loss: 0.011017799377441406\n",
      "Epoch: 49, Samples: 544/5760, Loss: 0.0074211955070495605\n",
      "Epoch: 49, Samples: 576/5760, Loss: 0.008086085319519043\n",
      "Epoch: 49, Samples: 608/5760, Loss: 0.007512807846069336\n",
      "Epoch: 49, Samples: 640/5760, Loss: 0.005683690309524536\n",
      "Epoch: 49, Samples: 672/5760, Loss: 0.009032607078552246\n",
      "Epoch: 49, Samples: 704/5760, Loss: 0.007263392210006714\n",
      "Epoch: 49, Samples: 736/5760, Loss: 0.002614140510559082\n",
      "Epoch: 49, Samples: 768/5760, Loss: 0.01665058732032776\n",
      "Epoch: 49, Samples: 800/5760, Loss: 0.006647080183029175\n",
      "Epoch: 49, Samples: 832/5760, Loss: 0.015388548374176025\n",
      "Epoch: 49, Samples: 864/5760, Loss: 0.006574660539627075\n",
      "Epoch: 49, Samples: 896/5760, Loss: 0.008985012769699097\n",
      "Epoch: 49, Samples: 928/5760, Loss: 0.011497199535369873\n",
      "Epoch: 49, Samples: 960/5760, Loss: 0.010977119207382202\n",
      "Epoch: 49, Samples: 992/5760, Loss: 0.01920458674430847\n",
      "Epoch: 49, Samples: 1024/5760, Loss: 0.00513911247253418\n",
      "Epoch: 49, Samples: 1056/5760, Loss: 0.009589225053787231\n",
      "Epoch: 49, Samples: 1088/5760, Loss: 0.01889348030090332\n",
      "Epoch: 49, Samples: 1120/5760, Loss: 0.010112196207046509\n",
      "Epoch: 49, Samples: 1152/5760, Loss: 0.014183729887008667\n",
      "Epoch: 49, Samples: 1184/5760, Loss: 0.008676409721374512\n",
      "Epoch: 49, Samples: 1216/5760, Loss: 0.005647212266921997\n",
      "Epoch: 49, Samples: 1248/5760, Loss: 0.005699127912521362\n",
      "Epoch: 49, Samples: 1280/5760, Loss: 0.01139676570892334\n",
      "Epoch: 49, Samples: 1312/5760, Loss: 0.006612002849578857\n",
      "Epoch: 49, Samples: 1344/5760, Loss: 0.012601912021636963\n",
      "Epoch: 49, Samples: 1376/5760, Loss: 0.004677146673202515\n",
      "Epoch: 49, Samples: 1408/5760, Loss: 0.0040434300899505615\n",
      "Epoch: 49, Samples: 1440/5760, Loss: 0.005737632513046265\n",
      "Epoch: 49, Samples: 1472/5760, Loss: 0.007991373538970947\n",
      "Epoch: 49, Samples: 1504/5760, Loss: 0.005590170621871948\n",
      "Epoch: 49, Samples: 1536/5760, Loss: 0.005214691162109375\n",
      "Epoch: 49, Samples: 1568/5760, Loss: 0.007356375455856323\n",
      "Epoch: 49, Samples: 1600/5760, Loss: 0.007293105125427246\n",
      "Epoch: 49, Samples: 1632/5760, Loss: 0.0067774951457977295\n",
      "Epoch: 49, Samples: 1664/5760, Loss: 0.007124006748199463\n",
      "Epoch: 49, Samples: 1696/5760, Loss: 0.005534559488296509\n",
      "Epoch: 49, Samples: 1728/5760, Loss: 0.005919426679611206\n",
      "Epoch: 49, Samples: 1760/5760, Loss: 0.00534212589263916\n",
      "Epoch: 49, Samples: 1792/5760, Loss: 0.010357588529586792\n",
      "Epoch: 49, Samples: 1824/5760, Loss: 0.013933241367340088\n",
      "Epoch: 49, Samples: 1856/5760, Loss: 0.0050469934940338135\n",
      "Epoch: 49, Samples: 1888/5760, Loss: 0.006298273801803589\n",
      "Epoch: 49, Samples: 1920/5760, Loss: 0.011935144662857056\n",
      "Epoch: 49, Samples: 1952/5760, Loss: 0.006600558757781982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49, Samples: 1984/5760, Loss: 0.008545279502868652\n",
      "Epoch: 49, Samples: 2016/5760, Loss: 0.008449345827102661\n",
      "Epoch: 49, Samples: 2048/5760, Loss: 0.007699847221374512\n",
      "Epoch: 49, Samples: 2080/5760, Loss: 0.008080869913101196\n",
      "Epoch: 49, Samples: 2112/5760, Loss: 0.006593376398086548\n",
      "Epoch: 49, Samples: 2144/5760, Loss: 0.007178068161010742\n",
      "Epoch: 49, Samples: 2176/5760, Loss: 0.0035816431045532227\n",
      "Epoch: 49, Samples: 2208/5760, Loss: 0.0047699809074401855\n",
      "Epoch: 49, Samples: 2240/5760, Loss: 0.005134373903274536\n",
      "Epoch: 49, Samples: 2272/5760, Loss: 0.007557928562164307\n",
      "Epoch: 49, Samples: 2304/5760, Loss: 0.004904389381408691\n",
      "Epoch: 49, Samples: 2336/5760, Loss: 0.006292432546615601\n",
      "Epoch: 49, Samples: 2368/5760, Loss: 0.007853895425796509\n",
      "Epoch: 49, Samples: 2400/5760, Loss: 0.007225692272186279\n",
      "Epoch: 49, Samples: 2432/5760, Loss: 0.006448477506637573\n",
      "Epoch: 49, Samples: 2464/5760, Loss: 0.007839202880859375\n",
      "Epoch: 49, Samples: 2496/5760, Loss: 0.006693989038467407\n",
      "Epoch: 49, Samples: 2528/5760, Loss: 0.008132576942443848\n",
      "Epoch: 49, Samples: 2560/5760, Loss: 0.007626146078109741\n",
      "Epoch: 49, Samples: 2592/5760, Loss: 0.008004933595657349\n",
      "Epoch: 49, Samples: 2624/5760, Loss: 0.009754955768585205\n",
      "Epoch: 49, Samples: 2656/5760, Loss: 0.016511350870132446\n",
      "Epoch: 49, Samples: 2688/5760, Loss: 0.009366244077682495\n",
      "Epoch: 49, Samples: 2720/5760, Loss: 0.01116335391998291\n",
      "Epoch: 49, Samples: 2752/5760, Loss: 0.013461410999298096\n",
      "Epoch: 49, Samples: 2784/5760, Loss: 0.010989964008331299\n",
      "Epoch: 49, Samples: 2816/5760, Loss: 0.009215056896209717\n",
      "Epoch: 49, Samples: 2848/5760, Loss: 0.012954771518707275\n",
      "Epoch: 49, Samples: 2880/5760, Loss: 0.006483972072601318\n",
      "Epoch: 49, Samples: 2912/5760, Loss: 0.011315077543258667\n",
      "Epoch: 49, Samples: 2944/5760, Loss: 0.0074073076248168945\n",
      "Epoch: 49, Samples: 2976/5760, Loss: 0.014344245195388794\n",
      "Epoch: 49, Samples: 3008/5760, Loss: 0.012210488319396973\n",
      "Epoch: 49, Samples: 3040/5760, Loss: 0.00503203272819519\n",
      "Epoch: 49, Samples: 3072/5760, Loss: 0.00422176718711853\n",
      "Epoch: 49, Samples: 3104/5760, Loss: 0.010605543851852417\n",
      "Epoch: 49, Samples: 3136/5760, Loss: 0.013126403093338013\n",
      "Epoch: 49, Samples: 3168/5760, Loss: 0.005314558744430542\n",
      "Epoch: 49, Samples: 3200/5760, Loss: 0.010003238916397095\n",
      "Epoch: 49, Samples: 3232/5760, Loss: 0.005193114280700684\n",
      "Epoch: 49, Samples: 3264/5760, Loss: 0.016842782497406006\n",
      "Epoch: 49, Samples: 3296/5760, Loss: 0.011861234903335571\n",
      "Epoch: 49, Samples: 3328/5760, Loss: 0.008539795875549316\n",
      "Epoch: 49, Samples: 3360/5760, Loss: 0.007899433374404907\n",
      "Epoch: 49, Samples: 3392/5760, Loss: 0.009557098150253296\n",
      "Epoch: 49, Samples: 3424/5760, Loss: 0.010576814413070679\n",
      "Epoch: 49, Samples: 3456/5760, Loss: 0.0037058591842651367\n",
      "Epoch: 49, Samples: 3488/5760, Loss: 0.009656816720962524\n",
      "Epoch: 49, Samples: 3520/5760, Loss: 0.013033777475357056\n",
      "Epoch: 49, Samples: 3552/5760, Loss: 0.012664347887039185\n",
      "Epoch: 49, Samples: 3584/5760, Loss: 0.008951574563980103\n",
      "Epoch: 49, Samples: 3616/5760, Loss: 0.005696386098861694\n",
      "Epoch: 49, Samples: 3648/5760, Loss: 0.0074793994426727295\n",
      "Epoch: 49, Samples: 3680/5760, Loss: 0.005383938550949097\n",
      "Epoch: 49, Samples: 3712/5760, Loss: 0.007298260927200317\n",
      "Epoch: 49, Samples: 3744/5760, Loss: 0.007605761289596558\n",
      "Epoch: 49, Samples: 3776/5760, Loss: 0.01071062684059143\n",
      "Epoch: 49, Samples: 3808/5760, Loss: 0.004326075315475464\n",
      "Epoch: 49, Samples: 3840/5760, Loss: 0.007980793714523315\n",
      "Epoch: 49, Samples: 3872/5760, Loss: 0.003939181566238403\n",
      "Epoch: 49, Samples: 3904/5760, Loss: 0.0059701502323150635\n",
      "Epoch: 49, Samples: 3936/5760, Loss: 0.010012626647949219\n",
      "Epoch: 49, Samples: 3968/5760, Loss: 0.002702683210372925\n",
      "Epoch: 49, Samples: 4000/5760, Loss: 0.0051500797271728516\n",
      "Epoch: 49, Samples: 4032/5760, Loss: 0.005548566579818726\n",
      "Epoch: 49, Samples: 4064/5760, Loss: 0.022033482789993286\n",
      "Epoch: 49, Samples: 4096/5760, Loss: 0.006818264722824097\n",
      "Epoch: 49, Samples: 4128/5760, Loss: 0.010932892560958862\n",
      "Epoch: 49, Samples: 4160/5760, Loss: 0.018698811531066895\n",
      "Epoch: 49, Samples: 4192/5760, Loss: 0.004377573728561401\n",
      "Epoch: 49, Samples: 4224/5760, Loss: 0.005122005939483643\n",
      "Epoch: 49, Samples: 4256/5760, Loss: 0.009855955839157104\n",
      "Epoch: 49, Samples: 4288/5760, Loss: 0.009347349405288696\n",
      "Epoch: 49, Samples: 4320/5760, Loss: 0.006231874227523804\n",
      "Epoch: 49, Samples: 4352/5760, Loss: 0.0203864723443985\n",
      "Epoch: 49, Samples: 4384/5760, Loss: 0.005251139402389526\n",
      "Epoch: 49, Samples: 4416/5760, Loss: 0.013580620288848877\n",
      "Epoch: 49, Samples: 4448/5760, Loss: 0.007445782423019409\n",
      "Epoch: 49, Samples: 4480/5760, Loss: 0.004938095808029175\n",
      "Epoch: 49, Samples: 4512/5760, Loss: 0.010409504175186157\n",
      "Epoch: 49, Samples: 4544/5760, Loss: 0.012745678424835205\n",
      "Epoch: 49, Samples: 4576/5760, Loss: 0.012605160474777222\n",
      "Epoch: 49, Samples: 4608/5760, Loss: 0.007618308067321777\n",
      "Epoch: 49, Samples: 4640/5760, Loss: 0.004327178001403809\n",
      "Epoch: 49, Samples: 4672/5760, Loss: 0.010088175535202026\n",
      "Epoch: 49, Samples: 4704/5760, Loss: 0.0052686333656311035\n",
      "Epoch: 49, Samples: 4736/5760, Loss: 0.006260931491851807\n",
      "Epoch: 49, Samples: 4768/5760, Loss: 0.007199525833129883\n",
      "Epoch: 49, Samples: 4800/5760, Loss: 0.010611891746520996\n",
      "Epoch: 49, Samples: 4832/5760, Loss: 0.009948521852493286\n",
      "Epoch: 49, Samples: 4864/5760, Loss: 0.004164934158325195\n",
      "Epoch: 49, Samples: 4896/5760, Loss: 0.005363136529922485\n",
      "Epoch: 49, Samples: 4928/5760, Loss: 0.007113128900527954\n",
      "Epoch: 49, Samples: 4960/5760, Loss: 0.007848590612411499\n",
      "Epoch: 49, Samples: 4992/5760, Loss: 0.007991135120391846\n",
      "Epoch: 49, Samples: 5024/5760, Loss: 0.005431562662124634\n",
      "Epoch: 49, Samples: 5056/5760, Loss: 0.005640894174575806\n",
      "Epoch: 49, Samples: 5088/5760, Loss: 0.006482064723968506\n",
      "Epoch: 49, Samples: 5120/5760, Loss: 0.012468189001083374\n",
      "Epoch: 49, Samples: 5152/5760, Loss: 0.003897547721862793\n",
      "Epoch: 49, Samples: 5184/5760, Loss: 0.004849791526794434\n",
      "Epoch: 49, Samples: 5216/5760, Loss: 0.015148431062698364\n",
      "Epoch: 49, Samples: 5248/5760, Loss: 0.004988342523574829\n",
      "Epoch: 49, Samples: 5280/5760, Loss: 0.012999773025512695\n",
      "Epoch: 49, Samples: 5312/5760, Loss: 0.004229605197906494\n",
      "Epoch: 49, Samples: 5344/5760, Loss: 0.004503518342971802\n",
      "Epoch: 49, Samples: 5376/5760, Loss: 0.0067858099937438965\n",
      "Epoch: 49, Samples: 5408/5760, Loss: 0.012341916561126709\n",
      "Epoch: 49, Samples: 5440/5760, Loss: 0.00650709867477417\n",
      "Epoch: 49, Samples: 5472/5760, Loss: 0.006830930709838867\n",
      "Epoch: 49, Samples: 5504/5760, Loss: 0.009236365556716919\n",
      "Epoch: 49, Samples: 5536/5760, Loss: 0.006877928972244263\n",
      "Epoch: 49, Samples: 5568/5760, Loss: 0.015822961926460266\n",
      "Epoch: 49, Samples: 5600/5760, Loss: 0.007057875394821167\n",
      "Epoch: 49, Samples: 5632/5760, Loss: 0.006026953458786011\n",
      "Epoch: 49, Samples: 5664/5760, Loss: 0.008572548627853394\n",
      "Epoch: 49, Samples: 5696/5760, Loss: 0.00717434287071228\n",
      "Epoch: 49, Samples: 5728/5760, Loss: 0.5364426374435425\n",
      "\n",
      "Epoch: 49\n",
      "Training set: Average loss: 0.0114\n",
      "Validation set: Average loss: 0.3148, Accuracy: 756/818 (92%)\n",
      "Saving model (epoch 49) with lowest validation loss: 0.31477418828469056\n",
      "Epoch: 50, Samples: 0/5760, Loss: 0.0054114460945129395\n",
      "Epoch: 50, Samples: 32/5760, Loss: 0.006193757057189941\n",
      "Epoch: 50, Samples: 64/5760, Loss: 0.005395293235778809\n",
      "Epoch: 50, Samples: 96/5760, Loss: 0.00573316216468811\n",
      "Epoch: 50, Samples: 128/5760, Loss: 0.009357154369354248\n",
      "Epoch: 50, Samples: 160/5760, Loss: 0.009974151849746704\n",
      "Epoch: 50, Samples: 192/5760, Loss: 0.008839935064315796\n",
      "Epoch: 50, Samples: 224/5760, Loss: 0.012529939413070679\n",
      "Epoch: 50, Samples: 256/5760, Loss: 0.02584061026573181\n",
      "Epoch: 50, Samples: 288/5760, Loss: 0.004154831171035767\n",
      "Epoch: 50, Samples: 320/5760, Loss: 0.010632574558258057\n",
      "Epoch: 50, Samples: 352/5760, Loss: 0.004844784736633301\n",
      "Epoch: 50, Samples: 384/5760, Loss: 0.006381809711456299\n",
      "Epoch: 50, Samples: 416/5760, Loss: 0.008568495512008667\n",
      "Epoch: 50, Samples: 448/5760, Loss: 0.011825382709503174\n",
      "Epoch: 50, Samples: 480/5760, Loss: 0.007012814283370972\n",
      "Epoch: 50, Samples: 512/5760, Loss: 0.005259126424789429\n",
      "Epoch: 50, Samples: 544/5760, Loss: 0.018564075231552124\n",
      "Epoch: 50, Samples: 576/5760, Loss: 0.006541520357131958\n",
      "Epoch: 50, Samples: 608/5760, Loss: 0.007718592882156372\n",
      "Epoch: 50, Samples: 640/5760, Loss: 0.009289145469665527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Samples: 672/5760, Loss: 0.0063148438930511475\n",
      "Epoch: 50, Samples: 704/5760, Loss: 0.006969362497329712\n",
      "Epoch: 50, Samples: 736/5760, Loss: 0.007262229919433594\n",
      "Epoch: 50, Samples: 768/5760, Loss: 0.0071776509284973145\n",
      "Epoch: 50, Samples: 800/5760, Loss: 0.006096154451370239\n",
      "Epoch: 50, Samples: 832/5760, Loss: 0.00690123438835144\n",
      "Epoch: 50, Samples: 864/5760, Loss: 0.008302360773086548\n",
      "Epoch: 50, Samples: 896/5760, Loss: 0.008712142705917358\n",
      "Epoch: 50, Samples: 928/5760, Loss: 0.018242061138153076\n",
      "Epoch: 50, Samples: 960/5760, Loss: 0.004609137773513794\n",
      "Epoch: 50, Samples: 992/5760, Loss: 0.013155758380889893\n",
      "Epoch: 50, Samples: 1024/5760, Loss: 0.006900101900100708\n",
      "Epoch: 50, Samples: 1056/5760, Loss: 0.012959092855453491\n",
      "Epoch: 50, Samples: 1088/5760, Loss: 0.015293240547180176\n",
      "Epoch: 50, Samples: 1120/5760, Loss: 0.003675401210784912\n",
      "Epoch: 50, Samples: 1152/5760, Loss: 0.01283198595046997\n",
      "Epoch: 50, Samples: 1184/5760, Loss: 0.005723297595977783\n",
      "Epoch: 50, Samples: 1216/5760, Loss: 0.006138235330581665\n",
      "Epoch: 50, Samples: 1248/5760, Loss: 0.012110739946365356\n",
      "Epoch: 50, Samples: 1280/5760, Loss: 0.00554773211479187\n",
      "Epoch: 50, Samples: 1312/5760, Loss: 0.00905415415763855\n",
      "Epoch: 50, Samples: 1344/5760, Loss: 0.0048195719718933105\n",
      "Epoch: 50, Samples: 1376/5760, Loss: 0.00503811240196228\n",
      "Epoch: 50, Samples: 1408/5760, Loss: 0.008710712194442749\n",
      "Epoch: 50, Samples: 1440/5760, Loss: 0.009863525629043579\n",
      "Epoch: 50, Samples: 1472/5760, Loss: 0.005469918251037598\n",
      "Epoch: 50, Samples: 1504/5760, Loss: 0.0046086907386779785\n",
      "Epoch: 50, Samples: 1536/5760, Loss: 0.012702226638793945\n",
      "Epoch: 50, Samples: 1568/5760, Loss: 0.009936004877090454\n",
      "Epoch: 50, Samples: 1600/5760, Loss: 0.009111911058425903\n",
      "Epoch: 50, Samples: 1632/5760, Loss: 0.008295178413391113\n",
      "Epoch: 50, Samples: 1664/5760, Loss: 0.008010059595108032\n",
      "Epoch: 50, Samples: 1696/5760, Loss: 0.015793800354003906\n",
      "Epoch: 50, Samples: 1728/5760, Loss: 0.005689412355422974\n",
      "Epoch: 50, Samples: 1760/5760, Loss: 0.006883293390274048\n",
      "Epoch: 50, Samples: 1792/5760, Loss: 0.006282985210418701\n",
      "Epoch: 50, Samples: 1824/5760, Loss: 0.0070618391036987305\n",
      "Epoch: 50, Samples: 1856/5760, Loss: 0.006244271993637085\n",
      "Epoch: 50, Samples: 1888/5760, Loss: 0.00283244252204895\n",
      "Epoch: 50, Samples: 1920/5760, Loss: 0.008543670177459717\n",
      "Epoch: 50, Samples: 1952/5760, Loss: 0.006424456834793091\n",
      "Epoch: 50, Samples: 1984/5760, Loss: 0.012996971607208252\n",
      "Epoch: 50, Samples: 2016/5760, Loss: 0.00518566370010376\n",
      "Epoch: 50, Samples: 2048/5760, Loss: 0.009460031986236572\n",
      "Epoch: 50, Samples: 2080/5760, Loss: 0.008726358413696289\n",
      "Epoch: 50, Samples: 2112/5760, Loss: 0.005788028240203857\n",
      "Epoch: 50, Samples: 2144/5760, Loss: 0.0056416988372802734\n",
      "Epoch: 50, Samples: 2176/5760, Loss: 0.009285062551498413\n",
      "Epoch: 50, Samples: 2208/5760, Loss: 0.015349060297012329\n",
      "Epoch: 50, Samples: 2240/5760, Loss: 0.010346055030822754\n",
      "Epoch: 50, Samples: 2272/5760, Loss: 0.010891377925872803\n",
      "Epoch: 50, Samples: 2304/5760, Loss: 0.016279876232147217\n",
      "Epoch: 50, Samples: 2336/5760, Loss: 0.008917897939682007\n",
      "Epoch: 50, Samples: 2368/5760, Loss: 0.011484980583190918\n",
      "Epoch: 50, Samples: 2400/5760, Loss: 0.005428194999694824\n",
      "Epoch: 50, Samples: 2432/5760, Loss: 0.007456392049789429\n",
      "Epoch: 50, Samples: 2464/5760, Loss: 0.006114482879638672\n",
      "Epoch: 50, Samples: 2496/5760, Loss: 0.0069043636322021484\n",
      "Epoch: 50, Samples: 2528/5760, Loss: 0.02304130792617798\n",
      "Epoch: 50, Samples: 2560/5760, Loss: 0.009821832180023193\n",
      "Epoch: 50, Samples: 2592/5760, Loss: 0.004612654447555542\n",
      "Epoch: 50, Samples: 2624/5760, Loss: 0.005320876836776733\n",
      "Epoch: 50, Samples: 2656/5760, Loss: 0.006241023540496826\n",
      "Epoch: 50, Samples: 2688/5760, Loss: 0.008727818727493286\n",
      "Epoch: 50, Samples: 2720/5760, Loss: 0.007090508937835693\n",
      "Epoch: 50, Samples: 2752/5760, Loss: 0.008001476526260376\n",
      "Epoch: 50, Samples: 2784/5760, Loss: 0.005480349063873291\n",
      "Epoch: 50, Samples: 2816/5760, Loss: 0.00637391209602356\n",
      "Epoch: 50, Samples: 2848/5760, Loss: 0.005922138690948486\n",
      "Epoch: 50, Samples: 2880/5760, Loss: 0.0067168474197387695\n",
      "Epoch: 50, Samples: 2912/5760, Loss: 0.005574852228164673\n",
      "Epoch: 50, Samples: 2944/5760, Loss: 0.008346527814865112\n",
      "Epoch: 50, Samples: 2976/5760, Loss: 0.003391444683074951\n",
      "Epoch: 50, Samples: 3008/5760, Loss: 0.008592844009399414\n",
      "Epoch: 50, Samples: 3040/5760, Loss: 0.025274962186813354\n",
      "Epoch: 50, Samples: 3072/5760, Loss: 0.008248478174209595\n",
      "Epoch: 50, Samples: 3104/5760, Loss: 0.010761409997940063\n",
      "Epoch: 50, Samples: 3136/5760, Loss: 0.006256192922592163\n",
      "Epoch: 50, Samples: 3168/5760, Loss: 0.012802034616470337\n",
      "Epoch: 50, Samples: 3200/5760, Loss: 0.01062193512916565\n",
      "Epoch: 50, Samples: 3232/5760, Loss: 0.0034709572792053223\n",
      "Epoch: 50, Samples: 3264/5760, Loss: 0.007542461156845093\n",
      "Epoch: 50, Samples: 3296/5760, Loss: 0.012306302785873413\n",
      "Epoch: 50, Samples: 3328/5760, Loss: 0.01156705617904663\n",
      "Epoch: 50, Samples: 3360/5760, Loss: 0.0100117027759552\n",
      "Epoch: 50, Samples: 3392/5760, Loss: 0.008396327495574951\n",
      "Epoch: 50, Samples: 3424/5760, Loss: 0.017224013805389404\n",
      "Epoch: 50, Samples: 3456/5760, Loss: 0.004416227340698242\n",
      "Epoch: 50, Samples: 3488/5760, Loss: 0.011187881231307983\n",
      "Epoch: 50, Samples: 3520/5760, Loss: 0.011580556631088257\n",
      "Epoch: 50, Samples: 3552/5760, Loss: 0.02039891481399536\n",
      "Epoch: 50, Samples: 3584/5760, Loss: 0.009780317544937134\n",
      "Epoch: 50, Samples: 3616/5760, Loss: 0.009194284677505493\n",
      "Epoch: 50, Samples: 3648/5760, Loss: 0.006280332803726196\n",
      "Epoch: 50, Samples: 3680/5760, Loss: 0.007841557264328003\n",
      "Epoch: 50, Samples: 3712/5760, Loss: 0.014392346143722534\n",
      "Epoch: 50, Samples: 3744/5760, Loss: 0.005345970392227173\n",
      "Epoch: 50, Samples: 3776/5760, Loss: 0.009040415287017822\n",
      "Epoch: 50, Samples: 3808/5760, Loss: 0.006319642066955566\n",
      "Epoch: 50, Samples: 3840/5760, Loss: 0.011773616075515747\n",
      "Epoch: 50, Samples: 3872/5760, Loss: 0.0072938501834869385\n",
      "Epoch: 50, Samples: 3904/5760, Loss: 0.008889704942703247\n",
      "Epoch: 50, Samples: 3936/5760, Loss: 0.0076482295989990234\n",
      "Epoch: 50, Samples: 3968/5760, Loss: 0.012419939041137695\n",
      "Epoch: 50, Samples: 4000/5760, Loss: 0.01645982265472412\n",
      "Epoch: 50, Samples: 4032/5760, Loss: 0.009051203727722168\n",
      "Epoch: 50, Samples: 4064/5760, Loss: 0.011124908924102783\n",
      "Epoch: 50, Samples: 4096/5760, Loss: 0.006535619497299194\n",
      "Epoch: 50, Samples: 4128/5760, Loss: 0.01566833257675171\n",
      "Epoch: 50, Samples: 4160/5760, Loss: 0.024376899003982544\n",
      "Epoch: 50, Samples: 4192/5760, Loss: 0.009297996759414673\n",
      "Epoch: 50, Samples: 4224/5760, Loss: 0.005137056112289429\n",
      "Epoch: 50, Samples: 4256/5760, Loss: 0.004752159118652344\n",
      "Epoch: 50, Samples: 4288/5760, Loss: 0.011704176664352417\n",
      "Epoch: 50, Samples: 4320/5760, Loss: 0.005795091390609741\n",
      "Epoch: 50, Samples: 4352/5760, Loss: 0.007697999477386475\n",
      "Epoch: 50, Samples: 4384/5760, Loss: 0.0061528682708740234\n",
      "Epoch: 50, Samples: 4416/5760, Loss: 0.0060159265995025635\n",
      "Epoch: 50, Samples: 4448/5760, Loss: 0.01734676957130432\n",
      "Epoch: 50, Samples: 4480/5760, Loss: 0.004913419485092163\n",
      "Epoch: 50, Samples: 4512/5760, Loss: 0.0057111382484436035\n",
      "Epoch: 50, Samples: 4544/5760, Loss: 0.00722852349281311\n",
      "Epoch: 50, Samples: 4576/5760, Loss: 0.005183100700378418\n",
      "Epoch: 50, Samples: 4608/5760, Loss: 0.014471977949142456\n",
      "Epoch: 50, Samples: 4640/5760, Loss: 0.011764228343963623\n",
      "Epoch: 50, Samples: 4672/5760, Loss: 0.006055772304534912\n",
      "Epoch: 50, Samples: 4704/5760, Loss: 0.005391240119934082\n",
      "Epoch: 50, Samples: 4736/5760, Loss: 0.004741013050079346\n",
      "Epoch: 50, Samples: 4768/5760, Loss: 0.006530672311782837\n",
      "Epoch: 50, Samples: 4800/5760, Loss: 0.01145029067993164\n",
      "Epoch: 50, Samples: 4832/5760, Loss: 0.00859612226486206\n",
      "Epoch: 50, Samples: 4864/5760, Loss: 0.013903379440307617\n",
      "Epoch: 50, Samples: 4896/5760, Loss: 0.005585461854934692\n",
      "Epoch: 50, Samples: 4928/5760, Loss: 0.013998538255691528\n",
      "Epoch: 50, Samples: 4960/5760, Loss: 0.007603585720062256\n",
      "Epoch: 50, Samples: 4992/5760, Loss: 0.00829005241394043\n",
      "Epoch: 50, Samples: 5024/5760, Loss: 0.006085306406021118\n",
      "Epoch: 50, Samples: 5056/5760, Loss: 0.008714735507965088\n",
      "Epoch: 50, Samples: 5088/5760, Loss: 0.005656152963638306\n",
      "Epoch: 50, Samples: 5120/5760, Loss: 0.006263375282287598\n",
      "Epoch: 50, Samples: 5152/5760, Loss: 0.010261952877044678\n",
      "Epoch: 50, Samples: 5184/5760, Loss: 0.007179439067840576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Samples: 5216/5760, Loss: 0.008838146924972534\n",
      "Epoch: 50, Samples: 5248/5760, Loss: 0.005125641822814941\n",
      "Epoch: 50, Samples: 5280/5760, Loss: 0.00511544942855835\n",
      "Epoch: 50, Samples: 5312/5760, Loss: 0.011164218187332153\n",
      "Epoch: 50, Samples: 5344/5760, Loss: 0.010093361139297485\n",
      "Epoch: 50, Samples: 5376/5760, Loss: 0.006878077983856201\n",
      "Epoch: 50, Samples: 5408/5760, Loss: 0.008244365453720093\n",
      "Epoch: 50, Samples: 5440/5760, Loss: 0.009478509426116943\n",
      "Epoch: 50, Samples: 5472/5760, Loss: 0.0061331093311309814\n",
      "Epoch: 50, Samples: 5504/5760, Loss: 0.011315643787384033\n",
      "Epoch: 50, Samples: 5536/5760, Loss: 0.012923866510391235\n",
      "Epoch: 50, Samples: 5568/5760, Loss: 0.005788236856460571\n",
      "Epoch: 50, Samples: 5600/5760, Loss: 0.010291129350662231\n",
      "Epoch: 50, Samples: 5632/5760, Loss: 0.012006700038909912\n",
      "Epoch: 50, Samples: 5664/5760, Loss: 0.0084189772605896\n",
      "Epoch: 50, Samples: 5696/5760, Loss: 0.01075795292854309\n",
      "Epoch: 50, Samples: 5728/5760, Loss: 0.33019542694091797\n",
      "\n",
      "Epoch: 50\n",
      "Training set: Average loss: 0.0107\n",
      "Validation set: Average loss: 0.3388, Accuracy: 748/818 (91%)\n",
      "Training and validation complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class FlowerDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_paths, label_file, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels = np.load(label_file)\n",
    "        self.image_label_pairs = self._load_paths(image_paths)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def train_val_test_split(self, train_ratio, val_ratio):\n",
    "        dataset_length = len(self.image_label_pairs)\n",
    "        train_length = int(train_ratio * dataset_length)\n",
    "        val_length = int(val_ratio * dataset_length)\n",
    "        test_length = len(self) - train_length - val_length\n",
    "        splits = [train_length, val_length, test_length]\n",
    "        return random_split(self, splits)\n",
    "        \n",
    "    def _load_paths(self, file_path):\n",
    "        \"\"\"\n",
    "        params:  file_path, a path pointing to where the image paths are stored.\n",
    "        returns: dictionary with keys 'full_image_path', and values 'label'\n",
    "        \"\"\"\n",
    "        split_set = {}\n",
    "        with open(file_path) as f:\n",
    "            lines = f.readlines()\n",
    "            num_lines = len(lines)\n",
    "            assert(num_lines == len(self.labels))\n",
    "            for line_num in range(num_lines):\n",
    "                full_image_path = os.path.join(self.image_dir, lines[line_num].strip('\\n'))\n",
    "                split_set[full_image_path] = self.labels[line_num]\n",
    "        return pd.DataFrame.from_dict(split_set, orient='index')\n",
    "        \n",
    "    def _load_image(self, image_path):\n",
    "        img = Image.open(image_path)\n",
    "        img.load()\n",
    "        img = np.array(img)\n",
    "        if len(img.shape) == 2:\n",
    "            img = np.expand_dims(img, 2)\n",
    "            img = np.repeat(img, 3, 2)\n",
    "        return Image.fromarray(img)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_label_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # apply transforms\n",
    "        image_path = self.image_label_pairs.index[idx]\n",
    "        image = self._load_image(image_path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return {'image': image,\n",
    "                'label': label}\n",
    "    \n",
    "transform = transforms.Compose([transforms.CenterCrop(200), transforms.ToTensor()])\n",
    "dataset = FlowerDataset('data', 'image_paths.txt', 'labels.npy', transform=transform)\n",
    "train_set, val_set, test_set = dataset.train_val_test_split(0.7, 0.1)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        data, target = batch['image'].to(device), batch['label'].long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = CE(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        print('Epoch: {}, Samples: {}/{}, Loss: {}'.format(epoch, idx*batch_size,\n",
    "                                                           len(train_loader)*batch_size,\n",
    "                                                           loss.item()))\n",
    "    train_loss = torch.mean(torch.tensor(train_losses))\n",
    "    print('\\nEpoch: {}'.format(epoch))\n",
    "    print('Training set: Average loss: {:.4f}'.format(train_loss))\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "def validate(model, device, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(val_loader):\n",
    "            data, target = batch['image'].to(device), batch['label'].long().to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            # compute the batch loss\n",
    "            batch_loss = CE(output, target).item()\n",
    "            val_loss += batch_loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    # divide by the number of batches of batch size 32\n",
    "    # get the average validation over all bins\n",
    "    val_loss /= len(val_loader)\n",
    "    print('Validation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        val_loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "DATA_DIRECTORY = 'data/'\n",
    "use_cuda = 1\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# freeze all layers before layer3\n",
    "freeze(model.conv1)\n",
    "freeze(model.bn1)\n",
    "freeze(model.layer1)\n",
    "freeze(model.layer2)\n",
    "model.fc = nn.Linear(512, 102)\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "CE = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "    val_loss = validate(model, device, val_loader)\n",
    "\n",
    "    if (len(val_losses) > 0) and (val_loss < min(val_losses)):\n",
    "        torch.save(model.state_dict(), \"best_model_c.pt\")\n",
    "        print(\"Saving model (epoch {}) with lowest validation loss: {}\"\n",
    "              .format(epoch, val_loss))\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "print(\"Training and validation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "epoch_list = np.arange(1, num_epochs+1)\n",
    "plt.xticks(epoch_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(epoch_list, train_losses, label=\"Training loss\")\n",
    "plt.plot(epoch_list, val_losses, label=\"Validation loss\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 1503/1639 (92%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model_c_49.pt'))\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(test_loader):\n",
    "        data = batch['image'].to(device)\n",
    "        labels = batch['label'].long().to(device)\n",
    "        result = model(data)\n",
    "        pred = result.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    \n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(correct, len(test_loader.dataset), \n",
    "                                                       100. * correct / len(test_loader.dataset)))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
