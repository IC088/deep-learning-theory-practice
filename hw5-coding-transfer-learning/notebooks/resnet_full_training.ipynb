{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Samples: 0/5760, Loss: 4.556001663208008\n",
      "Epoch: 1, Samples: 32/5760, Loss: 4.807998180389404\n",
      "Epoch: 1, Samples: 64/5760, Loss: 4.754427433013916\n",
      "Epoch: 1, Samples: 96/5760, Loss: 4.662336826324463\n",
      "Epoch: 1, Samples: 128/5760, Loss: 4.687515735626221\n",
      "Epoch: 1, Samples: 160/5760, Loss: 4.847537040710449\n",
      "Epoch: 1, Samples: 192/5760, Loss: 4.754984378814697\n",
      "Epoch: 1, Samples: 224/5760, Loss: 4.707015037536621\n",
      "Epoch: 1, Samples: 256/5760, Loss: 4.539435386657715\n",
      "Epoch: 1, Samples: 288/5760, Loss: 4.579208850860596\n",
      "Epoch: 1, Samples: 320/5760, Loss: 4.562300205230713\n",
      "Epoch: 1, Samples: 352/5760, Loss: 4.760418891906738\n",
      "Epoch: 1, Samples: 384/5760, Loss: 4.568358898162842\n",
      "Epoch: 1, Samples: 416/5760, Loss: 4.677227973937988\n",
      "Epoch: 1, Samples: 448/5760, Loss: 4.6275129318237305\n",
      "Epoch: 1, Samples: 480/5760, Loss: 4.637516975402832\n",
      "Epoch: 1, Samples: 512/5760, Loss: 4.7410197257995605\n",
      "Epoch: 1, Samples: 544/5760, Loss: 4.494808197021484\n",
      "Epoch: 1, Samples: 576/5760, Loss: 4.241174697875977\n",
      "Epoch: 1, Samples: 608/5760, Loss: 4.448087215423584\n",
      "Epoch: 1, Samples: 640/5760, Loss: 4.527251720428467\n",
      "Epoch: 1, Samples: 672/5760, Loss: 4.638151168823242\n",
      "Epoch: 1, Samples: 704/5760, Loss: 4.590157508850098\n",
      "Epoch: 1, Samples: 736/5760, Loss: 4.582883834838867\n",
      "Epoch: 1, Samples: 768/5760, Loss: 4.559414386749268\n",
      "Epoch: 1, Samples: 800/5760, Loss: 4.475213050842285\n",
      "Epoch: 1, Samples: 832/5760, Loss: 4.40742301940918\n",
      "Epoch: 1, Samples: 864/5760, Loss: 4.620551586151123\n",
      "Epoch: 1, Samples: 896/5760, Loss: 4.33072566986084\n",
      "Epoch: 1, Samples: 928/5760, Loss: 4.467406272888184\n",
      "Epoch: 1, Samples: 960/5760, Loss: 4.615632057189941\n",
      "Epoch: 1, Samples: 992/5760, Loss: 4.368832111358643\n",
      "Epoch: 1, Samples: 1024/5760, Loss: 4.6110334396362305\n",
      "Epoch: 1, Samples: 1056/5760, Loss: 4.569580554962158\n",
      "Epoch: 1, Samples: 1088/5760, Loss: 4.5302815437316895\n",
      "Epoch: 1, Samples: 1120/5760, Loss: 4.402219295501709\n",
      "Epoch: 1, Samples: 1152/5760, Loss: 4.484617233276367\n",
      "Epoch: 1, Samples: 1184/5760, Loss: 4.514373779296875\n",
      "Epoch: 1, Samples: 1216/5760, Loss: 4.2650604248046875\n",
      "Epoch: 1, Samples: 1248/5760, Loss: 4.4315690994262695\n",
      "Epoch: 1, Samples: 1280/5760, Loss: 4.436064720153809\n",
      "Epoch: 1, Samples: 1312/5760, Loss: 4.337247848510742\n",
      "Epoch: 1, Samples: 1344/5760, Loss: 4.446765422821045\n",
      "Epoch: 1, Samples: 1376/5760, Loss: 4.392609119415283\n",
      "Epoch: 1, Samples: 1408/5760, Loss: 4.390357494354248\n",
      "Epoch: 1, Samples: 1440/5760, Loss: 4.439810276031494\n",
      "Epoch: 1, Samples: 1472/5760, Loss: 4.395724296569824\n",
      "Epoch: 1, Samples: 1504/5760, Loss: 4.279160022735596\n",
      "Epoch: 1, Samples: 1536/5760, Loss: 4.559245586395264\n",
      "Epoch: 1, Samples: 1568/5760, Loss: 4.328770637512207\n",
      "Epoch: 1, Samples: 1600/5760, Loss: 4.6021857261657715\n",
      "Epoch: 1, Samples: 1632/5760, Loss: 4.470682144165039\n",
      "Epoch: 1, Samples: 1664/5760, Loss: 4.228268623352051\n",
      "Epoch: 1, Samples: 1696/5760, Loss: 4.5894575119018555\n",
      "Epoch: 1, Samples: 1728/5760, Loss: 4.10403299331665\n",
      "Epoch: 1, Samples: 1760/5760, Loss: 4.172352313995361\n",
      "Epoch: 1, Samples: 1792/5760, Loss: 4.367371082305908\n",
      "Epoch: 1, Samples: 1824/5760, Loss: 4.542252540588379\n",
      "Epoch: 1, Samples: 1856/5760, Loss: 4.275721549987793\n",
      "Epoch: 1, Samples: 1888/5760, Loss: 4.468758583068848\n",
      "Epoch: 1, Samples: 1920/5760, Loss: 4.2699384689331055\n",
      "Epoch: 1, Samples: 1952/5760, Loss: 4.081319808959961\n",
      "Epoch: 1, Samples: 1984/5760, Loss: 4.436388969421387\n",
      "Epoch: 1, Samples: 2016/5760, Loss: 4.387842178344727\n",
      "Epoch: 1, Samples: 2048/5760, Loss: 4.425904273986816\n",
      "Epoch: 1, Samples: 2080/5760, Loss: 4.452483654022217\n",
      "Epoch: 1, Samples: 2112/5760, Loss: 4.323346138000488\n",
      "Epoch: 1, Samples: 2144/5760, Loss: 4.084749698638916\n",
      "Epoch: 1, Samples: 2176/5760, Loss: 4.324972152709961\n",
      "Epoch: 1, Samples: 2208/5760, Loss: 4.632793426513672\n",
      "Epoch: 1, Samples: 2240/5760, Loss: 4.601955413818359\n",
      "Epoch: 1, Samples: 2272/5760, Loss: 4.299226760864258\n",
      "Epoch: 1, Samples: 2304/5760, Loss: 4.351442813873291\n",
      "Epoch: 1, Samples: 2336/5760, Loss: 4.368512153625488\n",
      "Epoch: 1, Samples: 2368/5760, Loss: 4.089112281799316\n",
      "Epoch: 1, Samples: 2400/5760, Loss: 4.2716498374938965\n",
      "Epoch: 1, Samples: 2432/5760, Loss: 4.198324203491211\n",
      "Epoch: 1, Samples: 2464/5760, Loss: 4.223389148712158\n",
      "Epoch: 1, Samples: 2496/5760, Loss: 4.366488456726074\n",
      "Epoch: 1, Samples: 2528/5760, Loss: 4.267821788787842\n",
      "Epoch: 1, Samples: 2560/5760, Loss: 4.1805739402771\n",
      "Epoch: 1, Samples: 2592/5760, Loss: 4.297630310058594\n",
      "Epoch: 1, Samples: 2624/5760, Loss: 4.256121635437012\n",
      "Epoch: 1, Samples: 2656/5760, Loss: 3.8908674716949463\n",
      "Epoch: 1, Samples: 2688/5760, Loss: 4.216191291809082\n",
      "Epoch: 1, Samples: 2720/5760, Loss: 4.220803737640381\n",
      "Epoch: 1, Samples: 2752/5760, Loss: 4.101247310638428\n",
      "Epoch: 1, Samples: 2784/5760, Loss: 4.028855323791504\n",
      "Epoch: 1, Samples: 2816/5760, Loss: 4.250973224639893\n",
      "Epoch: 1, Samples: 2848/5760, Loss: 4.463037490844727\n",
      "Epoch: 1, Samples: 2880/5760, Loss: 4.258540153503418\n",
      "Epoch: 1, Samples: 2912/5760, Loss: 4.257992267608643\n",
      "Epoch: 1, Samples: 2944/5760, Loss: 4.080597877502441\n",
      "Epoch: 1, Samples: 2976/5760, Loss: 4.13377571105957\n",
      "Epoch: 1, Samples: 3008/5760, Loss: 4.374239444732666\n",
      "Epoch: 1, Samples: 3040/5760, Loss: 4.366663932800293\n",
      "Epoch: 1, Samples: 3072/5760, Loss: 4.13137674331665\n",
      "Epoch: 1, Samples: 3104/5760, Loss: 4.27885103225708\n",
      "Epoch: 1, Samples: 3136/5760, Loss: 4.263840198516846\n",
      "Epoch: 1, Samples: 3168/5760, Loss: 4.2565789222717285\n",
      "Epoch: 1, Samples: 3200/5760, Loss: 4.160191059112549\n",
      "Epoch: 1, Samples: 3232/5760, Loss: 4.125793933868408\n",
      "Epoch: 1, Samples: 3264/5760, Loss: 4.1953816413879395\n",
      "Epoch: 1, Samples: 3296/5760, Loss: 4.067821025848389\n",
      "Epoch: 1, Samples: 3328/5760, Loss: 4.19175910949707\n",
      "Epoch: 1, Samples: 3360/5760, Loss: 4.1150054931640625\n",
      "Epoch: 1, Samples: 3392/5760, Loss: 4.124655723571777\n",
      "Epoch: 1, Samples: 3424/5760, Loss: 3.9903945922851562\n",
      "Epoch: 1, Samples: 3456/5760, Loss: 4.234726905822754\n",
      "Epoch: 1, Samples: 3488/5760, Loss: 3.787736654281616\n",
      "Epoch: 1, Samples: 3520/5760, Loss: 4.151171684265137\n",
      "Epoch: 1, Samples: 3552/5760, Loss: 3.993364095687866\n",
      "Epoch: 1, Samples: 3584/5760, Loss: 4.324995040893555\n",
      "Epoch: 1, Samples: 3616/5760, Loss: 3.8375182151794434\n",
      "Epoch: 1, Samples: 3648/5760, Loss: 3.940735101699829\n",
      "Epoch: 1, Samples: 3680/5760, Loss: 4.465645790100098\n",
      "Epoch: 1, Samples: 3712/5760, Loss: 4.062503337860107\n",
      "Epoch: 1, Samples: 3744/5760, Loss: 4.336764335632324\n",
      "Epoch: 1, Samples: 3776/5760, Loss: 4.146986484527588\n",
      "Epoch: 1, Samples: 3808/5760, Loss: 3.9947237968444824\n",
      "Epoch: 1, Samples: 3840/5760, Loss: 3.9251279830932617\n",
      "Epoch: 1, Samples: 3872/5760, Loss: 4.431895732879639\n",
      "Epoch: 1, Samples: 3904/5760, Loss: 4.247068405151367\n",
      "Epoch: 1, Samples: 3936/5760, Loss: 3.880436420440674\n",
      "Epoch: 1, Samples: 3968/5760, Loss: 3.9991278648376465\n",
      "Epoch: 1, Samples: 4000/5760, Loss: 3.9888031482696533\n",
      "Epoch: 1, Samples: 4032/5760, Loss: 4.236588478088379\n",
      "Epoch: 1, Samples: 4064/5760, Loss: 4.103470802307129\n",
      "Epoch: 1, Samples: 4096/5760, Loss: 4.206191539764404\n",
      "Epoch: 1, Samples: 4128/5760, Loss: 3.6628451347351074\n",
      "Epoch: 1, Samples: 4160/5760, Loss: 3.901463270187378\n",
      "Epoch: 1, Samples: 4192/5760, Loss: 3.7615272998809814\n",
      "Epoch: 1, Samples: 4224/5760, Loss: 3.9308669567108154\n",
      "Epoch: 1, Samples: 4256/5760, Loss: 3.8627164363861084\n",
      "Epoch: 1, Samples: 4288/5760, Loss: 4.187256813049316\n",
      "Epoch: 1, Samples: 4320/5760, Loss: 3.9740476608276367\n",
      "Epoch: 1, Samples: 4352/5760, Loss: 3.7448811531066895\n",
      "Epoch: 1, Samples: 4384/5760, Loss: 3.792898178100586\n",
      "Epoch: 1, Samples: 4416/5760, Loss: 3.857654333114624\n",
      "Epoch: 1, Samples: 4448/5760, Loss: 3.7574880123138428\n",
      "Epoch: 1, Samples: 4480/5760, Loss: 3.970961093902588\n",
      "Epoch: 1, Samples: 4512/5760, Loss: 3.8009166717529297\n",
      "Epoch: 1, Samples: 4544/5760, Loss: 3.6960785388946533\n",
      "Epoch: 1, Samples: 4576/5760, Loss: 4.09439754486084\n",
      "Epoch: 1, Samples: 4608/5760, Loss: 4.137295722961426\n",
      "Epoch: 1, Samples: 4640/5760, Loss: 3.6490225791931152\n",
      "Epoch: 1, Samples: 4672/5760, Loss: 3.9743869304656982\n",
      "Epoch: 1, Samples: 4704/5760, Loss: 3.956462860107422\n",
      "Epoch: 1, Samples: 4736/5760, Loss: 4.0064496994018555\n",
      "Epoch: 1, Samples: 4768/5760, Loss: 3.711158037185669\n",
      "Epoch: 1, Samples: 4800/5760, Loss: 4.151452541351318\n",
      "Epoch: 1, Samples: 4832/5760, Loss: 3.99503493309021\n",
      "Epoch: 1, Samples: 4864/5760, Loss: 3.7858142852783203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Samples: 4896/5760, Loss: 3.9042606353759766\n",
      "Epoch: 1, Samples: 4928/5760, Loss: 4.04185152053833\n",
      "Epoch: 1, Samples: 4960/5760, Loss: 4.2513108253479\n",
      "Epoch: 1, Samples: 4992/5760, Loss: 3.9645705223083496\n",
      "Epoch: 1, Samples: 5024/5760, Loss: 4.2627058029174805\n",
      "Epoch: 1, Samples: 5056/5760, Loss: 4.0413737297058105\n",
      "Epoch: 1, Samples: 5088/5760, Loss: 4.150453090667725\n",
      "Epoch: 1, Samples: 5120/5760, Loss: 3.8399953842163086\n",
      "Epoch: 1, Samples: 5152/5760, Loss: 3.6247997283935547\n",
      "Epoch: 1, Samples: 5184/5760, Loss: 3.7275891304016113\n",
      "Epoch: 1, Samples: 5216/5760, Loss: 3.7734262943267822\n",
      "Epoch: 1, Samples: 5248/5760, Loss: 3.8946216106414795\n",
      "Epoch: 1, Samples: 5280/5760, Loss: 3.6527299880981445\n",
      "Epoch: 1, Samples: 5312/5760, Loss: 3.6566922664642334\n",
      "Epoch: 1, Samples: 5344/5760, Loss: 3.814462184906006\n",
      "Epoch: 1, Samples: 5376/5760, Loss: 3.797210216522217\n",
      "Epoch: 1, Samples: 5408/5760, Loss: 3.6923487186431885\n",
      "Epoch: 1, Samples: 5440/5760, Loss: 3.8697147369384766\n",
      "Epoch: 1, Samples: 5472/5760, Loss: 3.976091146469116\n",
      "Epoch: 1, Samples: 5504/5760, Loss: 3.6381523609161377\n",
      "Epoch: 1, Samples: 5536/5760, Loss: 3.7589375972747803\n",
      "Epoch: 1, Samples: 5568/5760, Loss: 3.8716535568237305\n",
      "Epoch: 1, Samples: 5600/5760, Loss: 3.7726635932922363\n",
      "Epoch: 1, Samples: 5632/5760, Loss: 3.949678421020508\n",
      "Epoch: 1, Samples: 5664/5760, Loss: 3.8430328369140625\n",
      "Epoch: 1, Samples: 5696/5760, Loss: 4.004870414733887\n",
      "Epoch: 1, Samples: 5728/5760, Loss: 4.831384181976318\n",
      "\n",
      "Epoch: 1\n",
      "Training set: Average loss: 4.2193\n",
      "Validation set: Average loss: 3.8092, Accuracy: 108/818 (13%)\n",
      "Epoch: 2, Samples: 0/5760, Loss: 3.5729575157165527\n",
      "Epoch: 2, Samples: 32/5760, Loss: 3.8879432678222656\n",
      "Epoch: 2, Samples: 64/5760, Loss: 3.9056918621063232\n",
      "Epoch: 2, Samples: 96/5760, Loss: 3.618738889694214\n",
      "Epoch: 2, Samples: 128/5760, Loss: 3.8199687004089355\n",
      "Epoch: 2, Samples: 160/5760, Loss: 3.7490668296813965\n",
      "Epoch: 2, Samples: 192/5760, Loss: 3.7322001457214355\n",
      "Epoch: 2, Samples: 224/5760, Loss: 3.8297293186187744\n",
      "Epoch: 2, Samples: 256/5760, Loss: 3.2973484992980957\n",
      "Epoch: 2, Samples: 288/5760, Loss: 3.756457805633545\n",
      "Epoch: 2, Samples: 320/5760, Loss: 3.544875383377075\n",
      "Epoch: 2, Samples: 352/5760, Loss: 3.6782093048095703\n",
      "Epoch: 2, Samples: 384/5760, Loss: 3.6732730865478516\n",
      "Epoch: 2, Samples: 416/5760, Loss: 3.5693156719207764\n",
      "Epoch: 2, Samples: 448/5760, Loss: 3.581144094467163\n",
      "Epoch: 2, Samples: 480/5760, Loss: 3.502627372741699\n",
      "Epoch: 2, Samples: 512/5760, Loss: 3.712817430496216\n",
      "Epoch: 2, Samples: 544/5760, Loss: 3.58075213432312\n",
      "Epoch: 2, Samples: 576/5760, Loss: 3.868865489959717\n",
      "Epoch: 2, Samples: 608/5760, Loss: 3.813915252685547\n",
      "Epoch: 2, Samples: 640/5760, Loss: 3.9306752681732178\n",
      "Epoch: 2, Samples: 672/5760, Loss: 3.946448564529419\n",
      "Epoch: 2, Samples: 704/5760, Loss: 3.7986528873443604\n",
      "Epoch: 2, Samples: 736/5760, Loss: 4.273003578186035\n",
      "Epoch: 2, Samples: 768/5760, Loss: 3.6115198135375977\n",
      "Epoch: 2, Samples: 800/5760, Loss: 3.6884281635284424\n",
      "Epoch: 2, Samples: 832/5760, Loss: 3.628392219543457\n",
      "Epoch: 2, Samples: 864/5760, Loss: 3.2136011123657227\n",
      "Epoch: 2, Samples: 896/5760, Loss: 3.7503502368927\n",
      "Epoch: 2, Samples: 928/5760, Loss: 3.6889097690582275\n",
      "Epoch: 2, Samples: 960/5760, Loss: 3.837230682373047\n",
      "Epoch: 2, Samples: 992/5760, Loss: 3.8124122619628906\n",
      "Epoch: 2, Samples: 1024/5760, Loss: 3.7461655139923096\n",
      "Epoch: 2, Samples: 1056/5760, Loss: 3.9532902240753174\n",
      "Epoch: 2, Samples: 1088/5760, Loss: 3.335130453109741\n",
      "Epoch: 2, Samples: 1120/5760, Loss: 3.588357448577881\n",
      "Epoch: 2, Samples: 1152/5760, Loss: 3.3109591007232666\n",
      "Epoch: 2, Samples: 1184/5760, Loss: 3.7369956970214844\n",
      "Epoch: 2, Samples: 1216/5760, Loss: 3.8731441497802734\n",
      "Epoch: 2, Samples: 1248/5760, Loss: 3.7231194972991943\n",
      "Epoch: 2, Samples: 1280/5760, Loss: 3.504410743713379\n",
      "Epoch: 2, Samples: 1312/5760, Loss: 3.824512481689453\n",
      "Epoch: 2, Samples: 1344/5760, Loss: 3.9968910217285156\n",
      "Epoch: 2, Samples: 1376/5760, Loss: 3.6077702045440674\n",
      "Epoch: 2, Samples: 1408/5760, Loss: 3.6910152435302734\n",
      "Epoch: 2, Samples: 1440/5760, Loss: 3.5613536834716797\n",
      "Epoch: 2, Samples: 1472/5760, Loss: 3.680263042449951\n",
      "Epoch: 2, Samples: 1504/5760, Loss: 3.5563783645629883\n",
      "Epoch: 2, Samples: 1536/5760, Loss: 3.2909739017486572\n",
      "Epoch: 2, Samples: 1568/5760, Loss: 3.6280689239501953\n",
      "Epoch: 2, Samples: 1600/5760, Loss: 3.7369399070739746\n",
      "Epoch: 2, Samples: 1632/5760, Loss: 3.5677976608276367\n",
      "Epoch: 2, Samples: 1664/5760, Loss: 3.494231700897217\n",
      "Epoch: 2, Samples: 1696/5760, Loss: 3.5600061416625977\n",
      "Epoch: 2, Samples: 1728/5760, Loss: 3.651630401611328\n",
      "Epoch: 2, Samples: 1760/5760, Loss: 3.81520938873291\n",
      "Epoch: 2, Samples: 1792/5760, Loss: 3.6733059883117676\n",
      "Epoch: 2, Samples: 1824/5760, Loss: 3.3203632831573486\n",
      "Epoch: 2, Samples: 1856/5760, Loss: 3.762669801712036\n",
      "Epoch: 2, Samples: 1888/5760, Loss: 3.809274911880493\n",
      "Epoch: 2, Samples: 1920/5760, Loss: 3.7404704093933105\n",
      "Epoch: 2, Samples: 1952/5760, Loss: 3.7624266147613525\n",
      "Epoch: 2, Samples: 1984/5760, Loss: 3.6017372608184814\n",
      "Epoch: 2, Samples: 2016/5760, Loss: 3.6314034461975098\n",
      "Epoch: 2, Samples: 2048/5760, Loss: 3.6607987880706787\n",
      "Epoch: 2, Samples: 2080/5760, Loss: 3.6150245666503906\n",
      "Epoch: 2, Samples: 2112/5760, Loss: 3.655575752258301\n",
      "Epoch: 2, Samples: 2144/5760, Loss: 3.431201457977295\n",
      "Epoch: 2, Samples: 2176/5760, Loss: 3.4480223655700684\n",
      "Epoch: 2, Samples: 2208/5760, Loss: 3.4722232818603516\n",
      "Epoch: 2, Samples: 2240/5760, Loss: 3.7967450618743896\n",
      "Epoch: 2, Samples: 2272/5760, Loss: 3.6447527408599854\n",
      "Epoch: 2, Samples: 2304/5760, Loss: 3.493121862411499\n",
      "Epoch: 2, Samples: 2336/5760, Loss: 3.6329619884490967\n",
      "Epoch: 2, Samples: 2368/5760, Loss: 3.5600826740264893\n",
      "Epoch: 2, Samples: 2400/5760, Loss: 3.8329555988311768\n",
      "Epoch: 2, Samples: 2432/5760, Loss: 3.49082088470459\n",
      "Epoch: 2, Samples: 2464/5760, Loss: 3.541606903076172\n",
      "Epoch: 2, Samples: 2496/5760, Loss: 3.613734483718872\n",
      "Epoch: 2, Samples: 2528/5760, Loss: 3.2949862480163574\n",
      "Epoch: 2, Samples: 2560/5760, Loss: 4.1043219566345215\n",
      "Epoch: 2, Samples: 2592/5760, Loss: 3.5496320724487305\n",
      "Epoch: 2, Samples: 2624/5760, Loss: 3.804579973220825\n",
      "Epoch: 2, Samples: 2656/5760, Loss: 3.4176864624023438\n",
      "Epoch: 2, Samples: 2688/5760, Loss: 3.4654741287231445\n",
      "Epoch: 2, Samples: 2720/5760, Loss: 3.5231781005859375\n",
      "Epoch: 2, Samples: 2752/5760, Loss: 3.5633575916290283\n",
      "Epoch: 2, Samples: 2784/5760, Loss: 3.4814507961273193\n",
      "Epoch: 2, Samples: 2816/5760, Loss: 3.949259042739868\n",
      "Epoch: 2, Samples: 2848/5760, Loss: 3.340078115463257\n",
      "Epoch: 2, Samples: 2880/5760, Loss: 3.6467044353485107\n",
      "Epoch: 2, Samples: 2912/5760, Loss: 3.6127724647521973\n",
      "Epoch: 2, Samples: 2944/5760, Loss: 3.68798565864563\n",
      "Epoch: 2, Samples: 2976/5760, Loss: 3.5896008014678955\n",
      "Epoch: 2, Samples: 3008/5760, Loss: 3.382362127304077\n",
      "Epoch: 2, Samples: 3040/5760, Loss: 3.496267557144165\n",
      "Epoch: 2, Samples: 3072/5760, Loss: 3.680283308029175\n",
      "Epoch: 2, Samples: 3104/5760, Loss: 3.7062182426452637\n",
      "Epoch: 2, Samples: 3136/5760, Loss: 3.4895665645599365\n",
      "Epoch: 2, Samples: 3168/5760, Loss: 3.5715296268463135\n",
      "Epoch: 2, Samples: 3200/5760, Loss: 3.50846791267395\n",
      "Epoch: 2, Samples: 3232/5760, Loss: 3.740438461303711\n",
      "Epoch: 2, Samples: 3264/5760, Loss: 3.586926221847534\n",
      "Epoch: 2, Samples: 3296/5760, Loss: 3.3861875534057617\n",
      "Epoch: 2, Samples: 3328/5760, Loss: 3.5995097160339355\n",
      "Epoch: 2, Samples: 3360/5760, Loss: 3.3396785259246826\n",
      "Epoch: 2, Samples: 3392/5760, Loss: 3.513690233230591\n",
      "Epoch: 2, Samples: 3424/5760, Loss: 3.434749126434326\n",
      "Epoch: 2, Samples: 3456/5760, Loss: 3.695462703704834\n",
      "Epoch: 2, Samples: 3488/5760, Loss: 3.4460113048553467\n",
      "Epoch: 2, Samples: 3520/5760, Loss: 3.20620059967041\n",
      "Epoch: 2, Samples: 3552/5760, Loss: 3.6733319759368896\n",
      "Epoch: 2, Samples: 3584/5760, Loss: 3.602648973464966\n",
      "Epoch: 2, Samples: 3616/5760, Loss: 3.5237338542938232\n",
      "Epoch: 2, Samples: 3648/5760, Loss: 3.6163320541381836\n",
      "Epoch: 2, Samples: 3680/5760, Loss: 3.303783893585205\n",
      "Epoch: 2, Samples: 3712/5760, Loss: 3.4066150188446045\n",
      "Epoch: 2, Samples: 3744/5760, Loss: 3.8573391437530518\n",
      "Epoch: 2, Samples: 3776/5760, Loss: 3.593015432357788\n",
      "Epoch: 2, Samples: 3808/5760, Loss: 3.502507448196411\n",
      "Epoch: 2, Samples: 3840/5760, Loss: 3.6442105770111084\n",
      "Epoch: 2, Samples: 3872/5760, Loss: 3.699836254119873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Samples: 3904/5760, Loss: 3.300143003463745\n",
      "Epoch: 2, Samples: 3936/5760, Loss: 3.5177464485168457\n",
      "Epoch: 2, Samples: 3968/5760, Loss: 2.8914601802825928\n",
      "Epoch: 2, Samples: 4000/5760, Loss: 3.719750165939331\n",
      "Epoch: 2, Samples: 4032/5760, Loss: 3.3223493099212646\n",
      "Epoch: 2, Samples: 4064/5760, Loss: 3.5301294326782227\n",
      "Epoch: 2, Samples: 4096/5760, Loss: 3.3233845233917236\n",
      "Epoch: 2, Samples: 4128/5760, Loss: 3.2264957427978516\n",
      "Epoch: 2, Samples: 4160/5760, Loss: 3.459146022796631\n",
      "Epoch: 2, Samples: 4192/5760, Loss: 3.5317118167877197\n",
      "Epoch: 2, Samples: 4224/5760, Loss: 3.4459304809570312\n",
      "Epoch: 2, Samples: 4256/5760, Loss: 3.774745225906372\n",
      "Epoch: 2, Samples: 4288/5760, Loss: 3.3446044921875\n",
      "Epoch: 2, Samples: 4320/5760, Loss: 3.7271814346313477\n",
      "Epoch: 2, Samples: 4352/5760, Loss: 3.1245148181915283\n",
      "Epoch: 2, Samples: 4384/5760, Loss: 3.1806492805480957\n",
      "Epoch: 2, Samples: 4416/5760, Loss: 3.368650436401367\n",
      "Epoch: 2, Samples: 4448/5760, Loss: 2.979912757873535\n",
      "Epoch: 2, Samples: 4480/5760, Loss: 3.3663904666900635\n",
      "Epoch: 2, Samples: 4512/5760, Loss: 3.8118462562561035\n",
      "Epoch: 2, Samples: 4544/5760, Loss: 3.858696460723877\n",
      "Epoch: 2, Samples: 4576/5760, Loss: 3.54201602935791\n",
      "Epoch: 2, Samples: 4608/5760, Loss: 3.457559585571289\n",
      "Epoch: 2, Samples: 4640/5760, Loss: 3.6990456581115723\n",
      "Epoch: 2, Samples: 4672/5760, Loss: 3.4409751892089844\n",
      "Epoch: 2, Samples: 4704/5760, Loss: 3.5140552520751953\n",
      "Epoch: 2, Samples: 4736/5760, Loss: 3.428626775741577\n",
      "Epoch: 2, Samples: 4768/5760, Loss: 3.4766039848327637\n",
      "Epoch: 2, Samples: 4800/5760, Loss: 3.4781062602996826\n",
      "Epoch: 2, Samples: 4832/5760, Loss: 3.5205814838409424\n",
      "Epoch: 2, Samples: 4864/5760, Loss: 3.576167583465576\n",
      "Epoch: 2, Samples: 4896/5760, Loss: 3.6490252017974854\n",
      "Epoch: 2, Samples: 4928/5760, Loss: 3.5381977558135986\n",
      "Epoch: 2, Samples: 4960/5760, Loss: 3.395719528198242\n",
      "Epoch: 2, Samples: 4992/5760, Loss: 3.288778305053711\n",
      "Epoch: 2, Samples: 5024/5760, Loss: 3.3241491317749023\n",
      "Epoch: 2, Samples: 5056/5760, Loss: 3.569223403930664\n",
      "Epoch: 2, Samples: 5088/5760, Loss: 3.574629068374634\n",
      "Epoch: 2, Samples: 5120/5760, Loss: 3.487638235092163\n",
      "Epoch: 2, Samples: 5152/5760, Loss: 3.3441755771636963\n",
      "Epoch: 2, Samples: 5184/5760, Loss: 3.1492974758148193\n",
      "Epoch: 2, Samples: 5216/5760, Loss: 3.404175281524658\n",
      "Epoch: 2, Samples: 5248/5760, Loss: 3.1504950523376465\n",
      "Epoch: 2, Samples: 5280/5760, Loss: 3.468186855316162\n",
      "Epoch: 2, Samples: 5312/5760, Loss: 3.060999870300293\n",
      "Epoch: 2, Samples: 5344/5760, Loss: 3.3006255626678467\n",
      "Epoch: 2, Samples: 5376/5760, Loss: 3.2736294269561768\n",
      "Epoch: 2, Samples: 5408/5760, Loss: 3.1723380088806152\n",
      "Epoch: 2, Samples: 5440/5760, Loss: 3.1072535514831543\n",
      "Epoch: 2, Samples: 5472/5760, Loss: 3.0575664043426514\n",
      "Epoch: 2, Samples: 5504/5760, Loss: 3.304708957672119\n",
      "Epoch: 2, Samples: 5536/5760, Loss: 3.3687283992767334\n",
      "Epoch: 2, Samples: 5568/5760, Loss: 3.824862241744995\n",
      "Epoch: 2, Samples: 5600/5760, Loss: 3.173523426055908\n",
      "Epoch: 2, Samples: 5632/5760, Loss: 3.0779829025268555\n",
      "Epoch: 2, Samples: 5664/5760, Loss: 3.3133034706115723\n",
      "Epoch: 2, Samples: 5696/5760, Loss: 3.3663244247436523\n",
      "Epoch: 2, Samples: 5728/5760, Loss: 2.691063404083252\n",
      "\n",
      "Epoch: 2\n",
      "Training set: Average loss: 3.5506\n",
      "Validation set: Average loss: 3.3075, Accuracy: 166/818 (20%)\n",
      "Saving model (epoch 2) with lowest validation loss: 3.3074573370126576\n",
      "Epoch: 3, Samples: 0/5760, Loss: 3.239230155944824\n",
      "Epoch: 3, Samples: 32/5760, Loss: 3.2188620567321777\n",
      "Epoch: 3, Samples: 64/5760, Loss: 3.182443380355835\n",
      "Epoch: 3, Samples: 96/5760, Loss: 3.076404094696045\n",
      "Epoch: 3, Samples: 128/5760, Loss: 3.1702017784118652\n",
      "Epoch: 3, Samples: 160/5760, Loss: 3.114671230316162\n",
      "Epoch: 3, Samples: 192/5760, Loss: 3.4887094497680664\n",
      "Epoch: 3, Samples: 224/5760, Loss: 3.1344733238220215\n",
      "Epoch: 3, Samples: 256/5760, Loss: 3.5710978507995605\n",
      "Epoch: 3, Samples: 288/5760, Loss: 3.5499751567840576\n",
      "Epoch: 3, Samples: 320/5760, Loss: 3.1849172115325928\n",
      "Epoch: 3, Samples: 352/5760, Loss: 3.1912567615509033\n",
      "Epoch: 3, Samples: 384/5760, Loss: 2.9878509044647217\n",
      "Epoch: 3, Samples: 416/5760, Loss: 3.35451078414917\n",
      "Epoch: 3, Samples: 448/5760, Loss: 3.3721327781677246\n",
      "Epoch: 3, Samples: 480/5760, Loss: 3.3991141319274902\n",
      "Epoch: 3, Samples: 512/5760, Loss: 3.347292423248291\n",
      "Epoch: 3, Samples: 544/5760, Loss: 3.087977409362793\n",
      "Epoch: 3, Samples: 576/5760, Loss: 3.4021196365356445\n",
      "Epoch: 3, Samples: 608/5760, Loss: 3.3037312030792236\n",
      "Epoch: 3, Samples: 640/5760, Loss: 3.410428285598755\n",
      "Epoch: 3, Samples: 672/5760, Loss: 3.1216912269592285\n",
      "Epoch: 3, Samples: 704/5760, Loss: 3.607909679412842\n",
      "Epoch: 3, Samples: 736/5760, Loss: 3.830493450164795\n",
      "Epoch: 3, Samples: 768/5760, Loss: 3.479048490524292\n",
      "Epoch: 3, Samples: 800/5760, Loss: 3.098618984222412\n",
      "Epoch: 3, Samples: 832/5760, Loss: 3.5516297817230225\n",
      "Epoch: 3, Samples: 864/5760, Loss: 3.533557176589966\n",
      "Epoch: 3, Samples: 896/5760, Loss: 3.5120983123779297\n",
      "Epoch: 3, Samples: 928/5760, Loss: 3.3786940574645996\n",
      "Epoch: 3, Samples: 960/5760, Loss: 3.1466307640075684\n",
      "Epoch: 3, Samples: 992/5760, Loss: 3.249722719192505\n",
      "Epoch: 3, Samples: 1024/5760, Loss: 3.290541648864746\n",
      "Epoch: 3, Samples: 1056/5760, Loss: 3.554704189300537\n",
      "Epoch: 3, Samples: 1088/5760, Loss: 3.6391682624816895\n",
      "Epoch: 3, Samples: 1120/5760, Loss: 3.02864408493042\n",
      "Epoch: 3, Samples: 1152/5760, Loss: 3.2991230487823486\n",
      "Epoch: 3, Samples: 1184/5760, Loss: 3.383495807647705\n",
      "Epoch: 3, Samples: 1216/5760, Loss: 3.348148822784424\n",
      "Epoch: 3, Samples: 1248/5760, Loss: 3.265453815460205\n",
      "Epoch: 3, Samples: 1280/5760, Loss: 3.351236343383789\n",
      "Epoch: 3, Samples: 1312/5760, Loss: 3.388284683227539\n",
      "Epoch: 3, Samples: 1344/5760, Loss: 3.2097418308258057\n",
      "Epoch: 3, Samples: 1376/5760, Loss: 3.584028482437134\n",
      "Epoch: 3, Samples: 1408/5760, Loss: 3.182001829147339\n",
      "Epoch: 3, Samples: 1440/5760, Loss: 3.173086404800415\n",
      "Epoch: 3, Samples: 1472/5760, Loss: 3.4973645210266113\n",
      "Epoch: 3, Samples: 1504/5760, Loss: 3.431511640548706\n",
      "Epoch: 3, Samples: 1536/5760, Loss: 3.5916061401367188\n",
      "Epoch: 3, Samples: 1568/5760, Loss: 3.0758073329925537\n",
      "Epoch: 3, Samples: 1600/5760, Loss: 3.1148087978363037\n",
      "Epoch: 3, Samples: 1632/5760, Loss: 3.2656197547912598\n",
      "Epoch: 3, Samples: 1664/5760, Loss: 3.1557090282440186\n",
      "Epoch: 3, Samples: 1696/5760, Loss: 3.4268314838409424\n",
      "Epoch: 3, Samples: 1728/5760, Loss: 3.6317026615142822\n",
      "Epoch: 3, Samples: 1760/5760, Loss: 3.231616258621216\n",
      "Epoch: 3, Samples: 1792/5760, Loss: 3.3074841499328613\n",
      "Epoch: 3, Samples: 1824/5760, Loss: 3.4725229740142822\n",
      "Epoch: 3, Samples: 1856/5760, Loss: 2.9781103134155273\n",
      "Epoch: 3, Samples: 1888/5760, Loss: 3.3278210163116455\n",
      "Epoch: 3, Samples: 1920/5760, Loss: 2.8372535705566406\n",
      "Epoch: 3, Samples: 1952/5760, Loss: 2.860015392303467\n",
      "Epoch: 3, Samples: 1984/5760, Loss: 3.3943960666656494\n",
      "Epoch: 3, Samples: 2016/5760, Loss: 3.303856611251831\n",
      "Epoch: 3, Samples: 2048/5760, Loss: 3.4029252529144287\n",
      "Epoch: 3, Samples: 2080/5760, Loss: 3.1661345958709717\n",
      "Epoch: 3, Samples: 2112/5760, Loss: 3.3444952964782715\n",
      "Epoch: 3, Samples: 2144/5760, Loss: 3.009345531463623\n",
      "Epoch: 3, Samples: 2176/5760, Loss: 3.533247232437134\n",
      "Epoch: 3, Samples: 2208/5760, Loss: 3.315044641494751\n",
      "Epoch: 3, Samples: 2240/5760, Loss: 3.083181619644165\n",
      "Epoch: 3, Samples: 2272/5760, Loss: 3.410383701324463\n",
      "Epoch: 3, Samples: 2304/5760, Loss: 3.0561885833740234\n",
      "Epoch: 3, Samples: 2336/5760, Loss: 2.9196431636810303\n",
      "Epoch: 3, Samples: 2368/5760, Loss: 3.1791229248046875\n",
      "Epoch: 3, Samples: 2400/5760, Loss: 3.377815008163452\n",
      "Epoch: 3, Samples: 2432/5760, Loss: 3.405418872833252\n",
      "Epoch: 3, Samples: 2464/5760, Loss: 3.358459949493408\n",
      "Epoch: 3, Samples: 2496/5760, Loss: 3.243755340576172\n",
      "Epoch: 3, Samples: 2528/5760, Loss: 3.0245554447174072\n",
      "Epoch: 3, Samples: 2560/5760, Loss: 3.1023170948028564\n",
      "Epoch: 3, Samples: 2592/5760, Loss: 3.13301944732666\n",
      "Epoch: 3, Samples: 2624/5760, Loss: 3.2909202575683594\n",
      "Epoch: 3, Samples: 2656/5760, Loss: 3.2213246822357178\n",
      "Epoch: 3, Samples: 2688/5760, Loss: 3.5032594203948975\n",
      "Epoch: 3, Samples: 2720/5760, Loss: 3.269655466079712\n",
      "Epoch: 3, Samples: 2752/5760, Loss: 3.2276926040649414\n",
      "Epoch: 3, Samples: 2784/5760, Loss: 2.8938095569610596\n",
      "Epoch: 3, Samples: 2816/5760, Loss: 3.07496976852417\n",
      "Epoch: 3, Samples: 2848/5760, Loss: 3.060110330581665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Samples: 2880/5760, Loss: 3.1219582557678223\n",
      "Epoch: 3, Samples: 2912/5760, Loss: 3.2512362003326416\n",
      "Epoch: 3, Samples: 2944/5760, Loss: 3.2126340866088867\n",
      "Epoch: 3, Samples: 2976/5760, Loss: 3.2187273502349854\n",
      "Epoch: 3, Samples: 3008/5760, Loss: 2.7114851474761963\n",
      "Epoch: 3, Samples: 3040/5760, Loss: 3.584916830062866\n",
      "Epoch: 3, Samples: 3072/5760, Loss: 3.082462787628174\n",
      "Epoch: 3, Samples: 3104/5760, Loss: 2.9165618419647217\n",
      "Epoch: 3, Samples: 3136/5760, Loss: 3.405723810195923\n",
      "Epoch: 3, Samples: 3168/5760, Loss: 3.1424598693847656\n",
      "Epoch: 3, Samples: 3200/5760, Loss: 3.163007974624634\n",
      "Epoch: 3, Samples: 3232/5760, Loss: 2.882164239883423\n",
      "Epoch: 3, Samples: 3264/5760, Loss: 3.170677661895752\n",
      "Epoch: 3, Samples: 3296/5760, Loss: 3.3496575355529785\n",
      "Epoch: 3, Samples: 3328/5760, Loss: 3.184298515319824\n",
      "Epoch: 3, Samples: 3360/5760, Loss: 3.0110490322113037\n",
      "Epoch: 3, Samples: 3392/5760, Loss: 3.734663724899292\n",
      "Epoch: 3, Samples: 3424/5760, Loss: 3.760154962539673\n",
      "Epoch: 3, Samples: 3456/5760, Loss: 3.148041009902954\n",
      "Epoch: 3, Samples: 3488/5760, Loss: 2.58766770362854\n",
      "Epoch: 3, Samples: 3520/5760, Loss: 3.045077323913574\n",
      "Epoch: 3, Samples: 3552/5760, Loss: 3.1367595195770264\n",
      "Epoch: 3, Samples: 3584/5760, Loss: 3.4682533740997314\n",
      "Epoch: 3, Samples: 3616/5760, Loss: 3.046823740005493\n",
      "Epoch: 3, Samples: 3648/5760, Loss: 3.0573298931121826\n",
      "Epoch: 3, Samples: 3680/5760, Loss: 3.21429443359375\n",
      "Epoch: 3, Samples: 3712/5760, Loss: 2.995913028717041\n",
      "Epoch: 3, Samples: 3744/5760, Loss: 2.6515440940856934\n",
      "Epoch: 3, Samples: 3776/5760, Loss: 2.843356132507324\n",
      "Epoch: 3, Samples: 3808/5760, Loss: 2.754952907562256\n",
      "Epoch: 3, Samples: 3840/5760, Loss: 2.905134916305542\n",
      "Epoch: 3, Samples: 3872/5760, Loss: 3.2915329933166504\n",
      "Epoch: 3, Samples: 3904/5760, Loss: 3.0067198276519775\n",
      "Epoch: 3, Samples: 3936/5760, Loss: 2.949612617492676\n",
      "Epoch: 3, Samples: 3968/5760, Loss: 3.291262149810791\n",
      "Epoch: 3, Samples: 4000/5760, Loss: 2.912898540496826\n",
      "Epoch: 3, Samples: 4032/5760, Loss: 3.611696481704712\n",
      "Epoch: 3, Samples: 4064/5760, Loss: 3.0308852195739746\n",
      "Epoch: 3, Samples: 4096/5760, Loss: 2.87674617767334\n",
      "Epoch: 3, Samples: 4128/5760, Loss: 3.019169330596924\n",
      "Epoch: 3, Samples: 4160/5760, Loss: 2.837742805480957\n",
      "Epoch: 3, Samples: 4192/5760, Loss: 3.123082160949707\n",
      "Epoch: 3, Samples: 4224/5760, Loss: 3.1146302223205566\n",
      "Epoch: 3, Samples: 4256/5760, Loss: 2.9912688732147217\n",
      "Epoch: 3, Samples: 4288/5760, Loss: 3.34816312789917\n",
      "Epoch: 3, Samples: 4320/5760, Loss: 3.130056619644165\n",
      "Epoch: 3, Samples: 4352/5760, Loss: 3.252716302871704\n",
      "Epoch: 3, Samples: 4384/5760, Loss: 3.4213383197784424\n",
      "Epoch: 3, Samples: 4416/5760, Loss: 3.2617621421813965\n",
      "Epoch: 3, Samples: 4448/5760, Loss: 2.944438934326172\n",
      "Epoch: 3, Samples: 4480/5760, Loss: 3.2201292514801025\n",
      "Epoch: 3, Samples: 4512/5760, Loss: 3.4897077083587646\n",
      "Epoch: 3, Samples: 4544/5760, Loss: 3.233765125274658\n",
      "Epoch: 3, Samples: 4576/5760, Loss: 2.8827922344207764\n",
      "Epoch: 3, Samples: 4608/5760, Loss: 3.4096710681915283\n",
      "Epoch: 3, Samples: 4640/5760, Loss: 2.8880789279937744\n",
      "Epoch: 3, Samples: 4672/5760, Loss: 2.932969093322754\n",
      "Epoch: 3, Samples: 4704/5760, Loss: 3.437042236328125\n",
      "Epoch: 3, Samples: 4736/5760, Loss: 3.0122454166412354\n",
      "Epoch: 3, Samples: 4768/5760, Loss: 3.764843702316284\n",
      "Epoch: 3, Samples: 4800/5760, Loss: 2.74405574798584\n",
      "Epoch: 3, Samples: 4832/5760, Loss: 3.1012372970581055\n",
      "Epoch: 3, Samples: 4864/5760, Loss: 2.8900177478790283\n",
      "Epoch: 3, Samples: 4896/5760, Loss: 2.4743144512176514\n",
      "Epoch: 3, Samples: 4928/5760, Loss: 3.1975150108337402\n",
      "Epoch: 3, Samples: 4960/5760, Loss: 2.8493125438690186\n",
      "Epoch: 3, Samples: 4992/5760, Loss: 3.3647193908691406\n",
      "Epoch: 3, Samples: 5024/5760, Loss: 3.044950485229492\n",
      "Epoch: 3, Samples: 5056/5760, Loss: 3.2021405696868896\n",
      "Epoch: 3, Samples: 5088/5760, Loss: 2.7815325260162354\n",
      "Epoch: 3, Samples: 5120/5760, Loss: 2.9932470321655273\n",
      "Epoch: 3, Samples: 5152/5760, Loss: 3.10662579536438\n",
      "Epoch: 3, Samples: 5184/5760, Loss: 2.7929720878601074\n",
      "Epoch: 3, Samples: 5216/5760, Loss: 3.508603572845459\n",
      "Epoch: 3, Samples: 5248/5760, Loss: 3.186798334121704\n",
      "Epoch: 3, Samples: 5280/5760, Loss: 3.0906074047088623\n",
      "Epoch: 3, Samples: 5312/5760, Loss: 2.9031577110290527\n",
      "Epoch: 3, Samples: 5344/5760, Loss: 3.2761154174804688\n",
      "Epoch: 3, Samples: 5376/5760, Loss: 3.0498666763305664\n",
      "Epoch: 3, Samples: 5408/5760, Loss: 3.140625476837158\n",
      "Epoch: 3, Samples: 5440/5760, Loss: 3.112955093383789\n",
      "Epoch: 3, Samples: 5472/5760, Loss: 3.109037160873413\n",
      "Epoch: 3, Samples: 5504/5760, Loss: 3.209804058074951\n",
      "Epoch: 3, Samples: 5536/5760, Loss: 3.4862911701202393\n",
      "Epoch: 3, Samples: 5568/5760, Loss: 3.190382480621338\n",
      "Epoch: 3, Samples: 5600/5760, Loss: 2.794527530670166\n",
      "Epoch: 3, Samples: 5632/5760, Loss: 3.2487192153930664\n",
      "Epoch: 3, Samples: 5664/5760, Loss: 2.8240768909454346\n",
      "Epoch: 3, Samples: 5696/5760, Loss: 3.3790602684020996\n",
      "Epoch: 3, Samples: 5728/5760, Loss: 3.1198086738586426\n",
      "\n",
      "Epoch: 3\n",
      "Training set: Average loss: 3.2007\n",
      "Validation set: Average loss: 3.1286, Accuracy: 189/818 (23%)\n",
      "Saving model (epoch 3) with lowest validation loss: 3.128558929149921\n",
      "Epoch: 4, Samples: 0/5760, Loss: 3.014583110809326\n",
      "Epoch: 4, Samples: 32/5760, Loss: 2.9913694858551025\n",
      "Epoch: 4, Samples: 64/5760, Loss: 2.979452133178711\n",
      "Epoch: 4, Samples: 96/5760, Loss: 2.9380366802215576\n",
      "Epoch: 4, Samples: 128/5760, Loss: 2.9116361141204834\n",
      "Epoch: 4, Samples: 160/5760, Loss: 3.175255537033081\n",
      "Epoch: 4, Samples: 192/5760, Loss: 3.3132896423339844\n",
      "Epoch: 4, Samples: 224/5760, Loss: 2.9105467796325684\n",
      "Epoch: 4, Samples: 256/5760, Loss: 3.2142422199249268\n",
      "Epoch: 4, Samples: 288/5760, Loss: 3.2758731842041016\n",
      "Epoch: 4, Samples: 320/5760, Loss: 2.8445286750793457\n",
      "Epoch: 4, Samples: 352/5760, Loss: 2.8937673568725586\n",
      "Epoch: 4, Samples: 384/5760, Loss: 3.1215460300445557\n",
      "Epoch: 4, Samples: 416/5760, Loss: 3.1499176025390625\n",
      "Epoch: 4, Samples: 448/5760, Loss: 3.022880792617798\n",
      "Epoch: 4, Samples: 480/5760, Loss: 3.2222561836242676\n",
      "Epoch: 4, Samples: 512/5760, Loss: 2.8062357902526855\n",
      "Epoch: 4, Samples: 544/5760, Loss: 3.1349685192108154\n",
      "Epoch: 4, Samples: 576/5760, Loss: 2.6684439182281494\n",
      "Epoch: 4, Samples: 608/5760, Loss: 3.054227590560913\n",
      "Epoch: 4, Samples: 640/5760, Loss: 2.826921224594116\n",
      "Epoch: 4, Samples: 672/5760, Loss: 2.8900246620178223\n",
      "Epoch: 4, Samples: 704/5760, Loss: 3.1396372318267822\n",
      "Epoch: 4, Samples: 736/5760, Loss: 2.696013927459717\n",
      "Epoch: 4, Samples: 768/5760, Loss: 3.2625861167907715\n",
      "Epoch: 4, Samples: 800/5760, Loss: 3.144599437713623\n",
      "Epoch: 4, Samples: 832/5760, Loss: 2.7220897674560547\n",
      "Epoch: 4, Samples: 864/5760, Loss: 2.8367607593536377\n",
      "Epoch: 4, Samples: 896/5760, Loss: 2.892336368560791\n",
      "Epoch: 4, Samples: 928/5760, Loss: 3.510267972946167\n",
      "Epoch: 4, Samples: 960/5760, Loss: 2.902555465698242\n",
      "Epoch: 4, Samples: 992/5760, Loss: 3.08024263381958\n",
      "Epoch: 4, Samples: 1024/5760, Loss: 2.823227882385254\n",
      "Epoch: 4, Samples: 1056/5760, Loss: 3.076580762863159\n",
      "Epoch: 4, Samples: 1088/5760, Loss: 3.0506017208099365\n",
      "Epoch: 4, Samples: 1120/5760, Loss: 3.336592197418213\n",
      "Epoch: 4, Samples: 1152/5760, Loss: 3.2484099864959717\n",
      "Epoch: 4, Samples: 1184/5760, Loss: 3.3089983463287354\n",
      "Epoch: 4, Samples: 1216/5760, Loss: 3.2398860454559326\n",
      "Epoch: 4, Samples: 1248/5760, Loss: 3.0102949142456055\n",
      "Epoch: 4, Samples: 1280/5760, Loss: 2.975740432739258\n",
      "Epoch: 4, Samples: 1312/5760, Loss: 2.7310240268707275\n",
      "Epoch: 4, Samples: 1344/5760, Loss: 2.8552279472351074\n",
      "Epoch: 4, Samples: 1376/5760, Loss: 2.9090564250946045\n",
      "Epoch: 4, Samples: 1408/5760, Loss: 2.931522846221924\n",
      "Epoch: 4, Samples: 1440/5760, Loss: 2.931109666824341\n",
      "Epoch: 4, Samples: 1472/5760, Loss: 2.8739137649536133\n",
      "Epoch: 4, Samples: 1504/5760, Loss: 2.579993486404419\n",
      "Epoch: 4, Samples: 1536/5760, Loss: 2.9636759757995605\n",
      "Epoch: 4, Samples: 1568/5760, Loss: 3.2050108909606934\n",
      "Epoch: 4, Samples: 1600/5760, Loss: 2.653684377670288\n",
      "Epoch: 4, Samples: 1632/5760, Loss: 2.5270557403564453\n",
      "Epoch: 4, Samples: 1664/5760, Loss: 3.271531820297241\n",
      "Epoch: 4, Samples: 1696/5760, Loss: 2.951965570449829\n",
      "Epoch: 4, Samples: 1728/5760, Loss: 2.966794013977051\n",
      "Epoch: 4, Samples: 1760/5760, Loss: 3.1993370056152344\n",
      "Epoch: 4, Samples: 1792/5760, Loss: 2.73705792427063\n",
      "Epoch: 4, Samples: 1824/5760, Loss: 3.1149368286132812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Samples: 1856/5760, Loss: 3.0655863285064697\n",
      "Epoch: 4, Samples: 1888/5760, Loss: 3.0183892250061035\n",
      "Epoch: 4, Samples: 1920/5760, Loss: 2.461433172225952\n",
      "Epoch: 4, Samples: 1952/5760, Loss: 2.8744583129882812\n",
      "Epoch: 4, Samples: 1984/5760, Loss: 3.0140933990478516\n",
      "Epoch: 4, Samples: 2016/5760, Loss: 2.9616477489471436\n",
      "Epoch: 4, Samples: 2048/5760, Loss: 2.5018625259399414\n",
      "Epoch: 4, Samples: 2080/5760, Loss: 2.7958719730377197\n",
      "Epoch: 4, Samples: 2112/5760, Loss: 2.865875482559204\n",
      "Epoch: 4, Samples: 2144/5760, Loss: 3.2129666805267334\n",
      "Epoch: 4, Samples: 2176/5760, Loss: 2.6496193408966064\n",
      "Epoch: 4, Samples: 2208/5760, Loss: 2.8483567237854004\n",
      "Epoch: 4, Samples: 2240/5760, Loss: 2.8374438285827637\n",
      "Epoch: 4, Samples: 2272/5760, Loss: 2.8167760372161865\n",
      "Epoch: 4, Samples: 2304/5760, Loss: 2.7389330863952637\n",
      "Epoch: 4, Samples: 2336/5760, Loss: 3.2449002265930176\n",
      "Epoch: 4, Samples: 2368/5760, Loss: 3.108225107192993\n",
      "Epoch: 4, Samples: 2400/5760, Loss: 3.3149452209472656\n",
      "Epoch: 4, Samples: 2432/5760, Loss: 3.1136369705200195\n",
      "Epoch: 4, Samples: 2464/5760, Loss: 3.012700080871582\n",
      "Epoch: 4, Samples: 2496/5760, Loss: 3.249455451965332\n",
      "Epoch: 4, Samples: 2528/5760, Loss: 3.0295634269714355\n",
      "Epoch: 4, Samples: 2560/5760, Loss: 3.049556016921997\n",
      "Epoch: 4, Samples: 2592/5760, Loss: 2.968534231185913\n",
      "Epoch: 4, Samples: 2624/5760, Loss: 2.9925713539123535\n",
      "Epoch: 4, Samples: 2656/5760, Loss: 3.380664348602295\n",
      "Epoch: 4, Samples: 2688/5760, Loss: 2.934023380279541\n",
      "Epoch: 4, Samples: 2720/5760, Loss: 3.2910845279693604\n",
      "Epoch: 4, Samples: 2752/5760, Loss: 2.865022897720337\n",
      "Epoch: 4, Samples: 2784/5760, Loss: 3.1063921451568604\n",
      "Epoch: 4, Samples: 2816/5760, Loss: 2.8584370613098145\n",
      "Epoch: 4, Samples: 2848/5760, Loss: 3.1580612659454346\n",
      "Epoch: 4, Samples: 2880/5760, Loss: 2.637042284011841\n",
      "Epoch: 4, Samples: 2912/5760, Loss: 2.912886142730713\n",
      "Epoch: 4, Samples: 2944/5760, Loss: 2.745664358139038\n",
      "Epoch: 4, Samples: 2976/5760, Loss: 3.0609474182128906\n",
      "Epoch: 4, Samples: 3008/5760, Loss: 3.19012188911438\n",
      "Epoch: 4, Samples: 3040/5760, Loss: 3.3882462978363037\n",
      "Epoch: 4, Samples: 3072/5760, Loss: 2.644993543624878\n",
      "Epoch: 4, Samples: 3104/5760, Loss: 3.0693001747131348\n",
      "Epoch: 4, Samples: 3136/5760, Loss: 2.8695125579833984\n",
      "Epoch: 4, Samples: 3168/5760, Loss: 2.4163458347320557\n",
      "Epoch: 4, Samples: 3200/5760, Loss: 3.0950779914855957\n",
      "Epoch: 4, Samples: 3232/5760, Loss: 2.7952539920806885\n",
      "Epoch: 4, Samples: 3264/5760, Loss: 2.8785338401794434\n",
      "Epoch: 4, Samples: 3296/5760, Loss: 2.9716625213623047\n",
      "Epoch: 4, Samples: 3328/5760, Loss: 2.7524471282958984\n",
      "Epoch: 4, Samples: 3360/5760, Loss: 2.8170676231384277\n",
      "Epoch: 4, Samples: 3392/5760, Loss: 3.096158742904663\n",
      "Epoch: 4, Samples: 3424/5760, Loss: 2.8257107734680176\n",
      "Epoch: 4, Samples: 3456/5760, Loss: 2.856048822402954\n",
      "Epoch: 4, Samples: 3488/5760, Loss: 2.972060203552246\n",
      "Epoch: 4, Samples: 3520/5760, Loss: 2.6441900730133057\n",
      "Epoch: 4, Samples: 3552/5760, Loss: 2.9730067253112793\n",
      "Epoch: 4, Samples: 3584/5760, Loss: 2.9895269870758057\n",
      "Epoch: 4, Samples: 3616/5760, Loss: 2.6261720657348633\n",
      "Epoch: 4, Samples: 3648/5760, Loss: 2.8933019638061523\n",
      "Epoch: 4, Samples: 3680/5760, Loss: 2.6916074752807617\n",
      "Epoch: 4, Samples: 3712/5760, Loss: 2.9863622188568115\n",
      "Epoch: 4, Samples: 3744/5760, Loss: 2.9766290187835693\n",
      "Epoch: 4, Samples: 3776/5760, Loss: 2.871798276901245\n",
      "Epoch: 4, Samples: 3808/5760, Loss: 2.993971586227417\n",
      "Epoch: 4, Samples: 3840/5760, Loss: 2.8412692546844482\n",
      "Epoch: 4, Samples: 3872/5760, Loss: 2.8189706802368164\n",
      "Epoch: 4, Samples: 3904/5760, Loss: 2.721733331680298\n",
      "Epoch: 4, Samples: 3936/5760, Loss: 3.4499001502990723\n",
      "Epoch: 4, Samples: 3968/5760, Loss: 2.915369749069214\n",
      "Epoch: 4, Samples: 4000/5760, Loss: 2.9868178367614746\n",
      "Epoch: 4, Samples: 4032/5760, Loss: 3.211416721343994\n",
      "Epoch: 4, Samples: 4064/5760, Loss: 3.0131595134735107\n",
      "Epoch: 4, Samples: 4096/5760, Loss: 2.543914794921875\n",
      "Epoch: 4, Samples: 4128/5760, Loss: 2.886504888534546\n",
      "Epoch: 4, Samples: 4160/5760, Loss: 2.449432373046875\n",
      "Epoch: 4, Samples: 4192/5760, Loss: 2.592207908630371\n",
      "Epoch: 4, Samples: 4224/5760, Loss: 2.936734199523926\n",
      "Epoch: 4, Samples: 4256/5760, Loss: 3.0516860485076904\n",
      "Epoch: 4, Samples: 4288/5760, Loss: 2.6487696170806885\n",
      "Epoch: 4, Samples: 4320/5760, Loss: 2.586089611053467\n",
      "Epoch: 4, Samples: 4352/5760, Loss: 2.7347357273101807\n",
      "Epoch: 4, Samples: 4384/5760, Loss: 2.7355663776397705\n",
      "Epoch: 4, Samples: 4416/5760, Loss: 2.7984161376953125\n",
      "Epoch: 4, Samples: 4448/5760, Loss: 3.408522367477417\n",
      "Epoch: 4, Samples: 4480/5760, Loss: 3.1357262134552\n",
      "Epoch: 4, Samples: 4512/5760, Loss: 2.6717076301574707\n",
      "Epoch: 4, Samples: 4544/5760, Loss: 2.8990113735198975\n",
      "Epoch: 4, Samples: 4576/5760, Loss: 2.8412253856658936\n",
      "Epoch: 4, Samples: 4608/5760, Loss: 2.851120710372925\n",
      "Epoch: 4, Samples: 4640/5760, Loss: 2.8312783241271973\n",
      "Epoch: 4, Samples: 4672/5760, Loss: 2.6325061321258545\n",
      "Epoch: 4, Samples: 4704/5760, Loss: 2.830659866333008\n",
      "Epoch: 4, Samples: 4736/5760, Loss: 3.1153018474578857\n",
      "Epoch: 4, Samples: 4768/5760, Loss: 2.6559765338897705\n",
      "Epoch: 4, Samples: 4800/5760, Loss: 3.166647434234619\n",
      "Epoch: 4, Samples: 4832/5760, Loss: 3.1231257915496826\n",
      "Epoch: 4, Samples: 4864/5760, Loss: 3.09895396232605\n",
      "Epoch: 4, Samples: 4896/5760, Loss: 2.8303658962249756\n",
      "Epoch: 4, Samples: 4928/5760, Loss: 2.7780215740203857\n",
      "Epoch: 4, Samples: 4960/5760, Loss: 2.7890045642852783\n",
      "Epoch: 4, Samples: 4992/5760, Loss: 2.6523759365081787\n",
      "Epoch: 4, Samples: 5024/5760, Loss: 3.310964345932007\n",
      "Epoch: 4, Samples: 5056/5760, Loss: 2.9178740978240967\n",
      "Epoch: 4, Samples: 5088/5760, Loss: 2.9959917068481445\n",
      "Epoch: 4, Samples: 5120/5760, Loss: 2.849766492843628\n",
      "Epoch: 4, Samples: 5152/5760, Loss: 2.7993345260620117\n",
      "Epoch: 4, Samples: 5184/5760, Loss: 3.0046379566192627\n",
      "Epoch: 4, Samples: 5216/5760, Loss: 2.813927173614502\n",
      "Epoch: 4, Samples: 5248/5760, Loss: 2.8741486072540283\n",
      "Epoch: 4, Samples: 5280/5760, Loss: 2.966172456741333\n",
      "Epoch: 4, Samples: 5312/5760, Loss: 3.053821086883545\n",
      "Epoch: 4, Samples: 5344/5760, Loss: 3.1175291538238525\n",
      "Epoch: 4, Samples: 5376/5760, Loss: 2.760434150695801\n",
      "Epoch: 4, Samples: 5408/5760, Loss: 2.5749006271362305\n",
      "Epoch: 4, Samples: 5440/5760, Loss: 2.8262135982513428\n",
      "Epoch: 4, Samples: 5472/5760, Loss: 2.997633457183838\n",
      "Epoch: 4, Samples: 5504/5760, Loss: 2.7750236988067627\n",
      "Epoch: 4, Samples: 5536/5760, Loss: 3.3307199478149414\n",
      "Epoch: 4, Samples: 5568/5760, Loss: 3.009629249572754\n",
      "Epoch: 4, Samples: 5600/5760, Loss: 3.0929501056671143\n",
      "Epoch: 4, Samples: 5632/5760, Loss: 2.8817367553710938\n",
      "Epoch: 4, Samples: 5664/5760, Loss: 2.793219566345215\n",
      "Epoch: 4, Samples: 5696/5760, Loss: 2.8607325553894043\n",
      "Epoch: 4, Samples: 5728/5760, Loss: 2.352996349334717\n",
      "\n",
      "Epoch: 4\n",
      "Training set: Average loss: 2.9400\n",
      "Validation set: Average loss: 3.0233, Accuracy: 198/818 (24%)\n",
      "Saving model (epoch 4) with lowest validation loss: 3.023250699043274\n",
      "Epoch: 5, Samples: 0/5760, Loss: 2.8709352016448975\n",
      "Epoch: 5, Samples: 32/5760, Loss: 2.7512831687927246\n",
      "Epoch: 5, Samples: 64/5760, Loss: 2.8129329681396484\n",
      "Epoch: 5, Samples: 96/5760, Loss: 3.113848924636841\n",
      "Epoch: 5, Samples: 128/5760, Loss: 3.475400686264038\n",
      "Epoch: 5, Samples: 160/5760, Loss: 2.950760841369629\n",
      "Epoch: 5, Samples: 192/5760, Loss: 3.1340689659118652\n",
      "Epoch: 5, Samples: 224/5760, Loss: 2.8560991287231445\n",
      "Epoch: 5, Samples: 256/5760, Loss: 3.187008857727051\n",
      "Epoch: 5, Samples: 288/5760, Loss: 3.3419837951660156\n",
      "Epoch: 5, Samples: 320/5760, Loss: 2.5830459594726562\n",
      "Epoch: 5, Samples: 352/5760, Loss: 2.6811656951904297\n",
      "Epoch: 5, Samples: 384/5760, Loss: 3.097707509994507\n",
      "Epoch: 5, Samples: 416/5760, Loss: 3.0221917629241943\n",
      "Epoch: 5, Samples: 448/5760, Loss: 2.8724493980407715\n",
      "Epoch: 5, Samples: 480/5760, Loss: 2.770252227783203\n",
      "Epoch: 5, Samples: 512/5760, Loss: 2.7840354442596436\n",
      "Epoch: 5, Samples: 544/5760, Loss: 2.909681558609009\n",
      "Epoch: 5, Samples: 576/5760, Loss: 2.4815165996551514\n",
      "Epoch: 5, Samples: 608/5760, Loss: 2.7812440395355225\n",
      "Epoch: 5, Samples: 640/5760, Loss: 2.8847365379333496\n",
      "Epoch: 5, Samples: 672/5760, Loss: 2.7101101875305176\n",
      "Epoch: 5, Samples: 704/5760, Loss: 3.0678908824920654\n",
      "Epoch: 5, Samples: 736/5760, Loss: 2.5153937339782715\n",
      "Epoch: 5, Samples: 768/5760, Loss: 3.0025367736816406\n",
      "Epoch: 5, Samples: 800/5760, Loss: 2.9026403427124023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Samples: 832/5760, Loss: 2.909635305404663\n",
      "Epoch: 5, Samples: 864/5760, Loss: 2.9611287117004395\n",
      "Epoch: 5, Samples: 896/5760, Loss: 2.747267723083496\n",
      "Epoch: 5, Samples: 928/5760, Loss: 3.1059653759002686\n",
      "Epoch: 5, Samples: 960/5760, Loss: 3.1615381240844727\n",
      "Epoch: 5, Samples: 992/5760, Loss: 2.82917857170105\n",
      "Epoch: 5, Samples: 1024/5760, Loss: 2.7060019969940186\n",
      "Epoch: 5, Samples: 1056/5760, Loss: 2.8469552993774414\n",
      "Epoch: 5, Samples: 1088/5760, Loss: 3.031766891479492\n",
      "Epoch: 5, Samples: 1120/5760, Loss: 2.5951027870178223\n",
      "Epoch: 5, Samples: 1152/5760, Loss: 2.3759922981262207\n",
      "Epoch: 5, Samples: 1184/5760, Loss: 2.609998941421509\n",
      "Epoch: 5, Samples: 1216/5760, Loss: 2.7445664405822754\n",
      "Epoch: 5, Samples: 1248/5760, Loss: 2.8977315425872803\n",
      "Epoch: 5, Samples: 1280/5760, Loss: 2.954219102859497\n",
      "Epoch: 5, Samples: 1312/5760, Loss: 2.584742784500122\n",
      "Epoch: 5, Samples: 1344/5760, Loss: 3.5395002365112305\n",
      "Epoch: 5, Samples: 1376/5760, Loss: 3.071563959121704\n",
      "Epoch: 5, Samples: 1408/5760, Loss: 3.0042781829833984\n",
      "Epoch: 5, Samples: 1440/5760, Loss: 2.916281223297119\n",
      "Epoch: 5, Samples: 1472/5760, Loss: 2.581430435180664\n",
      "Epoch: 5, Samples: 1504/5760, Loss: 3.153493642807007\n",
      "Epoch: 5, Samples: 1536/5760, Loss: 2.8728134632110596\n",
      "Epoch: 5, Samples: 1568/5760, Loss: 3.100559949874878\n",
      "Epoch: 5, Samples: 1600/5760, Loss: 2.6510636806488037\n",
      "Epoch: 5, Samples: 1632/5760, Loss: 2.4492437839508057\n",
      "Epoch: 5, Samples: 1664/5760, Loss: 2.248964786529541\n",
      "Epoch: 5, Samples: 1696/5760, Loss: 2.5926551818847656\n",
      "Epoch: 5, Samples: 1728/5760, Loss: 2.5523035526275635\n",
      "Epoch: 5, Samples: 1760/5760, Loss: 2.879610776901245\n",
      "Epoch: 5, Samples: 1792/5760, Loss: 2.7753593921661377\n",
      "Epoch: 5, Samples: 1824/5760, Loss: 2.5347933769226074\n",
      "Epoch: 5, Samples: 1856/5760, Loss: 2.455648899078369\n",
      "Epoch: 5, Samples: 1888/5760, Loss: 2.685356616973877\n",
      "Epoch: 5, Samples: 1920/5760, Loss: 2.522075891494751\n",
      "Epoch: 5, Samples: 1952/5760, Loss: 3.155832290649414\n",
      "Epoch: 5, Samples: 1984/5760, Loss: 2.3442091941833496\n",
      "Epoch: 5, Samples: 2016/5760, Loss: 2.5681910514831543\n",
      "Epoch: 5, Samples: 2048/5760, Loss: 3.169429302215576\n",
      "Epoch: 5, Samples: 2080/5760, Loss: 2.8815693855285645\n",
      "Epoch: 5, Samples: 2112/5760, Loss: 2.6745259761810303\n",
      "Epoch: 5, Samples: 2144/5760, Loss: 2.9823930263519287\n",
      "Epoch: 5, Samples: 2176/5760, Loss: 2.743605136871338\n",
      "Epoch: 5, Samples: 2208/5760, Loss: 2.256486415863037\n",
      "Epoch: 5, Samples: 2240/5760, Loss: 2.7968344688415527\n",
      "Epoch: 5, Samples: 2272/5760, Loss: 2.8863067626953125\n",
      "Epoch: 5, Samples: 2304/5760, Loss: 3.3327319622039795\n",
      "Epoch: 5, Samples: 2336/5760, Loss: 2.731106758117676\n",
      "Epoch: 5, Samples: 2368/5760, Loss: 2.674673080444336\n",
      "Epoch: 5, Samples: 2400/5760, Loss: 2.7845818996429443\n",
      "Epoch: 5, Samples: 2432/5760, Loss: 2.8402037620544434\n",
      "Epoch: 5, Samples: 2464/5760, Loss: 2.7195324897766113\n",
      "Epoch: 5, Samples: 2496/5760, Loss: 3.075441360473633\n",
      "Epoch: 5, Samples: 2528/5760, Loss: 2.8569185733795166\n",
      "Epoch: 5, Samples: 2560/5760, Loss: 2.6761786937713623\n",
      "Epoch: 5, Samples: 2592/5760, Loss: 2.9970996379852295\n",
      "Epoch: 5, Samples: 2624/5760, Loss: 2.3507773876190186\n",
      "Epoch: 5, Samples: 2656/5760, Loss: 3.1717655658721924\n",
      "Epoch: 5, Samples: 2688/5760, Loss: 2.5549774169921875\n",
      "Epoch: 5, Samples: 2720/5760, Loss: 2.8542072772979736\n",
      "Epoch: 5, Samples: 2752/5760, Loss: 2.3828654289245605\n",
      "Epoch: 5, Samples: 2784/5760, Loss: 2.9387447834014893\n",
      "Epoch: 5, Samples: 2816/5760, Loss: 2.7516398429870605\n",
      "Epoch: 5, Samples: 2848/5760, Loss: 2.6897072792053223\n",
      "Epoch: 5, Samples: 2880/5760, Loss: 2.6481149196624756\n",
      "Epoch: 5, Samples: 2912/5760, Loss: 2.2717111110687256\n",
      "Epoch: 5, Samples: 2944/5760, Loss: 2.6832921504974365\n",
      "Epoch: 5, Samples: 2976/5760, Loss: 2.59025502204895\n",
      "Epoch: 5, Samples: 3008/5760, Loss: 2.6286959648132324\n",
      "Epoch: 5, Samples: 3040/5760, Loss: 3.0908939838409424\n",
      "Epoch: 5, Samples: 3072/5760, Loss: 2.6735589504241943\n",
      "Epoch: 5, Samples: 3104/5760, Loss: 2.4421794414520264\n",
      "Epoch: 5, Samples: 3136/5760, Loss: 2.752706289291382\n",
      "Epoch: 5, Samples: 3168/5760, Loss: 3.144836187362671\n",
      "Epoch: 5, Samples: 3200/5760, Loss: 2.6670312881469727\n",
      "Epoch: 5, Samples: 3232/5760, Loss: 2.9811275005340576\n",
      "Epoch: 5, Samples: 3264/5760, Loss: 2.619945764541626\n",
      "Epoch: 5, Samples: 3296/5760, Loss: 2.7254786491394043\n",
      "Epoch: 5, Samples: 3328/5760, Loss: 2.3364644050598145\n",
      "Epoch: 5, Samples: 3360/5760, Loss: 2.682755708694458\n",
      "Epoch: 5, Samples: 3392/5760, Loss: 3.113898754119873\n",
      "Epoch: 5, Samples: 3424/5760, Loss: 2.996873140335083\n",
      "Epoch: 5, Samples: 3456/5760, Loss: 2.5660877227783203\n",
      "Epoch: 5, Samples: 3488/5760, Loss: 2.9012341499328613\n",
      "Epoch: 5, Samples: 3520/5760, Loss: 2.396348476409912\n",
      "Epoch: 5, Samples: 3552/5760, Loss: 2.782367467880249\n",
      "Epoch: 5, Samples: 3584/5760, Loss: 2.7148659229278564\n",
      "Epoch: 5, Samples: 3616/5760, Loss: 2.666478157043457\n",
      "Epoch: 5, Samples: 3648/5760, Loss: 2.392814874649048\n",
      "Epoch: 5, Samples: 3680/5760, Loss: 3.1158366203308105\n",
      "Epoch: 5, Samples: 3712/5760, Loss: 2.764867067337036\n",
      "Epoch: 5, Samples: 3744/5760, Loss: 2.8843648433685303\n",
      "Epoch: 5, Samples: 3776/5760, Loss: 2.9937307834625244\n",
      "Epoch: 5, Samples: 3808/5760, Loss: 2.713754415512085\n",
      "Epoch: 5, Samples: 3840/5760, Loss: 2.895700216293335\n",
      "Epoch: 5, Samples: 3872/5760, Loss: 2.6730167865753174\n",
      "Epoch: 5, Samples: 3904/5760, Loss: 3.1271042823791504\n",
      "Epoch: 5, Samples: 3936/5760, Loss: 2.500558853149414\n",
      "Epoch: 5, Samples: 3968/5760, Loss: 2.9027810096740723\n",
      "Epoch: 5, Samples: 4000/5760, Loss: 2.830166816711426\n",
      "Epoch: 5, Samples: 4032/5760, Loss: 2.441216468811035\n",
      "Epoch: 5, Samples: 4064/5760, Loss: 2.592712163925171\n",
      "Epoch: 5, Samples: 4096/5760, Loss: 2.65645694732666\n",
      "Epoch: 5, Samples: 4128/5760, Loss: 3.388028144836426\n",
      "Epoch: 5, Samples: 4160/5760, Loss: 2.7333498001098633\n",
      "Epoch: 5, Samples: 4192/5760, Loss: 2.448219060897827\n",
      "Epoch: 5, Samples: 4224/5760, Loss: 2.716768503189087\n",
      "Epoch: 5, Samples: 4256/5760, Loss: 2.582432270050049\n",
      "Epoch: 5, Samples: 4288/5760, Loss: 2.7225310802459717\n",
      "Epoch: 5, Samples: 4320/5760, Loss: 2.6768341064453125\n",
      "Epoch: 5, Samples: 4352/5760, Loss: 2.5594050884246826\n",
      "Epoch: 5, Samples: 4384/5760, Loss: 2.706195592880249\n",
      "Epoch: 5, Samples: 4416/5760, Loss: 3.1638898849487305\n",
      "Epoch: 5, Samples: 4448/5760, Loss: 2.915928840637207\n",
      "Epoch: 5, Samples: 4480/5760, Loss: 3.088724136352539\n",
      "Epoch: 5, Samples: 4512/5760, Loss: 2.548769235610962\n",
      "Epoch: 5, Samples: 4544/5760, Loss: 2.8999857902526855\n",
      "Epoch: 5, Samples: 4576/5760, Loss: 2.7208895683288574\n",
      "Epoch: 5, Samples: 4608/5760, Loss: 2.7192230224609375\n",
      "Epoch: 5, Samples: 4640/5760, Loss: 1.9467077255249023\n",
      "Epoch: 5, Samples: 4672/5760, Loss: 2.7391417026519775\n",
      "Epoch: 5, Samples: 4704/5760, Loss: 2.7925527095794678\n",
      "Epoch: 5, Samples: 4736/5760, Loss: 3.0051584243774414\n",
      "Epoch: 5, Samples: 4768/5760, Loss: 2.7272422313690186\n",
      "Epoch: 5, Samples: 4800/5760, Loss: 2.938133716583252\n",
      "Epoch: 5, Samples: 4832/5760, Loss: 2.872814178466797\n",
      "Epoch: 5, Samples: 4864/5760, Loss: 2.346735954284668\n",
      "Epoch: 5, Samples: 4896/5760, Loss: 2.6611618995666504\n",
      "Epoch: 5, Samples: 4928/5760, Loss: 2.5433857440948486\n",
      "Epoch: 5, Samples: 4960/5760, Loss: 3.126016616821289\n",
      "Epoch: 5, Samples: 4992/5760, Loss: 2.367934465408325\n",
      "Epoch: 5, Samples: 5024/5760, Loss: 2.3595592975616455\n",
      "Epoch: 5, Samples: 5056/5760, Loss: 2.9717204570770264\n",
      "Epoch: 5, Samples: 5088/5760, Loss: 2.686809778213501\n",
      "Epoch: 5, Samples: 5120/5760, Loss: 2.2685749530792236\n",
      "Epoch: 5, Samples: 5152/5760, Loss: 2.5364301204681396\n",
      "Epoch: 5, Samples: 5184/5760, Loss: 2.5721044540405273\n",
      "Epoch: 5, Samples: 5216/5760, Loss: 3.0977933406829834\n",
      "Epoch: 5, Samples: 5248/5760, Loss: 2.8447420597076416\n",
      "Epoch: 5, Samples: 5280/5760, Loss: 3.1282095909118652\n",
      "Epoch: 5, Samples: 5312/5760, Loss: 2.647601842880249\n",
      "Epoch: 5, Samples: 5344/5760, Loss: 2.90578031539917\n",
      "Epoch: 5, Samples: 5376/5760, Loss: 2.946509599685669\n",
      "Epoch: 5, Samples: 5408/5760, Loss: 3.0012059211730957\n",
      "Epoch: 5, Samples: 5440/5760, Loss: 2.472644567489624\n",
      "Epoch: 5, Samples: 5472/5760, Loss: 2.9716384410858154\n",
      "Epoch: 5, Samples: 5504/5760, Loss: 2.644291639328003\n",
      "Epoch: 5, Samples: 5536/5760, Loss: 2.7061777114868164\n",
      "Epoch: 5, Samples: 5568/5760, Loss: 2.562983751296997\n",
      "Epoch: 5, Samples: 5600/5760, Loss: 3.046440601348877\n",
      "Epoch: 5, Samples: 5632/5760, Loss: 2.8278985023498535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Samples: 5664/5760, Loss: 2.724876642227173\n",
      "Epoch: 5, Samples: 5696/5760, Loss: 2.6027870178222656\n",
      "Epoch: 5, Samples: 5728/5760, Loss: 3.361424446105957\n",
      "\n",
      "Epoch: 5\n",
      "Training set: Average loss: 2.7842\n",
      "Validation set: Average loss: 3.1761, Accuracy: 187/818 (23%)\n",
      "Epoch: 6, Samples: 0/5760, Loss: 2.4344515800476074\n",
      "Epoch: 6, Samples: 32/5760, Loss: 3.0856099128723145\n",
      "Epoch: 6, Samples: 64/5760, Loss: 2.587679862976074\n",
      "Epoch: 6, Samples: 96/5760, Loss: 2.2801973819732666\n",
      "Epoch: 6, Samples: 128/5760, Loss: 2.8249897956848145\n",
      "Epoch: 6, Samples: 160/5760, Loss: 2.5254900455474854\n",
      "Epoch: 6, Samples: 192/5760, Loss: 2.5074551105499268\n",
      "Epoch: 6, Samples: 224/5760, Loss: 2.3055121898651123\n",
      "Epoch: 6, Samples: 256/5760, Loss: 2.5563855171203613\n",
      "Epoch: 6, Samples: 288/5760, Loss: 3.0244858264923096\n",
      "Epoch: 6, Samples: 320/5760, Loss: 2.6709976196289062\n",
      "Epoch: 6, Samples: 352/5760, Loss: 3.0514636039733887\n",
      "Epoch: 6, Samples: 384/5760, Loss: 2.4978628158569336\n",
      "Epoch: 6, Samples: 416/5760, Loss: 2.5355608463287354\n",
      "Epoch: 6, Samples: 448/5760, Loss: 2.6119706630706787\n",
      "Epoch: 6, Samples: 480/5760, Loss: 2.616140604019165\n",
      "Epoch: 6, Samples: 512/5760, Loss: 2.4751012325286865\n",
      "Epoch: 6, Samples: 544/5760, Loss: 2.592808961868286\n",
      "Epoch: 6, Samples: 576/5760, Loss: 3.017648935317993\n",
      "Epoch: 6, Samples: 608/5760, Loss: 2.6261861324310303\n",
      "Epoch: 6, Samples: 640/5760, Loss: 2.7098333835601807\n",
      "Epoch: 6, Samples: 672/5760, Loss: 2.44815731048584\n",
      "Epoch: 6, Samples: 704/5760, Loss: 2.6177477836608887\n",
      "Epoch: 6, Samples: 736/5760, Loss: 2.5592188835144043\n",
      "Epoch: 6, Samples: 768/5760, Loss: 2.4245293140411377\n",
      "Epoch: 6, Samples: 800/5760, Loss: 2.302098274230957\n",
      "Epoch: 6, Samples: 832/5760, Loss: 2.5343055725097656\n",
      "Epoch: 6, Samples: 864/5760, Loss: 2.4953267574310303\n",
      "Epoch: 6, Samples: 896/5760, Loss: 3.030458450317383\n",
      "Epoch: 6, Samples: 928/5760, Loss: 2.6597650051116943\n",
      "Epoch: 6, Samples: 960/5760, Loss: 2.660039186477661\n",
      "Epoch: 6, Samples: 992/5760, Loss: 2.549332618713379\n",
      "Epoch: 6, Samples: 1024/5760, Loss: 2.8313705921173096\n",
      "Epoch: 6, Samples: 1056/5760, Loss: 2.4960238933563232\n",
      "Epoch: 6, Samples: 1088/5760, Loss: 2.64410662651062\n",
      "Epoch: 6, Samples: 1120/5760, Loss: 2.787142515182495\n",
      "Epoch: 6, Samples: 1152/5760, Loss: 2.4475173950195312\n",
      "Epoch: 6, Samples: 1184/5760, Loss: 2.67899751663208\n",
      "Epoch: 6, Samples: 1216/5760, Loss: 2.715574026107788\n",
      "Epoch: 6, Samples: 1248/5760, Loss: 2.7189059257507324\n",
      "Epoch: 6, Samples: 1280/5760, Loss: 2.5724711418151855\n",
      "Epoch: 6, Samples: 1312/5760, Loss: 2.5950369834899902\n",
      "Epoch: 6, Samples: 1344/5760, Loss: 2.548107624053955\n",
      "Epoch: 6, Samples: 1376/5760, Loss: 2.636025905609131\n",
      "Epoch: 6, Samples: 1408/5760, Loss: 2.6006901264190674\n",
      "Epoch: 6, Samples: 1440/5760, Loss: 2.5032973289489746\n",
      "Epoch: 6, Samples: 1472/5760, Loss: 2.5736091136932373\n",
      "Epoch: 6, Samples: 1504/5760, Loss: 2.426740884780884\n",
      "Epoch: 6, Samples: 1536/5760, Loss: 2.2682878971099854\n",
      "Epoch: 6, Samples: 1568/5760, Loss: 2.6629183292388916\n",
      "Epoch: 6, Samples: 1600/5760, Loss: 2.3248934745788574\n",
      "Epoch: 6, Samples: 1632/5760, Loss: 3.1007790565490723\n",
      "Epoch: 6, Samples: 1664/5760, Loss: 2.5747385025024414\n",
      "Epoch: 6, Samples: 1696/5760, Loss: 2.6401190757751465\n",
      "Epoch: 6, Samples: 1728/5760, Loss: 2.655742883682251\n",
      "Epoch: 6, Samples: 1760/5760, Loss: 2.907338857650757\n",
      "Epoch: 6, Samples: 1792/5760, Loss: 2.7419681549072266\n",
      "Epoch: 6, Samples: 1824/5760, Loss: 2.303131341934204\n",
      "Epoch: 6, Samples: 1856/5760, Loss: 2.824553966522217\n",
      "Epoch: 6, Samples: 1888/5760, Loss: 2.9835832118988037\n",
      "Epoch: 6, Samples: 1920/5760, Loss: 3.5708022117614746\n",
      "Epoch: 6, Samples: 1952/5760, Loss: 2.6605618000030518\n",
      "Epoch: 6, Samples: 1984/5760, Loss: 2.461623430252075\n",
      "Epoch: 6, Samples: 2016/5760, Loss: 2.519116163253784\n",
      "Epoch: 6, Samples: 2048/5760, Loss: 2.3512020111083984\n",
      "Epoch: 6, Samples: 2080/5760, Loss: 2.7393884658813477\n",
      "Epoch: 6, Samples: 2112/5760, Loss: 2.4544029235839844\n",
      "Epoch: 6, Samples: 2144/5760, Loss: 2.995560884475708\n",
      "Epoch: 6, Samples: 2176/5760, Loss: 2.7668137550354004\n",
      "Epoch: 6, Samples: 2208/5760, Loss: 2.794724464416504\n",
      "Epoch: 6, Samples: 2240/5760, Loss: 3.095557689666748\n",
      "Epoch: 6, Samples: 2272/5760, Loss: 2.560195207595825\n",
      "Epoch: 6, Samples: 2304/5760, Loss: 2.826017141342163\n",
      "Epoch: 6, Samples: 2336/5760, Loss: 3.0646204948425293\n",
      "Epoch: 6, Samples: 2368/5760, Loss: 2.568218231201172\n",
      "Epoch: 6, Samples: 2400/5760, Loss: 2.620415449142456\n",
      "Epoch: 6, Samples: 2432/5760, Loss: 2.4765543937683105\n",
      "Epoch: 6, Samples: 2464/5760, Loss: 2.839813232421875\n",
      "Epoch: 6, Samples: 2496/5760, Loss: 2.868227481842041\n",
      "Epoch: 6, Samples: 2528/5760, Loss: 2.5281784534454346\n",
      "Epoch: 6, Samples: 2560/5760, Loss: 2.6219024658203125\n",
      "Epoch: 6, Samples: 2592/5760, Loss: 3.0490853786468506\n",
      "Epoch: 6, Samples: 2624/5760, Loss: 2.57546067237854\n",
      "Epoch: 6, Samples: 2656/5760, Loss: 2.5868637561798096\n",
      "Epoch: 6, Samples: 2688/5760, Loss: 2.4291584491729736\n",
      "Epoch: 6, Samples: 2720/5760, Loss: 2.4218485355377197\n",
      "Epoch: 6, Samples: 2752/5760, Loss: 2.283905267715454\n",
      "Epoch: 6, Samples: 2784/5760, Loss: 2.7229819297790527\n",
      "Epoch: 6, Samples: 2816/5760, Loss: 2.9479353427886963\n",
      "Epoch: 6, Samples: 2848/5760, Loss: 2.241609811782837\n",
      "Epoch: 6, Samples: 2880/5760, Loss: 2.502044439315796\n",
      "Epoch: 6, Samples: 2912/5760, Loss: 2.7085347175598145\n",
      "Epoch: 6, Samples: 2944/5760, Loss: 2.8427908420562744\n",
      "Epoch: 6, Samples: 2976/5760, Loss: 2.5362682342529297\n",
      "Epoch: 6, Samples: 3008/5760, Loss: 2.829495429992676\n",
      "Epoch: 6, Samples: 3040/5760, Loss: 2.91188907623291\n",
      "Epoch: 6, Samples: 3072/5760, Loss: 2.9748778343200684\n",
      "Epoch: 6, Samples: 3104/5760, Loss: 2.4887068271636963\n",
      "Epoch: 6, Samples: 3136/5760, Loss: 2.3765530586242676\n",
      "Epoch: 6, Samples: 3168/5760, Loss: 2.552820920944214\n",
      "Epoch: 6, Samples: 3200/5760, Loss: 3.1653385162353516\n",
      "Epoch: 6, Samples: 3232/5760, Loss: 2.8592259883880615\n",
      "Epoch: 6, Samples: 3264/5760, Loss: 2.620927095413208\n",
      "Epoch: 6, Samples: 3296/5760, Loss: 2.5118916034698486\n",
      "Epoch: 6, Samples: 3328/5760, Loss: 2.3367526531219482\n",
      "Epoch: 6, Samples: 3360/5760, Loss: 2.8428826332092285\n",
      "Epoch: 6, Samples: 3392/5760, Loss: 2.6354806423187256\n",
      "Epoch: 6, Samples: 3424/5760, Loss: 3.3419415950775146\n",
      "Epoch: 6, Samples: 3456/5760, Loss: 2.426287889480591\n",
      "Epoch: 6, Samples: 3488/5760, Loss: 2.553356170654297\n",
      "Epoch: 6, Samples: 3520/5760, Loss: 2.454542636871338\n",
      "Epoch: 6, Samples: 3552/5760, Loss: 2.489346742630005\n",
      "Epoch: 6, Samples: 3584/5760, Loss: 2.9356119632720947\n",
      "Epoch: 6, Samples: 3616/5760, Loss: 2.7774617671966553\n",
      "Epoch: 6, Samples: 3648/5760, Loss: 2.938730478286743\n",
      "Epoch: 6, Samples: 3680/5760, Loss: 2.458716869354248\n",
      "Epoch: 6, Samples: 3712/5760, Loss: 2.2831497192382812\n",
      "Epoch: 6, Samples: 3744/5760, Loss: 2.1588401794433594\n",
      "Epoch: 6, Samples: 3776/5760, Loss: 2.191849708557129\n",
      "Epoch: 6, Samples: 3808/5760, Loss: 2.6595520973205566\n",
      "Epoch: 6, Samples: 3840/5760, Loss: 2.809915542602539\n",
      "Epoch: 6, Samples: 3872/5760, Loss: 2.6247963905334473\n",
      "Epoch: 6, Samples: 3904/5760, Loss: 2.703043222427368\n",
      "Epoch: 6, Samples: 3936/5760, Loss: 2.709665298461914\n",
      "Epoch: 6, Samples: 3968/5760, Loss: 2.649057149887085\n",
      "Epoch: 6, Samples: 4000/5760, Loss: 2.3743109703063965\n",
      "Epoch: 6, Samples: 4032/5760, Loss: 2.9987850189208984\n",
      "Epoch: 6, Samples: 4064/5760, Loss: 2.649878740310669\n",
      "Epoch: 6, Samples: 4096/5760, Loss: 2.319650650024414\n",
      "Epoch: 6, Samples: 4128/5760, Loss: 2.397888660430908\n",
      "Epoch: 6, Samples: 4160/5760, Loss: 2.199857473373413\n",
      "Epoch: 6, Samples: 4192/5760, Loss: 2.586127996444702\n",
      "Epoch: 6, Samples: 4224/5760, Loss: 2.5283901691436768\n",
      "Epoch: 6, Samples: 4256/5760, Loss: 2.94412899017334\n",
      "Epoch: 6, Samples: 4288/5760, Loss: 2.314375162124634\n",
      "Epoch: 6, Samples: 4320/5760, Loss: 2.474343776702881\n",
      "Epoch: 6, Samples: 4352/5760, Loss: 2.1111814975738525\n",
      "Epoch: 6, Samples: 4384/5760, Loss: 2.8262126445770264\n",
      "Epoch: 6, Samples: 4416/5760, Loss: 2.864213228225708\n",
      "Epoch: 6, Samples: 4448/5760, Loss: 2.46287202835083\n",
      "Epoch: 6, Samples: 4480/5760, Loss: 2.716899871826172\n",
      "Epoch: 6, Samples: 4512/5760, Loss: 2.648627519607544\n",
      "Epoch: 6, Samples: 4544/5760, Loss: 2.8432512283325195\n",
      "Epoch: 6, Samples: 4576/5760, Loss: 2.9040932655334473\n",
      "Epoch: 6, Samples: 4608/5760, Loss: 2.571789026260376\n",
      "Epoch: 6, Samples: 4640/5760, Loss: 2.675767183303833\n",
      "Epoch: 6, Samples: 4672/5760, Loss: 2.2025163173675537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Samples: 4704/5760, Loss: 2.8705098628997803\n",
      "Epoch: 6, Samples: 4736/5760, Loss: 2.7785842418670654\n",
      "Epoch: 6, Samples: 4768/5760, Loss: 2.483412504196167\n",
      "Epoch: 6, Samples: 4800/5760, Loss: 2.3265902996063232\n",
      "Epoch: 6, Samples: 4832/5760, Loss: 2.427028179168701\n",
      "Epoch: 6, Samples: 4864/5760, Loss: 3.0037198066711426\n",
      "Epoch: 6, Samples: 4896/5760, Loss: 2.952378511428833\n",
      "Epoch: 6, Samples: 4928/5760, Loss: 3.152470350265503\n",
      "Epoch: 6, Samples: 4960/5760, Loss: 2.736105442047119\n",
      "Epoch: 6, Samples: 4992/5760, Loss: 2.3144443035125732\n",
      "Epoch: 6, Samples: 5024/5760, Loss: 2.5982093811035156\n",
      "Epoch: 6, Samples: 5056/5760, Loss: 2.5190298557281494\n",
      "Epoch: 6, Samples: 5088/5760, Loss: 2.859984874725342\n",
      "Epoch: 6, Samples: 5120/5760, Loss: 2.909430980682373\n",
      "Epoch: 6, Samples: 5152/5760, Loss: 2.4807746410369873\n",
      "Epoch: 6, Samples: 5184/5760, Loss: 2.7575039863586426\n",
      "Epoch: 6, Samples: 5216/5760, Loss: 2.389739513397217\n",
      "Epoch: 6, Samples: 5248/5760, Loss: 2.4125773906707764\n",
      "Epoch: 6, Samples: 5280/5760, Loss: 2.4200847148895264\n",
      "Epoch: 6, Samples: 5312/5760, Loss: 2.63814115524292\n",
      "Epoch: 6, Samples: 5344/5760, Loss: 2.7690980434417725\n",
      "Epoch: 6, Samples: 5376/5760, Loss: 2.2063047885894775\n",
      "Epoch: 6, Samples: 5408/5760, Loss: 2.6962602138519287\n",
      "Epoch: 6, Samples: 5440/5760, Loss: 2.4451076984405518\n",
      "Epoch: 6, Samples: 5472/5760, Loss: 2.8021938800811768\n",
      "Epoch: 6, Samples: 5504/5760, Loss: 2.483661651611328\n",
      "Epoch: 6, Samples: 5536/5760, Loss: 2.815852403640747\n",
      "Epoch: 6, Samples: 5568/5760, Loss: 2.387014627456665\n",
      "Epoch: 6, Samples: 5600/5760, Loss: 2.191216230392456\n",
      "Epoch: 6, Samples: 5632/5760, Loss: 2.7382113933563232\n",
      "Epoch: 6, Samples: 5664/5760, Loss: 2.6313858032226562\n",
      "Epoch: 6, Samples: 5696/5760, Loss: 2.630192756652832\n",
      "Epoch: 6, Samples: 5728/5760, Loss: 3.326254367828369\n",
      "\n",
      "Epoch: 6\n",
      "Training set: Average loss: 2.6381\n",
      "Validation set: Average loss: 2.6354, Accuracy: 293/818 (36%)\n",
      "Saving model (epoch 6) with lowest validation loss: 2.6354125096247745\n",
      "Epoch: 7, Samples: 0/5760, Loss: 2.8785953521728516\n",
      "Epoch: 7, Samples: 32/5760, Loss: 2.3789875507354736\n",
      "Epoch: 7, Samples: 64/5760, Loss: 2.466414451599121\n",
      "Epoch: 7, Samples: 96/5760, Loss: 2.5935616493225098\n",
      "Epoch: 7, Samples: 128/5760, Loss: 2.6228108406066895\n",
      "Epoch: 7, Samples: 160/5760, Loss: 2.2428929805755615\n",
      "Epoch: 7, Samples: 192/5760, Loss: 2.6541593074798584\n",
      "Epoch: 7, Samples: 224/5760, Loss: 2.2989988327026367\n",
      "Epoch: 7, Samples: 256/5760, Loss: 2.3620574474334717\n",
      "Epoch: 7, Samples: 288/5760, Loss: 2.431565761566162\n",
      "Epoch: 7, Samples: 320/5760, Loss: 2.800194025039673\n",
      "Epoch: 7, Samples: 352/5760, Loss: 2.7261617183685303\n",
      "Epoch: 7, Samples: 384/5760, Loss: 2.2471446990966797\n",
      "Epoch: 7, Samples: 416/5760, Loss: 1.8927676677703857\n",
      "Epoch: 7, Samples: 448/5760, Loss: 2.2530829906463623\n",
      "Epoch: 7, Samples: 480/5760, Loss: 2.285740613937378\n",
      "Epoch: 7, Samples: 512/5760, Loss: 2.892005681991577\n",
      "Epoch: 7, Samples: 544/5760, Loss: 2.410125494003296\n",
      "Epoch: 7, Samples: 576/5760, Loss: 2.8809499740600586\n",
      "Epoch: 7, Samples: 608/5760, Loss: 2.6701877117156982\n",
      "Epoch: 7, Samples: 640/5760, Loss: 2.4761464595794678\n",
      "Epoch: 7, Samples: 672/5760, Loss: 2.414191246032715\n",
      "Epoch: 7, Samples: 704/5760, Loss: 2.9153554439544678\n",
      "Epoch: 7, Samples: 736/5760, Loss: 2.889124631881714\n",
      "Epoch: 7, Samples: 768/5760, Loss: 2.2031853199005127\n",
      "Epoch: 7, Samples: 800/5760, Loss: 2.8326122760772705\n",
      "Epoch: 7, Samples: 832/5760, Loss: 2.673860788345337\n",
      "Epoch: 7, Samples: 864/5760, Loss: 2.9338433742523193\n",
      "Epoch: 7, Samples: 896/5760, Loss: 2.918941020965576\n",
      "Epoch: 7, Samples: 928/5760, Loss: 2.385374069213867\n",
      "Epoch: 7, Samples: 960/5760, Loss: 2.185072898864746\n",
      "Epoch: 7, Samples: 992/5760, Loss: 2.5750303268432617\n",
      "Epoch: 7, Samples: 1024/5760, Loss: 2.5833470821380615\n",
      "Epoch: 7, Samples: 1056/5760, Loss: 2.793951988220215\n",
      "Epoch: 7, Samples: 1088/5760, Loss: 2.5575037002563477\n",
      "Epoch: 7, Samples: 1120/5760, Loss: 2.6300582885742188\n",
      "Epoch: 7, Samples: 1152/5760, Loss: 2.498845338821411\n",
      "Epoch: 7, Samples: 1184/5760, Loss: 2.411611318588257\n",
      "Epoch: 7, Samples: 1216/5760, Loss: 2.50234317779541\n",
      "Epoch: 7, Samples: 1248/5760, Loss: 2.2960612773895264\n",
      "Epoch: 7, Samples: 1280/5760, Loss: 2.5056707859039307\n",
      "Epoch: 7, Samples: 1312/5760, Loss: 2.5251760482788086\n",
      "Epoch: 7, Samples: 1344/5760, Loss: 3.156536102294922\n",
      "Epoch: 7, Samples: 1376/5760, Loss: 2.4442193508148193\n",
      "Epoch: 7, Samples: 1408/5760, Loss: 2.065535306930542\n",
      "Epoch: 7, Samples: 1440/5760, Loss: 2.6645686626434326\n",
      "Epoch: 7, Samples: 1472/5760, Loss: 2.419050693511963\n",
      "Epoch: 7, Samples: 1504/5760, Loss: 2.4073855876922607\n",
      "Epoch: 7, Samples: 1536/5760, Loss: 2.237823724746704\n",
      "Epoch: 7, Samples: 1568/5760, Loss: 2.5077641010284424\n",
      "Epoch: 7, Samples: 1600/5760, Loss: 2.2383835315704346\n",
      "Epoch: 7, Samples: 1632/5760, Loss: 2.5465025901794434\n",
      "Epoch: 7, Samples: 1664/5760, Loss: 2.614015817642212\n",
      "Epoch: 7, Samples: 1696/5760, Loss: 2.67777943611145\n",
      "Epoch: 7, Samples: 1728/5760, Loss: 2.714834213256836\n",
      "Epoch: 7, Samples: 1760/5760, Loss: 2.7401325702667236\n",
      "Epoch: 7, Samples: 1792/5760, Loss: 2.1480395793914795\n",
      "Epoch: 7, Samples: 1824/5760, Loss: 2.2743585109710693\n",
      "Epoch: 7, Samples: 1856/5760, Loss: 2.441279649734497\n",
      "Epoch: 7, Samples: 1888/5760, Loss: 2.2805066108703613\n",
      "Epoch: 7, Samples: 1920/5760, Loss: 2.1318750381469727\n",
      "Epoch: 7, Samples: 1952/5760, Loss: 2.5197861194610596\n",
      "Epoch: 7, Samples: 1984/5760, Loss: 2.192613363265991\n",
      "Epoch: 7, Samples: 2016/5760, Loss: 2.340477228164673\n",
      "Epoch: 7, Samples: 2048/5760, Loss: 2.2087042331695557\n",
      "Epoch: 7, Samples: 2080/5760, Loss: 2.5161283016204834\n",
      "Epoch: 7, Samples: 2112/5760, Loss: 2.157172679901123\n",
      "Epoch: 7, Samples: 2144/5760, Loss: 3.044982671737671\n",
      "Epoch: 7, Samples: 2176/5760, Loss: 2.423865556716919\n",
      "Epoch: 7, Samples: 2208/5760, Loss: 2.1076173782348633\n",
      "Epoch: 7, Samples: 2240/5760, Loss: 2.6329588890075684\n",
      "Epoch: 7, Samples: 2272/5760, Loss: 3.287631034851074\n",
      "Epoch: 7, Samples: 2304/5760, Loss: 2.2845191955566406\n",
      "Epoch: 7, Samples: 2336/5760, Loss: 2.252255439758301\n",
      "Epoch: 7, Samples: 2368/5760, Loss: 2.4693124294281006\n",
      "Epoch: 7, Samples: 2400/5760, Loss: 2.5182583332061768\n",
      "Epoch: 7, Samples: 2432/5760, Loss: 2.2537169456481934\n",
      "Epoch: 7, Samples: 2464/5760, Loss: 2.5295536518096924\n",
      "Epoch: 7, Samples: 2496/5760, Loss: 2.6037371158599854\n",
      "Epoch: 7, Samples: 2528/5760, Loss: 2.5526695251464844\n",
      "Epoch: 7, Samples: 2560/5760, Loss: 2.404341697692871\n",
      "Epoch: 7, Samples: 2592/5760, Loss: 2.373430013656616\n",
      "Epoch: 7, Samples: 2624/5760, Loss: 2.5579111576080322\n",
      "Epoch: 7, Samples: 2656/5760, Loss: 2.776427984237671\n",
      "Epoch: 7, Samples: 2688/5760, Loss: 2.8885483741760254\n",
      "Epoch: 7, Samples: 2720/5760, Loss: 1.8957757949829102\n",
      "Epoch: 7, Samples: 2752/5760, Loss: 2.7947466373443604\n",
      "Epoch: 7, Samples: 2784/5760, Loss: 2.1952431201934814\n",
      "Epoch: 7, Samples: 2816/5760, Loss: 2.3275554180145264\n",
      "Epoch: 7, Samples: 2848/5760, Loss: 2.0221760272979736\n",
      "Epoch: 7, Samples: 2880/5760, Loss: 2.236988067626953\n",
      "Epoch: 7, Samples: 2912/5760, Loss: 2.519587993621826\n",
      "Epoch: 7, Samples: 2944/5760, Loss: 2.518766403198242\n",
      "Epoch: 7, Samples: 2976/5760, Loss: 2.5997121334075928\n",
      "Epoch: 7, Samples: 3008/5760, Loss: 2.747098445892334\n",
      "Epoch: 7, Samples: 3040/5760, Loss: 2.5211727619171143\n",
      "Epoch: 7, Samples: 3072/5760, Loss: 2.7769956588745117\n",
      "Epoch: 7, Samples: 3104/5760, Loss: 2.627718448638916\n",
      "Epoch: 7, Samples: 3136/5760, Loss: 2.8157083988189697\n",
      "Epoch: 7, Samples: 3168/5760, Loss: 1.8822072744369507\n",
      "Epoch: 7, Samples: 3200/5760, Loss: 2.1880064010620117\n",
      "Epoch: 7, Samples: 3232/5760, Loss: 2.6900641918182373\n",
      "Epoch: 7, Samples: 3264/5760, Loss: 2.300584077835083\n",
      "Epoch: 7, Samples: 3296/5760, Loss: 2.7057297229766846\n",
      "Epoch: 7, Samples: 3328/5760, Loss: 2.628818988800049\n",
      "Epoch: 7, Samples: 3360/5760, Loss: 2.0410048961639404\n",
      "Epoch: 7, Samples: 3392/5760, Loss: 2.8201849460601807\n",
      "Epoch: 7, Samples: 3424/5760, Loss: 2.128967046737671\n",
      "Epoch: 7, Samples: 3456/5760, Loss: 2.3397216796875\n",
      "Epoch: 7, Samples: 3488/5760, Loss: 2.502462148666382\n",
      "Epoch: 7, Samples: 3520/5760, Loss: 2.3650686740875244\n",
      "Epoch: 7, Samples: 3552/5760, Loss: 2.177879571914673\n",
      "Epoch: 7, Samples: 3584/5760, Loss: 2.524653911590576\n",
      "Epoch: 7, Samples: 3616/5760, Loss: 2.5758514404296875\n",
      "Epoch: 7, Samples: 3648/5760, Loss: 3.033865451812744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Samples: 3680/5760, Loss: 2.0376391410827637\n",
      "Epoch: 7, Samples: 3712/5760, Loss: 2.6118288040161133\n",
      "Epoch: 7, Samples: 3744/5760, Loss: 2.4621455669403076\n",
      "Epoch: 7, Samples: 3776/5760, Loss: 2.4477458000183105\n",
      "Epoch: 7, Samples: 3808/5760, Loss: 2.5941953659057617\n",
      "Epoch: 7, Samples: 3840/5760, Loss: 2.4220685958862305\n",
      "Epoch: 7, Samples: 3872/5760, Loss: 2.2125391960144043\n",
      "Epoch: 7, Samples: 3904/5760, Loss: 2.7410707473754883\n",
      "Epoch: 7, Samples: 3936/5760, Loss: 2.5608980655670166\n",
      "Epoch: 7, Samples: 3968/5760, Loss: 2.4778409004211426\n",
      "Epoch: 7, Samples: 4000/5760, Loss: 2.7518038749694824\n",
      "Epoch: 7, Samples: 4032/5760, Loss: 2.558506488800049\n",
      "Epoch: 7, Samples: 4064/5760, Loss: 2.4217729568481445\n",
      "Epoch: 7, Samples: 4096/5760, Loss: 2.217268943786621\n",
      "Epoch: 7, Samples: 4128/5760, Loss: 2.5589613914489746\n",
      "Epoch: 7, Samples: 4160/5760, Loss: 2.3015804290771484\n",
      "Epoch: 7, Samples: 4192/5760, Loss: 2.4432976245880127\n",
      "Epoch: 7, Samples: 4224/5760, Loss: 1.9358853101730347\n",
      "Epoch: 7, Samples: 4256/5760, Loss: 2.014721393585205\n",
      "Epoch: 7, Samples: 4288/5760, Loss: 1.9055101871490479\n",
      "Epoch: 7, Samples: 4320/5760, Loss: 2.3184475898742676\n",
      "Epoch: 7, Samples: 4352/5760, Loss: 2.7822682857513428\n",
      "Epoch: 7, Samples: 4384/5760, Loss: 2.255496025085449\n",
      "Epoch: 7, Samples: 4416/5760, Loss: 2.5316684246063232\n",
      "Epoch: 7, Samples: 4448/5760, Loss: 2.7732250690460205\n",
      "Epoch: 7, Samples: 4480/5760, Loss: 2.719576835632324\n",
      "Epoch: 7, Samples: 4512/5760, Loss: 2.4103188514709473\n",
      "Epoch: 7, Samples: 4544/5760, Loss: 2.6813197135925293\n",
      "Epoch: 7, Samples: 4576/5760, Loss: 2.448012590408325\n",
      "Epoch: 7, Samples: 4608/5760, Loss: 2.441070318222046\n",
      "Epoch: 7, Samples: 4640/5760, Loss: 2.7569754123687744\n",
      "Epoch: 7, Samples: 4672/5760, Loss: 2.5670487880706787\n",
      "Epoch: 7, Samples: 4704/5760, Loss: 2.6145548820495605\n",
      "Epoch: 7, Samples: 4736/5760, Loss: 2.635871410369873\n",
      "Epoch: 7, Samples: 4768/5760, Loss: 2.5249452590942383\n",
      "Epoch: 7, Samples: 4800/5760, Loss: 2.5005435943603516\n",
      "Epoch: 7, Samples: 4832/5760, Loss: 2.3360166549682617\n",
      "Epoch: 7, Samples: 4864/5760, Loss: 2.9398815631866455\n",
      "Epoch: 7, Samples: 4896/5760, Loss: 2.5203604698181152\n",
      "Epoch: 7, Samples: 4928/5760, Loss: 2.7434990406036377\n",
      "Epoch: 7, Samples: 4960/5760, Loss: 2.5584492683410645\n",
      "Epoch: 7, Samples: 4992/5760, Loss: 2.9151179790496826\n",
      "Epoch: 7, Samples: 5024/5760, Loss: 2.3121633529663086\n",
      "Epoch: 7, Samples: 5056/5760, Loss: 2.550673007965088\n",
      "Epoch: 7, Samples: 5088/5760, Loss: 2.272439479827881\n",
      "Epoch: 7, Samples: 5120/5760, Loss: 2.5096747875213623\n",
      "Epoch: 7, Samples: 5152/5760, Loss: 2.296952486038208\n",
      "Epoch: 7, Samples: 5184/5760, Loss: 3.097843885421753\n",
      "Epoch: 7, Samples: 5216/5760, Loss: 2.5870754718780518\n",
      "Epoch: 7, Samples: 5248/5760, Loss: 2.3224737644195557\n",
      "Epoch: 7, Samples: 5280/5760, Loss: 2.460843801498413\n",
      "Epoch: 7, Samples: 5312/5760, Loss: 2.5871591567993164\n",
      "Epoch: 7, Samples: 5344/5760, Loss: 2.2772045135498047\n",
      "Epoch: 7, Samples: 5376/5760, Loss: 2.3476288318634033\n",
      "Epoch: 7, Samples: 5408/5760, Loss: 2.2351949214935303\n",
      "Epoch: 7, Samples: 5440/5760, Loss: 2.302558183670044\n",
      "Epoch: 7, Samples: 5472/5760, Loss: 2.9958908557891846\n",
      "Epoch: 7, Samples: 5504/5760, Loss: 2.2913739681243896\n",
      "Epoch: 7, Samples: 5536/5760, Loss: 2.0035250186920166\n",
      "Epoch: 7, Samples: 5568/5760, Loss: 2.169241189956665\n",
      "Epoch: 7, Samples: 5600/5760, Loss: 2.3143672943115234\n",
      "Epoch: 7, Samples: 5632/5760, Loss: 2.6436188220977783\n",
      "Epoch: 7, Samples: 5664/5760, Loss: 2.9059154987335205\n",
      "Epoch: 7, Samples: 5696/5760, Loss: 2.142422676086426\n",
      "Epoch: 7, Samples: 5728/5760, Loss: 3.742830276489258\n",
      "\n",
      "Epoch: 7\n",
      "Training set: Average loss: 2.4935\n",
      "Validation set: Average loss: 2.5647, Accuracy: 312/818 (38%)\n",
      "Saving model (epoch 7) with lowest validation loss: 2.5646719198960524\n",
      "Epoch: 8, Samples: 0/5760, Loss: 2.0316250324249268\n",
      "Epoch: 8, Samples: 32/5760, Loss: 2.6439785957336426\n",
      "Epoch: 8, Samples: 64/5760, Loss: 2.5063576698303223\n",
      "Epoch: 8, Samples: 96/5760, Loss: 2.4745001792907715\n",
      "Epoch: 8, Samples: 128/5760, Loss: 2.1382555961608887\n",
      "Epoch: 8, Samples: 160/5760, Loss: 2.444505214691162\n",
      "Epoch: 8, Samples: 192/5760, Loss: 2.116414785385132\n",
      "Epoch: 8, Samples: 224/5760, Loss: 2.366694450378418\n",
      "Epoch: 8, Samples: 256/5760, Loss: 2.297006607055664\n",
      "Epoch: 8, Samples: 288/5760, Loss: 2.5226502418518066\n",
      "Epoch: 8, Samples: 320/5760, Loss: 2.103069543838501\n",
      "Epoch: 8, Samples: 352/5760, Loss: 2.1731677055358887\n",
      "Epoch: 8, Samples: 384/5760, Loss: 2.514756202697754\n",
      "Epoch: 8, Samples: 416/5760, Loss: 2.4890754222869873\n",
      "Epoch: 8, Samples: 448/5760, Loss: 2.4750139713287354\n",
      "Epoch: 8, Samples: 480/5760, Loss: 2.3264260292053223\n",
      "Epoch: 8, Samples: 512/5760, Loss: 2.5072004795074463\n",
      "Epoch: 8, Samples: 544/5760, Loss: 2.715991497039795\n",
      "Epoch: 8, Samples: 576/5760, Loss: 2.590101480484009\n",
      "Epoch: 8, Samples: 608/5760, Loss: 2.2749743461608887\n",
      "Epoch: 8, Samples: 640/5760, Loss: 2.2448742389678955\n",
      "Epoch: 8, Samples: 672/5760, Loss: 3.0827348232269287\n",
      "Epoch: 8, Samples: 704/5760, Loss: 2.4787967205047607\n",
      "Epoch: 8, Samples: 736/5760, Loss: 2.343296527862549\n",
      "Epoch: 8, Samples: 768/5760, Loss: 2.104567527770996\n",
      "Epoch: 8, Samples: 800/5760, Loss: 2.1332297325134277\n",
      "Epoch: 8, Samples: 832/5760, Loss: 1.7839195728302002\n",
      "Epoch: 8, Samples: 864/5760, Loss: 2.4343199729919434\n",
      "Epoch: 8, Samples: 896/5760, Loss: 2.6189351081848145\n",
      "Epoch: 8, Samples: 928/5760, Loss: 2.1249094009399414\n",
      "Epoch: 8, Samples: 960/5760, Loss: 2.071763038635254\n",
      "Epoch: 8, Samples: 992/5760, Loss: 2.428379535675049\n",
      "Epoch: 8, Samples: 1024/5760, Loss: 1.918195366859436\n",
      "Epoch: 8, Samples: 1056/5760, Loss: 2.9491031169891357\n",
      "Epoch: 8, Samples: 1088/5760, Loss: 2.7213945388793945\n",
      "Epoch: 8, Samples: 1120/5760, Loss: 2.438673496246338\n",
      "Epoch: 8, Samples: 1152/5760, Loss: 2.3658711910247803\n",
      "Epoch: 8, Samples: 1184/5760, Loss: 2.2579314708709717\n",
      "Epoch: 8, Samples: 1216/5760, Loss: 2.49623966217041\n",
      "Epoch: 8, Samples: 1248/5760, Loss: 2.6790173053741455\n",
      "Epoch: 8, Samples: 1280/5760, Loss: 2.3146286010742188\n",
      "Epoch: 8, Samples: 1312/5760, Loss: 2.886629581451416\n",
      "Epoch: 8, Samples: 1344/5760, Loss: 2.68361234664917\n",
      "Epoch: 8, Samples: 1376/5760, Loss: 2.493375539779663\n",
      "Epoch: 8, Samples: 1408/5760, Loss: 1.895155429840088\n",
      "Epoch: 8, Samples: 1440/5760, Loss: 2.3438916206359863\n",
      "Epoch: 8, Samples: 1472/5760, Loss: 2.435516834259033\n",
      "Epoch: 8, Samples: 1504/5760, Loss: 2.153071641921997\n",
      "Epoch: 8, Samples: 1536/5760, Loss: 2.696460247039795\n",
      "Epoch: 8, Samples: 1568/5760, Loss: 2.3182127475738525\n",
      "Epoch: 8, Samples: 1600/5760, Loss: 2.260983467102051\n",
      "Epoch: 8, Samples: 1632/5760, Loss: 2.755260705947876\n",
      "Epoch: 8, Samples: 1664/5760, Loss: 2.5083141326904297\n",
      "Epoch: 8, Samples: 1696/5760, Loss: 2.2976131439208984\n",
      "Epoch: 8, Samples: 1728/5760, Loss: 2.6160216331481934\n",
      "Epoch: 8, Samples: 1760/5760, Loss: 2.664527654647827\n",
      "Epoch: 8, Samples: 1792/5760, Loss: 2.7535457611083984\n",
      "Epoch: 8, Samples: 1824/5760, Loss: 2.5805447101593018\n",
      "Epoch: 8, Samples: 1856/5760, Loss: 2.2509942054748535\n",
      "Epoch: 8, Samples: 1888/5760, Loss: 2.426082134246826\n",
      "Epoch: 8, Samples: 1920/5760, Loss: 2.2538533210754395\n",
      "Epoch: 8, Samples: 1952/5760, Loss: 2.2977406978607178\n",
      "Epoch: 8, Samples: 1984/5760, Loss: 2.597511053085327\n",
      "Epoch: 8, Samples: 2016/5760, Loss: 2.128884792327881\n",
      "Epoch: 8, Samples: 2048/5760, Loss: 2.7513959407806396\n",
      "Epoch: 8, Samples: 2080/5760, Loss: 2.2868289947509766\n",
      "Epoch: 8, Samples: 2112/5760, Loss: 2.6536507606506348\n",
      "Epoch: 8, Samples: 2144/5760, Loss: 1.6720741987228394\n",
      "Epoch: 8, Samples: 2176/5760, Loss: 2.514622449874878\n",
      "Epoch: 8, Samples: 2208/5760, Loss: 2.403982162475586\n",
      "Epoch: 8, Samples: 2240/5760, Loss: 2.4807724952697754\n",
      "Epoch: 8, Samples: 2272/5760, Loss: 2.2324912548065186\n",
      "Epoch: 8, Samples: 2304/5760, Loss: 2.3364498615264893\n",
      "Epoch: 8, Samples: 2336/5760, Loss: 2.588533401489258\n",
      "Epoch: 8, Samples: 2368/5760, Loss: 2.0299155712127686\n",
      "Epoch: 8, Samples: 2400/5760, Loss: 2.059788703918457\n",
      "Epoch: 8, Samples: 2432/5760, Loss: 2.763343334197998\n",
      "Epoch: 8, Samples: 2464/5760, Loss: 2.1103687286376953\n",
      "Epoch: 8, Samples: 2496/5760, Loss: 2.34222412109375\n",
      "Epoch: 8, Samples: 2528/5760, Loss: 2.422122001647949\n",
      "Epoch: 8, Samples: 2560/5760, Loss: 2.1832916736602783\n",
      "Epoch: 8, Samples: 2592/5760, Loss: 2.1438827514648438\n",
      "Epoch: 8, Samples: 2624/5760, Loss: 2.8693935871124268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Samples: 2656/5760, Loss: 2.5283570289611816\n",
      "Epoch: 8, Samples: 2688/5760, Loss: 2.6920552253723145\n",
      "Epoch: 8, Samples: 2720/5760, Loss: 2.062702178955078\n",
      "Epoch: 8, Samples: 2752/5760, Loss: 1.978890299797058\n",
      "Epoch: 8, Samples: 2784/5760, Loss: 2.150510787963867\n",
      "Epoch: 8, Samples: 2816/5760, Loss: 2.455759048461914\n",
      "Epoch: 8, Samples: 2848/5760, Loss: 2.3262360095977783\n",
      "Epoch: 8, Samples: 2880/5760, Loss: 2.076261520385742\n",
      "Epoch: 8, Samples: 2912/5760, Loss: 2.4372689723968506\n",
      "Epoch: 8, Samples: 2944/5760, Loss: 2.4396021366119385\n",
      "Epoch: 8, Samples: 2976/5760, Loss: 2.971327543258667\n",
      "Epoch: 8, Samples: 3008/5760, Loss: 2.321545124053955\n",
      "Epoch: 8, Samples: 3040/5760, Loss: 2.6167542934417725\n",
      "Epoch: 8, Samples: 3072/5760, Loss: 2.421415090560913\n",
      "Epoch: 8, Samples: 3104/5760, Loss: 2.597032070159912\n",
      "Epoch: 8, Samples: 3136/5760, Loss: 2.6051902770996094\n",
      "Epoch: 8, Samples: 3168/5760, Loss: 2.4698848724365234\n",
      "Epoch: 8, Samples: 3200/5760, Loss: 2.4272971153259277\n",
      "Epoch: 8, Samples: 3232/5760, Loss: 2.5018811225891113\n",
      "Epoch: 8, Samples: 3264/5760, Loss: 2.2719767093658447\n",
      "Epoch: 8, Samples: 3296/5760, Loss: 2.3373866081237793\n",
      "Epoch: 8, Samples: 3328/5760, Loss: 2.5475594997406006\n",
      "Epoch: 8, Samples: 3360/5760, Loss: 2.790156126022339\n",
      "Epoch: 8, Samples: 3392/5760, Loss: 2.3811943531036377\n",
      "Epoch: 8, Samples: 3424/5760, Loss: 2.2108192443847656\n",
      "Epoch: 8, Samples: 3456/5760, Loss: 2.3117048740386963\n",
      "Epoch: 8, Samples: 3488/5760, Loss: 2.906679630279541\n",
      "Epoch: 8, Samples: 3520/5760, Loss: 1.6465930938720703\n",
      "Epoch: 8, Samples: 3552/5760, Loss: 2.5389180183410645\n",
      "Epoch: 8, Samples: 3584/5760, Loss: 2.6006786823272705\n",
      "Epoch: 8, Samples: 3616/5760, Loss: 2.127257823944092\n",
      "Epoch: 8, Samples: 3648/5760, Loss: 2.519660234451294\n",
      "Epoch: 8, Samples: 3680/5760, Loss: 2.3240697383880615\n",
      "Epoch: 8, Samples: 3712/5760, Loss: 2.1102209091186523\n",
      "Epoch: 8, Samples: 3744/5760, Loss: 2.35229754447937\n",
      "Epoch: 8, Samples: 3776/5760, Loss: 2.2958741188049316\n",
      "Epoch: 8, Samples: 3808/5760, Loss: 2.1900548934936523\n",
      "Epoch: 8, Samples: 3840/5760, Loss: 2.7705702781677246\n",
      "Epoch: 8, Samples: 3872/5760, Loss: 2.462144136428833\n",
      "Epoch: 8, Samples: 3904/5760, Loss: 2.2275264263153076\n",
      "Epoch: 8, Samples: 3936/5760, Loss: 2.5883936882019043\n",
      "Epoch: 8, Samples: 3968/5760, Loss: 2.130396842956543\n",
      "Epoch: 8, Samples: 4000/5760, Loss: 2.5346975326538086\n",
      "Epoch: 8, Samples: 4032/5760, Loss: 3.1950623989105225\n",
      "Epoch: 8, Samples: 4064/5760, Loss: 2.457653522491455\n",
      "Epoch: 8, Samples: 4096/5760, Loss: 2.3492865562438965\n",
      "Epoch: 8, Samples: 4128/5760, Loss: 2.283797025680542\n",
      "Epoch: 8, Samples: 4160/5760, Loss: 2.3050873279571533\n",
      "Epoch: 8, Samples: 4192/5760, Loss: 2.436635732650757\n",
      "Epoch: 8, Samples: 4224/5760, Loss: 2.3990581035614014\n",
      "Epoch: 8, Samples: 4256/5760, Loss: 2.210923671722412\n",
      "Epoch: 8, Samples: 4288/5760, Loss: 2.26252818107605\n",
      "Epoch: 8, Samples: 4320/5760, Loss: 2.4584133625030518\n",
      "Epoch: 8, Samples: 4352/5760, Loss: 2.2255325317382812\n",
      "Epoch: 8, Samples: 4384/5760, Loss: 2.245675802230835\n",
      "Epoch: 8, Samples: 4416/5760, Loss: 1.942509412765503\n",
      "Epoch: 8, Samples: 4448/5760, Loss: 2.7146756649017334\n",
      "Epoch: 8, Samples: 4480/5760, Loss: 2.450350046157837\n",
      "Epoch: 8, Samples: 4512/5760, Loss: 2.4534881114959717\n",
      "Epoch: 8, Samples: 4544/5760, Loss: 2.3507511615753174\n",
      "Epoch: 8, Samples: 4576/5760, Loss: 1.8231472969055176\n",
      "Epoch: 8, Samples: 4608/5760, Loss: 2.1416375637054443\n",
      "Epoch: 8, Samples: 4640/5760, Loss: 2.1913411617279053\n",
      "Epoch: 8, Samples: 4672/5760, Loss: 2.4331531524658203\n",
      "Epoch: 8, Samples: 4704/5760, Loss: 1.9225292205810547\n",
      "Epoch: 8, Samples: 4736/5760, Loss: 2.3778316974639893\n",
      "Epoch: 8, Samples: 4768/5760, Loss: 2.1092896461486816\n",
      "Epoch: 8, Samples: 4800/5760, Loss: 2.6792027950286865\n",
      "Epoch: 8, Samples: 4832/5760, Loss: 2.680931568145752\n",
      "Epoch: 8, Samples: 4864/5760, Loss: 2.4920401573181152\n",
      "Epoch: 8, Samples: 4896/5760, Loss: 2.2762818336486816\n",
      "Epoch: 8, Samples: 4928/5760, Loss: 2.8782927989959717\n",
      "Epoch: 8, Samples: 4960/5760, Loss: 2.3487985134124756\n",
      "Epoch: 8, Samples: 4992/5760, Loss: 2.1153950691223145\n",
      "Epoch: 8, Samples: 5024/5760, Loss: 2.0602867603302\n",
      "Epoch: 8, Samples: 5056/5760, Loss: 2.204737663269043\n",
      "Epoch: 8, Samples: 5088/5760, Loss: 2.253120183944702\n",
      "Epoch: 8, Samples: 5120/5760, Loss: 1.911760926246643\n",
      "Epoch: 8, Samples: 5152/5760, Loss: 2.433245897293091\n",
      "Epoch: 8, Samples: 5184/5760, Loss: 2.237238883972168\n",
      "Epoch: 8, Samples: 5216/5760, Loss: 2.5116360187530518\n",
      "Epoch: 8, Samples: 5248/5760, Loss: 2.309105157852173\n",
      "Epoch: 8, Samples: 5280/5760, Loss: 2.4158008098602295\n",
      "Epoch: 8, Samples: 5312/5760, Loss: 2.184404134750366\n",
      "Epoch: 8, Samples: 5344/5760, Loss: 2.3558716773986816\n",
      "Epoch: 8, Samples: 5376/5760, Loss: 2.0864360332489014\n",
      "Epoch: 8, Samples: 5408/5760, Loss: 2.2422993183135986\n",
      "Epoch: 8, Samples: 5440/5760, Loss: 2.225881338119507\n",
      "Epoch: 8, Samples: 5472/5760, Loss: 2.2692418098449707\n",
      "Epoch: 8, Samples: 5504/5760, Loss: 2.4411234855651855\n",
      "Epoch: 8, Samples: 5536/5760, Loss: 2.3133790493011475\n",
      "Epoch: 8, Samples: 5568/5760, Loss: 2.152834177017212\n",
      "Epoch: 8, Samples: 5600/5760, Loss: 2.3407785892486572\n",
      "Epoch: 8, Samples: 5632/5760, Loss: 2.251988410949707\n",
      "Epoch: 8, Samples: 5664/5760, Loss: 2.224677801132202\n",
      "Epoch: 8, Samples: 5696/5760, Loss: 2.5295400619506836\n",
      "Epoch: 8, Samples: 5728/5760, Loss: 3.3487095832824707\n",
      "\n",
      "Epoch: 8\n",
      "Training set: Average loss: 2.3812\n",
      "Validation set: Average loss: 3.2013, Accuracy: 190/818 (23%)\n",
      "Epoch: 9, Samples: 0/5760, Loss: 2.0505199432373047\n",
      "Epoch: 9, Samples: 32/5760, Loss: 2.2005043029785156\n",
      "Epoch: 9, Samples: 64/5760, Loss: 2.3811240196228027\n",
      "Epoch: 9, Samples: 96/5760, Loss: 2.223060369491577\n",
      "Epoch: 9, Samples: 128/5760, Loss: 2.399974822998047\n",
      "Epoch: 9, Samples: 160/5760, Loss: 2.751661777496338\n",
      "Epoch: 9, Samples: 192/5760, Loss: 2.157600164413452\n",
      "Epoch: 9, Samples: 224/5760, Loss: 1.7580863237380981\n",
      "Epoch: 9, Samples: 256/5760, Loss: 2.049022674560547\n",
      "Epoch: 9, Samples: 288/5760, Loss: 2.5970265865325928\n",
      "Epoch: 9, Samples: 320/5760, Loss: 2.1919729709625244\n",
      "Epoch: 9, Samples: 352/5760, Loss: 2.208662986755371\n",
      "Epoch: 9, Samples: 384/5760, Loss: 2.6396567821502686\n",
      "Epoch: 9, Samples: 416/5760, Loss: 2.185432195663452\n",
      "Epoch: 9, Samples: 448/5760, Loss: 2.0695393085479736\n",
      "Epoch: 9, Samples: 480/5760, Loss: 2.431924343109131\n",
      "Epoch: 9, Samples: 512/5760, Loss: 2.4714903831481934\n",
      "Epoch: 9, Samples: 544/5760, Loss: 2.378573417663574\n",
      "Epoch: 9, Samples: 576/5760, Loss: 2.3452811241149902\n",
      "Epoch: 9, Samples: 608/5760, Loss: 2.5288705825805664\n",
      "Epoch: 9, Samples: 640/5760, Loss: 2.4309349060058594\n",
      "Epoch: 9, Samples: 672/5760, Loss: 2.1788485050201416\n",
      "Epoch: 9, Samples: 704/5760, Loss: 2.0095620155334473\n",
      "Epoch: 9, Samples: 736/5760, Loss: 1.9858918190002441\n",
      "Epoch: 9, Samples: 768/5760, Loss: 2.0782546997070312\n",
      "Epoch: 9, Samples: 800/5760, Loss: 2.1941943168640137\n",
      "Epoch: 9, Samples: 832/5760, Loss: 2.035399913787842\n",
      "Epoch: 9, Samples: 864/5760, Loss: 2.0886800289154053\n",
      "Epoch: 9, Samples: 896/5760, Loss: 2.3707938194274902\n",
      "Epoch: 9, Samples: 928/5760, Loss: 2.3879542350769043\n",
      "Epoch: 9, Samples: 960/5760, Loss: 2.1892309188842773\n",
      "Epoch: 9, Samples: 992/5760, Loss: 2.490102767944336\n",
      "Epoch: 9, Samples: 1024/5760, Loss: 2.3760998249053955\n",
      "Epoch: 9, Samples: 1056/5760, Loss: 2.3771517276763916\n",
      "Epoch: 9, Samples: 1088/5760, Loss: 2.2081637382507324\n",
      "Epoch: 9, Samples: 1120/5760, Loss: 2.4225335121154785\n",
      "Epoch: 9, Samples: 1152/5760, Loss: 2.1678311824798584\n",
      "Epoch: 9, Samples: 1184/5760, Loss: 2.8499391078948975\n",
      "Epoch: 9, Samples: 1216/5760, Loss: 2.331413507461548\n",
      "Epoch: 9, Samples: 1248/5760, Loss: 2.1916496753692627\n",
      "Epoch: 9, Samples: 1280/5760, Loss: 2.4097864627838135\n",
      "Epoch: 9, Samples: 1312/5760, Loss: 1.8812977075576782\n",
      "Epoch: 9, Samples: 1344/5760, Loss: 3.014219284057617\n",
      "Epoch: 9, Samples: 1376/5760, Loss: 2.118901252746582\n",
      "Epoch: 9, Samples: 1408/5760, Loss: 2.122304677963257\n",
      "Epoch: 9, Samples: 1440/5760, Loss: 2.0877795219421387\n",
      "Epoch: 9, Samples: 1472/5760, Loss: 2.537537097930908\n",
      "Epoch: 9, Samples: 1504/5760, Loss: 2.3594281673431396\n",
      "Epoch: 9, Samples: 1536/5760, Loss: 2.2507355213165283\n",
      "Epoch: 9, Samples: 1568/5760, Loss: 2.258363962173462\n",
      "Epoch: 9, Samples: 1600/5760, Loss: 2.0396904945373535\n",
      "Epoch: 9, Samples: 1632/5760, Loss: 2.035285234451294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Samples: 1664/5760, Loss: 2.1623103618621826\n",
      "Epoch: 9, Samples: 1696/5760, Loss: 2.0610640048980713\n",
      "Epoch: 9, Samples: 1728/5760, Loss: 2.3543736934661865\n",
      "Epoch: 9, Samples: 1760/5760, Loss: 2.3368782997131348\n",
      "Epoch: 9, Samples: 1792/5760, Loss: 1.8727778196334839\n",
      "Epoch: 9, Samples: 1824/5760, Loss: 2.3884143829345703\n",
      "Epoch: 9, Samples: 1856/5760, Loss: 2.1381115913391113\n",
      "Epoch: 9, Samples: 1888/5760, Loss: 2.4067792892456055\n",
      "Epoch: 9, Samples: 1920/5760, Loss: 2.855273723602295\n",
      "Epoch: 9, Samples: 1952/5760, Loss: 2.8611292839050293\n",
      "Epoch: 9, Samples: 1984/5760, Loss: 2.253412961959839\n",
      "Epoch: 9, Samples: 2016/5760, Loss: 2.039872884750366\n",
      "Epoch: 9, Samples: 2048/5760, Loss: 2.19783353805542\n",
      "Epoch: 9, Samples: 2080/5760, Loss: 2.2351982593536377\n",
      "Epoch: 9, Samples: 2112/5760, Loss: 2.3991100788116455\n",
      "Epoch: 9, Samples: 2144/5760, Loss: 2.190955400466919\n",
      "Epoch: 9, Samples: 2176/5760, Loss: 2.158809185028076\n",
      "Epoch: 9, Samples: 2208/5760, Loss: 1.8287014961242676\n",
      "Epoch: 9, Samples: 2240/5760, Loss: 2.6179282665252686\n",
      "Epoch: 9, Samples: 2272/5760, Loss: 1.9791841506958008\n",
      "Epoch: 9, Samples: 2304/5760, Loss: 2.297083854675293\n",
      "Epoch: 9, Samples: 2336/5760, Loss: 2.3088014125823975\n",
      "Epoch: 9, Samples: 2368/5760, Loss: 2.444837808609009\n",
      "Epoch: 9, Samples: 2400/5760, Loss: 2.502323865890503\n",
      "Epoch: 9, Samples: 2432/5760, Loss: 2.0348939895629883\n",
      "Epoch: 9, Samples: 2464/5760, Loss: 2.652947425842285\n",
      "Epoch: 9, Samples: 2496/5760, Loss: 1.8910186290740967\n",
      "Epoch: 9, Samples: 2528/5760, Loss: 2.2226433753967285\n",
      "Epoch: 9, Samples: 2560/5760, Loss: 2.516817808151245\n",
      "Epoch: 9, Samples: 2592/5760, Loss: 2.2010231018066406\n",
      "Epoch: 9, Samples: 2624/5760, Loss: 2.0673394203186035\n",
      "Epoch: 9, Samples: 2656/5760, Loss: 2.0203700065612793\n",
      "Epoch: 9, Samples: 2688/5760, Loss: 2.5000526905059814\n",
      "Epoch: 9, Samples: 2720/5760, Loss: 2.0903711318969727\n",
      "Epoch: 9, Samples: 2752/5760, Loss: 2.411668539047241\n",
      "Epoch: 9, Samples: 2784/5760, Loss: 2.738877534866333\n",
      "Epoch: 9, Samples: 2816/5760, Loss: 2.2360432147979736\n",
      "Epoch: 9, Samples: 2848/5760, Loss: 2.749631404876709\n",
      "Epoch: 9, Samples: 2880/5760, Loss: 2.1910083293914795\n",
      "Epoch: 9, Samples: 2912/5760, Loss: 2.38436222076416\n",
      "Epoch: 9, Samples: 2944/5760, Loss: 1.9788776636123657\n",
      "Epoch: 9, Samples: 2976/5760, Loss: 2.7259438037872314\n",
      "Epoch: 9, Samples: 3008/5760, Loss: 1.8276997804641724\n",
      "Epoch: 9, Samples: 3040/5760, Loss: 2.198084592819214\n",
      "Epoch: 9, Samples: 3072/5760, Loss: 2.473602294921875\n",
      "Epoch: 9, Samples: 3104/5760, Loss: 2.1589813232421875\n",
      "Epoch: 9, Samples: 3136/5760, Loss: 2.257622480392456\n",
      "Epoch: 9, Samples: 3168/5760, Loss: 1.8022390604019165\n",
      "Epoch: 9, Samples: 3200/5760, Loss: 1.9987552165985107\n",
      "Epoch: 9, Samples: 3232/5760, Loss: 2.424539089202881\n",
      "Epoch: 9, Samples: 3264/5760, Loss: 1.8872361183166504\n",
      "Epoch: 9, Samples: 3296/5760, Loss: 2.198413372039795\n",
      "Epoch: 9, Samples: 3328/5760, Loss: 2.5374863147735596\n",
      "Epoch: 9, Samples: 3360/5760, Loss: 2.066565752029419\n",
      "Epoch: 9, Samples: 3392/5760, Loss: 2.234736919403076\n",
      "Epoch: 9, Samples: 3424/5760, Loss: 2.249699115753174\n",
      "Epoch: 9, Samples: 3456/5760, Loss: 2.0908076763153076\n",
      "Epoch: 9, Samples: 3488/5760, Loss: 2.1594338417053223\n",
      "Epoch: 9, Samples: 3520/5760, Loss: 1.8664594888687134\n",
      "Epoch: 9, Samples: 3552/5760, Loss: 2.59179949760437\n",
      "Epoch: 9, Samples: 3584/5760, Loss: 2.0408809185028076\n",
      "Epoch: 9, Samples: 3616/5760, Loss: 1.9640198945999146\n",
      "Epoch: 9, Samples: 3648/5760, Loss: 2.159745931625366\n",
      "Epoch: 9, Samples: 3680/5760, Loss: 2.612292766571045\n",
      "Epoch: 9, Samples: 3712/5760, Loss: 2.6461315155029297\n",
      "Epoch: 9, Samples: 3744/5760, Loss: 2.339712381362915\n",
      "Epoch: 9, Samples: 3776/5760, Loss: 2.5363664627075195\n",
      "Epoch: 9, Samples: 3808/5760, Loss: 2.2413501739501953\n",
      "Epoch: 9, Samples: 3840/5760, Loss: 2.4647817611694336\n",
      "Epoch: 9, Samples: 3872/5760, Loss: 2.103666305541992\n",
      "Epoch: 9, Samples: 3904/5760, Loss: 2.3752119541168213\n",
      "Epoch: 9, Samples: 3936/5760, Loss: 2.5365443229675293\n",
      "Epoch: 9, Samples: 3968/5760, Loss: 1.6443843841552734\n",
      "Epoch: 9, Samples: 4000/5760, Loss: 2.215899705886841\n",
      "Epoch: 9, Samples: 4032/5760, Loss: 2.421818733215332\n",
      "Epoch: 9, Samples: 4064/5760, Loss: 2.1099352836608887\n",
      "Epoch: 9, Samples: 4096/5760, Loss: 2.3111214637756348\n",
      "Epoch: 9, Samples: 4128/5760, Loss: 2.0037589073181152\n",
      "Epoch: 9, Samples: 4160/5760, Loss: 2.023003339767456\n",
      "Epoch: 9, Samples: 4192/5760, Loss: 2.34140682220459\n",
      "Epoch: 9, Samples: 4224/5760, Loss: 2.7347021102905273\n",
      "Epoch: 9, Samples: 4256/5760, Loss: 1.9465765953063965\n",
      "Epoch: 9, Samples: 4288/5760, Loss: 2.0840113162994385\n",
      "Epoch: 9, Samples: 4320/5760, Loss: 2.418043613433838\n",
      "Epoch: 9, Samples: 4352/5760, Loss: 2.1639785766601562\n",
      "Epoch: 9, Samples: 4384/5760, Loss: 2.800055742263794\n",
      "Epoch: 9, Samples: 4416/5760, Loss: 2.4245333671569824\n",
      "Epoch: 9, Samples: 4448/5760, Loss: 1.8246791362762451\n",
      "Epoch: 9, Samples: 4480/5760, Loss: 2.498781442642212\n",
      "Epoch: 9, Samples: 4512/5760, Loss: 2.5742530822753906\n",
      "Epoch: 9, Samples: 4544/5760, Loss: 2.603132963180542\n",
      "Epoch: 9, Samples: 4576/5760, Loss: 2.1132562160491943\n",
      "Epoch: 9, Samples: 4608/5760, Loss: 2.655233144760132\n",
      "Epoch: 9, Samples: 4640/5760, Loss: 2.1709654331207275\n",
      "Epoch: 9, Samples: 4672/5760, Loss: 2.560533285140991\n",
      "Epoch: 9, Samples: 4704/5760, Loss: 2.4570064544677734\n",
      "Epoch: 9, Samples: 4736/5760, Loss: 2.220045566558838\n",
      "Epoch: 9, Samples: 4768/5760, Loss: 2.2106995582580566\n",
      "Epoch: 9, Samples: 4800/5760, Loss: 2.2527108192443848\n",
      "Epoch: 9, Samples: 4832/5760, Loss: 2.1847715377807617\n",
      "Epoch: 9, Samples: 4864/5760, Loss: 2.148365020751953\n",
      "Epoch: 9, Samples: 4896/5760, Loss: 2.0518996715545654\n",
      "Epoch: 9, Samples: 4928/5760, Loss: 2.464189291000366\n",
      "Epoch: 9, Samples: 4960/5760, Loss: 2.305574417114258\n",
      "Epoch: 9, Samples: 4992/5760, Loss: 2.505664825439453\n",
      "Epoch: 9, Samples: 5024/5760, Loss: 2.5341250896453857\n",
      "Epoch: 9, Samples: 5056/5760, Loss: 2.4347164630889893\n",
      "Epoch: 9, Samples: 5088/5760, Loss: 2.212048053741455\n",
      "Epoch: 9, Samples: 5120/5760, Loss: 2.354949474334717\n",
      "Epoch: 9, Samples: 5152/5760, Loss: 2.137948751449585\n",
      "Epoch: 9, Samples: 5184/5760, Loss: 2.1748299598693848\n",
      "Epoch: 9, Samples: 5216/5760, Loss: 2.4842677116394043\n",
      "Epoch: 9, Samples: 5248/5760, Loss: 2.4763705730438232\n",
      "Epoch: 9, Samples: 5280/5760, Loss: 2.0155391693115234\n",
      "Epoch: 9, Samples: 5312/5760, Loss: 2.1690452098846436\n",
      "Epoch: 9, Samples: 5344/5760, Loss: 1.957741379737854\n",
      "Epoch: 9, Samples: 5376/5760, Loss: 2.1541495323181152\n",
      "Epoch: 9, Samples: 5408/5760, Loss: 2.347116708755493\n",
      "Epoch: 9, Samples: 5440/5760, Loss: 2.4786159992218018\n",
      "Epoch: 9, Samples: 5472/5760, Loss: 2.177420139312744\n",
      "Epoch: 9, Samples: 5504/5760, Loss: 2.5003716945648193\n",
      "Epoch: 9, Samples: 5536/5760, Loss: 2.166405200958252\n",
      "Epoch: 9, Samples: 5568/5760, Loss: 2.6294150352478027\n",
      "Epoch: 9, Samples: 5600/5760, Loss: 2.4636754989624023\n",
      "Epoch: 9, Samples: 5632/5760, Loss: 2.653981924057007\n",
      "Epoch: 9, Samples: 5664/5760, Loss: 2.11399507522583\n",
      "Epoch: 9, Samples: 5696/5760, Loss: 2.2981338500976562\n",
      "Epoch: 9, Samples: 5728/5760, Loss: 3.5421669483184814\n",
      "\n",
      "Epoch: 9\n",
      "Training set: Average loss: 2.2887\n",
      "Validation set: Average loss: 2.3680, Accuracy: 316/818 (39%)\n",
      "Saving model (epoch 9) with lowest validation loss: 2.367986899155837\n",
      "Epoch: 10, Samples: 0/5760, Loss: 2.2243704795837402\n",
      "Epoch: 10, Samples: 32/5760, Loss: 2.2984201908111572\n",
      "Epoch: 10, Samples: 64/5760, Loss: 2.3055942058563232\n",
      "Epoch: 10, Samples: 96/5760, Loss: 2.0028419494628906\n",
      "Epoch: 10, Samples: 128/5760, Loss: 1.8619606494903564\n",
      "Epoch: 10, Samples: 160/5760, Loss: 1.8949658870697021\n",
      "Epoch: 10, Samples: 192/5760, Loss: 2.336700677871704\n",
      "Epoch: 10, Samples: 224/5760, Loss: 2.4388508796691895\n",
      "Epoch: 10, Samples: 256/5760, Loss: 1.8982185125350952\n",
      "Epoch: 10, Samples: 288/5760, Loss: 2.2390785217285156\n",
      "Epoch: 10, Samples: 320/5760, Loss: 2.123548984527588\n",
      "Epoch: 10, Samples: 352/5760, Loss: 2.043793201446533\n",
      "Epoch: 10, Samples: 384/5760, Loss: 1.9581799507141113\n",
      "Epoch: 10, Samples: 416/5760, Loss: 2.2471158504486084\n",
      "Epoch: 10, Samples: 448/5760, Loss: 2.0648624897003174\n",
      "Epoch: 10, Samples: 480/5760, Loss: 2.3539695739746094\n",
      "Epoch: 10, Samples: 512/5760, Loss: 2.1911094188690186\n",
      "Epoch: 10, Samples: 544/5760, Loss: 1.803568720817566\n",
      "Epoch: 10, Samples: 576/5760, Loss: 2.0684032440185547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Samples: 608/5760, Loss: 2.299004077911377\n",
      "Epoch: 10, Samples: 640/5760, Loss: 2.7201883792877197\n",
      "Epoch: 10, Samples: 672/5760, Loss: 2.082369089126587\n",
      "Epoch: 10, Samples: 704/5760, Loss: 1.997148871421814\n",
      "Epoch: 10, Samples: 736/5760, Loss: 2.022599458694458\n",
      "Epoch: 10, Samples: 768/5760, Loss: 2.373727560043335\n",
      "Epoch: 10, Samples: 800/5760, Loss: 2.2798190116882324\n",
      "Epoch: 10, Samples: 832/5760, Loss: 2.486157178878784\n",
      "Epoch: 10, Samples: 864/5760, Loss: 2.328449249267578\n",
      "Epoch: 10, Samples: 896/5760, Loss: 2.1012609004974365\n",
      "Epoch: 10, Samples: 928/5760, Loss: 2.0102806091308594\n",
      "Epoch: 10, Samples: 960/5760, Loss: 2.1800663471221924\n",
      "Epoch: 10, Samples: 992/5760, Loss: 1.954963207244873\n",
      "Epoch: 10, Samples: 1024/5760, Loss: 2.125729560852051\n",
      "Epoch: 10, Samples: 1056/5760, Loss: 2.608816146850586\n",
      "Epoch: 10, Samples: 1088/5760, Loss: 2.599696397781372\n",
      "Epoch: 10, Samples: 1120/5760, Loss: 2.3587677478790283\n",
      "Epoch: 10, Samples: 1152/5760, Loss: 2.3640339374542236\n",
      "Epoch: 10, Samples: 1184/5760, Loss: 2.450070858001709\n",
      "Epoch: 10, Samples: 1216/5760, Loss: 2.330305576324463\n",
      "Epoch: 10, Samples: 1248/5760, Loss: 2.5142810344696045\n",
      "Epoch: 10, Samples: 1280/5760, Loss: 1.8854025602340698\n",
      "Epoch: 10, Samples: 1312/5760, Loss: 2.2999532222747803\n",
      "Epoch: 10, Samples: 1344/5760, Loss: 2.216546058654785\n",
      "Epoch: 10, Samples: 1376/5760, Loss: 1.9859598875045776\n",
      "Epoch: 10, Samples: 1408/5760, Loss: 2.0106468200683594\n",
      "Epoch: 10, Samples: 1440/5760, Loss: 2.6527819633483887\n",
      "Epoch: 10, Samples: 1472/5760, Loss: 2.1724612712860107\n",
      "Epoch: 10, Samples: 1504/5760, Loss: 2.0853796005249023\n",
      "Epoch: 10, Samples: 1536/5760, Loss: 2.118259906768799\n",
      "Epoch: 10, Samples: 1568/5760, Loss: 1.9952173233032227\n",
      "Epoch: 10, Samples: 1600/5760, Loss: 2.474001169204712\n",
      "Epoch: 10, Samples: 1632/5760, Loss: 2.217440605163574\n",
      "Epoch: 10, Samples: 1664/5760, Loss: 2.479771375656128\n",
      "Epoch: 10, Samples: 1696/5760, Loss: 1.9905966520309448\n",
      "Epoch: 10, Samples: 1728/5760, Loss: 2.4195680618286133\n",
      "Epoch: 10, Samples: 1760/5760, Loss: 2.3257081508636475\n",
      "Epoch: 10, Samples: 1792/5760, Loss: 2.5573017597198486\n",
      "Epoch: 10, Samples: 1824/5760, Loss: 1.913964867591858\n",
      "Epoch: 10, Samples: 1856/5760, Loss: 1.8218048810958862\n",
      "Epoch: 10, Samples: 1888/5760, Loss: 2.138883352279663\n",
      "Epoch: 10, Samples: 1920/5760, Loss: 1.962788701057434\n",
      "Epoch: 10, Samples: 1952/5760, Loss: 1.7476041316986084\n",
      "Epoch: 10, Samples: 1984/5760, Loss: 1.9905424118041992\n",
      "Epoch: 10, Samples: 2016/5760, Loss: 2.269146203994751\n",
      "Epoch: 10, Samples: 2048/5760, Loss: 1.948095440864563\n",
      "Epoch: 10, Samples: 2080/5760, Loss: 2.394016981124878\n",
      "Epoch: 10, Samples: 2112/5760, Loss: 2.2998135089874268\n",
      "Epoch: 10, Samples: 2144/5760, Loss: 2.347071886062622\n",
      "Epoch: 10, Samples: 2176/5760, Loss: 2.302530527114868\n",
      "Epoch: 10, Samples: 2208/5760, Loss: 2.3784923553466797\n",
      "Epoch: 10, Samples: 2240/5760, Loss: 2.1789841651916504\n",
      "Epoch: 10, Samples: 2272/5760, Loss: 2.360945224761963\n",
      "Epoch: 10, Samples: 2304/5760, Loss: 2.571336030960083\n",
      "Epoch: 10, Samples: 2336/5760, Loss: 2.3752472400665283\n",
      "Epoch: 10, Samples: 2368/5760, Loss: 2.1225528717041016\n",
      "Epoch: 10, Samples: 2400/5760, Loss: 2.135363817214966\n",
      "Epoch: 10, Samples: 2432/5760, Loss: 1.7490986585617065\n",
      "Epoch: 10, Samples: 2464/5760, Loss: 2.026374578475952\n",
      "Epoch: 10, Samples: 2496/5760, Loss: 2.2360122203826904\n",
      "Epoch: 10, Samples: 2528/5760, Loss: 2.242520332336426\n",
      "Epoch: 10, Samples: 2560/5760, Loss: 2.6187562942504883\n",
      "Epoch: 10, Samples: 2592/5760, Loss: 2.255190372467041\n",
      "Epoch: 10, Samples: 2624/5760, Loss: 2.2123098373413086\n",
      "Epoch: 10, Samples: 2656/5760, Loss: 2.474468469619751\n",
      "Epoch: 10, Samples: 2688/5760, Loss: 2.3999509811401367\n",
      "Epoch: 10, Samples: 2720/5760, Loss: 1.743152141571045\n",
      "Epoch: 10, Samples: 2752/5760, Loss: 2.443211793899536\n",
      "Epoch: 10, Samples: 2784/5760, Loss: 2.0173256397247314\n",
      "Epoch: 10, Samples: 2816/5760, Loss: 2.1258609294891357\n",
      "Epoch: 10, Samples: 2848/5760, Loss: 2.078972578048706\n",
      "Epoch: 10, Samples: 2880/5760, Loss: 2.2199201583862305\n",
      "Epoch: 10, Samples: 2912/5760, Loss: 2.257086753845215\n",
      "Epoch: 10, Samples: 2944/5760, Loss: 2.521209478378296\n",
      "Epoch: 10, Samples: 2976/5760, Loss: 2.181715726852417\n",
      "Epoch: 10, Samples: 3008/5760, Loss: 2.055999279022217\n",
      "Epoch: 10, Samples: 3040/5760, Loss: 2.120795488357544\n",
      "Epoch: 10, Samples: 3072/5760, Loss: 1.9760801792144775\n",
      "Epoch: 10, Samples: 3104/5760, Loss: 1.9986553192138672\n",
      "Epoch: 10, Samples: 3136/5760, Loss: 2.3809711933135986\n",
      "Epoch: 10, Samples: 3168/5760, Loss: 1.9332596063613892\n",
      "Epoch: 10, Samples: 3200/5760, Loss: 2.015615940093994\n",
      "Epoch: 10, Samples: 3232/5760, Loss: 2.0887606143951416\n",
      "Epoch: 10, Samples: 3264/5760, Loss: 2.098938226699829\n",
      "Epoch: 10, Samples: 3296/5760, Loss: 2.5275604724884033\n",
      "Epoch: 10, Samples: 3328/5760, Loss: 2.169586181640625\n",
      "Epoch: 10, Samples: 3360/5760, Loss: 2.198122978210449\n",
      "Epoch: 10, Samples: 3392/5760, Loss: 2.1830296516418457\n",
      "Epoch: 10, Samples: 3424/5760, Loss: 1.9760135412216187\n",
      "Epoch: 10, Samples: 3456/5760, Loss: 1.8011716604232788\n",
      "Epoch: 10, Samples: 3488/5760, Loss: 2.719249963760376\n",
      "Epoch: 10, Samples: 3520/5760, Loss: 2.037538766860962\n",
      "Epoch: 10, Samples: 3552/5760, Loss: 1.9485268592834473\n",
      "Epoch: 10, Samples: 3584/5760, Loss: 2.0350189208984375\n",
      "Epoch: 10, Samples: 3616/5760, Loss: 2.1021203994750977\n",
      "Epoch: 10, Samples: 3648/5760, Loss: 2.1360409259796143\n",
      "Epoch: 10, Samples: 3680/5760, Loss: 2.1823487281799316\n",
      "Epoch: 10, Samples: 3712/5760, Loss: 1.9333915710449219\n",
      "Epoch: 10, Samples: 3744/5760, Loss: 2.1147301197052\n",
      "Epoch: 10, Samples: 3776/5760, Loss: 1.6881301403045654\n",
      "Epoch: 10, Samples: 3808/5760, Loss: 1.6952871084213257\n",
      "Epoch: 10, Samples: 3840/5760, Loss: 2.0030789375305176\n",
      "Epoch: 10, Samples: 3872/5760, Loss: 1.8482154607772827\n",
      "Epoch: 10, Samples: 3904/5760, Loss: 2.2645657062530518\n",
      "Epoch: 10, Samples: 3936/5760, Loss: 2.1215381622314453\n",
      "Epoch: 10, Samples: 3968/5760, Loss: 1.6866122484207153\n",
      "Epoch: 10, Samples: 4000/5760, Loss: 2.0881495475769043\n",
      "Epoch: 10, Samples: 4032/5760, Loss: 2.3937225341796875\n",
      "Epoch: 10, Samples: 4064/5760, Loss: 2.2865190505981445\n",
      "Epoch: 10, Samples: 4096/5760, Loss: 2.148592948913574\n",
      "Epoch: 10, Samples: 4128/5760, Loss: 2.2545933723449707\n",
      "Epoch: 10, Samples: 4160/5760, Loss: 2.2031972408294678\n",
      "Epoch: 10, Samples: 4192/5760, Loss: 2.479646921157837\n",
      "Epoch: 10, Samples: 4224/5760, Loss: 2.253093957901001\n",
      "Epoch: 10, Samples: 4256/5760, Loss: 1.8395859003067017\n",
      "Epoch: 10, Samples: 4288/5760, Loss: 1.7743680477142334\n",
      "Epoch: 10, Samples: 4320/5760, Loss: 2.3597817420959473\n",
      "Epoch: 10, Samples: 4352/5760, Loss: 1.981178879737854\n",
      "Epoch: 10, Samples: 4384/5760, Loss: 2.3523075580596924\n",
      "Epoch: 10, Samples: 4416/5760, Loss: 2.0820136070251465\n",
      "Epoch: 10, Samples: 4448/5760, Loss: 2.2692835330963135\n",
      "Epoch: 10, Samples: 4480/5760, Loss: 2.4669747352600098\n",
      "Epoch: 10, Samples: 4512/5760, Loss: 2.4101431369781494\n",
      "Epoch: 10, Samples: 4544/5760, Loss: 2.0474987030029297\n",
      "Epoch: 10, Samples: 4576/5760, Loss: 2.33439302444458\n",
      "Epoch: 10, Samples: 4608/5760, Loss: 2.01857328414917\n",
      "Epoch: 10, Samples: 4640/5760, Loss: 1.9511910676956177\n",
      "Epoch: 10, Samples: 4672/5760, Loss: 1.9964722394943237\n",
      "Epoch: 10, Samples: 4704/5760, Loss: 2.0679163932800293\n",
      "Epoch: 10, Samples: 4736/5760, Loss: 1.9738316535949707\n",
      "Epoch: 10, Samples: 4768/5760, Loss: 2.109494686126709\n",
      "Epoch: 10, Samples: 4800/5760, Loss: 2.2268049716949463\n",
      "Epoch: 10, Samples: 4832/5760, Loss: 2.410674571990967\n",
      "Epoch: 10, Samples: 4864/5760, Loss: 2.6617116928100586\n",
      "Epoch: 10, Samples: 4896/5760, Loss: 1.890191674232483\n",
      "Epoch: 10, Samples: 4928/5760, Loss: 2.726156711578369\n",
      "Epoch: 10, Samples: 4960/5760, Loss: 2.3643264770507812\n",
      "Epoch: 10, Samples: 4992/5760, Loss: 2.13759446144104\n",
      "Epoch: 10, Samples: 5024/5760, Loss: 2.194671392440796\n",
      "Epoch: 10, Samples: 5056/5760, Loss: 2.037562131881714\n",
      "Epoch: 10, Samples: 5088/5760, Loss: 2.0313003063201904\n",
      "Epoch: 10, Samples: 5120/5760, Loss: 2.2773125171661377\n",
      "Epoch: 10, Samples: 5152/5760, Loss: 2.4011735916137695\n",
      "Epoch: 10, Samples: 5184/5760, Loss: 2.1701552867889404\n",
      "Epoch: 10, Samples: 5216/5760, Loss: 2.0378634929656982\n",
      "Epoch: 10, Samples: 5248/5760, Loss: 2.4279987812042236\n",
      "Epoch: 10, Samples: 5280/5760, Loss: 1.7633447647094727\n",
      "Epoch: 10, Samples: 5312/5760, Loss: 2.180072784423828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Samples: 5344/5760, Loss: 2.1856088638305664\n",
      "Epoch: 10, Samples: 5376/5760, Loss: 2.5129730701446533\n",
      "Epoch: 10, Samples: 5408/5760, Loss: 2.421226978302002\n",
      "Epoch: 10, Samples: 5440/5760, Loss: 2.0561435222625732\n",
      "Epoch: 10, Samples: 5472/5760, Loss: 2.657179594039917\n",
      "Epoch: 10, Samples: 5504/5760, Loss: 2.117480993270874\n",
      "Epoch: 10, Samples: 5536/5760, Loss: 2.035529136657715\n",
      "Epoch: 10, Samples: 5568/5760, Loss: 1.6797447204589844\n",
      "Epoch: 10, Samples: 5600/5760, Loss: 1.803661584854126\n",
      "Epoch: 10, Samples: 5632/5760, Loss: 2.1347527503967285\n",
      "Epoch: 10, Samples: 5664/5760, Loss: 2.2678654193878174\n",
      "Epoch: 10, Samples: 5696/5760, Loss: 2.327617645263672\n",
      "Epoch: 10, Samples: 5728/5760, Loss: 2.235200881958008\n",
      "\n",
      "Epoch: 10\n",
      "Training set: Average loss: 2.1781\n",
      "Validation set: Average loss: 2.2295, Accuracy: 350/818 (43%)\n",
      "Saving model (epoch 10) with lowest validation loss: 2.229487180709839\n",
      "Epoch: 11, Samples: 0/5760, Loss: 1.953113079071045\n",
      "Epoch: 11, Samples: 32/5760, Loss: 2.2415709495544434\n",
      "Epoch: 11, Samples: 64/5760, Loss: 2.379379987716675\n",
      "Epoch: 11, Samples: 96/5760, Loss: 1.7713900804519653\n",
      "Epoch: 11, Samples: 128/5760, Loss: 1.9815685749053955\n",
      "Epoch: 11, Samples: 160/5760, Loss: 1.9564578533172607\n",
      "Epoch: 11, Samples: 192/5760, Loss: 2.3145253658294678\n",
      "Epoch: 11, Samples: 224/5760, Loss: 2.069253921508789\n",
      "Epoch: 11, Samples: 256/5760, Loss: 2.045056104660034\n",
      "Epoch: 11, Samples: 288/5760, Loss: 2.0410892963409424\n",
      "Epoch: 11, Samples: 320/5760, Loss: 2.3012027740478516\n",
      "Epoch: 11, Samples: 352/5760, Loss: 2.395399570465088\n",
      "Epoch: 11, Samples: 384/5760, Loss: 1.9507757425308228\n",
      "Epoch: 11, Samples: 416/5760, Loss: 2.419321298599243\n",
      "Epoch: 11, Samples: 448/5760, Loss: 2.2165613174438477\n",
      "Epoch: 11, Samples: 480/5760, Loss: 2.1172237396240234\n",
      "Epoch: 11, Samples: 512/5760, Loss: 2.3180365562438965\n",
      "Epoch: 11, Samples: 544/5760, Loss: 1.8246210813522339\n",
      "Epoch: 11, Samples: 576/5760, Loss: 1.7891348600387573\n",
      "Epoch: 11, Samples: 608/5760, Loss: 2.0646474361419678\n",
      "Epoch: 11, Samples: 640/5760, Loss: 2.149650812149048\n",
      "Epoch: 11, Samples: 672/5760, Loss: 1.8355653285980225\n",
      "Epoch: 11, Samples: 704/5760, Loss: 2.215550422668457\n",
      "Epoch: 11, Samples: 736/5760, Loss: 2.2596020698547363\n",
      "Epoch: 11, Samples: 768/5760, Loss: 2.0287392139434814\n",
      "Epoch: 11, Samples: 800/5760, Loss: 2.0534842014312744\n",
      "Epoch: 11, Samples: 832/5760, Loss: 1.9475804567337036\n",
      "Epoch: 11, Samples: 864/5760, Loss: 2.1792685985565186\n",
      "Epoch: 11, Samples: 896/5760, Loss: 2.510000228881836\n",
      "Epoch: 11, Samples: 928/5760, Loss: 2.2264366149902344\n",
      "Epoch: 11, Samples: 960/5760, Loss: 2.1720428466796875\n",
      "Epoch: 11, Samples: 992/5760, Loss: 1.8084710836410522\n",
      "Epoch: 11, Samples: 1024/5760, Loss: 2.065237045288086\n",
      "Epoch: 11, Samples: 1056/5760, Loss: 2.2965638637542725\n",
      "Epoch: 11, Samples: 1088/5760, Loss: 1.5957547426223755\n",
      "Epoch: 11, Samples: 1120/5760, Loss: 2.321596145629883\n",
      "Epoch: 11, Samples: 1152/5760, Loss: 2.7571861743927\n",
      "Epoch: 11, Samples: 1184/5760, Loss: 2.037339925765991\n",
      "Epoch: 11, Samples: 1216/5760, Loss: 2.1177120208740234\n",
      "Epoch: 11, Samples: 1248/5760, Loss: 1.774958610534668\n",
      "Epoch: 11, Samples: 1280/5760, Loss: 2.214374542236328\n",
      "Epoch: 11, Samples: 1312/5760, Loss: 1.9618744850158691\n",
      "Epoch: 11, Samples: 1344/5760, Loss: 2.41709041595459\n",
      "Epoch: 11, Samples: 1376/5760, Loss: 2.2270987033843994\n",
      "Epoch: 11, Samples: 1408/5760, Loss: 2.0799083709716797\n",
      "Epoch: 11, Samples: 1440/5760, Loss: 2.2508559226989746\n",
      "Epoch: 11, Samples: 1472/5760, Loss: 2.4831180572509766\n",
      "Epoch: 11, Samples: 1504/5760, Loss: 2.375433921813965\n",
      "Epoch: 11, Samples: 1536/5760, Loss: 2.283905506134033\n",
      "Epoch: 11, Samples: 1568/5760, Loss: 2.2379963397979736\n",
      "Epoch: 11, Samples: 1600/5760, Loss: 2.2864739894866943\n",
      "Epoch: 11, Samples: 1632/5760, Loss: 1.9191313982009888\n",
      "Epoch: 11, Samples: 1664/5760, Loss: 1.867050290107727\n",
      "Epoch: 11, Samples: 1696/5760, Loss: 2.035019636154175\n",
      "Epoch: 11, Samples: 1728/5760, Loss: 2.168860673904419\n",
      "Epoch: 11, Samples: 1760/5760, Loss: 1.908871054649353\n",
      "Epoch: 11, Samples: 1792/5760, Loss: 1.9573222398757935\n",
      "Epoch: 11, Samples: 1824/5760, Loss: 2.424375295639038\n",
      "Epoch: 11, Samples: 1856/5760, Loss: 2.2912821769714355\n",
      "Epoch: 11, Samples: 1888/5760, Loss: 1.9104410409927368\n",
      "Epoch: 11, Samples: 1920/5760, Loss: 2.0455803871154785\n",
      "Epoch: 11, Samples: 1952/5760, Loss: 2.2660608291625977\n",
      "Epoch: 11, Samples: 1984/5760, Loss: 2.0845727920532227\n",
      "Epoch: 11, Samples: 2016/5760, Loss: 2.0678980350494385\n",
      "Epoch: 11, Samples: 2048/5760, Loss: 2.151277780532837\n",
      "Epoch: 11, Samples: 2080/5760, Loss: 2.3165955543518066\n",
      "Epoch: 11, Samples: 2112/5760, Loss: 2.1341333389282227\n",
      "Epoch: 11, Samples: 2144/5760, Loss: 2.3779232501983643\n",
      "Epoch: 11, Samples: 2176/5760, Loss: 2.2335798740386963\n",
      "Epoch: 11, Samples: 2208/5760, Loss: 1.9562608003616333\n",
      "Epoch: 11, Samples: 2240/5760, Loss: 2.039788007736206\n",
      "Epoch: 11, Samples: 2272/5760, Loss: 2.168316125869751\n",
      "Epoch: 11, Samples: 2304/5760, Loss: 1.6016490459442139\n",
      "Epoch: 11, Samples: 2336/5760, Loss: 2.1804776191711426\n",
      "Epoch: 11, Samples: 2368/5760, Loss: 1.8132290840148926\n",
      "Epoch: 11, Samples: 2400/5760, Loss: 2.439471483230591\n",
      "Epoch: 11, Samples: 2432/5760, Loss: 1.9935132265090942\n",
      "Epoch: 11, Samples: 2464/5760, Loss: 1.8655740022659302\n",
      "Epoch: 11, Samples: 2496/5760, Loss: 1.9015623331069946\n",
      "Epoch: 11, Samples: 2528/5760, Loss: 1.8157305717468262\n",
      "Epoch: 11, Samples: 2560/5760, Loss: 2.182560920715332\n",
      "Epoch: 11, Samples: 2592/5760, Loss: 2.315443277359009\n",
      "Epoch: 11, Samples: 2624/5760, Loss: 2.136427879333496\n",
      "Epoch: 11, Samples: 2656/5760, Loss: 2.348271131515503\n",
      "Epoch: 11, Samples: 2688/5760, Loss: 2.309124231338501\n",
      "Epoch: 11, Samples: 2720/5760, Loss: 2.1690239906311035\n",
      "Epoch: 11, Samples: 2752/5760, Loss: 1.701860785484314\n",
      "Epoch: 11, Samples: 2784/5760, Loss: 2.045283555984497\n",
      "Epoch: 11, Samples: 2816/5760, Loss: 1.9045740365982056\n",
      "Epoch: 11, Samples: 2848/5760, Loss: 1.824354887008667\n",
      "Epoch: 11, Samples: 2880/5760, Loss: 2.2475790977478027\n",
      "Epoch: 11, Samples: 2912/5760, Loss: 1.620145320892334\n",
      "Epoch: 11, Samples: 2944/5760, Loss: 1.9703457355499268\n",
      "Epoch: 11, Samples: 2976/5760, Loss: 2.138923168182373\n",
      "Epoch: 11, Samples: 3008/5760, Loss: 1.9703145027160645\n",
      "Epoch: 11, Samples: 3040/5760, Loss: 2.143690824508667\n",
      "Epoch: 11, Samples: 3072/5760, Loss: 2.14931583404541\n",
      "Epoch: 11, Samples: 3104/5760, Loss: 2.0572943687438965\n",
      "Epoch: 11, Samples: 3136/5760, Loss: 2.1948118209838867\n",
      "Epoch: 11, Samples: 3168/5760, Loss: 1.959336280822754\n",
      "Epoch: 11, Samples: 3200/5760, Loss: 2.0255396366119385\n",
      "Epoch: 11, Samples: 3232/5760, Loss: 1.914811134338379\n",
      "Epoch: 11, Samples: 3264/5760, Loss: 1.8825936317443848\n",
      "Epoch: 11, Samples: 3296/5760, Loss: 2.04356050491333\n",
      "Epoch: 11, Samples: 3328/5760, Loss: 2.3437283039093018\n",
      "Epoch: 11, Samples: 3360/5760, Loss: 1.4400771856307983\n",
      "Epoch: 11, Samples: 3392/5760, Loss: 2.0731019973754883\n",
      "Epoch: 11, Samples: 3424/5760, Loss: 1.4318434000015259\n",
      "Epoch: 11, Samples: 3456/5760, Loss: 2.7137510776519775\n",
      "Epoch: 11, Samples: 3488/5760, Loss: 2.3752803802490234\n",
      "Epoch: 11, Samples: 3520/5760, Loss: 1.9362151622772217\n",
      "Epoch: 11, Samples: 3552/5760, Loss: 2.3877832889556885\n",
      "Epoch: 11, Samples: 3584/5760, Loss: 2.191331148147583\n",
      "Epoch: 11, Samples: 3616/5760, Loss: 1.9690258502960205\n",
      "Epoch: 11, Samples: 3648/5760, Loss: 2.3028507232666016\n",
      "Epoch: 11, Samples: 3680/5760, Loss: 1.9837347269058228\n",
      "Epoch: 11, Samples: 3712/5760, Loss: 1.789330005645752\n",
      "Epoch: 11, Samples: 3744/5760, Loss: 2.1106116771698\n",
      "Epoch: 11, Samples: 3776/5760, Loss: 2.267171859741211\n",
      "Epoch: 11, Samples: 3808/5760, Loss: 2.2297310829162598\n",
      "Epoch: 11, Samples: 3840/5760, Loss: 2.4576473236083984\n",
      "Epoch: 11, Samples: 3872/5760, Loss: 2.1687960624694824\n",
      "Epoch: 11, Samples: 3904/5760, Loss: 1.7319815158843994\n",
      "Epoch: 11, Samples: 3936/5760, Loss: 2.012909412384033\n",
      "Epoch: 11, Samples: 3968/5760, Loss: 1.9025901556015015\n",
      "Epoch: 11, Samples: 4000/5760, Loss: 2.2879538536071777\n",
      "Epoch: 11, Samples: 4032/5760, Loss: 2.075587511062622\n",
      "Epoch: 11, Samples: 4064/5760, Loss: 1.5549310445785522\n",
      "Epoch: 11, Samples: 4096/5760, Loss: 2.2568776607513428\n",
      "Epoch: 11, Samples: 4128/5760, Loss: 1.9646811485290527\n",
      "Epoch: 11, Samples: 4160/5760, Loss: 1.90281081199646\n",
      "Epoch: 11, Samples: 4192/5760, Loss: 2.0341103076934814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Samples: 4224/5760, Loss: 2.046588659286499\n",
      "Epoch: 11, Samples: 4256/5760, Loss: 1.3786967992782593\n",
      "Epoch: 11, Samples: 4288/5760, Loss: 1.9213570356369019\n",
      "Epoch: 11, Samples: 4320/5760, Loss: 2.6073875427246094\n",
      "Epoch: 11, Samples: 4352/5760, Loss: 1.9888873100280762\n",
      "Epoch: 11, Samples: 4384/5760, Loss: 2.051579713821411\n",
      "Epoch: 11, Samples: 4416/5760, Loss: 2.2275848388671875\n",
      "Epoch: 11, Samples: 4448/5760, Loss: 1.734865427017212\n",
      "Epoch: 11, Samples: 4480/5760, Loss: 1.6226189136505127\n",
      "Epoch: 11, Samples: 4512/5760, Loss: 2.4105112552642822\n",
      "Epoch: 11, Samples: 4544/5760, Loss: 2.1184096336364746\n",
      "Epoch: 11, Samples: 4576/5760, Loss: 1.9088726043701172\n",
      "Epoch: 11, Samples: 4608/5760, Loss: 2.0561914443969727\n",
      "Epoch: 11, Samples: 4640/5760, Loss: 1.880398154258728\n",
      "Epoch: 11, Samples: 4672/5760, Loss: 1.899340271949768\n",
      "Epoch: 11, Samples: 4704/5760, Loss: 2.1558284759521484\n",
      "Epoch: 11, Samples: 4736/5760, Loss: 2.224109649658203\n",
      "Epoch: 11, Samples: 4768/5760, Loss: 2.415642261505127\n",
      "Epoch: 11, Samples: 4800/5760, Loss: 1.8198480606079102\n",
      "Epoch: 11, Samples: 4832/5760, Loss: 2.012446403503418\n",
      "Epoch: 11, Samples: 4864/5760, Loss: 2.356942653656006\n",
      "Epoch: 11, Samples: 4896/5760, Loss: 2.233534336090088\n",
      "Epoch: 11, Samples: 4928/5760, Loss: 2.3699653148651123\n",
      "Epoch: 11, Samples: 4960/5760, Loss: 2.2030608654022217\n",
      "Epoch: 11, Samples: 4992/5760, Loss: 2.016106128692627\n",
      "Epoch: 11, Samples: 5024/5760, Loss: 1.7092161178588867\n",
      "Epoch: 11, Samples: 5056/5760, Loss: 2.4557440280914307\n",
      "Epoch: 11, Samples: 5088/5760, Loss: 2.4342055320739746\n",
      "Epoch: 11, Samples: 5120/5760, Loss: 2.1631505489349365\n",
      "Epoch: 11, Samples: 5152/5760, Loss: 1.9129337072372437\n",
      "Epoch: 11, Samples: 5184/5760, Loss: 2.734422206878662\n",
      "Epoch: 11, Samples: 5216/5760, Loss: 1.9476302862167358\n",
      "Epoch: 11, Samples: 5248/5760, Loss: 1.9265669584274292\n",
      "Epoch: 11, Samples: 5280/5760, Loss: 1.6201099157333374\n",
      "Epoch: 11, Samples: 5312/5760, Loss: 1.649848222732544\n",
      "Epoch: 11, Samples: 5344/5760, Loss: 1.8689849376678467\n",
      "Epoch: 11, Samples: 5376/5760, Loss: 1.9892542362213135\n",
      "Epoch: 11, Samples: 5408/5760, Loss: 2.3320250511169434\n",
      "Epoch: 11, Samples: 5440/5760, Loss: 2.0398693084716797\n",
      "Epoch: 11, Samples: 5472/5760, Loss: 2.245436429977417\n",
      "Epoch: 11, Samples: 5504/5760, Loss: 2.4531803131103516\n",
      "Epoch: 11, Samples: 5536/5760, Loss: 2.004335880279541\n",
      "Epoch: 11, Samples: 5568/5760, Loss: 1.9825259447097778\n",
      "Epoch: 11, Samples: 5600/5760, Loss: 2.1368203163146973\n",
      "Epoch: 11, Samples: 5632/5760, Loss: 1.7594904899597168\n",
      "Epoch: 11, Samples: 5664/5760, Loss: 1.9698529243469238\n",
      "Epoch: 11, Samples: 5696/5760, Loss: 1.8659436702728271\n",
      "Epoch: 11, Samples: 5728/5760, Loss: 3.559211492538452\n",
      "\n",
      "Epoch: 11\n",
      "Training set: Average loss: 2.0935\n",
      "Validation set: Average loss: 2.2991, Accuracy: 352/818 (43%)\n",
      "Epoch: 12, Samples: 0/5760, Loss: 1.6952165365219116\n",
      "Epoch: 12, Samples: 32/5760, Loss: 1.9924821853637695\n",
      "Epoch: 12, Samples: 64/5760, Loss: 1.9762738943099976\n",
      "Epoch: 12, Samples: 96/5760, Loss: 2.301302433013916\n",
      "Epoch: 12, Samples: 128/5760, Loss: 2.3371012210845947\n",
      "Epoch: 12, Samples: 160/5760, Loss: 2.130476474761963\n",
      "Epoch: 12, Samples: 192/5760, Loss: 1.8135478496551514\n",
      "Epoch: 12, Samples: 224/5760, Loss: 1.7593693733215332\n",
      "Epoch: 12, Samples: 256/5760, Loss: 1.6894646883010864\n",
      "Epoch: 12, Samples: 288/5760, Loss: 1.962206244468689\n",
      "Epoch: 12, Samples: 320/5760, Loss: 2.1733996868133545\n",
      "Epoch: 12, Samples: 352/5760, Loss: 2.0710012912750244\n",
      "Epoch: 12, Samples: 384/5760, Loss: 2.105912208557129\n",
      "Epoch: 12, Samples: 416/5760, Loss: 2.1447272300720215\n",
      "Epoch: 12, Samples: 448/5760, Loss: 2.4495930671691895\n",
      "Epoch: 12, Samples: 480/5760, Loss: 2.171344041824341\n",
      "Epoch: 12, Samples: 512/5760, Loss: 1.560652256011963\n",
      "Epoch: 12, Samples: 544/5760, Loss: 2.052022695541382\n",
      "Epoch: 12, Samples: 576/5760, Loss: 2.1615610122680664\n",
      "Epoch: 12, Samples: 608/5760, Loss: 1.8067855834960938\n",
      "Epoch: 12, Samples: 640/5760, Loss: 1.9160517454147339\n",
      "Epoch: 12, Samples: 672/5760, Loss: 2.147834539413452\n",
      "Epoch: 12, Samples: 704/5760, Loss: 1.8824462890625\n",
      "Epoch: 12, Samples: 736/5760, Loss: 1.5708979368209839\n",
      "Epoch: 12, Samples: 768/5760, Loss: 1.7896093130111694\n",
      "Epoch: 12, Samples: 800/5760, Loss: 2.3145804405212402\n",
      "Epoch: 12, Samples: 832/5760, Loss: 1.7341251373291016\n",
      "Epoch: 12, Samples: 864/5760, Loss: 1.7915618419647217\n",
      "Epoch: 12, Samples: 896/5760, Loss: 2.282590866088867\n",
      "Epoch: 12, Samples: 928/5760, Loss: 1.6347222328186035\n",
      "Epoch: 12, Samples: 960/5760, Loss: 1.5413010120391846\n",
      "Epoch: 12, Samples: 992/5760, Loss: 1.8646705150604248\n",
      "Epoch: 12, Samples: 1024/5760, Loss: 1.825645923614502\n",
      "Epoch: 12, Samples: 1056/5760, Loss: 1.9998730421066284\n",
      "Epoch: 12, Samples: 1088/5760, Loss: 2.287581205368042\n",
      "Epoch: 12, Samples: 1120/5760, Loss: 2.5341086387634277\n",
      "Epoch: 12, Samples: 1152/5760, Loss: 2.0795483589172363\n",
      "Epoch: 12, Samples: 1184/5760, Loss: 1.5501551628112793\n",
      "Epoch: 12, Samples: 1216/5760, Loss: 2.4782466888427734\n",
      "Epoch: 12, Samples: 1248/5760, Loss: 1.9148237705230713\n",
      "Epoch: 12, Samples: 1280/5760, Loss: 2.0869956016540527\n",
      "Epoch: 12, Samples: 1312/5760, Loss: 1.757517695426941\n",
      "Epoch: 12, Samples: 1344/5760, Loss: 2.105874538421631\n",
      "Epoch: 12, Samples: 1376/5760, Loss: 2.112978219985962\n",
      "Epoch: 12, Samples: 1408/5760, Loss: 1.5989561080932617\n",
      "Epoch: 12, Samples: 1440/5760, Loss: 1.7225117683410645\n",
      "Epoch: 12, Samples: 1472/5760, Loss: 1.704406499862671\n",
      "Epoch: 12, Samples: 1504/5760, Loss: 1.8469806909561157\n",
      "Epoch: 12, Samples: 1536/5760, Loss: 2.2018215656280518\n",
      "Epoch: 12, Samples: 1568/5760, Loss: 1.8187568187713623\n",
      "Epoch: 12, Samples: 1600/5760, Loss: 1.6475791931152344\n",
      "Epoch: 12, Samples: 1632/5760, Loss: 2.1246607303619385\n",
      "Epoch: 12, Samples: 1664/5760, Loss: 1.9874969720840454\n",
      "Epoch: 12, Samples: 1696/5760, Loss: 1.9342466592788696\n",
      "Epoch: 12, Samples: 1728/5760, Loss: 2.1627197265625\n",
      "Epoch: 12, Samples: 1760/5760, Loss: 2.1779253482818604\n",
      "Epoch: 12, Samples: 1792/5760, Loss: 2.0633838176727295\n",
      "Epoch: 12, Samples: 1824/5760, Loss: 1.7236992120742798\n",
      "Epoch: 12, Samples: 1856/5760, Loss: 2.0200161933898926\n",
      "Epoch: 12, Samples: 1888/5760, Loss: 1.7570164203643799\n",
      "Epoch: 12, Samples: 1920/5760, Loss: 1.6398178339004517\n",
      "Epoch: 12, Samples: 1952/5760, Loss: 2.2155392169952393\n",
      "Epoch: 12, Samples: 1984/5760, Loss: 2.0633959770202637\n",
      "Epoch: 12, Samples: 2016/5760, Loss: 1.930627703666687\n",
      "Epoch: 12, Samples: 2048/5760, Loss: 1.4877454042434692\n",
      "Epoch: 12, Samples: 2080/5760, Loss: 2.05529522895813\n",
      "Epoch: 12, Samples: 2112/5760, Loss: 1.759566068649292\n",
      "Epoch: 12, Samples: 2144/5760, Loss: 2.146928310394287\n",
      "Epoch: 12, Samples: 2176/5760, Loss: 2.337027072906494\n",
      "Epoch: 12, Samples: 2208/5760, Loss: 1.7664191722869873\n",
      "Epoch: 12, Samples: 2240/5760, Loss: 2.2156999111175537\n",
      "Epoch: 12, Samples: 2272/5760, Loss: 2.171171188354492\n",
      "Epoch: 12, Samples: 2304/5760, Loss: 1.8364521265029907\n",
      "Epoch: 12, Samples: 2336/5760, Loss: 1.4720388650894165\n",
      "Epoch: 12, Samples: 2368/5760, Loss: 1.920548915863037\n",
      "Epoch: 12, Samples: 2400/5760, Loss: 1.8377633094787598\n",
      "Epoch: 12, Samples: 2432/5760, Loss: 1.8218815326690674\n",
      "Epoch: 12, Samples: 2464/5760, Loss: 2.319611072540283\n",
      "Epoch: 12, Samples: 2496/5760, Loss: 1.9232680797576904\n",
      "Epoch: 12, Samples: 2528/5760, Loss: 1.464057207107544\n",
      "Epoch: 12, Samples: 2560/5760, Loss: 1.8976191282272339\n",
      "Epoch: 12, Samples: 2592/5760, Loss: 2.0836305618286133\n",
      "Epoch: 12, Samples: 2624/5760, Loss: 1.7121535539627075\n",
      "Epoch: 12, Samples: 2656/5760, Loss: 2.4393274784088135\n",
      "Epoch: 12, Samples: 2688/5760, Loss: 1.7942222356796265\n",
      "Epoch: 12, Samples: 2720/5760, Loss: 2.185114622116089\n",
      "Epoch: 12, Samples: 2752/5760, Loss: 2.264768600463867\n",
      "Epoch: 12, Samples: 2784/5760, Loss: 1.7481610774993896\n",
      "Epoch: 12, Samples: 2816/5760, Loss: 2.2392826080322266\n",
      "Epoch: 12, Samples: 2848/5760, Loss: 1.8853172063827515\n",
      "Epoch: 12, Samples: 2880/5760, Loss: 1.886757254600525\n",
      "Epoch: 12, Samples: 2912/5760, Loss: 1.6796576976776123\n",
      "Epoch: 12, Samples: 2944/5760, Loss: 2.1139607429504395\n",
      "Epoch: 12, Samples: 2976/5760, Loss: 1.8944644927978516\n",
      "Epoch: 12, Samples: 3008/5760, Loss: 2.0633697509765625\n",
      "Epoch: 12, Samples: 3040/5760, Loss: 1.9160126447677612\n",
      "Epoch: 12, Samples: 3072/5760, Loss: 1.6002777814865112\n",
      "Epoch: 12, Samples: 3104/5760, Loss: 2.5107171535491943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Samples: 3136/5760, Loss: 1.647549033164978\n",
      "Epoch: 12, Samples: 3168/5760, Loss: 1.8766212463378906\n",
      "Epoch: 12, Samples: 3200/5760, Loss: 1.6953147649765015\n",
      "Epoch: 12, Samples: 3232/5760, Loss: 1.6764812469482422\n",
      "Epoch: 12, Samples: 3264/5760, Loss: 2.3250889778137207\n",
      "Epoch: 12, Samples: 3296/5760, Loss: 2.0679831504821777\n",
      "Epoch: 12, Samples: 3328/5760, Loss: 2.0029141902923584\n",
      "Epoch: 12, Samples: 3360/5760, Loss: 1.886782169342041\n",
      "Epoch: 12, Samples: 3392/5760, Loss: 1.8455568552017212\n",
      "Epoch: 12, Samples: 3424/5760, Loss: 2.003485918045044\n",
      "Epoch: 12, Samples: 3456/5760, Loss: 1.9677449464797974\n",
      "Epoch: 12, Samples: 3488/5760, Loss: 1.7074592113494873\n",
      "Epoch: 12, Samples: 3520/5760, Loss: 2.069706678390503\n",
      "Epoch: 12, Samples: 3552/5760, Loss: 1.8565587997436523\n",
      "Epoch: 12, Samples: 3584/5760, Loss: 1.814156174659729\n",
      "Epoch: 12, Samples: 3616/5760, Loss: 2.3643763065338135\n",
      "Epoch: 12, Samples: 3648/5760, Loss: 2.2828993797302246\n",
      "Epoch: 12, Samples: 3680/5760, Loss: 2.0046446323394775\n",
      "Epoch: 12, Samples: 3712/5760, Loss: 1.8780531883239746\n",
      "Epoch: 12, Samples: 3744/5760, Loss: 2.029813528060913\n",
      "Epoch: 12, Samples: 3776/5760, Loss: 2.2036261558532715\n",
      "Epoch: 12, Samples: 3808/5760, Loss: 2.1835975646972656\n",
      "Epoch: 12, Samples: 3840/5760, Loss: 2.202977180480957\n",
      "Epoch: 12, Samples: 3872/5760, Loss: 1.7174551486968994\n",
      "Epoch: 12, Samples: 3904/5760, Loss: 1.9176597595214844\n",
      "Epoch: 12, Samples: 3936/5760, Loss: 2.132854700088501\n",
      "Epoch: 12, Samples: 3968/5760, Loss: 1.3887003660202026\n",
      "Epoch: 12, Samples: 4000/5760, Loss: 2.384202718734741\n",
      "Epoch: 12, Samples: 4032/5760, Loss: 1.6674808263778687\n",
      "Epoch: 12, Samples: 4064/5760, Loss: 1.5528045892715454\n",
      "Epoch: 12, Samples: 4096/5760, Loss: 1.632144570350647\n",
      "Epoch: 12, Samples: 4128/5760, Loss: 1.8290561437606812\n",
      "Epoch: 12, Samples: 4160/5760, Loss: 1.7819230556488037\n",
      "Epoch: 12, Samples: 4192/5760, Loss: 1.88326895236969\n",
      "Epoch: 12, Samples: 4224/5760, Loss: 2.1370227336883545\n",
      "Epoch: 12, Samples: 4256/5760, Loss: 2.575730323791504\n",
      "Epoch: 12, Samples: 4288/5760, Loss: 1.9879584312438965\n",
      "Epoch: 12, Samples: 4320/5760, Loss: 1.9629120826721191\n",
      "Epoch: 12, Samples: 4352/5760, Loss: 1.847434163093567\n",
      "Epoch: 12, Samples: 4384/5760, Loss: 2.555577278137207\n",
      "Epoch: 12, Samples: 4416/5760, Loss: 2.3407764434814453\n",
      "Epoch: 12, Samples: 4448/5760, Loss: 2.1569488048553467\n",
      "Epoch: 12, Samples: 4480/5760, Loss: 2.093045234680176\n",
      "Epoch: 12, Samples: 4512/5760, Loss: 2.381669044494629\n",
      "Epoch: 12, Samples: 4544/5760, Loss: 1.845934510231018\n",
      "Epoch: 12, Samples: 4576/5760, Loss: 1.6952729225158691\n",
      "Epoch: 12, Samples: 4608/5760, Loss: 1.6604641675949097\n",
      "Epoch: 12, Samples: 4640/5760, Loss: 2.2839672565460205\n",
      "Epoch: 12, Samples: 4672/5760, Loss: 1.9457597732543945\n",
      "Epoch: 12, Samples: 4704/5760, Loss: 2.3056678771972656\n",
      "Epoch: 12, Samples: 4736/5760, Loss: 2.0048649311065674\n",
      "Epoch: 12, Samples: 4768/5760, Loss: 2.258025646209717\n",
      "Epoch: 12, Samples: 4800/5760, Loss: 1.825804352760315\n",
      "Epoch: 12, Samples: 4832/5760, Loss: 2.160202741622925\n",
      "Epoch: 12, Samples: 4864/5760, Loss: 1.9997549057006836\n",
      "Epoch: 12, Samples: 4896/5760, Loss: 1.857619285583496\n",
      "Epoch: 12, Samples: 4928/5760, Loss: 2.2185301780700684\n",
      "Epoch: 12, Samples: 4960/5760, Loss: 2.0742974281311035\n",
      "Epoch: 12, Samples: 4992/5760, Loss: 2.114999294281006\n",
      "Epoch: 12, Samples: 5024/5760, Loss: 2.276644468307495\n",
      "Epoch: 12, Samples: 5056/5760, Loss: 2.267437219619751\n",
      "Epoch: 12, Samples: 5088/5760, Loss: 1.9556589126586914\n",
      "Epoch: 12, Samples: 5120/5760, Loss: 1.710646152496338\n",
      "Epoch: 12, Samples: 5152/5760, Loss: 1.9287241697311401\n",
      "Epoch: 12, Samples: 5184/5760, Loss: 1.8319785594940186\n",
      "Epoch: 12, Samples: 5216/5760, Loss: 1.9586338996887207\n",
      "Epoch: 12, Samples: 5248/5760, Loss: 2.342310905456543\n",
      "Epoch: 12, Samples: 5280/5760, Loss: 1.6426746845245361\n",
      "Epoch: 12, Samples: 5312/5760, Loss: 1.9986180067062378\n",
      "Epoch: 12, Samples: 5344/5760, Loss: 1.8611668348312378\n",
      "Epoch: 12, Samples: 5376/5760, Loss: 1.9250270128250122\n",
      "Epoch: 12, Samples: 5408/5760, Loss: 2.1796231269836426\n",
      "Epoch: 12, Samples: 5440/5760, Loss: 2.027400016784668\n",
      "Epoch: 12, Samples: 5472/5760, Loss: 1.6849946975708008\n",
      "Epoch: 12, Samples: 5504/5760, Loss: 1.8148128986358643\n",
      "Epoch: 12, Samples: 5536/5760, Loss: 2.24764084815979\n",
      "Epoch: 12, Samples: 5568/5760, Loss: 1.9503666162490845\n",
      "Epoch: 12, Samples: 5600/5760, Loss: 2.304743766784668\n",
      "Epoch: 12, Samples: 5632/5760, Loss: 1.9234944581985474\n",
      "Epoch: 12, Samples: 5664/5760, Loss: 2.1440367698669434\n",
      "Epoch: 12, Samples: 5696/5760, Loss: 2.0180320739746094\n",
      "Epoch: 12, Samples: 5728/5760, Loss: 2.6720376014709473\n",
      "\n",
      "Epoch: 12\n",
      "Training set: Average loss: 1.9822\n",
      "Validation set: Average loss: 2.1288, Accuracy: 365/818 (45%)\n",
      "Saving model (epoch 12) with lowest validation loss: 2.1287955137399526\n",
      "Epoch: 13, Samples: 0/5760, Loss: 1.6028425693511963\n",
      "Epoch: 13, Samples: 32/5760, Loss: 1.7931740283966064\n",
      "Epoch: 13, Samples: 64/5760, Loss: 2.119400978088379\n",
      "Epoch: 13, Samples: 96/5760, Loss: 2.1689653396606445\n",
      "Epoch: 13, Samples: 128/5760, Loss: 1.6765938997268677\n",
      "Epoch: 13, Samples: 160/5760, Loss: 2.2035224437713623\n",
      "Epoch: 13, Samples: 192/5760, Loss: 2.3028180599212646\n",
      "Epoch: 13, Samples: 224/5760, Loss: 2.2298166751861572\n",
      "Epoch: 13, Samples: 256/5760, Loss: 2.3558552265167236\n",
      "Epoch: 13, Samples: 288/5760, Loss: 2.1185457706451416\n",
      "Epoch: 13, Samples: 320/5760, Loss: 2.067213296890259\n",
      "Epoch: 13, Samples: 352/5760, Loss: 1.3903521299362183\n",
      "Epoch: 13, Samples: 384/5760, Loss: 1.6894700527191162\n",
      "Epoch: 13, Samples: 416/5760, Loss: 2.726047992706299\n",
      "Epoch: 13, Samples: 448/5760, Loss: 1.5689671039581299\n",
      "Epoch: 13, Samples: 480/5760, Loss: 1.881327509880066\n",
      "Epoch: 13, Samples: 512/5760, Loss: 2.1391685009002686\n",
      "Epoch: 13, Samples: 544/5760, Loss: 1.734947919845581\n",
      "Epoch: 13, Samples: 576/5760, Loss: 1.7473182678222656\n",
      "Epoch: 13, Samples: 608/5760, Loss: 2.0563740730285645\n",
      "Epoch: 13, Samples: 640/5760, Loss: 2.1743147373199463\n",
      "Epoch: 13, Samples: 672/5760, Loss: 1.8310480117797852\n",
      "Epoch: 13, Samples: 704/5760, Loss: 2.0186808109283447\n",
      "Epoch: 13, Samples: 736/5760, Loss: 2.3441576957702637\n",
      "Epoch: 13, Samples: 768/5760, Loss: 1.6313443183898926\n",
      "Epoch: 13, Samples: 800/5760, Loss: 1.719328761100769\n",
      "Epoch: 13, Samples: 832/5760, Loss: 1.818877935409546\n",
      "Epoch: 13, Samples: 864/5760, Loss: 1.659576177597046\n",
      "Epoch: 13, Samples: 896/5760, Loss: 2.2512948513031006\n",
      "Epoch: 13, Samples: 928/5760, Loss: 1.679378628730774\n",
      "Epoch: 13, Samples: 960/5760, Loss: 1.8341113328933716\n",
      "Epoch: 13, Samples: 992/5760, Loss: 1.9071276187896729\n",
      "Epoch: 13, Samples: 1024/5760, Loss: 1.8839396238327026\n",
      "Epoch: 13, Samples: 1056/5760, Loss: 1.7658146619796753\n",
      "Epoch: 13, Samples: 1088/5760, Loss: 2.0379555225372314\n",
      "Epoch: 13, Samples: 1120/5760, Loss: 1.9223718643188477\n",
      "Epoch: 13, Samples: 1152/5760, Loss: 2.8890202045440674\n",
      "Epoch: 13, Samples: 1184/5760, Loss: 1.5593405961990356\n",
      "Epoch: 13, Samples: 1216/5760, Loss: 2.0158097743988037\n",
      "Epoch: 13, Samples: 1248/5760, Loss: 2.0573854446411133\n",
      "Epoch: 13, Samples: 1280/5760, Loss: 1.8455350399017334\n",
      "Epoch: 13, Samples: 1312/5760, Loss: 1.9885305166244507\n",
      "Epoch: 13, Samples: 1344/5760, Loss: 1.7464427947998047\n",
      "Epoch: 13, Samples: 1376/5760, Loss: 1.9333425760269165\n",
      "Epoch: 13, Samples: 1408/5760, Loss: 1.9436674118041992\n",
      "Epoch: 13, Samples: 1440/5760, Loss: 2.341078996658325\n",
      "Epoch: 13, Samples: 1472/5760, Loss: 1.552001953125\n",
      "Epoch: 13, Samples: 1504/5760, Loss: 1.7277584075927734\n",
      "Epoch: 13, Samples: 1536/5760, Loss: 1.8051468133926392\n",
      "Epoch: 13, Samples: 1568/5760, Loss: 2.0266966819763184\n",
      "Epoch: 13, Samples: 1600/5760, Loss: 1.7020798921585083\n",
      "Epoch: 13, Samples: 1632/5760, Loss: 1.9533663988113403\n",
      "Epoch: 13, Samples: 1664/5760, Loss: 2.312258243560791\n",
      "Epoch: 13, Samples: 1696/5760, Loss: 2.349282741546631\n",
      "Epoch: 13, Samples: 1728/5760, Loss: 1.5305166244506836\n",
      "Epoch: 13, Samples: 1760/5760, Loss: 1.7658857107162476\n",
      "Epoch: 13, Samples: 1792/5760, Loss: 2.168221950531006\n",
      "Epoch: 13, Samples: 1824/5760, Loss: 1.6689544916152954\n",
      "Epoch: 13, Samples: 1856/5760, Loss: 2.4039597511291504\n",
      "Epoch: 13, Samples: 1888/5760, Loss: 1.8019193410873413\n",
      "Epoch: 13, Samples: 1920/5760, Loss: 2.2140400409698486\n",
      "Epoch: 13, Samples: 1952/5760, Loss: 2.0848398208618164\n",
      "Epoch: 13, Samples: 1984/5760, Loss: 1.6706749200820923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Samples: 2016/5760, Loss: 1.534727692604065\n",
      "Epoch: 13, Samples: 2048/5760, Loss: 2.197338104248047\n",
      "Epoch: 13, Samples: 2080/5760, Loss: 1.5042582750320435\n",
      "Epoch: 13, Samples: 2112/5760, Loss: 1.8365765810012817\n",
      "Epoch: 13, Samples: 2144/5760, Loss: 1.8963079452514648\n",
      "Epoch: 13, Samples: 2176/5760, Loss: 1.4729136228561401\n",
      "Epoch: 13, Samples: 2208/5760, Loss: 1.836929202079773\n",
      "Epoch: 13, Samples: 2240/5760, Loss: 2.1586148738861084\n",
      "Epoch: 13, Samples: 2272/5760, Loss: 1.5941252708435059\n",
      "Epoch: 13, Samples: 2304/5760, Loss: 1.8926215171813965\n",
      "Epoch: 13, Samples: 2336/5760, Loss: 1.816311240196228\n",
      "Epoch: 13, Samples: 2368/5760, Loss: 2.1074206829071045\n",
      "Epoch: 13, Samples: 2400/5760, Loss: 1.4845540523529053\n",
      "Epoch: 13, Samples: 2432/5760, Loss: 1.9379181861877441\n",
      "Epoch: 13, Samples: 2464/5760, Loss: 1.530747890472412\n",
      "Epoch: 13, Samples: 2496/5760, Loss: 1.8063037395477295\n",
      "Epoch: 13, Samples: 2528/5760, Loss: 1.5065486431121826\n",
      "Epoch: 13, Samples: 2560/5760, Loss: 1.8767467737197876\n",
      "Epoch: 13, Samples: 2592/5760, Loss: 2.095003128051758\n",
      "Epoch: 13, Samples: 2624/5760, Loss: 2.3309428691864014\n",
      "Epoch: 13, Samples: 2656/5760, Loss: 2.06941819190979\n",
      "Epoch: 13, Samples: 2688/5760, Loss: 2.217833995819092\n",
      "Epoch: 13, Samples: 2720/5760, Loss: 1.7328264713287354\n",
      "Epoch: 13, Samples: 2752/5760, Loss: 1.5435388088226318\n",
      "Epoch: 13, Samples: 2784/5760, Loss: 1.9345886707305908\n",
      "Epoch: 13, Samples: 2816/5760, Loss: 1.8468092679977417\n",
      "Epoch: 13, Samples: 2848/5760, Loss: 1.8554630279541016\n",
      "Epoch: 13, Samples: 2880/5760, Loss: 2.3844854831695557\n",
      "Epoch: 13, Samples: 2912/5760, Loss: 1.2398335933685303\n",
      "Epoch: 13, Samples: 2944/5760, Loss: 2.4524779319763184\n",
      "Epoch: 13, Samples: 2976/5760, Loss: 2.1435673236846924\n",
      "Epoch: 13, Samples: 3008/5760, Loss: 2.087028741836548\n",
      "Epoch: 13, Samples: 3040/5760, Loss: 2.190039873123169\n",
      "Epoch: 13, Samples: 3072/5760, Loss: 2.0671141147613525\n",
      "Epoch: 13, Samples: 3104/5760, Loss: 2.035372257232666\n",
      "Epoch: 13, Samples: 3136/5760, Loss: 2.1034817695617676\n",
      "Epoch: 13, Samples: 3168/5760, Loss: 2.0464115142822266\n",
      "Epoch: 13, Samples: 3200/5760, Loss: 1.8417985439300537\n",
      "Epoch: 13, Samples: 3232/5760, Loss: 2.3939244747161865\n",
      "Epoch: 13, Samples: 3264/5760, Loss: 1.9445388317108154\n",
      "Epoch: 13, Samples: 3296/5760, Loss: 1.7207006216049194\n",
      "Epoch: 13, Samples: 3328/5760, Loss: 1.598663330078125\n",
      "Epoch: 13, Samples: 3360/5760, Loss: 1.7544903755187988\n",
      "Epoch: 13, Samples: 3392/5760, Loss: 1.8977174758911133\n",
      "Epoch: 13, Samples: 3424/5760, Loss: 1.9565341472625732\n",
      "Epoch: 13, Samples: 3456/5760, Loss: 1.8367406129837036\n",
      "Epoch: 13, Samples: 3488/5760, Loss: 1.7803740501403809\n",
      "Epoch: 13, Samples: 3520/5760, Loss: 2.1110434532165527\n",
      "Epoch: 13, Samples: 3552/5760, Loss: 2.0652449131011963\n",
      "Epoch: 13, Samples: 3584/5760, Loss: 1.9505678415298462\n",
      "Epoch: 13, Samples: 3616/5760, Loss: 1.8704593181610107\n",
      "Epoch: 13, Samples: 3648/5760, Loss: 1.678159475326538\n",
      "Epoch: 13, Samples: 3680/5760, Loss: 2.041957378387451\n",
      "Epoch: 13, Samples: 3712/5760, Loss: 1.9969652891159058\n",
      "Epoch: 13, Samples: 3744/5760, Loss: 1.6933046579360962\n",
      "Epoch: 13, Samples: 3776/5760, Loss: 2.294057607650757\n",
      "Epoch: 13, Samples: 3808/5760, Loss: 1.72041654586792\n",
      "Epoch: 13, Samples: 3840/5760, Loss: 1.919315218925476\n",
      "Epoch: 13, Samples: 3872/5760, Loss: 2.1716527938842773\n",
      "Epoch: 13, Samples: 3904/5760, Loss: 2.1109514236450195\n",
      "Epoch: 13, Samples: 3936/5760, Loss: 1.5046483278274536\n",
      "Epoch: 13, Samples: 3968/5760, Loss: 2.1216206550598145\n",
      "Epoch: 13, Samples: 4000/5760, Loss: 2.552793025970459\n",
      "Epoch: 13, Samples: 4032/5760, Loss: 1.7354456186294556\n",
      "Epoch: 13, Samples: 4064/5760, Loss: 2.3685567378997803\n",
      "Epoch: 13, Samples: 4096/5760, Loss: 2.314340114593506\n",
      "Epoch: 13, Samples: 4128/5760, Loss: 2.057514190673828\n",
      "Epoch: 13, Samples: 4160/5760, Loss: 1.9978739023208618\n",
      "Epoch: 13, Samples: 4192/5760, Loss: 1.9623680114746094\n",
      "Epoch: 13, Samples: 4224/5760, Loss: 1.8374019861221313\n",
      "Epoch: 13, Samples: 4256/5760, Loss: 2.1315817832946777\n",
      "Epoch: 13, Samples: 4288/5760, Loss: 1.692348599433899\n",
      "Epoch: 13, Samples: 4320/5760, Loss: 2.0469162464141846\n",
      "Epoch: 13, Samples: 4352/5760, Loss: 1.627406120300293\n",
      "Epoch: 13, Samples: 4384/5760, Loss: 1.7146507501602173\n",
      "Epoch: 13, Samples: 4416/5760, Loss: 2.162973165512085\n",
      "Epoch: 13, Samples: 4448/5760, Loss: 2.2581167221069336\n",
      "Epoch: 13, Samples: 4480/5760, Loss: 2.0217151641845703\n",
      "Epoch: 13, Samples: 4512/5760, Loss: 1.5645725727081299\n",
      "Epoch: 13, Samples: 4544/5760, Loss: 1.8274459838867188\n",
      "Epoch: 13, Samples: 4576/5760, Loss: 1.8160227537155151\n",
      "Epoch: 13, Samples: 4608/5760, Loss: 1.487917423248291\n",
      "Epoch: 13, Samples: 4640/5760, Loss: 2.187983512878418\n",
      "Epoch: 13, Samples: 4672/5760, Loss: 1.7173594236373901\n",
      "Epoch: 13, Samples: 4704/5760, Loss: 2.2713418006896973\n",
      "Epoch: 13, Samples: 4736/5760, Loss: 1.9373385906219482\n",
      "Epoch: 13, Samples: 4768/5760, Loss: 2.0549542903900146\n",
      "Epoch: 13, Samples: 4800/5760, Loss: 2.149662494659424\n",
      "Epoch: 13, Samples: 4832/5760, Loss: 1.937623381614685\n",
      "Epoch: 13, Samples: 4864/5760, Loss: 1.8616129159927368\n",
      "Epoch: 13, Samples: 4896/5760, Loss: 1.7257232666015625\n",
      "Epoch: 13, Samples: 4928/5760, Loss: 1.897641658782959\n",
      "Epoch: 13, Samples: 4960/5760, Loss: 1.8551617860794067\n",
      "Epoch: 13, Samples: 4992/5760, Loss: 2.952249526977539\n",
      "Epoch: 13, Samples: 5024/5760, Loss: 2.176445245742798\n",
      "Epoch: 13, Samples: 5056/5760, Loss: 1.5588841438293457\n",
      "Epoch: 13, Samples: 5088/5760, Loss: 1.88931143283844\n",
      "Epoch: 13, Samples: 5120/5760, Loss: 1.930875301361084\n",
      "Epoch: 13, Samples: 5152/5760, Loss: 1.842921257019043\n",
      "Epoch: 13, Samples: 5184/5760, Loss: 1.838742971420288\n",
      "Epoch: 13, Samples: 5216/5760, Loss: 1.7822247743606567\n",
      "Epoch: 13, Samples: 5248/5760, Loss: 1.9111796617507935\n",
      "Epoch: 13, Samples: 5280/5760, Loss: 1.714394450187683\n",
      "Epoch: 13, Samples: 5312/5760, Loss: 1.85146963596344\n",
      "Epoch: 13, Samples: 5344/5760, Loss: 1.6743415594100952\n",
      "Epoch: 13, Samples: 5376/5760, Loss: 1.9500471353530884\n",
      "Epoch: 13, Samples: 5408/5760, Loss: 2.180739641189575\n",
      "Epoch: 13, Samples: 5440/5760, Loss: 2.1538939476013184\n",
      "Epoch: 13, Samples: 5472/5760, Loss: 1.9355148077011108\n",
      "Epoch: 13, Samples: 5504/5760, Loss: 1.3627333641052246\n",
      "Epoch: 13, Samples: 5536/5760, Loss: 2.286785125732422\n",
      "Epoch: 13, Samples: 5568/5760, Loss: 2.2918269634246826\n",
      "Epoch: 13, Samples: 5600/5760, Loss: 1.4240443706512451\n",
      "Epoch: 13, Samples: 5632/5760, Loss: 1.804173469543457\n",
      "Epoch: 13, Samples: 5664/5760, Loss: 1.779056429862976\n",
      "Epoch: 13, Samples: 5696/5760, Loss: 1.9703494310379028\n",
      "Epoch: 13, Samples: 5728/5760, Loss: 2.87927508354187\n",
      "\n",
      "Epoch: 13\n",
      "Training set: Average loss: 1.9409\n",
      "Validation set: Average loss: 2.3099, Accuracy: 363/818 (44%)\n",
      "Epoch: 14, Samples: 0/5760, Loss: 1.8260557651519775\n",
      "Epoch: 14, Samples: 32/5760, Loss: 1.213670253753662\n",
      "Epoch: 14, Samples: 64/5760, Loss: 1.6384276151657104\n",
      "Epoch: 14, Samples: 96/5760, Loss: 1.9934293031692505\n",
      "Epoch: 14, Samples: 128/5760, Loss: 1.388475775718689\n",
      "Epoch: 14, Samples: 160/5760, Loss: 1.6184906959533691\n",
      "Epoch: 14, Samples: 192/5760, Loss: 1.7072895765304565\n",
      "Epoch: 14, Samples: 224/5760, Loss: 1.8949437141418457\n",
      "Epoch: 14, Samples: 256/5760, Loss: 2.126131296157837\n",
      "Epoch: 14, Samples: 288/5760, Loss: 1.8868298530578613\n",
      "Epoch: 14, Samples: 320/5760, Loss: 1.991936445236206\n",
      "Epoch: 14, Samples: 352/5760, Loss: 2.1226093769073486\n",
      "Epoch: 14, Samples: 384/5760, Loss: 1.7555913925170898\n",
      "Epoch: 14, Samples: 416/5760, Loss: 1.732513427734375\n",
      "Epoch: 14, Samples: 448/5760, Loss: 2.181332588195801\n",
      "Epoch: 14, Samples: 480/5760, Loss: 2.1629583835601807\n",
      "Epoch: 14, Samples: 512/5760, Loss: 1.2110117673873901\n",
      "Epoch: 14, Samples: 544/5760, Loss: 1.6528047323226929\n",
      "Epoch: 14, Samples: 576/5760, Loss: 1.946549654006958\n",
      "Epoch: 14, Samples: 608/5760, Loss: 1.978564739227295\n",
      "Epoch: 14, Samples: 640/5760, Loss: 1.913923978805542\n",
      "Epoch: 14, Samples: 672/5760, Loss: 1.861667275428772\n",
      "Epoch: 14, Samples: 704/5760, Loss: 2.075798273086548\n",
      "Epoch: 14, Samples: 736/5760, Loss: 1.6737427711486816\n",
      "Epoch: 14, Samples: 768/5760, Loss: 1.7257531881332397\n",
      "Epoch: 14, Samples: 800/5760, Loss: 1.645370364189148\n",
      "Epoch: 14, Samples: 832/5760, Loss: 2.0647106170654297\n",
      "Epoch: 14, Samples: 864/5760, Loss: 2.010784149169922\n",
      "Epoch: 14, Samples: 896/5760, Loss: 1.4076467752456665\n",
      "Epoch: 14, Samples: 928/5760, Loss: 1.810459852218628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Samples: 960/5760, Loss: 2.121288299560547\n",
      "Epoch: 14, Samples: 992/5760, Loss: 2.400543689727783\n",
      "Epoch: 14, Samples: 1024/5760, Loss: 1.7144964933395386\n",
      "Epoch: 14, Samples: 1056/5760, Loss: 1.9817688465118408\n",
      "Epoch: 14, Samples: 1088/5760, Loss: 2.0600168704986572\n",
      "Epoch: 14, Samples: 1120/5760, Loss: 2.5658187866210938\n",
      "Epoch: 14, Samples: 1152/5760, Loss: 1.619140625\n",
      "Epoch: 14, Samples: 1184/5760, Loss: 1.602161169052124\n",
      "Epoch: 14, Samples: 1216/5760, Loss: 1.9704395532608032\n",
      "Epoch: 14, Samples: 1248/5760, Loss: 1.8704993724822998\n",
      "Epoch: 14, Samples: 1280/5760, Loss: 1.811090111732483\n",
      "Epoch: 14, Samples: 1312/5760, Loss: 1.8436384201049805\n",
      "Epoch: 14, Samples: 1344/5760, Loss: 2.115649938583374\n",
      "Epoch: 14, Samples: 1376/5760, Loss: 2.0953595638275146\n",
      "Epoch: 14, Samples: 1408/5760, Loss: 1.6249592304229736\n",
      "Epoch: 14, Samples: 1440/5760, Loss: 1.8604321479797363\n",
      "Epoch: 14, Samples: 1472/5760, Loss: 1.8794441223144531\n",
      "Epoch: 14, Samples: 1504/5760, Loss: 1.8537302017211914\n",
      "Epoch: 14, Samples: 1536/5760, Loss: 1.452617883682251\n",
      "Epoch: 14, Samples: 1568/5760, Loss: 1.9390426874160767\n",
      "Epoch: 14, Samples: 1600/5760, Loss: 1.995344877243042\n",
      "Epoch: 14, Samples: 1632/5760, Loss: 1.9151763916015625\n",
      "Epoch: 14, Samples: 1664/5760, Loss: 1.8093323707580566\n",
      "Epoch: 14, Samples: 1696/5760, Loss: 1.6665334701538086\n",
      "Epoch: 14, Samples: 1728/5760, Loss: 1.700390338897705\n",
      "Epoch: 14, Samples: 1760/5760, Loss: 1.2568352222442627\n",
      "Epoch: 14, Samples: 1792/5760, Loss: 2.0319361686706543\n",
      "Epoch: 14, Samples: 1824/5760, Loss: 1.8840378522872925\n",
      "Epoch: 14, Samples: 1856/5760, Loss: 2.2062957286834717\n",
      "Epoch: 14, Samples: 1888/5760, Loss: 1.9231094121932983\n",
      "Epoch: 14, Samples: 1920/5760, Loss: 2.00836181640625\n",
      "Epoch: 14, Samples: 1952/5760, Loss: 1.4791319370269775\n",
      "Epoch: 14, Samples: 1984/5760, Loss: 2.161663055419922\n",
      "Epoch: 14, Samples: 2016/5760, Loss: 1.6558513641357422\n",
      "Epoch: 14, Samples: 2048/5760, Loss: 2.156287908554077\n",
      "Epoch: 14, Samples: 2080/5760, Loss: 2.2504935264587402\n",
      "Epoch: 14, Samples: 2112/5760, Loss: 1.6697430610656738\n",
      "Epoch: 14, Samples: 2144/5760, Loss: 2.020308494567871\n",
      "Epoch: 14, Samples: 2176/5760, Loss: 1.6242096424102783\n",
      "Epoch: 14, Samples: 2208/5760, Loss: 1.7785751819610596\n",
      "Epoch: 14, Samples: 2240/5760, Loss: 1.8314447402954102\n",
      "Epoch: 14, Samples: 2272/5760, Loss: 2.2510039806365967\n",
      "Epoch: 14, Samples: 2304/5760, Loss: 1.484071135520935\n",
      "Epoch: 14, Samples: 2336/5760, Loss: 1.655753254890442\n",
      "Epoch: 14, Samples: 2368/5760, Loss: 2.289433479309082\n",
      "Epoch: 14, Samples: 2400/5760, Loss: 1.4526240825653076\n",
      "Epoch: 14, Samples: 2432/5760, Loss: 2.1815476417541504\n",
      "Epoch: 14, Samples: 2464/5760, Loss: 1.6455172300338745\n",
      "Epoch: 14, Samples: 2496/5760, Loss: 1.8286373615264893\n",
      "Epoch: 14, Samples: 2528/5760, Loss: 1.483901858329773\n",
      "Epoch: 14, Samples: 2560/5760, Loss: 1.577164649963379\n",
      "Epoch: 14, Samples: 2592/5760, Loss: 1.9785925149917603\n",
      "Epoch: 14, Samples: 2624/5760, Loss: 2.3692097663879395\n",
      "Epoch: 14, Samples: 2656/5760, Loss: 1.632743239402771\n",
      "Epoch: 14, Samples: 2688/5760, Loss: 1.5255742073059082\n",
      "Epoch: 14, Samples: 2720/5760, Loss: 2.343289375305176\n",
      "Epoch: 14, Samples: 2752/5760, Loss: 1.8098701238632202\n",
      "Epoch: 14, Samples: 2784/5760, Loss: 2.2797462940216064\n",
      "Epoch: 14, Samples: 2816/5760, Loss: 1.870709776878357\n",
      "Epoch: 14, Samples: 2848/5760, Loss: 1.655849575996399\n",
      "Epoch: 14, Samples: 2880/5760, Loss: 1.388220191001892\n",
      "Epoch: 14, Samples: 2912/5760, Loss: 1.5208873748779297\n",
      "Epoch: 14, Samples: 2944/5760, Loss: 1.4884518384933472\n",
      "Epoch: 14, Samples: 2976/5760, Loss: 1.8636332750320435\n",
      "Epoch: 14, Samples: 3008/5760, Loss: 2.2626850605010986\n",
      "Epoch: 14, Samples: 3040/5760, Loss: 1.7863266468048096\n",
      "Epoch: 14, Samples: 3072/5760, Loss: 1.9589163064956665\n",
      "Epoch: 14, Samples: 3104/5760, Loss: 1.8994358777999878\n",
      "Epoch: 14, Samples: 3136/5760, Loss: 2.44915509223938\n",
      "Epoch: 14, Samples: 3168/5760, Loss: 1.650044322013855\n",
      "Epoch: 14, Samples: 3200/5760, Loss: 1.5896018743515015\n",
      "Epoch: 14, Samples: 3232/5760, Loss: 1.4272658824920654\n",
      "Epoch: 14, Samples: 3264/5760, Loss: 2.1518328189849854\n",
      "Epoch: 14, Samples: 3296/5760, Loss: 1.5022692680358887\n",
      "Epoch: 14, Samples: 3328/5760, Loss: 1.5888360738754272\n",
      "Epoch: 14, Samples: 3360/5760, Loss: 1.429874300956726\n",
      "Epoch: 14, Samples: 3392/5760, Loss: 1.7052979469299316\n",
      "Epoch: 14, Samples: 3424/5760, Loss: 1.9463274478912354\n",
      "Epoch: 14, Samples: 3456/5760, Loss: 2.431382417678833\n",
      "Epoch: 14, Samples: 3488/5760, Loss: 1.8473002910614014\n",
      "Epoch: 14, Samples: 3520/5760, Loss: 2.3588039875030518\n",
      "Epoch: 14, Samples: 3552/5760, Loss: 1.4930340051651\n",
      "Epoch: 14, Samples: 3584/5760, Loss: 1.6296741962432861\n",
      "Epoch: 14, Samples: 3616/5760, Loss: 1.869993805885315\n",
      "Epoch: 14, Samples: 3648/5760, Loss: 1.7931249141693115\n",
      "Epoch: 14, Samples: 3680/5760, Loss: 1.4223730564117432\n",
      "Epoch: 14, Samples: 3712/5760, Loss: 1.7635749578475952\n",
      "Epoch: 14, Samples: 3744/5760, Loss: 1.819675326347351\n",
      "Epoch: 14, Samples: 3776/5760, Loss: 1.5736840963363647\n",
      "Epoch: 14, Samples: 3808/5760, Loss: 1.6093047857284546\n",
      "Epoch: 14, Samples: 3840/5760, Loss: 1.9077320098876953\n",
      "Epoch: 14, Samples: 3872/5760, Loss: 1.5469154119491577\n",
      "Epoch: 14, Samples: 3904/5760, Loss: 1.8985249996185303\n",
      "Epoch: 14, Samples: 3936/5760, Loss: 1.6775352954864502\n",
      "Epoch: 14, Samples: 3968/5760, Loss: 2.099426746368408\n",
      "Epoch: 14, Samples: 4000/5760, Loss: 1.7582464218139648\n",
      "Epoch: 14, Samples: 4032/5760, Loss: 1.8022829294204712\n",
      "Epoch: 14, Samples: 4064/5760, Loss: 1.6706321239471436\n",
      "Epoch: 14, Samples: 4096/5760, Loss: 1.564496636390686\n",
      "Epoch: 14, Samples: 4128/5760, Loss: 1.6695407629013062\n",
      "Epoch: 14, Samples: 4160/5760, Loss: 1.8178960084915161\n",
      "Epoch: 14, Samples: 4192/5760, Loss: 1.777703046798706\n",
      "Epoch: 14, Samples: 4224/5760, Loss: 1.687631368637085\n",
      "Epoch: 14, Samples: 4256/5760, Loss: 1.9221135377883911\n",
      "Epoch: 14, Samples: 4288/5760, Loss: 1.661180019378662\n",
      "Epoch: 14, Samples: 4320/5760, Loss: 1.2928946018218994\n",
      "Epoch: 14, Samples: 4352/5760, Loss: 1.7773544788360596\n",
      "Epoch: 14, Samples: 4384/5760, Loss: 1.8187052011489868\n",
      "Epoch: 14, Samples: 4416/5760, Loss: 1.7209594249725342\n",
      "Epoch: 14, Samples: 4448/5760, Loss: 1.3713221549987793\n",
      "Epoch: 14, Samples: 4480/5760, Loss: 1.5828189849853516\n",
      "Epoch: 14, Samples: 4512/5760, Loss: 1.8983339071273804\n",
      "Epoch: 14, Samples: 4544/5760, Loss: 1.6825834512710571\n",
      "Epoch: 14, Samples: 4576/5760, Loss: 2.341242551803589\n",
      "Epoch: 14, Samples: 4608/5760, Loss: 1.805794358253479\n",
      "Epoch: 14, Samples: 4640/5760, Loss: 1.6504881381988525\n",
      "Epoch: 14, Samples: 4672/5760, Loss: 1.8665074110031128\n",
      "Epoch: 14, Samples: 4704/5760, Loss: 1.9830347299575806\n",
      "Epoch: 14, Samples: 4736/5760, Loss: 2.125335693359375\n",
      "Epoch: 14, Samples: 4768/5760, Loss: 1.4416913986206055\n",
      "Epoch: 14, Samples: 4800/5760, Loss: 1.3352187871932983\n",
      "Epoch: 14, Samples: 4832/5760, Loss: 1.6314973831176758\n",
      "Epoch: 14, Samples: 4864/5760, Loss: 1.9920997619628906\n",
      "Epoch: 14, Samples: 4896/5760, Loss: 2.1107594966888428\n",
      "Epoch: 14, Samples: 4928/5760, Loss: 2.1593565940856934\n",
      "Epoch: 14, Samples: 4960/5760, Loss: 1.8731489181518555\n",
      "Epoch: 14, Samples: 4992/5760, Loss: 1.908140778541565\n",
      "Epoch: 14, Samples: 5024/5760, Loss: 1.8498672246932983\n",
      "Epoch: 14, Samples: 5056/5760, Loss: 1.5989906787872314\n",
      "Epoch: 14, Samples: 5088/5760, Loss: 2.262848138809204\n",
      "Epoch: 14, Samples: 5120/5760, Loss: 1.9770334959030151\n",
      "Epoch: 14, Samples: 5152/5760, Loss: 1.8514512777328491\n",
      "Epoch: 14, Samples: 5184/5760, Loss: 2.1249825954437256\n",
      "Epoch: 14, Samples: 5216/5760, Loss: 1.628991961479187\n",
      "Epoch: 14, Samples: 5248/5760, Loss: 2.0243284702301025\n",
      "Epoch: 14, Samples: 5280/5760, Loss: 1.7222894430160522\n",
      "Epoch: 14, Samples: 5312/5760, Loss: 1.9249851703643799\n",
      "Epoch: 14, Samples: 5344/5760, Loss: 1.9108023643493652\n",
      "Epoch: 14, Samples: 5376/5760, Loss: 1.7583227157592773\n",
      "Epoch: 14, Samples: 5408/5760, Loss: 1.459531545639038\n",
      "Epoch: 14, Samples: 5440/5760, Loss: 2.3032217025756836\n",
      "Epoch: 14, Samples: 5472/5760, Loss: 1.775306224822998\n",
      "Epoch: 14, Samples: 5504/5760, Loss: 1.7826107740402222\n",
      "Epoch: 14, Samples: 5536/5760, Loss: 1.4401261806488037\n",
      "Epoch: 14, Samples: 5568/5760, Loss: 1.7182021141052246\n",
      "Epoch: 14, Samples: 5600/5760, Loss: 2.0088179111480713\n",
      "Epoch: 14, Samples: 5632/5760, Loss: 1.663866400718689\n",
      "Epoch: 14, Samples: 5664/5760, Loss: 1.4445383548736572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Samples: 5696/5760, Loss: 2.1109061241149902\n",
      "Epoch: 14, Samples: 5728/5760, Loss: 2.9917659759521484\n",
      "\n",
      "Epoch: 14\n",
      "Training set: Average loss: 1.8296\n",
      "Validation set: Average loss: 2.3706, Accuracy: 330/818 (40%)\n",
      "Epoch: 15, Samples: 0/5760, Loss: 1.732187271118164\n",
      "Epoch: 15, Samples: 32/5760, Loss: 1.5324273109436035\n",
      "Epoch: 15, Samples: 64/5760, Loss: 2.251075029373169\n",
      "Epoch: 15, Samples: 96/5760, Loss: 1.897691011428833\n",
      "Epoch: 15, Samples: 128/5760, Loss: 1.6945065259933472\n",
      "Epoch: 15, Samples: 160/5760, Loss: 1.501399040222168\n",
      "Epoch: 15, Samples: 192/5760, Loss: 1.7338589429855347\n",
      "Epoch: 15, Samples: 224/5760, Loss: 2.262721538543701\n",
      "Epoch: 15, Samples: 256/5760, Loss: 1.6953985691070557\n",
      "Epoch: 15, Samples: 288/5760, Loss: 1.6809202432632446\n",
      "Epoch: 15, Samples: 320/5760, Loss: 1.5691817998886108\n",
      "Epoch: 15, Samples: 352/5760, Loss: 1.6081212759017944\n",
      "Epoch: 15, Samples: 384/5760, Loss: 1.509387731552124\n",
      "Epoch: 15, Samples: 416/5760, Loss: 2.1270716190338135\n",
      "Epoch: 15, Samples: 448/5760, Loss: 1.8553546667099\n",
      "Epoch: 15, Samples: 480/5760, Loss: 1.579385757446289\n",
      "Epoch: 15, Samples: 512/5760, Loss: 1.7828700542449951\n",
      "Epoch: 15, Samples: 544/5760, Loss: 1.485504150390625\n",
      "Epoch: 15, Samples: 576/5760, Loss: 1.4891653060913086\n",
      "Epoch: 15, Samples: 608/5760, Loss: 2.2192111015319824\n",
      "Epoch: 15, Samples: 640/5760, Loss: 1.2334954738616943\n",
      "Epoch: 15, Samples: 672/5760, Loss: 1.6943085193634033\n",
      "Epoch: 15, Samples: 704/5760, Loss: 1.664769172668457\n",
      "Epoch: 15, Samples: 736/5760, Loss: 1.6955552101135254\n",
      "Epoch: 15, Samples: 768/5760, Loss: 2.083995819091797\n",
      "Epoch: 15, Samples: 800/5760, Loss: 1.4908982515335083\n",
      "Epoch: 15, Samples: 832/5760, Loss: 1.7077134847640991\n",
      "Epoch: 15, Samples: 864/5760, Loss: 1.8735543489456177\n",
      "Epoch: 15, Samples: 896/5760, Loss: 1.3900647163391113\n",
      "Epoch: 15, Samples: 928/5760, Loss: 1.6575157642364502\n",
      "Epoch: 15, Samples: 960/5760, Loss: 2.1349377632141113\n",
      "Epoch: 15, Samples: 992/5760, Loss: 1.7589175701141357\n",
      "Epoch: 15, Samples: 1024/5760, Loss: 1.5378730297088623\n",
      "Epoch: 15, Samples: 1056/5760, Loss: 1.639950156211853\n",
      "Epoch: 15, Samples: 1088/5760, Loss: 1.6028099060058594\n",
      "Epoch: 15, Samples: 1120/5760, Loss: 1.7001926898956299\n",
      "Epoch: 15, Samples: 1152/5760, Loss: 1.3995143175125122\n",
      "Epoch: 15, Samples: 1184/5760, Loss: 1.729353904724121\n",
      "Epoch: 15, Samples: 1216/5760, Loss: 1.862997055053711\n",
      "Epoch: 15, Samples: 1248/5760, Loss: 2.1550443172454834\n",
      "Epoch: 15, Samples: 1280/5760, Loss: 1.8912107944488525\n",
      "Epoch: 15, Samples: 1312/5760, Loss: 1.90109121799469\n",
      "Epoch: 15, Samples: 1344/5760, Loss: 2.1150240898132324\n",
      "Epoch: 15, Samples: 1376/5760, Loss: 2.071521282196045\n",
      "Epoch: 15, Samples: 1408/5760, Loss: 1.8323460817337036\n",
      "Epoch: 15, Samples: 1440/5760, Loss: 2.115663528442383\n",
      "Epoch: 15, Samples: 1472/5760, Loss: 1.468196153640747\n",
      "Epoch: 15, Samples: 1504/5760, Loss: 1.548805832862854\n",
      "Epoch: 15, Samples: 1536/5760, Loss: 2.2946624755859375\n",
      "Epoch: 15, Samples: 1568/5760, Loss: 1.9014085531234741\n",
      "Epoch: 15, Samples: 1600/5760, Loss: 1.7182366847991943\n",
      "Epoch: 15, Samples: 1632/5760, Loss: 1.925537347793579\n",
      "Epoch: 15, Samples: 1664/5760, Loss: 2.058539867401123\n",
      "Epoch: 15, Samples: 1696/5760, Loss: 2.5239126682281494\n",
      "Epoch: 15, Samples: 1728/5760, Loss: 1.8707693815231323\n",
      "Epoch: 15, Samples: 1760/5760, Loss: 1.890081763267517\n",
      "Epoch: 15, Samples: 1792/5760, Loss: 1.8217170238494873\n",
      "Epoch: 15, Samples: 1824/5760, Loss: 1.3905270099639893\n",
      "Epoch: 15, Samples: 1856/5760, Loss: 1.3896026611328125\n",
      "Epoch: 15, Samples: 1888/5760, Loss: 1.7142783403396606\n",
      "Epoch: 15, Samples: 1920/5760, Loss: 1.591626524925232\n",
      "Epoch: 15, Samples: 1952/5760, Loss: 1.9539541006088257\n",
      "Epoch: 15, Samples: 1984/5760, Loss: 1.485978364944458\n",
      "Epoch: 15, Samples: 2016/5760, Loss: 1.6250760555267334\n",
      "Epoch: 15, Samples: 2048/5760, Loss: 1.6196529865264893\n",
      "Epoch: 15, Samples: 2080/5760, Loss: 1.903417706489563\n",
      "Epoch: 15, Samples: 2112/5760, Loss: 1.5034631490707397\n",
      "Epoch: 15, Samples: 2144/5760, Loss: 1.9905221462249756\n",
      "Epoch: 15, Samples: 2176/5760, Loss: 1.6091915369033813\n",
      "Epoch: 15, Samples: 2208/5760, Loss: 1.5846543312072754\n",
      "Epoch: 15, Samples: 2240/5760, Loss: 1.848029375076294\n",
      "Epoch: 15, Samples: 2272/5760, Loss: 1.9936023950576782\n",
      "Epoch: 15, Samples: 2304/5760, Loss: 1.4255450963974\n",
      "Epoch: 15, Samples: 2336/5760, Loss: 1.9059834480285645\n",
      "Epoch: 15, Samples: 2368/5760, Loss: 1.9939132928848267\n",
      "Epoch: 15, Samples: 2400/5760, Loss: 1.5336787700653076\n",
      "Epoch: 15, Samples: 2432/5760, Loss: 1.1419942378997803\n",
      "Epoch: 15, Samples: 2464/5760, Loss: 1.4914789199829102\n",
      "Epoch: 15, Samples: 2496/5760, Loss: 1.588952660560608\n",
      "Epoch: 15, Samples: 2528/5760, Loss: 1.4885817766189575\n",
      "Epoch: 15, Samples: 2560/5760, Loss: 1.72489333152771\n",
      "Epoch: 15, Samples: 2592/5760, Loss: 1.667478084564209\n",
      "Epoch: 15, Samples: 2624/5760, Loss: 1.9211297035217285\n",
      "Epoch: 15, Samples: 2656/5760, Loss: 1.7930891513824463\n",
      "Epoch: 15, Samples: 2688/5760, Loss: 2.033109426498413\n",
      "Epoch: 15, Samples: 2720/5760, Loss: 1.9257290363311768\n",
      "Epoch: 15, Samples: 2752/5760, Loss: 1.4593266248703003\n",
      "Epoch: 15, Samples: 2784/5760, Loss: 2.109055519104004\n",
      "Epoch: 15, Samples: 2816/5760, Loss: 1.4824573993682861\n",
      "Epoch: 15, Samples: 2848/5760, Loss: 1.8232332468032837\n",
      "Epoch: 15, Samples: 2880/5760, Loss: 1.6910591125488281\n",
      "Epoch: 15, Samples: 2912/5760, Loss: 2.186065673828125\n",
      "Epoch: 15, Samples: 2944/5760, Loss: 1.5515598058700562\n",
      "Epoch: 15, Samples: 2976/5760, Loss: 1.8164621591567993\n",
      "Epoch: 15, Samples: 3008/5760, Loss: 1.829003930091858\n",
      "Epoch: 15, Samples: 3040/5760, Loss: 1.6439106464385986\n",
      "Epoch: 15, Samples: 3072/5760, Loss: 1.946197271347046\n",
      "Epoch: 15, Samples: 3104/5760, Loss: 1.4476882219314575\n",
      "Epoch: 15, Samples: 3136/5760, Loss: 2.3237056732177734\n",
      "Epoch: 15, Samples: 3168/5760, Loss: 1.7338581085205078\n",
      "Epoch: 15, Samples: 3200/5760, Loss: 1.9871366024017334\n",
      "Epoch: 15, Samples: 3232/5760, Loss: 1.5832245349884033\n",
      "Epoch: 15, Samples: 3264/5760, Loss: 1.5551598072052002\n",
      "Epoch: 15, Samples: 3296/5760, Loss: 1.705992341041565\n",
      "Epoch: 15, Samples: 3328/5760, Loss: 1.656843662261963\n",
      "Epoch: 15, Samples: 3360/5760, Loss: 1.726197361946106\n",
      "Epoch: 15, Samples: 3392/5760, Loss: 1.4572110176086426\n",
      "Epoch: 15, Samples: 3424/5760, Loss: 1.4691599607467651\n",
      "Epoch: 15, Samples: 3456/5760, Loss: 1.579429268836975\n",
      "Epoch: 15, Samples: 3488/5760, Loss: 1.327252984046936\n",
      "Epoch: 15, Samples: 3520/5760, Loss: 1.6969808340072632\n",
      "Epoch: 15, Samples: 3552/5760, Loss: 1.7952311038970947\n",
      "Epoch: 15, Samples: 3584/5760, Loss: 1.6862881183624268\n",
      "Epoch: 15, Samples: 3616/5760, Loss: 1.576323390007019\n",
      "Epoch: 15, Samples: 3648/5760, Loss: 1.3929189443588257\n",
      "Epoch: 15, Samples: 3680/5760, Loss: 1.2263270616531372\n",
      "Epoch: 15, Samples: 3712/5760, Loss: 2.0583901405334473\n",
      "Epoch: 15, Samples: 3744/5760, Loss: 1.9603242874145508\n",
      "Epoch: 15, Samples: 3776/5760, Loss: 1.8890612125396729\n",
      "Epoch: 15, Samples: 3808/5760, Loss: 1.919400691986084\n",
      "Epoch: 15, Samples: 3840/5760, Loss: 1.8325166702270508\n",
      "Epoch: 15, Samples: 3872/5760, Loss: 1.5925095081329346\n",
      "Epoch: 15, Samples: 3904/5760, Loss: 1.5858262777328491\n",
      "Epoch: 15, Samples: 3936/5760, Loss: 1.5458106994628906\n",
      "Epoch: 15, Samples: 3968/5760, Loss: 1.8186626434326172\n",
      "Epoch: 15, Samples: 4000/5760, Loss: 1.9503298997879028\n",
      "Epoch: 15, Samples: 4032/5760, Loss: 1.8999437093734741\n",
      "Epoch: 15, Samples: 4064/5760, Loss: 1.9835788011550903\n",
      "Epoch: 15, Samples: 4096/5760, Loss: 2.0484859943389893\n",
      "Epoch: 15, Samples: 4128/5760, Loss: 1.8080288171768188\n",
      "Epoch: 15, Samples: 4160/5760, Loss: 1.5244797468185425\n",
      "Epoch: 15, Samples: 4192/5760, Loss: 1.7999411821365356\n",
      "Epoch: 15, Samples: 4224/5760, Loss: 1.8093565702438354\n",
      "Epoch: 15, Samples: 4256/5760, Loss: 1.4090760946273804\n",
      "Epoch: 15, Samples: 4288/5760, Loss: 1.8922650814056396\n",
      "Epoch: 15, Samples: 4320/5760, Loss: 1.4416916370391846\n",
      "Epoch: 15, Samples: 4352/5760, Loss: 1.7464637756347656\n",
      "Epoch: 15, Samples: 4384/5760, Loss: 1.6628541946411133\n",
      "Epoch: 15, Samples: 4416/5760, Loss: 2.0574333667755127\n",
      "Epoch: 15, Samples: 4448/5760, Loss: 1.9489789009094238\n",
      "Epoch: 15, Samples: 4480/5760, Loss: 1.805082082748413\n",
      "Epoch: 15, Samples: 4512/5760, Loss: 1.96723473072052\n",
      "Epoch: 15, Samples: 4544/5760, Loss: 1.8862757682800293\n",
      "Epoch: 15, Samples: 4576/5760, Loss: 1.9115363359451294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Samples: 4608/5760, Loss: 1.7285740375518799\n",
      "Epoch: 15, Samples: 4640/5760, Loss: 1.3232288360595703\n",
      "Epoch: 15, Samples: 4672/5760, Loss: 1.8549648523330688\n",
      "Epoch: 15, Samples: 4704/5760, Loss: 1.8785343170166016\n",
      "Epoch: 15, Samples: 4736/5760, Loss: 1.5460383892059326\n",
      "Epoch: 15, Samples: 4768/5760, Loss: 1.9070788621902466\n",
      "Epoch: 15, Samples: 4800/5760, Loss: 1.5628365278244019\n",
      "Epoch: 15, Samples: 4832/5760, Loss: 1.7125972509384155\n",
      "Epoch: 15, Samples: 4864/5760, Loss: 1.6468418836593628\n",
      "Epoch: 15, Samples: 4896/5760, Loss: 1.3200196027755737\n",
      "Epoch: 15, Samples: 4928/5760, Loss: 1.3825231790542603\n",
      "Epoch: 15, Samples: 4960/5760, Loss: 1.1973227262496948\n",
      "Epoch: 15, Samples: 4992/5760, Loss: 1.59977126121521\n",
      "Epoch: 15, Samples: 5024/5760, Loss: 2.275951385498047\n",
      "Epoch: 15, Samples: 5056/5760, Loss: 1.7090905904769897\n",
      "Epoch: 15, Samples: 5088/5760, Loss: 1.808107614517212\n",
      "Epoch: 15, Samples: 5120/5760, Loss: 1.0857411623001099\n",
      "Epoch: 15, Samples: 5152/5760, Loss: 1.5787698030471802\n",
      "Epoch: 15, Samples: 5184/5760, Loss: 2.088202714920044\n",
      "Epoch: 15, Samples: 5216/5760, Loss: 1.5744768381118774\n",
      "Epoch: 15, Samples: 5248/5760, Loss: 1.6949541568756104\n",
      "Epoch: 15, Samples: 5280/5760, Loss: 2.000701427459717\n",
      "Epoch: 15, Samples: 5312/5760, Loss: 1.852002739906311\n",
      "Epoch: 15, Samples: 5344/5760, Loss: 1.776365876197815\n",
      "Epoch: 15, Samples: 5376/5760, Loss: 2.3223013877868652\n",
      "Epoch: 15, Samples: 5408/5760, Loss: 1.837005853652954\n",
      "Epoch: 15, Samples: 5440/5760, Loss: 1.616565227508545\n",
      "Epoch: 15, Samples: 5472/5760, Loss: 1.6755685806274414\n",
      "Epoch: 15, Samples: 5504/5760, Loss: 1.6719200611114502\n",
      "Epoch: 15, Samples: 5536/5760, Loss: 1.559147596359253\n",
      "Epoch: 15, Samples: 5568/5760, Loss: 1.6531860828399658\n",
      "Epoch: 15, Samples: 5600/5760, Loss: 1.6827811002731323\n",
      "Epoch: 15, Samples: 5632/5760, Loss: 2.01531982421875\n",
      "Epoch: 15, Samples: 5664/5760, Loss: 1.634246587753296\n",
      "Epoch: 15, Samples: 5696/5760, Loss: 2.518568277359009\n",
      "Epoch: 15, Samples: 5728/5760, Loss: 3.232529640197754\n",
      "\n",
      "Epoch: 15\n",
      "Training set: Average loss: 1.7542\n",
      "Validation set: Average loss: 2.1191, Accuracy: 374/818 (46%)\n",
      "Saving model (epoch 15) with lowest validation loss: 2.119075518388015\n",
      "Epoch: 16, Samples: 0/5760, Loss: 1.465074062347412\n",
      "Epoch: 16, Samples: 32/5760, Loss: 1.5372384786605835\n",
      "Epoch: 16, Samples: 64/5760, Loss: 1.4633545875549316\n",
      "Epoch: 16, Samples: 96/5760, Loss: 1.754849910736084\n",
      "Epoch: 16, Samples: 128/5760, Loss: 1.493554949760437\n",
      "Epoch: 16, Samples: 160/5760, Loss: 1.8437328338623047\n",
      "Epoch: 16, Samples: 192/5760, Loss: 1.3667997121810913\n",
      "Epoch: 16, Samples: 224/5760, Loss: 2.333496332168579\n",
      "Epoch: 16, Samples: 256/5760, Loss: 1.704964518547058\n",
      "Epoch: 16, Samples: 288/5760, Loss: 2.1811442375183105\n",
      "Epoch: 16, Samples: 320/5760, Loss: 2.119591236114502\n",
      "Epoch: 16, Samples: 352/5760, Loss: 1.9561676979064941\n",
      "Epoch: 16, Samples: 384/5760, Loss: 1.3588366508483887\n",
      "Epoch: 16, Samples: 416/5760, Loss: 1.757552981376648\n",
      "Epoch: 16, Samples: 448/5760, Loss: 1.5571777820587158\n",
      "Epoch: 16, Samples: 480/5760, Loss: 2.107693910598755\n",
      "Epoch: 16, Samples: 512/5760, Loss: 1.957270622253418\n",
      "Epoch: 16, Samples: 544/5760, Loss: 1.6762313842773438\n",
      "Epoch: 16, Samples: 576/5760, Loss: 1.7457458972930908\n",
      "Epoch: 16, Samples: 608/5760, Loss: 1.5837738513946533\n",
      "Epoch: 16, Samples: 640/5760, Loss: 1.8406699895858765\n",
      "Epoch: 16, Samples: 672/5760, Loss: 1.2763714790344238\n",
      "Epoch: 16, Samples: 704/5760, Loss: 1.909803867340088\n",
      "Epoch: 16, Samples: 736/5760, Loss: 1.8650871515274048\n",
      "Epoch: 16, Samples: 768/5760, Loss: 1.7699098587036133\n",
      "Epoch: 16, Samples: 800/5760, Loss: 1.499917984008789\n",
      "Epoch: 16, Samples: 832/5760, Loss: 2.0200653076171875\n",
      "Epoch: 16, Samples: 864/5760, Loss: 1.8341622352600098\n",
      "Epoch: 16, Samples: 896/5760, Loss: 1.844224452972412\n",
      "Epoch: 16, Samples: 928/5760, Loss: 1.2581520080566406\n",
      "Epoch: 16, Samples: 960/5760, Loss: 1.5531013011932373\n",
      "Epoch: 16, Samples: 992/5760, Loss: 1.3536086082458496\n",
      "Epoch: 16, Samples: 1024/5760, Loss: 1.3184489011764526\n",
      "Epoch: 16, Samples: 1056/5760, Loss: 1.6334607601165771\n",
      "Epoch: 16, Samples: 1088/5760, Loss: 1.5484857559204102\n",
      "Epoch: 16, Samples: 1120/5760, Loss: 1.4463571310043335\n",
      "Epoch: 16, Samples: 1152/5760, Loss: 1.8138896226882935\n",
      "Epoch: 16, Samples: 1184/5760, Loss: 1.5576149225234985\n",
      "Epoch: 16, Samples: 1216/5760, Loss: 1.298064112663269\n",
      "Epoch: 16, Samples: 1248/5760, Loss: 1.3930413722991943\n",
      "Epoch: 16, Samples: 1280/5760, Loss: 1.9949252605438232\n",
      "Epoch: 16, Samples: 1312/5760, Loss: 1.5479995012283325\n",
      "Epoch: 16, Samples: 1344/5760, Loss: 1.6368924379348755\n",
      "Epoch: 16, Samples: 1376/5760, Loss: 1.75722074508667\n",
      "Epoch: 16, Samples: 1408/5760, Loss: 1.4701457023620605\n",
      "Epoch: 16, Samples: 1440/5760, Loss: 1.520405888557434\n",
      "Epoch: 16, Samples: 1472/5760, Loss: 1.8368617296218872\n",
      "Epoch: 16, Samples: 1504/5760, Loss: 1.9237334728240967\n",
      "Epoch: 16, Samples: 1536/5760, Loss: 2.1603739261627197\n",
      "Epoch: 16, Samples: 1568/5760, Loss: 1.397871494293213\n",
      "Epoch: 16, Samples: 1600/5760, Loss: 1.411698579788208\n",
      "Epoch: 16, Samples: 1632/5760, Loss: 1.7948713302612305\n",
      "Epoch: 16, Samples: 1664/5760, Loss: 1.7249042987823486\n",
      "Epoch: 16, Samples: 1696/5760, Loss: 1.8191745281219482\n",
      "Epoch: 16, Samples: 1728/5760, Loss: 2.171212911605835\n",
      "Epoch: 16, Samples: 1760/5760, Loss: 1.4717121124267578\n",
      "Epoch: 16, Samples: 1792/5760, Loss: 1.3595775365829468\n",
      "Epoch: 16, Samples: 1824/5760, Loss: 1.2228283882141113\n",
      "Epoch: 16, Samples: 1856/5760, Loss: 1.6097073554992676\n",
      "Epoch: 16, Samples: 1888/5760, Loss: 1.5885748863220215\n",
      "Epoch: 16, Samples: 1920/5760, Loss: 1.736332654953003\n",
      "Epoch: 16, Samples: 1952/5760, Loss: 1.5671862363815308\n",
      "Epoch: 16, Samples: 1984/5760, Loss: 1.4683278799057007\n",
      "Epoch: 16, Samples: 2016/5760, Loss: 1.5775156021118164\n",
      "Epoch: 16, Samples: 2048/5760, Loss: 1.7612775564193726\n",
      "Epoch: 16, Samples: 2080/5760, Loss: 1.384524941444397\n",
      "Epoch: 16, Samples: 2112/5760, Loss: 1.789618730545044\n",
      "Epoch: 16, Samples: 2144/5760, Loss: 1.7418577671051025\n",
      "Epoch: 16, Samples: 2176/5760, Loss: 1.9507269859313965\n",
      "Epoch: 16, Samples: 2208/5760, Loss: 1.859652042388916\n",
      "Epoch: 16, Samples: 2240/5760, Loss: 1.3513309955596924\n",
      "Epoch: 16, Samples: 2272/5760, Loss: 1.6674232482910156\n",
      "Epoch: 16, Samples: 2304/5760, Loss: 1.2974904775619507\n",
      "Epoch: 16, Samples: 2336/5760, Loss: 1.6627458333969116\n",
      "Epoch: 16, Samples: 2368/5760, Loss: 1.6617292165756226\n",
      "Epoch: 16, Samples: 2400/5760, Loss: 2.2025856971740723\n",
      "Epoch: 16, Samples: 2432/5760, Loss: 2.25732421875\n",
      "Epoch: 16, Samples: 2464/5760, Loss: 1.438240885734558\n",
      "Epoch: 16, Samples: 2496/5760, Loss: 1.8304624557495117\n",
      "Epoch: 16, Samples: 2528/5760, Loss: 1.5331519842147827\n",
      "Epoch: 16, Samples: 2560/5760, Loss: 1.9861351251602173\n",
      "Epoch: 16, Samples: 2592/5760, Loss: 1.8285949230194092\n",
      "Epoch: 16, Samples: 2624/5760, Loss: 1.363976001739502\n",
      "Epoch: 16, Samples: 2656/5760, Loss: 1.429828405380249\n",
      "Epoch: 16, Samples: 2688/5760, Loss: 2.569302558898926\n",
      "Epoch: 16, Samples: 2720/5760, Loss: 1.4437265396118164\n",
      "Epoch: 16, Samples: 2752/5760, Loss: 1.7188477516174316\n",
      "Epoch: 16, Samples: 2784/5760, Loss: 2.000281810760498\n",
      "Epoch: 16, Samples: 2816/5760, Loss: 1.7919565439224243\n",
      "Epoch: 16, Samples: 2848/5760, Loss: 1.5771291255950928\n",
      "Epoch: 16, Samples: 2880/5760, Loss: 1.549260139465332\n",
      "Epoch: 16, Samples: 2912/5760, Loss: 1.6240187883377075\n",
      "Epoch: 16, Samples: 2944/5760, Loss: 1.8718814849853516\n",
      "Epoch: 16, Samples: 2976/5760, Loss: 1.5040255784988403\n",
      "Epoch: 16, Samples: 3008/5760, Loss: 1.8894284963607788\n",
      "Epoch: 16, Samples: 3040/5760, Loss: 1.3111125230789185\n",
      "Epoch: 16, Samples: 3072/5760, Loss: 1.5769144296646118\n",
      "Epoch: 16, Samples: 3104/5760, Loss: 1.8706021308898926\n",
      "Epoch: 16, Samples: 3136/5760, Loss: 1.7213122844696045\n",
      "Epoch: 16, Samples: 3168/5760, Loss: 1.834068775177002\n",
      "Epoch: 16, Samples: 3200/5760, Loss: 1.7574105262756348\n",
      "Epoch: 16, Samples: 3232/5760, Loss: 1.9044883251190186\n",
      "Epoch: 16, Samples: 3264/5760, Loss: 1.9559506177902222\n",
      "Epoch: 16, Samples: 3296/5760, Loss: 1.7049239873886108\n",
      "Epoch: 16, Samples: 3328/5760, Loss: 1.3389101028442383\n",
      "Epoch: 16, Samples: 3360/5760, Loss: 1.3506872653961182\n",
      "Epoch: 16, Samples: 3392/5760, Loss: 1.4211230278015137\n",
      "Epoch: 16, Samples: 3424/5760, Loss: 1.3186391592025757\n",
      "Epoch: 16, Samples: 3456/5760, Loss: 1.7284798622131348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Samples: 3488/5760, Loss: 1.5155150890350342\n",
      "Epoch: 16, Samples: 3520/5760, Loss: 1.8325145244598389\n",
      "Epoch: 16, Samples: 3552/5760, Loss: 1.5097676515579224\n",
      "Epoch: 16, Samples: 3584/5760, Loss: 2.095449924468994\n",
      "Epoch: 16, Samples: 3616/5760, Loss: 1.5750319957733154\n",
      "Epoch: 16, Samples: 3648/5760, Loss: 1.5174399614334106\n",
      "Epoch: 16, Samples: 3680/5760, Loss: 1.4152612686157227\n",
      "Epoch: 16, Samples: 3712/5760, Loss: 1.7612491846084595\n",
      "Epoch: 16, Samples: 3744/5760, Loss: 1.7654497623443604\n",
      "Epoch: 16, Samples: 3776/5760, Loss: 1.7563257217407227\n",
      "Epoch: 16, Samples: 3808/5760, Loss: 1.4015552997589111\n",
      "Epoch: 16, Samples: 3840/5760, Loss: 2.305758476257324\n",
      "Epoch: 16, Samples: 3872/5760, Loss: 1.4195393323898315\n",
      "Epoch: 16, Samples: 3904/5760, Loss: 2.1733345985412598\n",
      "Epoch: 16, Samples: 3936/5760, Loss: 1.3167905807495117\n",
      "Epoch: 16, Samples: 3968/5760, Loss: 1.5100648403167725\n",
      "Epoch: 16, Samples: 4000/5760, Loss: 1.5098450183868408\n",
      "Epoch: 16, Samples: 4032/5760, Loss: 1.5532760620117188\n",
      "Epoch: 16, Samples: 4064/5760, Loss: 1.7841031551361084\n",
      "Epoch: 16, Samples: 4096/5760, Loss: 1.0679742097854614\n",
      "Epoch: 16, Samples: 4128/5760, Loss: 1.729845404624939\n",
      "Epoch: 16, Samples: 4160/5760, Loss: 1.6429741382598877\n",
      "Epoch: 16, Samples: 4192/5760, Loss: 1.3731460571289062\n",
      "Epoch: 16, Samples: 4224/5760, Loss: 1.670973300933838\n",
      "Epoch: 16, Samples: 4256/5760, Loss: 1.3584692478179932\n",
      "Epoch: 16, Samples: 4288/5760, Loss: 1.2647898197174072\n",
      "Epoch: 16, Samples: 4320/5760, Loss: 1.9191910028457642\n",
      "Epoch: 16, Samples: 4352/5760, Loss: 1.3507336378097534\n",
      "Epoch: 16, Samples: 4384/5760, Loss: 1.5251988172531128\n",
      "Epoch: 16, Samples: 4416/5760, Loss: 1.7639174461364746\n",
      "Epoch: 16, Samples: 4448/5760, Loss: 1.8182953596115112\n",
      "Epoch: 16, Samples: 4480/5760, Loss: 1.622607946395874\n",
      "Epoch: 16, Samples: 4512/5760, Loss: 1.972103238105774\n",
      "Epoch: 16, Samples: 4544/5760, Loss: 1.7970032691955566\n",
      "Epoch: 16, Samples: 4576/5760, Loss: 1.666191577911377\n",
      "Epoch: 16, Samples: 4608/5760, Loss: 1.539020299911499\n",
      "Epoch: 16, Samples: 4640/5760, Loss: 1.555642008781433\n",
      "Epoch: 16, Samples: 4672/5760, Loss: 1.7193809747695923\n",
      "Epoch: 16, Samples: 4704/5760, Loss: 1.7147618532180786\n",
      "Epoch: 16, Samples: 4736/5760, Loss: 1.548425555229187\n",
      "Epoch: 16, Samples: 4768/5760, Loss: 1.4726377725601196\n",
      "Epoch: 16, Samples: 4800/5760, Loss: 2.417527675628662\n",
      "Epoch: 16, Samples: 4832/5760, Loss: 1.7770684957504272\n",
      "Epoch: 16, Samples: 4864/5760, Loss: 1.8792598247528076\n",
      "Epoch: 16, Samples: 4896/5760, Loss: 1.2027854919433594\n",
      "Epoch: 16, Samples: 4928/5760, Loss: 1.5675073862075806\n",
      "Epoch: 16, Samples: 4960/5760, Loss: 1.6166493892669678\n",
      "Epoch: 16, Samples: 4992/5760, Loss: 1.4334276914596558\n",
      "Epoch: 16, Samples: 5024/5760, Loss: 1.2330185174942017\n",
      "Epoch: 16, Samples: 5056/5760, Loss: 1.9159150123596191\n",
      "Epoch: 16, Samples: 5088/5760, Loss: 1.4822782278060913\n",
      "Epoch: 16, Samples: 5120/5760, Loss: 1.9170775413513184\n",
      "Epoch: 16, Samples: 5152/5760, Loss: 2.169468402862549\n",
      "Epoch: 16, Samples: 5184/5760, Loss: 1.832490086555481\n",
      "Epoch: 16, Samples: 5216/5760, Loss: 1.856508493423462\n",
      "Epoch: 16, Samples: 5248/5760, Loss: 1.081974983215332\n",
      "Epoch: 16, Samples: 5280/5760, Loss: 1.5445150136947632\n",
      "Epoch: 16, Samples: 5312/5760, Loss: 1.9385840892791748\n",
      "Epoch: 16, Samples: 5344/5760, Loss: 1.7608760595321655\n",
      "Epoch: 16, Samples: 5376/5760, Loss: 1.490051031112671\n",
      "Epoch: 16, Samples: 5408/5760, Loss: 2.292914867401123\n",
      "Epoch: 16, Samples: 5440/5760, Loss: 1.3793447017669678\n",
      "Epoch: 16, Samples: 5472/5760, Loss: 1.7141897678375244\n",
      "Epoch: 16, Samples: 5504/5760, Loss: 2.2012765407562256\n",
      "Epoch: 16, Samples: 5536/5760, Loss: 1.5910683870315552\n",
      "Epoch: 16, Samples: 5568/5760, Loss: 1.16124427318573\n",
      "Epoch: 16, Samples: 5600/5760, Loss: 1.6091302633285522\n",
      "Epoch: 16, Samples: 5632/5760, Loss: 1.4973208904266357\n",
      "Epoch: 16, Samples: 5664/5760, Loss: 1.7446701526641846\n",
      "Epoch: 16, Samples: 5696/5760, Loss: 1.8761941194534302\n",
      "Epoch: 16, Samples: 5728/5760, Loss: 2.8349781036376953\n",
      "\n",
      "Epoch: 16\n",
      "Training set: Average loss: 1.6774\n",
      "Validation set: Average loss: 2.0877, Accuracy: 388/818 (47%)\n",
      "Saving model (epoch 16) with lowest validation loss: 2.0877357033582835\n",
      "Epoch: 17, Samples: 0/5760, Loss: 1.2420321702957153\n",
      "Epoch: 17, Samples: 32/5760, Loss: 1.6305124759674072\n",
      "Epoch: 17, Samples: 64/5760, Loss: 1.545173168182373\n",
      "Epoch: 17, Samples: 96/5760, Loss: 2.074939250946045\n",
      "Epoch: 17, Samples: 128/5760, Loss: 1.4754149913787842\n",
      "Epoch: 17, Samples: 160/5760, Loss: 1.931811809539795\n",
      "Epoch: 17, Samples: 192/5760, Loss: 1.544975996017456\n",
      "Epoch: 17, Samples: 224/5760, Loss: 1.7305357456207275\n",
      "Epoch: 17, Samples: 256/5760, Loss: 1.5948233604431152\n",
      "Epoch: 17, Samples: 288/5760, Loss: 1.6114615201950073\n",
      "Epoch: 17, Samples: 320/5760, Loss: 1.8461087942123413\n",
      "Epoch: 17, Samples: 352/5760, Loss: 1.7901911735534668\n",
      "Epoch: 17, Samples: 384/5760, Loss: 1.419989824295044\n",
      "Epoch: 17, Samples: 416/5760, Loss: 1.8412771224975586\n",
      "Epoch: 17, Samples: 448/5760, Loss: 1.4089988470077515\n",
      "Epoch: 17, Samples: 480/5760, Loss: 1.7256474494934082\n",
      "Epoch: 17, Samples: 512/5760, Loss: 1.4156346321105957\n",
      "Epoch: 17, Samples: 544/5760, Loss: 1.6144050359725952\n",
      "Epoch: 17, Samples: 576/5760, Loss: 1.5025386810302734\n",
      "Epoch: 17, Samples: 608/5760, Loss: 1.6226961612701416\n",
      "Epoch: 17, Samples: 640/5760, Loss: 1.539599895477295\n",
      "Epoch: 17, Samples: 672/5760, Loss: 1.4555895328521729\n",
      "Epoch: 17, Samples: 704/5760, Loss: 1.5025103092193604\n",
      "Epoch: 17, Samples: 736/5760, Loss: 1.8042783737182617\n",
      "Epoch: 17, Samples: 768/5760, Loss: 2.127938985824585\n",
      "Epoch: 17, Samples: 800/5760, Loss: 1.4074236154556274\n",
      "Epoch: 17, Samples: 832/5760, Loss: 2.0884995460510254\n",
      "Epoch: 17, Samples: 864/5760, Loss: 1.9050912857055664\n",
      "Epoch: 17, Samples: 896/5760, Loss: 1.3682196140289307\n",
      "Epoch: 17, Samples: 928/5760, Loss: 1.6618050336837769\n",
      "Epoch: 17, Samples: 960/5760, Loss: 1.4971680641174316\n",
      "Epoch: 17, Samples: 992/5760, Loss: 1.744483232498169\n",
      "Epoch: 17, Samples: 1024/5760, Loss: 1.2568225860595703\n",
      "Epoch: 17, Samples: 1056/5760, Loss: 2.1712486743927\n",
      "Epoch: 17, Samples: 1088/5760, Loss: 1.2983901500701904\n",
      "Epoch: 17, Samples: 1120/5760, Loss: 1.6751086711883545\n",
      "Epoch: 17, Samples: 1152/5760, Loss: 1.6934047937393188\n",
      "Epoch: 17, Samples: 1184/5760, Loss: 1.189895749092102\n",
      "Epoch: 17, Samples: 1216/5760, Loss: 1.6517974138259888\n",
      "Epoch: 17, Samples: 1248/5760, Loss: 1.8197426795959473\n",
      "Epoch: 17, Samples: 1280/5760, Loss: 1.9709970951080322\n",
      "Epoch: 17, Samples: 1312/5760, Loss: 1.368807077407837\n",
      "Epoch: 17, Samples: 1344/5760, Loss: 1.4098154306411743\n",
      "Epoch: 17, Samples: 1376/5760, Loss: 1.513866901397705\n",
      "Epoch: 17, Samples: 1408/5760, Loss: 1.4654312133789062\n",
      "Epoch: 17, Samples: 1440/5760, Loss: 1.7597646713256836\n",
      "Epoch: 17, Samples: 1472/5760, Loss: 1.5836451053619385\n",
      "Epoch: 17, Samples: 1504/5760, Loss: 1.7683191299438477\n",
      "Epoch: 17, Samples: 1536/5760, Loss: 1.4828053712844849\n",
      "Epoch: 17, Samples: 1568/5760, Loss: 1.5951423645019531\n",
      "Epoch: 17, Samples: 1600/5760, Loss: 1.942297339439392\n",
      "Epoch: 17, Samples: 1632/5760, Loss: 1.96140456199646\n",
      "Epoch: 17, Samples: 1664/5760, Loss: 1.948203206062317\n",
      "Epoch: 17, Samples: 1696/5760, Loss: 1.4157748222351074\n",
      "Epoch: 17, Samples: 1728/5760, Loss: 1.5829551219940186\n",
      "Epoch: 17, Samples: 1760/5760, Loss: 1.860259771347046\n",
      "Epoch: 17, Samples: 1792/5760, Loss: 1.7074342966079712\n",
      "Epoch: 17, Samples: 1824/5760, Loss: 1.7434906959533691\n",
      "Epoch: 17, Samples: 1856/5760, Loss: 1.6880505084991455\n",
      "Epoch: 17, Samples: 1888/5760, Loss: 1.55275297164917\n",
      "Epoch: 17, Samples: 1920/5760, Loss: 1.423386573791504\n",
      "Epoch: 17, Samples: 1952/5760, Loss: 1.2277491092681885\n",
      "Epoch: 17, Samples: 1984/5760, Loss: 1.5517159700393677\n",
      "Epoch: 17, Samples: 2016/5760, Loss: 1.8697608709335327\n",
      "Epoch: 17, Samples: 2048/5760, Loss: 1.3846217393875122\n",
      "Epoch: 17, Samples: 2080/5760, Loss: 1.539811611175537\n",
      "Epoch: 17, Samples: 2112/5760, Loss: 1.6662955284118652\n",
      "Epoch: 17, Samples: 2144/5760, Loss: 1.2652928829193115\n",
      "Epoch: 17, Samples: 2176/5760, Loss: 1.6843980550765991\n",
      "Epoch: 17, Samples: 2208/5760, Loss: 1.14852774143219\n",
      "Epoch: 17, Samples: 2240/5760, Loss: 1.6409698724746704\n",
      "Epoch: 17, Samples: 2272/5760, Loss: 1.985168218612671\n",
      "Epoch: 17, Samples: 2304/5760, Loss: 1.7571467161178589\n",
      "Epoch: 17, Samples: 2336/5760, Loss: 1.4018797874450684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Samples: 2368/5760, Loss: 1.3077553510665894\n",
      "Epoch: 17, Samples: 2400/5760, Loss: 2.0488946437835693\n",
      "Epoch: 17, Samples: 2432/5760, Loss: 1.7974413633346558\n",
      "Epoch: 17, Samples: 2464/5760, Loss: 2.5106639862060547\n",
      "Epoch: 17, Samples: 2496/5760, Loss: 1.528387188911438\n",
      "Epoch: 17, Samples: 2528/5760, Loss: 1.2314471006393433\n",
      "Epoch: 17, Samples: 2560/5760, Loss: 1.3051567077636719\n",
      "Epoch: 17, Samples: 2592/5760, Loss: 1.2918977737426758\n",
      "Epoch: 17, Samples: 2624/5760, Loss: 1.5797446966171265\n",
      "Epoch: 17, Samples: 2656/5760, Loss: 1.7904309034347534\n",
      "Epoch: 17, Samples: 2688/5760, Loss: 1.7598214149475098\n",
      "Epoch: 17, Samples: 2720/5760, Loss: 1.5488331317901611\n",
      "Epoch: 17, Samples: 2752/5760, Loss: 1.615858793258667\n",
      "Epoch: 17, Samples: 2784/5760, Loss: 1.7777657508850098\n",
      "Epoch: 17, Samples: 2816/5760, Loss: 1.6841014623641968\n",
      "Epoch: 17, Samples: 2848/5760, Loss: 1.6854201555252075\n",
      "Epoch: 17, Samples: 2880/5760, Loss: 1.2797869443893433\n",
      "Epoch: 17, Samples: 2912/5760, Loss: 1.995662808418274\n",
      "Epoch: 17, Samples: 2944/5760, Loss: 1.6380125284194946\n",
      "Epoch: 17, Samples: 2976/5760, Loss: 1.5897119045257568\n",
      "Epoch: 17, Samples: 3008/5760, Loss: 1.6061758995056152\n",
      "Epoch: 17, Samples: 3040/5760, Loss: 1.5055890083312988\n",
      "Epoch: 17, Samples: 3072/5760, Loss: 2.0632483959198\n",
      "Epoch: 17, Samples: 3104/5760, Loss: 1.9541606903076172\n",
      "Epoch: 17, Samples: 3136/5760, Loss: 1.459599494934082\n",
      "Epoch: 17, Samples: 3168/5760, Loss: 1.5167534351348877\n",
      "Epoch: 17, Samples: 3200/5760, Loss: 1.4398983716964722\n",
      "Epoch: 17, Samples: 3232/5760, Loss: 1.567847728729248\n",
      "Epoch: 17, Samples: 3264/5760, Loss: 1.5763168334960938\n",
      "Epoch: 17, Samples: 3296/5760, Loss: 1.8470362424850464\n",
      "Epoch: 17, Samples: 3328/5760, Loss: 1.963948130607605\n",
      "Epoch: 17, Samples: 3360/5760, Loss: 1.2632687091827393\n",
      "Epoch: 17, Samples: 3392/5760, Loss: 1.2089776992797852\n",
      "Epoch: 17, Samples: 3424/5760, Loss: 1.7637947797775269\n",
      "Epoch: 17, Samples: 3456/5760, Loss: 1.6288317441940308\n",
      "Epoch: 17, Samples: 3488/5760, Loss: 1.5565850734710693\n",
      "Epoch: 17, Samples: 3520/5760, Loss: 1.3903422355651855\n",
      "Epoch: 17, Samples: 3552/5760, Loss: 1.3280013799667358\n",
      "Epoch: 17, Samples: 3584/5760, Loss: 1.6014808416366577\n",
      "Epoch: 17, Samples: 3616/5760, Loss: 1.3595812320709229\n",
      "Epoch: 17, Samples: 3648/5760, Loss: 1.7781345844268799\n",
      "Epoch: 17, Samples: 3680/5760, Loss: 1.6271629333496094\n",
      "Epoch: 17, Samples: 3712/5760, Loss: 1.235722541809082\n",
      "Epoch: 17, Samples: 3744/5760, Loss: 1.444913387298584\n",
      "Epoch: 17, Samples: 3776/5760, Loss: 1.572251558303833\n",
      "Epoch: 17, Samples: 3808/5760, Loss: 1.5979254245758057\n",
      "Epoch: 17, Samples: 3840/5760, Loss: 1.551237940788269\n",
      "Epoch: 17, Samples: 3872/5760, Loss: 2.4884543418884277\n",
      "Epoch: 17, Samples: 3904/5760, Loss: 0.9961081743240356\n",
      "Epoch: 17, Samples: 3936/5760, Loss: 1.7807197570800781\n",
      "Epoch: 17, Samples: 3968/5760, Loss: 1.4297311305999756\n",
      "Epoch: 17, Samples: 4000/5760, Loss: 1.252294898033142\n",
      "Epoch: 17, Samples: 4032/5760, Loss: 1.3486006259918213\n",
      "Epoch: 17, Samples: 4064/5760, Loss: 1.5686025619506836\n",
      "Epoch: 17, Samples: 4096/5760, Loss: 2.1435701847076416\n",
      "Epoch: 17, Samples: 4128/5760, Loss: 1.7654653787612915\n",
      "Epoch: 17, Samples: 4160/5760, Loss: 1.436184287071228\n",
      "Epoch: 17, Samples: 4192/5760, Loss: 1.4692175388336182\n",
      "Epoch: 17, Samples: 4224/5760, Loss: 1.3252537250518799\n",
      "Epoch: 17, Samples: 4256/5760, Loss: 1.5015925168991089\n",
      "Epoch: 17, Samples: 4288/5760, Loss: 1.461868166923523\n",
      "Epoch: 17, Samples: 4320/5760, Loss: 1.8557558059692383\n",
      "Epoch: 17, Samples: 4352/5760, Loss: 2.3762965202331543\n",
      "Epoch: 17, Samples: 4384/5760, Loss: 1.2373831272125244\n",
      "Epoch: 17, Samples: 4416/5760, Loss: 1.8209857940673828\n",
      "Epoch: 17, Samples: 4448/5760, Loss: 1.3470780849456787\n",
      "Epoch: 17, Samples: 4480/5760, Loss: 1.3780486583709717\n",
      "Epoch: 17, Samples: 4512/5760, Loss: 1.468605637550354\n",
      "Epoch: 17, Samples: 4544/5760, Loss: 1.481419563293457\n",
      "Epoch: 17, Samples: 4576/5760, Loss: 1.5932424068450928\n",
      "Epoch: 17, Samples: 4608/5760, Loss: 1.6543331146240234\n",
      "Epoch: 17, Samples: 4640/5760, Loss: 1.4791879653930664\n",
      "Epoch: 17, Samples: 4672/5760, Loss: 1.4432785511016846\n",
      "Epoch: 17, Samples: 4704/5760, Loss: 1.4828665256500244\n",
      "Epoch: 17, Samples: 4736/5760, Loss: 1.726707935333252\n",
      "Epoch: 17, Samples: 4768/5760, Loss: 1.3727935552597046\n",
      "Epoch: 17, Samples: 4800/5760, Loss: 1.5896189212799072\n",
      "Epoch: 17, Samples: 4832/5760, Loss: 1.5623273849487305\n",
      "Epoch: 17, Samples: 4864/5760, Loss: 1.8901563882827759\n",
      "Epoch: 17, Samples: 4896/5760, Loss: 1.2803547382354736\n",
      "Epoch: 17, Samples: 4928/5760, Loss: 1.6851812601089478\n",
      "Epoch: 17, Samples: 4960/5760, Loss: 1.2041300535202026\n",
      "Epoch: 17, Samples: 4992/5760, Loss: 1.360060453414917\n",
      "Epoch: 17, Samples: 5024/5760, Loss: 1.9693390130996704\n",
      "Epoch: 17, Samples: 5056/5760, Loss: 1.6250265836715698\n",
      "Epoch: 17, Samples: 5088/5760, Loss: 1.082015037536621\n",
      "Epoch: 17, Samples: 5120/5760, Loss: 1.7253897190093994\n",
      "Epoch: 17, Samples: 5152/5760, Loss: 1.3378318548202515\n",
      "Epoch: 17, Samples: 5184/5760, Loss: 1.5171253681182861\n",
      "Epoch: 17, Samples: 5216/5760, Loss: 1.7817938327789307\n",
      "Epoch: 17, Samples: 5248/5760, Loss: 1.2847812175750732\n",
      "Epoch: 17, Samples: 5280/5760, Loss: 1.7624472379684448\n",
      "Epoch: 17, Samples: 5312/5760, Loss: 1.656937599182129\n",
      "Epoch: 17, Samples: 5344/5760, Loss: 1.3774940967559814\n",
      "Epoch: 17, Samples: 5376/5760, Loss: 1.695297122001648\n",
      "Epoch: 17, Samples: 5408/5760, Loss: 1.5612149238586426\n",
      "Epoch: 17, Samples: 5440/5760, Loss: 1.428248405456543\n",
      "Epoch: 17, Samples: 5472/5760, Loss: 1.9977691173553467\n",
      "Epoch: 17, Samples: 5504/5760, Loss: 1.3101249933242798\n",
      "Epoch: 17, Samples: 5536/5760, Loss: 1.5624468326568604\n",
      "Epoch: 17, Samples: 5568/5760, Loss: 1.2829861640930176\n",
      "Epoch: 17, Samples: 5600/5760, Loss: 1.297888994216919\n",
      "Epoch: 17, Samples: 5632/5760, Loss: 1.8881512880325317\n",
      "Epoch: 17, Samples: 5664/5760, Loss: 1.7783012390136719\n",
      "Epoch: 17, Samples: 5696/5760, Loss: 1.7215614318847656\n",
      "Epoch: 17, Samples: 5728/5760, Loss: 2.5233287811279297\n",
      "\n",
      "Epoch: 17\n",
      "Training set: Average loss: 1.6061\n",
      "Validation set: Average loss: 2.1886, Accuracy: 375/818 (46%)\n",
      "Epoch: 18, Samples: 0/5760, Loss: 1.7093045711517334\n",
      "Epoch: 18, Samples: 32/5760, Loss: 1.5614838600158691\n",
      "Epoch: 18, Samples: 64/5760, Loss: 2.171901226043701\n",
      "Epoch: 18, Samples: 96/5760, Loss: 1.7322568893432617\n",
      "Epoch: 18, Samples: 128/5760, Loss: 1.9485327005386353\n",
      "Epoch: 18, Samples: 160/5760, Loss: 1.6788318157196045\n",
      "Epoch: 18, Samples: 192/5760, Loss: 1.8487950563430786\n",
      "Epoch: 18, Samples: 224/5760, Loss: 1.8381409645080566\n",
      "Epoch: 18, Samples: 256/5760, Loss: 1.933916687965393\n",
      "Epoch: 18, Samples: 288/5760, Loss: 1.5185065269470215\n",
      "Epoch: 18, Samples: 320/5760, Loss: 1.1156331300735474\n",
      "Epoch: 18, Samples: 352/5760, Loss: 1.6641881465911865\n",
      "Epoch: 18, Samples: 384/5760, Loss: 1.1733840703964233\n",
      "Epoch: 18, Samples: 416/5760, Loss: 1.6432029008865356\n",
      "Epoch: 18, Samples: 448/5760, Loss: 1.4520936012268066\n",
      "Epoch: 18, Samples: 480/5760, Loss: 1.2674697637557983\n",
      "Epoch: 18, Samples: 512/5760, Loss: 1.513744592666626\n",
      "Epoch: 18, Samples: 544/5760, Loss: 1.6227513551712036\n",
      "Epoch: 18, Samples: 576/5760, Loss: 1.585396409034729\n",
      "Epoch: 18, Samples: 608/5760, Loss: 1.4926072359085083\n",
      "Epoch: 18, Samples: 640/5760, Loss: 1.5422708988189697\n",
      "Epoch: 18, Samples: 672/5760, Loss: 1.4238742589950562\n",
      "Epoch: 18, Samples: 704/5760, Loss: 1.7423166036605835\n",
      "Epoch: 18, Samples: 736/5760, Loss: 1.54085111618042\n",
      "Epoch: 18, Samples: 768/5760, Loss: 1.5976359844207764\n",
      "Epoch: 18, Samples: 800/5760, Loss: 1.5561587810516357\n",
      "Epoch: 18, Samples: 832/5760, Loss: 1.4093414545059204\n",
      "Epoch: 18, Samples: 864/5760, Loss: 1.3986634016036987\n",
      "Epoch: 18, Samples: 896/5760, Loss: 1.715975046157837\n",
      "Epoch: 18, Samples: 928/5760, Loss: 1.092993974685669\n",
      "Epoch: 18, Samples: 960/5760, Loss: 1.6464959383010864\n",
      "Epoch: 18, Samples: 992/5760, Loss: 1.76661217212677\n",
      "Epoch: 18, Samples: 1024/5760, Loss: 1.6114671230316162\n",
      "Epoch: 18, Samples: 1056/5760, Loss: 1.441361427307129\n",
      "Epoch: 18, Samples: 1088/5760, Loss: 1.351457118988037\n",
      "Epoch: 18, Samples: 1120/5760, Loss: 1.6026346683502197\n",
      "Epoch: 18, Samples: 1152/5760, Loss: 1.5544288158416748\n",
      "Epoch: 18, Samples: 1184/5760, Loss: 1.3621934652328491\n",
      "Epoch: 18, Samples: 1216/5760, Loss: 1.2175687551498413\n",
      "Epoch: 18, Samples: 1248/5760, Loss: 1.5385693311691284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Samples: 1280/5760, Loss: 1.609125018119812\n",
      "Epoch: 18, Samples: 1312/5760, Loss: 1.4328728914260864\n",
      "Epoch: 18, Samples: 1344/5760, Loss: 1.3525182008743286\n",
      "Epoch: 18, Samples: 1376/5760, Loss: 1.364898681640625\n",
      "Epoch: 18, Samples: 1408/5760, Loss: 1.1191420555114746\n",
      "Epoch: 18, Samples: 1440/5760, Loss: 1.7398431301116943\n",
      "Epoch: 18, Samples: 1472/5760, Loss: 1.1929247379302979\n",
      "Epoch: 18, Samples: 1504/5760, Loss: 1.5456372499465942\n",
      "Epoch: 18, Samples: 1536/5760, Loss: 1.3379175662994385\n",
      "Epoch: 18, Samples: 1568/5760, Loss: 1.3877277374267578\n",
      "Epoch: 18, Samples: 1600/5760, Loss: 2.1540987491607666\n",
      "Epoch: 18, Samples: 1632/5760, Loss: 1.7795907258987427\n",
      "Epoch: 18, Samples: 1664/5760, Loss: 1.7108161449432373\n",
      "Epoch: 18, Samples: 1696/5760, Loss: 1.634100079536438\n",
      "Epoch: 18, Samples: 1728/5760, Loss: 1.9648321866989136\n",
      "Epoch: 18, Samples: 1760/5760, Loss: 1.6465610265731812\n",
      "Epoch: 18, Samples: 1792/5760, Loss: 1.2653958797454834\n",
      "Epoch: 18, Samples: 1824/5760, Loss: 1.5780924558639526\n",
      "Epoch: 18, Samples: 1856/5760, Loss: 1.8164464235305786\n",
      "Epoch: 18, Samples: 1888/5760, Loss: 1.7166087627410889\n",
      "Epoch: 18, Samples: 1920/5760, Loss: 1.261709213256836\n",
      "Epoch: 18, Samples: 1952/5760, Loss: 1.5377306938171387\n",
      "Epoch: 18, Samples: 1984/5760, Loss: 1.495016098022461\n",
      "Epoch: 18, Samples: 2016/5760, Loss: 1.384995460510254\n",
      "Epoch: 18, Samples: 2048/5760, Loss: 1.6192808151245117\n",
      "Epoch: 18, Samples: 2080/5760, Loss: 1.5199960470199585\n",
      "Epoch: 18, Samples: 2112/5760, Loss: 1.3732279539108276\n",
      "Epoch: 18, Samples: 2144/5760, Loss: 1.6562440395355225\n",
      "Epoch: 18, Samples: 2176/5760, Loss: 1.5237401723861694\n",
      "Epoch: 18, Samples: 2208/5760, Loss: 1.4701480865478516\n",
      "Epoch: 18, Samples: 2240/5760, Loss: 1.4673885107040405\n",
      "Epoch: 18, Samples: 2272/5760, Loss: 1.343940019607544\n",
      "Epoch: 18, Samples: 2304/5760, Loss: 1.3321950435638428\n",
      "Epoch: 18, Samples: 2336/5760, Loss: 1.748867392539978\n",
      "Epoch: 18, Samples: 2368/5760, Loss: 2.111860990524292\n",
      "Epoch: 18, Samples: 2400/5760, Loss: 1.1662465333938599\n",
      "Epoch: 18, Samples: 2432/5760, Loss: 1.9438244104385376\n",
      "Epoch: 18, Samples: 2464/5760, Loss: 1.3954694271087646\n",
      "Epoch: 18, Samples: 2496/5760, Loss: 1.517368197441101\n",
      "Epoch: 18, Samples: 2528/5760, Loss: 1.299347162246704\n",
      "Epoch: 18, Samples: 2560/5760, Loss: 1.4311268329620361\n",
      "Epoch: 18, Samples: 2592/5760, Loss: 1.2729737758636475\n",
      "Epoch: 18, Samples: 2624/5760, Loss: 1.4317963123321533\n",
      "Epoch: 18, Samples: 2656/5760, Loss: 1.7305681705474854\n",
      "Epoch: 18, Samples: 2688/5760, Loss: 1.1422523260116577\n",
      "Epoch: 18, Samples: 2720/5760, Loss: 1.227710485458374\n",
      "Epoch: 18, Samples: 2752/5760, Loss: 2.3979697227478027\n",
      "Epoch: 18, Samples: 2784/5760, Loss: 1.7349836826324463\n",
      "Epoch: 18, Samples: 2816/5760, Loss: 1.4550633430480957\n",
      "Epoch: 18, Samples: 2848/5760, Loss: 1.7110668420791626\n",
      "Epoch: 18, Samples: 2880/5760, Loss: 1.353238821029663\n",
      "Epoch: 18, Samples: 2912/5760, Loss: 1.4926164150238037\n",
      "Epoch: 18, Samples: 2944/5760, Loss: 1.4371144771575928\n",
      "Epoch: 18, Samples: 2976/5760, Loss: 1.810583472251892\n",
      "Epoch: 18, Samples: 3008/5760, Loss: 1.1777374744415283\n",
      "Epoch: 18, Samples: 3040/5760, Loss: 1.450135588645935\n",
      "Epoch: 18, Samples: 3072/5760, Loss: 1.4146226644515991\n",
      "Epoch: 18, Samples: 3104/5760, Loss: 1.4265457391738892\n",
      "Epoch: 18, Samples: 3136/5760, Loss: 1.3265584707260132\n",
      "Epoch: 18, Samples: 3168/5760, Loss: 1.7342034578323364\n",
      "Epoch: 18, Samples: 3200/5760, Loss: 1.6069469451904297\n",
      "Epoch: 18, Samples: 3232/5760, Loss: 1.4717650413513184\n",
      "Epoch: 18, Samples: 3264/5760, Loss: 1.4648045301437378\n",
      "Epoch: 18, Samples: 3296/5760, Loss: 1.8926844596862793\n",
      "Epoch: 18, Samples: 3328/5760, Loss: 1.5849443674087524\n",
      "Epoch: 18, Samples: 3360/5760, Loss: 2.0323140621185303\n",
      "Epoch: 18, Samples: 3392/5760, Loss: 1.6172364950180054\n",
      "Epoch: 18, Samples: 3424/5760, Loss: 1.4329208135604858\n",
      "Epoch: 18, Samples: 3456/5760, Loss: 1.4199001789093018\n",
      "Epoch: 18, Samples: 3488/5760, Loss: 1.2780795097351074\n",
      "Epoch: 18, Samples: 3520/5760, Loss: 1.0930426120758057\n",
      "Epoch: 18, Samples: 3552/5760, Loss: 1.8633869886398315\n",
      "Epoch: 18, Samples: 3584/5760, Loss: 1.8876354694366455\n",
      "Epoch: 18, Samples: 3616/5760, Loss: 1.6683903932571411\n",
      "Epoch: 18, Samples: 3648/5760, Loss: 1.6055254936218262\n",
      "Epoch: 18, Samples: 3680/5760, Loss: 1.3069043159484863\n",
      "Epoch: 18, Samples: 3712/5760, Loss: 1.4065048694610596\n",
      "Epoch: 18, Samples: 3744/5760, Loss: 1.3967570066452026\n",
      "Epoch: 18, Samples: 3776/5760, Loss: 1.6531895399093628\n",
      "Epoch: 18, Samples: 3808/5760, Loss: 1.5839626789093018\n",
      "Epoch: 18, Samples: 3840/5760, Loss: 1.1771541833877563\n",
      "Epoch: 18, Samples: 3872/5760, Loss: 1.1809508800506592\n",
      "Epoch: 18, Samples: 3904/5760, Loss: 1.7794017791748047\n",
      "Epoch: 18, Samples: 3936/5760, Loss: 1.39541494846344\n",
      "Epoch: 18, Samples: 3968/5760, Loss: 1.67621648311615\n",
      "Epoch: 18, Samples: 4000/5760, Loss: 1.8934125900268555\n",
      "Epoch: 18, Samples: 4032/5760, Loss: 1.858271837234497\n",
      "Epoch: 18, Samples: 4064/5760, Loss: 2.0608606338500977\n",
      "Epoch: 18, Samples: 4096/5760, Loss: 1.9176428318023682\n",
      "Epoch: 18, Samples: 4128/5760, Loss: 1.605958342552185\n",
      "Epoch: 18, Samples: 4160/5760, Loss: 1.573623538017273\n",
      "Epoch: 18, Samples: 4192/5760, Loss: 1.2467451095581055\n",
      "Epoch: 18, Samples: 4224/5760, Loss: 1.6939786672592163\n",
      "Epoch: 18, Samples: 4256/5760, Loss: 1.6766797304153442\n",
      "Epoch: 18, Samples: 4288/5760, Loss: 1.4179108142852783\n",
      "Epoch: 18, Samples: 4320/5760, Loss: 1.583814263343811\n",
      "Epoch: 18, Samples: 4352/5760, Loss: 1.569930911064148\n",
      "Epoch: 18, Samples: 4384/5760, Loss: 1.3463221788406372\n",
      "Epoch: 18, Samples: 4416/5760, Loss: 1.4023469686508179\n",
      "Epoch: 18, Samples: 4448/5760, Loss: 2.1887829303741455\n",
      "Epoch: 18, Samples: 4480/5760, Loss: 1.450057029724121\n",
      "Epoch: 18, Samples: 4512/5760, Loss: 1.5194592475891113\n",
      "Epoch: 18, Samples: 4544/5760, Loss: 1.4967292547225952\n",
      "Epoch: 18, Samples: 4576/5760, Loss: 1.5270472764968872\n",
      "Epoch: 18, Samples: 4608/5760, Loss: 1.095280647277832\n",
      "Epoch: 18, Samples: 4640/5760, Loss: 1.5498623847961426\n",
      "Epoch: 18, Samples: 4672/5760, Loss: 1.9022042751312256\n",
      "Epoch: 18, Samples: 4704/5760, Loss: 1.3987817764282227\n",
      "Epoch: 18, Samples: 4736/5760, Loss: 1.3020684719085693\n",
      "Epoch: 18, Samples: 4768/5760, Loss: 1.6217268705368042\n",
      "Epoch: 18, Samples: 4800/5760, Loss: 1.301432490348816\n",
      "Epoch: 18, Samples: 4832/5760, Loss: 1.5540457963943481\n",
      "Epoch: 18, Samples: 4864/5760, Loss: 1.6739286184310913\n",
      "Epoch: 18, Samples: 4896/5760, Loss: 1.487095594406128\n",
      "Epoch: 18, Samples: 4928/5760, Loss: 1.3134281635284424\n",
      "Epoch: 18, Samples: 4960/5760, Loss: 1.1817870140075684\n",
      "Epoch: 18, Samples: 4992/5760, Loss: 1.2463616132736206\n",
      "Epoch: 18, Samples: 5024/5760, Loss: 1.6032508611679077\n",
      "Epoch: 18, Samples: 5056/5760, Loss: 1.6579691171646118\n",
      "Epoch: 18, Samples: 5088/5760, Loss: 1.4377011060714722\n",
      "Epoch: 18, Samples: 5120/5760, Loss: 1.3564759492874146\n",
      "Epoch: 18, Samples: 5152/5760, Loss: 1.4578551054000854\n",
      "Epoch: 18, Samples: 5184/5760, Loss: 1.2502851486206055\n",
      "Epoch: 18, Samples: 5216/5760, Loss: 1.2291357517242432\n",
      "Epoch: 18, Samples: 5248/5760, Loss: 1.4025462865829468\n",
      "Epoch: 18, Samples: 5280/5760, Loss: 1.4631130695343018\n",
      "Epoch: 18, Samples: 5312/5760, Loss: 1.6381967067718506\n",
      "Epoch: 18, Samples: 5344/5760, Loss: 1.4545214176177979\n",
      "Epoch: 18, Samples: 5376/5760, Loss: 1.675615906715393\n",
      "Epoch: 18, Samples: 5408/5760, Loss: 1.6361730098724365\n",
      "Epoch: 18, Samples: 5440/5760, Loss: 1.721325159072876\n",
      "Epoch: 18, Samples: 5472/5760, Loss: 1.5796141624450684\n",
      "Epoch: 18, Samples: 5504/5760, Loss: 2.15221905708313\n",
      "Epoch: 18, Samples: 5536/5760, Loss: 1.9052845239639282\n",
      "Epoch: 18, Samples: 5568/5760, Loss: 1.3667371273040771\n",
      "Epoch: 18, Samples: 5600/5760, Loss: 1.4765739440917969\n",
      "Epoch: 18, Samples: 5632/5760, Loss: 1.5986512899398804\n",
      "Epoch: 18, Samples: 5664/5760, Loss: 1.7664387226104736\n",
      "Epoch: 18, Samples: 5696/5760, Loss: 2.296945095062256\n",
      "Epoch: 18, Samples: 5728/5760, Loss: 1.7120566368103027\n",
      "\n",
      "Epoch: 18\n",
      "Training set: Average loss: 1.5513\n",
      "Validation set: Average loss: 2.2360, Accuracy: 369/818 (45%)\n",
      "Epoch: 19, Samples: 0/5760, Loss: 1.2659655809402466\n",
      "Epoch: 19, Samples: 32/5760, Loss: 1.3737874031066895\n",
      "Epoch: 19, Samples: 64/5760, Loss: 1.0732030868530273\n",
      "Epoch: 19, Samples: 96/5760, Loss: 1.3821589946746826\n",
      "Epoch: 19, Samples: 128/5760, Loss: 1.9892269372940063\n",
      "Epoch: 19, Samples: 160/5760, Loss: 1.291641354560852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Samples: 192/5760, Loss: 1.72951340675354\n",
      "Epoch: 19, Samples: 224/5760, Loss: 1.6760386228561401\n",
      "Epoch: 19, Samples: 256/5760, Loss: 1.7304563522338867\n",
      "Epoch: 19, Samples: 288/5760, Loss: 1.8174042701721191\n",
      "Epoch: 19, Samples: 320/5760, Loss: 2.065816879272461\n",
      "Epoch: 19, Samples: 352/5760, Loss: 1.4006725549697876\n",
      "Epoch: 19, Samples: 384/5760, Loss: 1.491361141204834\n",
      "Epoch: 19, Samples: 416/5760, Loss: 2.033696413040161\n",
      "Epoch: 19, Samples: 448/5760, Loss: 1.5837593078613281\n",
      "Epoch: 19, Samples: 480/5760, Loss: 1.665041446685791\n",
      "Epoch: 19, Samples: 512/5760, Loss: 1.2737648487091064\n",
      "Epoch: 19, Samples: 544/5760, Loss: 1.4551031589508057\n",
      "Epoch: 19, Samples: 576/5760, Loss: 1.3711304664611816\n",
      "Epoch: 19, Samples: 608/5760, Loss: 1.2809689044952393\n",
      "Epoch: 19, Samples: 640/5760, Loss: 2.0948541164398193\n",
      "Epoch: 19, Samples: 672/5760, Loss: 1.702958583831787\n",
      "Epoch: 19, Samples: 704/5760, Loss: 1.4451254606246948\n",
      "Epoch: 19, Samples: 736/5760, Loss: 1.3928093910217285\n",
      "Epoch: 19, Samples: 768/5760, Loss: 1.1650352478027344\n",
      "Epoch: 19, Samples: 800/5760, Loss: 1.2257981300354004\n",
      "Epoch: 19, Samples: 832/5760, Loss: 1.6562923192977905\n",
      "Epoch: 19, Samples: 864/5760, Loss: 1.1302512884140015\n",
      "Epoch: 19, Samples: 896/5760, Loss: 1.119445562362671\n",
      "Epoch: 19, Samples: 928/5760, Loss: 1.9098052978515625\n",
      "Epoch: 19, Samples: 960/5760, Loss: 1.399147868156433\n",
      "Epoch: 19, Samples: 992/5760, Loss: 1.588142991065979\n",
      "Epoch: 19, Samples: 1024/5760, Loss: 1.3931833505630493\n",
      "Epoch: 19, Samples: 1056/5760, Loss: 1.688081979751587\n",
      "Epoch: 19, Samples: 1088/5760, Loss: 1.7025867700576782\n",
      "Epoch: 19, Samples: 1120/5760, Loss: 1.6606976985931396\n",
      "Epoch: 19, Samples: 1152/5760, Loss: 1.1763337850570679\n",
      "Epoch: 19, Samples: 1184/5760, Loss: 1.6098955869674683\n",
      "Epoch: 19, Samples: 1216/5760, Loss: 1.4574227333068848\n",
      "Epoch: 19, Samples: 1248/5760, Loss: 1.2447233200073242\n",
      "Epoch: 19, Samples: 1280/5760, Loss: 1.7358245849609375\n",
      "Epoch: 19, Samples: 1312/5760, Loss: 1.1724694967269897\n",
      "Epoch: 19, Samples: 1344/5760, Loss: 0.9859146475791931\n",
      "Epoch: 19, Samples: 1376/5760, Loss: 1.548743724822998\n",
      "Epoch: 19, Samples: 1408/5760, Loss: 1.7301182746887207\n",
      "Epoch: 19, Samples: 1440/5760, Loss: 1.9418389797210693\n",
      "Epoch: 19, Samples: 1472/5760, Loss: 1.069458246231079\n",
      "Epoch: 19, Samples: 1504/5760, Loss: 1.6554415225982666\n",
      "Epoch: 19, Samples: 1536/5760, Loss: 1.9138431549072266\n",
      "Epoch: 19, Samples: 1568/5760, Loss: 1.471825122833252\n",
      "Epoch: 19, Samples: 1600/5760, Loss: 2.0648066997528076\n",
      "Epoch: 19, Samples: 1632/5760, Loss: 1.823541522026062\n",
      "Epoch: 19, Samples: 1664/5760, Loss: 1.3193421363830566\n",
      "Epoch: 19, Samples: 1696/5760, Loss: 1.398875117301941\n",
      "Epoch: 19, Samples: 1728/5760, Loss: 1.298775553703308\n",
      "Epoch: 19, Samples: 1760/5760, Loss: 1.3932466506958008\n",
      "Epoch: 19, Samples: 1792/5760, Loss: 1.5951181650161743\n",
      "Epoch: 19, Samples: 1824/5760, Loss: 1.338819980621338\n",
      "Epoch: 19, Samples: 1856/5760, Loss: 1.3566912412643433\n",
      "Epoch: 19, Samples: 1888/5760, Loss: 1.5018200874328613\n",
      "Epoch: 19, Samples: 1920/5760, Loss: 1.6811476945877075\n",
      "Epoch: 19, Samples: 1952/5760, Loss: 1.4048902988433838\n",
      "Epoch: 19, Samples: 1984/5760, Loss: 1.446702480316162\n",
      "Epoch: 19, Samples: 2016/5760, Loss: 1.3223406076431274\n",
      "Epoch: 19, Samples: 2048/5760, Loss: 1.4527053833007812\n",
      "Epoch: 19, Samples: 2080/5760, Loss: 1.4198304414749146\n",
      "Epoch: 19, Samples: 2112/5760, Loss: 1.278804063796997\n",
      "Epoch: 19, Samples: 2144/5760, Loss: 1.659321904182434\n",
      "Epoch: 19, Samples: 2176/5760, Loss: 1.4536628723144531\n",
      "Epoch: 19, Samples: 2208/5760, Loss: 1.4855071306228638\n",
      "Epoch: 19, Samples: 2240/5760, Loss: 1.4272648096084595\n",
      "Epoch: 19, Samples: 2272/5760, Loss: 1.6082109212875366\n",
      "Epoch: 19, Samples: 2304/5760, Loss: 1.4261337518692017\n",
      "Epoch: 19, Samples: 2336/5760, Loss: 1.7900712490081787\n",
      "Epoch: 19, Samples: 2368/5760, Loss: 1.4933123588562012\n",
      "Epoch: 19, Samples: 2400/5760, Loss: 1.3913164138793945\n",
      "Epoch: 19, Samples: 2432/5760, Loss: 1.1623845100402832\n",
      "Epoch: 19, Samples: 2464/5760, Loss: 1.2646877765655518\n",
      "Epoch: 19, Samples: 2496/5760, Loss: 1.5827651023864746\n",
      "Epoch: 19, Samples: 2528/5760, Loss: 1.337447166442871\n",
      "Epoch: 19, Samples: 2560/5760, Loss: 1.235543966293335\n",
      "Epoch: 19, Samples: 2592/5760, Loss: 1.535294532775879\n",
      "Epoch: 19, Samples: 2624/5760, Loss: 1.2907161712646484\n",
      "Epoch: 19, Samples: 2656/5760, Loss: 1.650354266166687\n",
      "Epoch: 19, Samples: 2688/5760, Loss: 1.9106979370117188\n",
      "Epoch: 19, Samples: 2720/5760, Loss: 1.472334623336792\n",
      "Epoch: 19, Samples: 2752/5760, Loss: 1.792327642440796\n",
      "Epoch: 19, Samples: 2784/5760, Loss: 1.475041389465332\n",
      "Epoch: 19, Samples: 2816/5760, Loss: 1.5779603719711304\n",
      "Epoch: 19, Samples: 2848/5760, Loss: 1.7512980699539185\n",
      "Epoch: 19, Samples: 2880/5760, Loss: 1.5268851518630981\n",
      "Epoch: 19, Samples: 2912/5760, Loss: 1.8020002841949463\n",
      "Epoch: 19, Samples: 2944/5760, Loss: 1.3875539302825928\n",
      "Epoch: 19, Samples: 2976/5760, Loss: 1.4666738510131836\n",
      "Epoch: 19, Samples: 3008/5760, Loss: 1.5061615705490112\n",
      "Epoch: 19, Samples: 3040/5760, Loss: 1.4283732175827026\n",
      "Epoch: 19, Samples: 3072/5760, Loss: 1.143669843673706\n",
      "Epoch: 19, Samples: 3104/5760, Loss: 0.938977837562561\n",
      "Epoch: 19, Samples: 3136/5760, Loss: 1.8739440441131592\n",
      "Epoch: 19, Samples: 3168/5760, Loss: 1.55756676197052\n",
      "Epoch: 19, Samples: 3200/5760, Loss: 1.0794421434402466\n",
      "Epoch: 19, Samples: 3232/5760, Loss: 1.2474751472473145\n",
      "Epoch: 19, Samples: 3264/5760, Loss: 1.32352614402771\n",
      "Epoch: 19, Samples: 3296/5760, Loss: 1.3928236961364746\n",
      "Epoch: 19, Samples: 3328/5760, Loss: 1.4725993871688843\n",
      "Epoch: 19, Samples: 3360/5760, Loss: 1.9671448469161987\n",
      "Epoch: 19, Samples: 3392/5760, Loss: 1.441070556640625\n",
      "Epoch: 19, Samples: 3424/5760, Loss: 1.6186954975128174\n",
      "Epoch: 19, Samples: 3456/5760, Loss: 1.0895893573760986\n",
      "Epoch: 19, Samples: 3488/5760, Loss: 1.153848648071289\n",
      "Epoch: 19, Samples: 3520/5760, Loss: 2.0735251903533936\n",
      "Epoch: 19, Samples: 3552/5760, Loss: 1.4611542224884033\n",
      "Epoch: 19, Samples: 3584/5760, Loss: 1.4593673944473267\n",
      "Epoch: 19, Samples: 3616/5760, Loss: 1.459643006324768\n",
      "Epoch: 19, Samples: 3648/5760, Loss: 1.8114029169082642\n",
      "Epoch: 19, Samples: 3680/5760, Loss: 1.520633339881897\n",
      "Epoch: 19, Samples: 3712/5760, Loss: 1.4781602621078491\n",
      "Epoch: 19, Samples: 3744/5760, Loss: 1.3253722190856934\n",
      "Epoch: 19, Samples: 3776/5760, Loss: 1.633392333984375\n",
      "Epoch: 19, Samples: 3808/5760, Loss: 1.3887758255004883\n",
      "Epoch: 19, Samples: 3840/5760, Loss: 1.746191143989563\n",
      "Epoch: 19, Samples: 3872/5760, Loss: 1.5336486101150513\n",
      "Epoch: 19, Samples: 3904/5760, Loss: 1.6289455890655518\n",
      "Epoch: 19, Samples: 3936/5760, Loss: 1.7408006191253662\n",
      "Epoch: 19, Samples: 3968/5760, Loss: 1.4815993309020996\n",
      "Epoch: 19, Samples: 4000/5760, Loss: 1.2784242630004883\n",
      "Epoch: 19, Samples: 4032/5760, Loss: 1.5334413051605225\n",
      "Epoch: 19, Samples: 4064/5760, Loss: 1.1619693040847778\n",
      "Epoch: 19, Samples: 4096/5760, Loss: 1.638974905014038\n",
      "Epoch: 19, Samples: 4128/5760, Loss: 1.5600990056991577\n",
      "Epoch: 19, Samples: 4160/5760, Loss: 1.5708818435668945\n",
      "Epoch: 19, Samples: 4192/5760, Loss: 1.7121831178665161\n",
      "Epoch: 19, Samples: 4224/5760, Loss: 1.5227079391479492\n",
      "Epoch: 19, Samples: 4256/5760, Loss: 1.534091591835022\n",
      "Epoch: 19, Samples: 4288/5760, Loss: 1.6301273107528687\n",
      "Epoch: 19, Samples: 4320/5760, Loss: 1.2390689849853516\n",
      "Epoch: 19, Samples: 4352/5760, Loss: 1.2168059349060059\n",
      "Epoch: 19, Samples: 4384/5760, Loss: 2.29679536819458\n",
      "Epoch: 19, Samples: 4416/5760, Loss: 1.147323489189148\n",
      "Epoch: 19, Samples: 4448/5760, Loss: 1.3284966945648193\n",
      "Epoch: 19, Samples: 4480/5760, Loss: 1.6589823961257935\n",
      "Epoch: 19, Samples: 4512/5760, Loss: 1.541085124015808\n",
      "Epoch: 19, Samples: 4544/5760, Loss: 1.904476523399353\n",
      "Epoch: 19, Samples: 4576/5760, Loss: 1.5039913654327393\n",
      "Epoch: 19, Samples: 4608/5760, Loss: 1.3465639352798462\n",
      "Epoch: 19, Samples: 4640/5760, Loss: 2.0043954849243164\n",
      "Epoch: 19, Samples: 4672/5760, Loss: 1.7974313497543335\n",
      "Epoch: 19, Samples: 4704/5760, Loss: 1.7867801189422607\n",
      "Epoch: 19, Samples: 4736/5760, Loss: 1.5539153814315796\n",
      "Epoch: 19, Samples: 4768/5760, Loss: 1.487746000289917\n",
      "Epoch: 19, Samples: 4800/5760, Loss: 1.6465864181518555\n",
      "Epoch: 19, Samples: 4832/5760, Loss: 1.327560544013977\n",
      "Epoch: 19, Samples: 4864/5760, Loss: 1.599117398262024\n",
      "Epoch: 19, Samples: 4896/5760, Loss: 1.2758853435516357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Samples: 4928/5760, Loss: 1.2931828498840332\n",
      "Epoch: 19, Samples: 4960/5760, Loss: 1.6940504312515259\n",
      "Epoch: 19, Samples: 4992/5760, Loss: 1.2833412885665894\n",
      "Epoch: 19, Samples: 5024/5760, Loss: 1.6649259328842163\n",
      "Epoch: 19, Samples: 5056/5760, Loss: 1.3676077127456665\n",
      "Epoch: 19, Samples: 5088/5760, Loss: 1.2293742895126343\n",
      "Epoch: 19, Samples: 5120/5760, Loss: 1.801966667175293\n",
      "Epoch: 19, Samples: 5152/5760, Loss: 1.727636694908142\n",
      "Epoch: 19, Samples: 5184/5760, Loss: 1.488801121711731\n",
      "Epoch: 19, Samples: 5216/5760, Loss: 1.7815513610839844\n",
      "Epoch: 19, Samples: 5248/5760, Loss: 1.3201096057891846\n",
      "Epoch: 19, Samples: 5280/5760, Loss: 0.9434020519256592\n",
      "Epoch: 19, Samples: 5312/5760, Loss: 1.6339408159255981\n",
      "Epoch: 19, Samples: 5344/5760, Loss: 1.359920859336853\n",
      "Epoch: 19, Samples: 5376/5760, Loss: 1.9635372161865234\n",
      "Epoch: 19, Samples: 5408/5760, Loss: 1.1836031675338745\n",
      "Epoch: 19, Samples: 5440/5760, Loss: 1.213452696800232\n",
      "Epoch: 19, Samples: 5472/5760, Loss: 1.743330717086792\n",
      "Epoch: 19, Samples: 5504/5760, Loss: 1.6893541812896729\n",
      "Epoch: 19, Samples: 5536/5760, Loss: 1.2757701873779297\n",
      "Epoch: 19, Samples: 5568/5760, Loss: 1.432188868522644\n",
      "Epoch: 19, Samples: 5600/5760, Loss: 1.2051544189453125\n",
      "Epoch: 19, Samples: 5632/5760, Loss: 2.1295783519744873\n",
      "Epoch: 19, Samples: 5664/5760, Loss: 1.4711997509002686\n",
      "Epoch: 19, Samples: 5696/5760, Loss: 1.3803799152374268\n",
      "Epoch: 19, Samples: 5728/5760, Loss: 2.505249500274658\n",
      "\n",
      "Epoch: 19\n",
      "Training set: Average loss: 1.5142\n",
      "Validation set: Average loss: 1.9094, Accuracy: 426/818 (52%)\n",
      "Saving model (epoch 19) with lowest validation loss: 1.9094030261039734\n",
      "Epoch: 20, Samples: 0/5760, Loss: 1.4940853118896484\n",
      "Epoch: 20, Samples: 32/5760, Loss: 1.8938206434249878\n",
      "Epoch: 20, Samples: 64/5760, Loss: 1.6411710977554321\n",
      "Epoch: 20, Samples: 96/5760, Loss: 1.6947624683380127\n",
      "Epoch: 20, Samples: 128/5760, Loss: 1.4963188171386719\n",
      "Epoch: 20, Samples: 160/5760, Loss: 1.6136295795440674\n",
      "Epoch: 20, Samples: 192/5760, Loss: 1.248500108718872\n",
      "Epoch: 20, Samples: 224/5760, Loss: 1.5105239152908325\n",
      "Epoch: 20, Samples: 256/5760, Loss: 1.275760531425476\n",
      "Epoch: 20, Samples: 288/5760, Loss: 1.5318734645843506\n",
      "Epoch: 20, Samples: 320/5760, Loss: 1.0384304523468018\n",
      "Epoch: 20, Samples: 352/5760, Loss: 1.2704089879989624\n",
      "Epoch: 20, Samples: 384/5760, Loss: 1.5436813831329346\n",
      "Epoch: 20, Samples: 416/5760, Loss: 1.4090518951416016\n",
      "Epoch: 20, Samples: 448/5760, Loss: 1.4009699821472168\n",
      "Epoch: 20, Samples: 480/5760, Loss: 1.2759257555007935\n",
      "Epoch: 20, Samples: 512/5760, Loss: 1.6481322050094604\n",
      "Epoch: 20, Samples: 544/5760, Loss: 1.2071216106414795\n",
      "Epoch: 20, Samples: 576/5760, Loss: 1.6266674995422363\n",
      "Epoch: 20, Samples: 608/5760, Loss: 1.5773696899414062\n",
      "Epoch: 20, Samples: 640/5760, Loss: 1.4093843698501587\n",
      "Epoch: 20, Samples: 672/5760, Loss: 1.2899980545043945\n",
      "Epoch: 20, Samples: 704/5760, Loss: 1.4695137739181519\n",
      "Epoch: 20, Samples: 736/5760, Loss: 1.4562749862670898\n",
      "Epoch: 20, Samples: 768/5760, Loss: 1.342821717262268\n",
      "Epoch: 20, Samples: 800/5760, Loss: 1.441820502281189\n",
      "Epoch: 20, Samples: 832/5760, Loss: 1.3185975551605225\n",
      "Epoch: 20, Samples: 864/5760, Loss: 1.3088264465332031\n",
      "Epoch: 20, Samples: 896/5760, Loss: 1.3319125175476074\n",
      "Epoch: 20, Samples: 928/5760, Loss: 1.4651658535003662\n",
      "Epoch: 20, Samples: 960/5760, Loss: 1.7128512859344482\n",
      "Epoch: 20, Samples: 992/5760, Loss: 1.4679486751556396\n",
      "Epoch: 20, Samples: 1024/5760, Loss: 1.2637412548065186\n",
      "Epoch: 20, Samples: 1056/5760, Loss: 1.7826696634292603\n",
      "Epoch: 20, Samples: 1088/5760, Loss: 1.4481682777404785\n",
      "Epoch: 20, Samples: 1120/5760, Loss: 1.2551777362823486\n",
      "Epoch: 20, Samples: 1152/5760, Loss: 1.7415030002593994\n",
      "Epoch: 20, Samples: 1184/5760, Loss: 0.9783589839935303\n",
      "Epoch: 20, Samples: 1216/5760, Loss: 1.399390459060669\n",
      "Epoch: 20, Samples: 1248/5760, Loss: 1.4402039051055908\n",
      "Epoch: 20, Samples: 1280/5760, Loss: 1.6724061965942383\n",
      "Epoch: 20, Samples: 1312/5760, Loss: 1.090032935142517\n",
      "Epoch: 20, Samples: 1344/5760, Loss: 1.3380059003829956\n",
      "Epoch: 20, Samples: 1376/5760, Loss: 1.3438626527786255\n",
      "Epoch: 20, Samples: 1408/5760, Loss: 1.563880443572998\n",
      "Epoch: 20, Samples: 1440/5760, Loss: 1.4654234647750854\n",
      "Epoch: 20, Samples: 1472/5760, Loss: 1.2920172214508057\n",
      "Epoch: 20, Samples: 1504/5760, Loss: 1.4952280521392822\n",
      "Epoch: 20, Samples: 1536/5760, Loss: 1.306291103363037\n",
      "Epoch: 20, Samples: 1568/5760, Loss: 1.462773323059082\n",
      "Epoch: 20, Samples: 1600/5760, Loss: 1.1418083906173706\n",
      "Epoch: 20, Samples: 1632/5760, Loss: 1.4635579586029053\n",
      "Epoch: 20, Samples: 1664/5760, Loss: 1.552860975265503\n",
      "Epoch: 20, Samples: 1696/5760, Loss: 1.9462041854858398\n",
      "Epoch: 20, Samples: 1728/5760, Loss: 1.2429842948913574\n",
      "Epoch: 20, Samples: 1760/5760, Loss: 1.0284802913665771\n",
      "Epoch: 20, Samples: 1792/5760, Loss: 1.3259050846099854\n",
      "Epoch: 20, Samples: 1824/5760, Loss: 1.1232398748397827\n",
      "Epoch: 20, Samples: 1856/5760, Loss: 1.1445517539978027\n",
      "Epoch: 20, Samples: 1888/5760, Loss: 1.4942377805709839\n",
      "Epoch: 20, Samples: 1920/5760, Loss: 1.462721824645996\n",
      "Epoch: 20, Samples: 1952/5760, Loss: 1.4441839456558228\n",
      "Epoch: 20, Samples: 1984/5760, Loss: 1.1833139657974243\n",
      "Epoch: 20, Samples: 2016/5760, Loss: 1.3297969102859497\n",
      "Epoch: 20, Samples: 2048/5760, Loss: 1.5402144193649292\n",
      "Epoch: 20, Samples: 2080/5760, Loss: 1.1053712368011475\n",
      "Epoch: 20, Samples: 2112/5760, Loss: 1.3792080879211426\n",
      "Epoch: 20, Samples: 2144/5760, Loss: 1.170668601989746\n",
      "Epoch: 20, Samples: 2176/5760, Loss: 1.5472965240478516\n",
      "Epoch: 20, Samples: 2208/5760, Loss: 1.456618309020996\n",
      "Epoch: 20, Samples: 2240/5760, Loss: 1.55983304977417\n",
      "Epoch: 20, Samples: 2272/5760, Loss: 1.3507599830627441\n",
      "Epoch: 20, Samples: 2304/5760, Loss: 1.2296700477600098\n",
      "Epoch: 20, Samples: 2336/5760, Loss: 1.5038055181503296\n",
      "Epoch: 20, Samples: 2368/5760, Loss: 1.4122347831726074\n",
      "Epoch: 20, Samples: 2400/5760, Loss: 1.358879804611206\n",
      "Epoch: 20, Samples: 2432/5760, Loss: 1.1984717845916748\n",
      "Epoch: 20, Samples: 2464/5760, Loss: 1.1746115684509277\n",
      "Epoch: 20, Samples: 2496/5760, Loss: 1.3600517511367798\n",
      "Epoch: 20, Samples: 2528/5760, Loss: 1.3295347690582275\n",
      "Epoch: 20, Samples: 2560/5760, Loss: 1.62880539894104\n",
      "Epoch: 20, Samples: 2592/5760, Loss: 1.6351006031036377\n",
      "Epoch: 20, Samples: 2624/5760, Loss: 1.2810440063476562\n",
      "Epoch: 20, Samples: 2656/5760, Loss: 1.5673049688339233\n",
      "Epoch: 20, Samples: 2688/5760, Loss: 2.136598825454712\n",
      "Epoch: 20, Samples: 2720/5760, Loss: 1.5422693490982056\n",
      "Epoch: 20, Samples: 2752/5760, Loss: 1.3742611408233643\n",
      "Epoch: 20, Samples: 2784/5760, Loss: 1.0687347650527954\n",
      "Epoch: 20, Samples: 2816/5760, Loss: 1.151511788368225\n",
      "Epoch: 20, Samples: 2848/5760, Loss: 1.3954414129257202\n",
      "Epoch: 20, Samples: 2880/5760, Loss: 1.5155659914016724\n",
      "Epoch: 20, Samples: 2912/5760, Loss: 1.430795669555664\n",
      "Epoch: 20, Samples: 2944/5760, Loss: 1.53952157497406\n",
      "Epoch: 20, Samples: 2976/5760, Loss: 1.5479565858840942\n",
      "Epoch: 20, Samples: 3008/5760, Loss: 1.1696295738220215\n",
      "Epoch: 20, Samples: 3040/5760, Loss: 1.7674731016159058\n",
      "Epoch: 20, Samples: 3072/5760, Loss: 1.2569425106048584\n",
      "Epoch: 20, Samples: 3104/5760, Loss: 1.5294463634490967\n",
      "Epoch: 20, Samples: 3136/5760, Loss: 1.4162578582763672\n",
      "Epoch: 20, Samples: 3168/5760, Loss: 1.4822583198547363\n",
      "Epoch: 20, Samples: 3200/5760, Loss: 1.6060258150100708\n",
      "Epoch: 20, Samples: 3232/5760, Loss: 1.4317532777786255\n",
      "Epoch: 20, Samples: 3264/5760, Loss: 1.142223596572876\n",
      "Epoch: 20, Samples: 3296/5760, Loss: 1.6527934074401855\n",
      "Epoch: 20, Samples: 3328/5760, Loss: 1.795469045639038\n",
      "Epoch: 20, Samples: 3360/5760, Loss: 1.5208275318145752\n",
      "Epoch: 20, Samples: 3392/5760, Loss: 1.1370357275009155\n",
      "Epoch: 20, Samples: 3424/5760, Loss: 1.7683608531951904\n",
      "Epoch: 20, Samples: 3456/5760, Loss: 1.3895745277404785\n",
      "Epoch: 20, Samples: 3488/5760, Loss: 1.1033459901809692\n",
      "Epoch: 20, Samples: 3520/5760, Loss: 1.0057209730148315\n",
      "Epoch: 20, Samples: 3552/5760, Loss: 1.5819942951202393\n",
      "Epoch: 20, Samples: 3584/5760, Loss: 1.8727760314941406\n",
      "Epoch: 20, Samples: 3616/5760, Loss: 1.5215520858764648\n",
      "Epoch: 20, Samples: 3648/5760, Loss: 1.3538635969161987\n",
      "Epoch: 20, Samples: 3680/5760, Loss: 1.5436712503433228\n",
      "Epoch: 20, Samples: 3712/5760, Loss: 1.3951903581619263\n",
      "Epoch: 20, Samples: 3744/5760, Loss: 1.1128411293029785\n",
      "Epoch: 20, Samples: 3776/5760, Loss: 1.001560926437378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Samples: 3808/5760, Loss: 1.236235499382019\n",
      "Epoch: 20, Samples: 3840/5760, Loss: 1.4102282524108887\n",
      "Epoch: 20, Samples: 3872/5760, Loss: 1.6822608709335327\n",
      "Epoch: 20, Samples: 3904/5760, Loss: 1.501030445098877\n",
      "Epoch: 20, Samples: 3936/5760, Loss: 1.4788378477096558\n",
      "Epoch: 20, Samples: 3968/5760, Loss: 1.035670280456543\n",
      "Epoch: 20, Samples: 4000/5760, Loss: 1.569867491722107\n",
      "Epoch: 20, Samples: 4032/5760, Loss: 1.7681058645248413\n",
      "Epoch: 20, Samples: 4064/5760, Loss: 1.4882690906524658\n",
      "Epoch: 20, Samples: 4096/5760, Loss: 1.5307129621505737\n",
      "Epoch: 20, Samples: 4128/5760, Loss: 1.5014252662658691\n",
      "Epoch: 20, Samples: 4160/5760, Loss: 1.3055168390274048\n",
      "Epoch: 20, Samples: 4192/5760, Loss: 1.4157297611236572\n",
      "Epoch: 20, Samples: 4224/5760, Loss: 1.6561022996902466\n",
      "Epoch: 20, Samples: 4256/5760, Loss: 1.5289769172668457\n",
      "Epoch: 20, Samples: 4288/5760, Loss: 1.5162677764892578\n",
      "Epoch: 20, Samples: 4320/5760, Loss: 1.422882318496704\n",
      "Epoch: 20, Samples: 4352/5760, Loss: 1.4561378955841064\n",
      "Epoch: 20, Samples: 4384/5760, Loss: 1.9437482357025146\n",
      "Epoch: 20, Samples: 4416/5760, Loss: 0.9381401538848877\n",
      "Epoch: 20, Samples: 4448/5760, Loss: 1.4692264795303345\n",
      "Epoch: 20, Samples: 4480/5760, Loss: 1.3433247804641724\n",
      "Epoch: 20, Samples: 4512/5760, Loss: 0.8794128894805908\n",
      "Epoch: 20, Samples: 4544/5760, Loss: 1.435120701789856\n",
      "Epoch: 20, Samples: 4576/5760, Loss: 1.3498950004577637\n",
      "Epoch: 20, Samples: 4608/5760, Loss: 1.6355409622192383\n",
      "Epoch: 20, Samples: 4640/5760, Loss: 1.9631738662719727\n",
      "Epoch: 20, Samples: 4672/5760, Loss: 1.1483179330825806\n",
      "Epoch: 20, Samples: 4704/5760, Loss: 1.1636559963226318\n",
      "Epoch: 20, Samples: 4736/5760, Loss: 1.4944713115692139\n",
      "Epoch: 20, Samples: 4768/5760, Loss: 1.3085218667984009\n",
      "Epoch: 20, Samples: 4800/5760, Loss: 1.4111835956573486\n",
      "Epoch: 20, Samples: 4832/5760, Loss: 1.2078857421875\n",
      "Epoch: 20, Samples: 4864/5760, Loss: 1.5112181901931763\n",
      "Epoch: 20, Samples: 4896/5760, Loss: 1.6154415607452393\n",
      "Epoch: 20, Samples: 4928/5760, Loss: 1.920580267906189\n",
      "Epoch: 20, Samples: 4960/5760, Loss: 1.6521601676940918\n",
      "Epoch: 20, Samples: 4992/5760, Loss: 1.285914421081543\n",
      "Epoch: 20, Samples: 5024/5760, Loss: 1.4929778575897217\n",
      "Epoch: 20, Samples: 5056/5760, Loss: 1.2172801494598389\n",
      "Epoch: 20, Samples: 5088/5760, Loss: 1.0387095212936401\n",
      "Epoch: 20, Samples: 5120/5760, Loss: 1.3377444744110107\n",
      "Epoch: 20, Samples: 5152/5760, Loss: 1.3050376176834106\n",
      "Epoch: 20, Samples: 5184/5760, Loss: 1.3928163051605225\n",
      "Epoch: 20, Samples: 5216/5760, Loss: 1.4749243259429932\n",
      "Epoch: 20, Samples: 5248/5760, Loss: 1.2960702180862427\n",
      "Epoch: 20, Samples: 5280/5760, Loss: 1.4155604839324951\n",
      "Epoch: 20, Samples: 5312/5760, Loss: 1.9948028326034546\n",
      "Epoch: 20, Samples: 5344/5760, Loss: 1.5700361728668213\n",
      "Epoch: 20, Samples: 5376/5760, Loss: 1.5790992975234985\n",
      "Epoch: 20, Samples: 5408/5760, Loss: 1.0338526964187622\n",
      "Epoch: 20, Samples: 5440/5760, Loss: 1.0699641704559326\n",
      "Epoch: 20, Samples: 5472/5760, Loss: 1.8378188610076904\n",
      "Epoch: 20, Samples: 5504/5760, Loss: 1.243490219116211\n",
      "Epoch: 20, Samples: 5536/5760, Loss: 1.1755571365356445\n",
      "Epoch: 20, Samples: 5568/5760, Loss: 1.6774916648864746\n",
      "Epoch: 20, Samples: 5600/5760, Loss: 1.3892762660980225\n",
      "Epoch: 20, Samples: 5632/5760, Loss: 1.4875527620315552\n",
      "Epoch: 20, Samples: 5664/5760, Loss: 1.5321440696716309\n",
      "Epoch: 20, Samples: 5696/5760, Loss: 1.4123446941375732\n",
      "Epoch: 20, Samples: 5728/5760, Loss: 3.1688313484191895\n",
      "\n",
      "Epoch: 20\n",
      "Training set: Average loss: 1.4334\n",
      "Validation set: Average loss: 2.1752, Accuracy: 371/818 (45%)\n",
      "Epoch: 21, Samples: 0/5760, Loss: 1.2284541130065918\n",
      "Epoch: 21, Samples: 32/5760, Loss: 1.1953593492507935\n",
      "Epoch: 21, Samples: 64/5760, Loss: 0.991722583770752\n",
      "Epoch: 21, Samples: 96/5760, Loss: 1.285194754600525\n",
      "Epoch: 21, Samples: 128/5760, Loss: 1.4091860055923462\n",
      "Epoch: 21, Samples: 160/5760, Loss: 1.3539502620697021\n",
      "Epoch: 21, Samples: 192/5760, Loss: 1.3814526796340942\n",
      "Epoch: 21, Samples: 224/5760, Loss: 1.672055721282959\n",
      "Epoch: 21, Samples: 256/5760, Loss: 1.3929321765899658\n",
      "Epoch: 21, Samples: 288/5760, Loss: 0.966152548789978\n",
      "Epoch: 21, Samples: 320/5760, Loss: 1.5317306518554688\n",
      "Epoch: 21, Samples: 352/5760, Loss: 1.714179515838623\n",
      "Epoch: 21, Samples: 384/5760, Loss: 1.5079421997070312\n",
      "Epoch: 21, Samples: 416/5760, Loss: 1.5030899047851562\n",
      "Epoch: 21, Samples: 448/5760, Loss: 1.2548108100891113\n",
      "Epoch: 21, Samples: 480/5760, Loss: 1.392600655555725\n",
      "Epoch: 21, Samples: 512/5760, Loss: 1.3773571252822876\n",
      "Epoch: 21, Samples: 544/5760, Loss: 1.3173177242279053\n",
      "Epoch: 21, Samples: 576/5760, Loss: 1.4788928031921387\n",
      "Epoch: 21, Samples: 608/5760, Loss: 1.6375913619995117\n",
      "Epoch: 21, Samples: 640/5760, Loss: 1.2158416509628296\n",
      "Epoch: 21, Samples: 672/5760, Loss: 1.3655471801757812\n",
      "Epoch: 21, Samples: 704/5760, Loss: 1.211374282836914\n",
      "Epoch: 21, Samples: 736/5760, Loss: 1.6662696599960327\n",
      "Epoch: 21, Samples: 768/5760, Loss: 1.4031429290771484\n",
      "Epoch: 21, Samples: 800/5760, Loss: 2.103780746459961\n",
      "Epoch: 21, Samples: 832/5760, Loss: 1.4387855529785156\n",
      "Epoch: 21, Samples: 864/5760, Loss: 1.6577420234680176\n",
      "Epoch: 21, Samples: 896/5760, Loss: 1.1610174179077148\n",
      "Epoch: 21, Samples: 928/5760, Loss: 1.650210976600647\n",
      "Epoch: 21, Samples: 960/5760, Loss: 1.8331249952316284\n",
      "Epoch: 21, Samples: 992/5760, Loss: 0.9596284627914429\n",
      "Epoch: 21, Samples: 1024/5760, Loss: 1.5833239555358887\n",
      "Epoch: 21, Samples: 1056/5760, Loss: 1.2722924947738647\n",
      "Epoch: 21, Samples: 1088/5760, Loss: 1.4276686906814575\n",
      "Epoch: 21, Samples: 1120/5760, Loss: 1.0747324228286743\n",
      "Epoch: 21, Samples: 1152/5760, Loss: 1.6457977294921875\n",
      "Epoch: 21, Samples: 1184/5760, Loss: 1.5639665126800537\n",
      "Epoch: 21, Samples: 1216/5760, Loss: 1.1151657104492188\n",
      "Epoch: 21, Samples: 1248/5760, Loss: 1.3352859020233154\n",
      "Epoch: 21, Samples: 1280/5760, Loss: 1.2521172761917114\n",
      "Epoch: 21, Samples: 1312/5760, Loss: 1.2270535230636597\n",
      "Epoch: 21, Samples: 1344/5760, Loss: 1.1894984245300293\n",
      "Epoch: 21, Samples: 1376/5760, Loss: 1.6573820114135742\n",
      "Epoch: 21, Samples: 1408/5760, Loss: 1.767835259437561\n",
      "Epoch: 21, Samples: 1440/5760, Loss: 1.5283432006835938\n",
      "Epoch: 21, Samples: 1472/5760, Loss: 1.168955683708191\n",
      "Epoch: 21, Samples: 1504/5760, Loss: 1.0762443542480469\n",
      "Epoch: 21, Samples: 1536/5760, Loss: 1.5340975522994995\n",
      "Epoch: 21, Samples: 1568/5760, Loss: 1.3630144596099854\n",
      "Epoch: 21, Samples: 1600/5760, Loss: 1.501835584640503\n",
      "Epoch: 21, Samples: 1632/5760, Loss: 1.079287052154541\n",
      "Epoch: 21, Samples: 1664/5760, Loss: 1.066352128982544\n",
      "Epoch: 21, Samples: 1696/5760, Loss: 1.428636074066162\n",
      "Epoch: 21, Samples: 1728/5760, Loss: 1.5545004606246948\n",
      "Epoch: 21, Samples: 1760/5760, Loss: 2.035541296005249\n",
      "Epoch: 21, Samples: 1792/5760, Loss: 1.1266909837722778\n",
      "Epoch: 21, Samples: 1824/5760, Loss: 1.2034707069396973\n",
      "Epoch: 21, Samples: 1856/5760, Loss: 1.0959248542785645\n",
      "Epoch: 21, Samples: 1888/5760, Loss: 1.4485222101211548\n",
      "Epoch: 21, Samples: 1920/5760, Loss: 0.9657366871833801\n",
      "Epoch: 21, Samples: 1952/5760, Loss: 1.4113476276397705\n",
      "Epoch: 21, Samples: 1984/5760, Loss: 1.2218905687332153\n",
      "Epoch: 21, Samples: 2016/5760, Loss: 0.9094977378845215\n",
      "Epoch: 21, Samples: 2048/5760, Loss: 1.1893130540847778\n",
      "Epoch: 21, Samples: 2080/5760, Loss: 1.339613437652588\n",
      "Epoch: 21, Samples: 2112/5760, Loss: 0.9269298911094666\n",
      "Epoch: 21, Samples: 2144/5760, Loss: 1.6828038692474365\n",
      "Epoch: 21, Samples: 2176/5760, Loss: 1.3251495361328125\n",
      "Epoch: 21, Samples: 2208/5760, Loss: 1.3463212251663208\n",
      "Epoch: 21, Samples: 2240/5760, Loss: 1.3061310052871704\n",
      "Epoch: 21, Samples: 2272/5760, Loss: 1.4345279932022095\n",
      "Epoch: 21, Samples: 2304/5760, Loss: 1.5612674951553345\n",
      "Epoch: 21, Samples: 2336/5760, Loss: 1.256716251373291\n",
      "Epoch: 21, Samples: 2368/5760, Loss: 1.2580143213272095\n",
      "Epoch: 21, Samples: 2400/5760, Loss: 1.358265995979309\n",
      "Epoch: 21, Samples: 2432/5760, Loss: 1.2593600749969482\n",
      "Epoch: 21, Samples: 2464/5760, Loss: 1.7575318813323975\n",
      "Epoch: 21, Samples: 2496/5760, Loss: 1.807053804397583\n",
      "Epoch: 21, Samples: 2528/5760, Loss: 1.3324875831604004\n",
      "Epoch: 21, Samples: 2560/5760, Loss: 1.7011706829071045\n",
      "Epoch: 21, Samples: 2592/5760, Loss: 1.0902987718582153\n",
      "Epoch: 21, Samples: 2624/5760, Loss: 1.3752473592758179\n",
      "Epoch: 21, Samples: 2656/5760, Loss: 1.2251060009002686\n",
      "Epoch: 21, Samples: 2688/5760, Loss: 1.242194652557373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Samples: 2720/5760, Loss: 1.4655582904815674\n",
      "Epoch: 21, Samples: 2752/5760, Loss: 1.5567761659622192\n",
      "Epoch: 21, Samples: 2784/5760, Loss: 2.147684097290039\n",
      "Epoch: 21, Samples: 2816/5760, Loss: 1.3637540340423584\n",
      "Epoch: 21, Samples: 2848/5760, Loss: 1.2771189212799072\n",
      "Epoch: 21, Samples: 2880/5760, Loss: 1.0818878412246704\n",
      "Epoch: 21, Samples: 2912/5760, Loss: 1.3070145845413208\n",
      "Epoch: 21, Samples: 2944/5760, Loss: 1.2509865760803223\n",
      "Epoch: 21, Samples: 2976/5760, Loss: 1.8003003597259521\n",
      "Epoch: 21, Samples: 3008/5760, Loss: 1.760238528251648\n",
      "Epoch: 21, Samples: 3040/5760, Loss: 1.738271951675415\n",
      "Epoch: 21, Samples: 3072/5760, Loss: 0.9274934530258179\n",
      "Epoch: 21, Samples: 3104/5760, Loss: 1.176619529724121\n",
      "Epoch: 21, Samples: 3136/5760, Loss: 1.579709529876709\n",
      "Epoch: 21, Samples: 3168/5760, Loss: 1.1980807781219482\n",
      "Epoch: 21, Samples: 3200/5760, Loss: 1.7319269180297852\n",
      "Epoch: 21, Samples: 3232/5760, Loss: 1.2186217308044434\n",
      "Epoch: 21, Samples: 3264/5760, Loss: 1.5291476249694824\n",
      "Epoch: 21, Samples: 3296/5760, Loss: 1.527531623840332\n",
      "Epoch: 21, Samples: 3328/5760, Loss: 1.6664226055145264\n",
      "Epoch: 21, Samples: 3360/5760, Loss: 0.9116745591163635\n",
      "Epoch: 21, Samples: 3392/5760, Loss: 1.4762349128723145\n",
      "Epoch: 21, Samples: 3424/5760, Loss: 1.7152924537658691\n",
      "Epoch: 21, Samples: 3456/5760, Loss: 1.5510122776031494\n",
      "Epoch: 21, Samples: 3488/5760, Loss: 1.338810682296753\n",
      "Epoch: 21, Samples: 3520/5760, Loss: 1.6715370416641235\n",
      "Epoch: 21, Samples: 3552/5760, Loss: 1.43099045753479\n",
      "Epoch: 21, Samples: 3584/5760, Loss: 1.5298357009887695\n",
      "Epoch: 21, Samples: 3616/5760, Loss: 1.2576191425323486\n",
      "Epoch: 21, Samples: 3648/5760, Loss: 1.7095890045166016\n",
      "Epoch: 21, Samples: 3680/5760, Loss: 1.4486825466156006\n",
      "Epoch: 21, Samples: 3712/5760, Loss: 1.193161129951477\n",
      "Epoch: 21, Samples: 3744/5760, Loss: 1.1383129358291626\n",
      "Epoch: 21, Samples: 3776/5760, Loss: 0.8536845445632935\n",
      "Epoch: 21, Samples: 3808/5760, Loss: 1.5528520345687866\n",
      "Epoch: 21, Samples: 3840/5760, Loss: 1.195737361907959\n",
      "Epoch: 21, Samples: 3872/5760, Loss: 0.7694000005722046\n",
      "Epoch: 21, Samples: 3904/5760, Loss: 1.0864001512527466\n",
      "Epoch: 21, Samples: 3936/5760, Loss: 1.4710160493850708\n",
      "Epoch: 21, Samples: 3968/5760, Loss: 1.5869994163513184\n",
      "Epoch: 21, Samples: 4000/5760, Loss: 1.2972054481506348\n",
      "Epoch: 21, Samples: 4032/5760, Loss: 1.8064489364624023\n",
      "Epoch: 21, Samples: 4064/5760, Loss: 1.2831158638000488\n",
      "Epoch: 21, Samples: 4096/5760, Loss: 1.353203535079956\n",
      "Epoch: 21, Samples: 4128/5760, Loss: 1.255450963973999\n",
      "Epoch: 21, Samples: 4160/5760, Loss: 1.2207648754119873\n",
      "Epoch: 21, Samples: 4192/5760, Loss: 1.473607063293457\n",
      "Epoch: 21, Samples: 4224/5760, Loss: 1.1586108207702637\n",
      "Epoch: 21, Samples: 4256/5760, Loss: 1.5083539485931396\n",
      "Epoch: 21, Samples: 4288/5760, Loss: 1.4091219902038574\n",
      "Epoch: 21, Samples: 4320/5760, Loss: 1.7359007596969604\n",
      "Epoch: 21, Samples: 4352/5760, Loss: 1.1472176313400269\n",
      "Epoch: 21, Samples: 4384/5760, Loss: 1.5080924034118652\n",
      "Epoch: 21, Samples: 4416/5760, Loss: 0.8295450210571289\n",
      "Epoch: 21, Samples: 4448/5760, Loss: 0.9069849848747253\n",
      "Epoch: 21, Samples: 4480/5760, Loss: 1.5410369634628296\n",
      "Epoch: 21, Samples: 4512/5760, Loss: 1.3629438877105713\n",
      "Epoch: 21, Samples: 4544/5760, Loss: 1.6323018074035645\n",
      "Epoch: 21, Samples: 4576/5760, Loss: 1.4312894344329834\n",
      "Epoch: 21, Samples: 4608/5760, Loss: 1.405930757522583\n",
      "Epoch: 21, Samples: 4640/5760, Loss: 1.4888471364974976\n",
      "Epoch: 21, Samples: 4672/5760, Loss: 1.3887139558792114\n",
      "Epoch: 21, Samples: 4704/5760, Loss: 1.2140990495681763\n",
      "Epoch: 21, Samples: 4736/5760, Loss: 1.578995943069458\n",
      "Epoch: 21, Samples: 4768/5760, Loss: 1.387763261795044\n",
      "Epoch: 21, Samples: 4800/5760, Loss: 1.6758540868759155\n",
      "Epoch: 21, Samples: 4832/5760, Loss: 1.6033687591552734\n",
      "Epoch: 21, Samples: 4864/5760, Loss: 1.3824387788772583\n",
      "Epoch: 21, Samples: 4896/5760, Loss: 1.2967506647109985\n",
      "Epoch: 21, Samples: 4928/5760, Loss: 1.1846036911010742\n",
      "Epoch: 21, Samples: 4960/5760, Loss: 1.2927889823913574\n",
      "Epoch: 21, Samples: 4992/5760, Loss: 1.1406201124191284\n",
      "Epoch: 21, Samples: 5024/5760, Loss: 1.0325120687484741\n",
      "Epoch: 21, Samples: 5056/5760, Loss: 1.3687870502471924\n",
      "Epoch: 21, Samples: 5088/5760, Loss: 1.8038297891616821\n",
      "Epoch: 21, Samples: 5120/5760, Loss: 1.2652771472930908\n",
      "Epoch: 21, Samples: 5152/5760, Loss: 1.5598524808883667\n",
      "Epoch: 21, Samples: 5184/5760, Loss: 1.14728844165802\n",
      "Epoch: 21, Samples: 5216/5760, Loss: 1.1946964263916016\n",
      "Epoch: 21, Samples: 5248/5760, Loss: 1.1951231956481934\n",
      "Epoch: 21, Samples: 5280/5760, Loss: 1.5584197044372559\n",
      "Epoch: 21, Samples: 5312/5760, Loss: 1.1243356466293335\n",
      "Epoch: 21, Samples: 5344/5760, Loss: 1.1290466785430908\n",
      "Epoch: 21, Samples: 5376/5760, Loss: 1.0916764736175537\n",
      "Epoch: 21, Samples: 5408/5760, Loss: 1.5453253984451294\n",
      "Epoch: 21, Samples: 5440/5760, Loss: 1.2878081798553467\n",
      "Epoch: 21, Samples: 5472/5760, Loss: 1.0965397357940674\n",
      "Epoch: 21, Samples: 5504/5760, Loss: 1.3422057628631592\n",
      "Epoch: 21, Samples: 5536/5760, Loss: 1.3085391521453857\n",
      "Epoch: 21, Samples: 5568/5760, Loss: 1.4487661123275757\n",
      "Epoch: 21, Samples: 5600/5760, Loss: 1.3003675937652588\n",
      "Epoch: 21, Samples: 5632/5760, Loss: 1.1330441236495972\n",
      "Epoch: 21, Samples: 5664/5760, Loss: 0.6773340702056885\n",
      "Epoch: 21, Samples: 5696/5760, Loss: 1.586806058883667\n",
      "Epoch: 21, Samples: 5728/5760, Loss: 2.979785919189453\n",
      "\n",
      "Epoch: 21\n",
      "Training set: Average loss: 1.3786\n",
      "Validation set: Average loss: 3.5128, Accuracy: 242/818 (30%)\n",
      "Epoch: 22, Samples: 0/5760, Loss: 1.0402547121047974\n",
      "Epoch: 22, Samples: 32/5760, Loss: 1.424845814704895\n",
      "Epoch: 22, Samples: 64/5760, Loss: 0.9617870450019836\n",
      "Epoch: 22, Samples: 96/5760, Loss: 1.6789456605911255\n",
      "Epoch: 22, Samples: 128/5760, Loss: 1.6127252578735352\n",
      "Epoch: 22, Samples: 160/5760, Loss: 1.2841442823410034\n",
      "Epoch: 22, Samples: 192/5760, Loss: 1.3624401092529297\n",
      "Epoch: 22, Samples: 224/5760, Loss: 1.234188199043274\n",
      "Epoch: 22, Samples: 256/5760, Loss: 1.063401460647583\n",
      "Epoch: 22, Samples: 288/5760, Loss: 1.3556897640228271\n",
      "Epoch: 22, Samples: 320/5760, Loss: 1.1133683919906616\n",
      "Epoch: 22, Samples: 352/5760, Loss: 2.0890798568725586\n",
      "Epoch: 22, Samples: 384/5760, Loss: 1.5024113655090332\n",
      "Epoch: 22, Samples: 416/5760, Loss: 1.3693783283233643\n",
      "Epoch: 22, Samples: 448/5760, Loss: 0.9525951147079468\n",
      "Epoch: 22, Samples: 480/5760, Loss: 1.2352139949798584\n",
      "Epoch: 22, Samples: 512/5760, Loss: 1.340327501296997\n",
      "Epoch: 22, Samples: 544/5760, Loss: 1.0732451677322388\n",
      "Epoch: 22, Samples: 576/5760, Loss: 1.0667402744293213\n",
      "Epoch: 22, Samples: 608/5760, Loss: 1.2416694164276123\n",
      "Epoch: 22, Samples: 640/5760, Loss: 1.3351248502731323\n",
      "Epoch: 22, Samples: 672/5760, Loss: 1.5625221729278564\n",
      "Epoch: 22, Samples: 704/5760, Loss: 1.073975682258606\n",
      "Epoch: 22, Samples: 736/5760, Loss: 1.4416455030441284\n",
      "Epoch: 22, Samples: 768/5760, Loss: 1.0787030458450317\n",
      "Epoch: 22, Samples: 800/5760, Loss: 1.3687629699707031\n",
      "Epoch: 22, Samples: 832/5760, Loss: 1.2926084995269775\n",
      "Epoch: 22, Samples: 864/5760, Loss: 1.478471279144287\n",
      "Epoch: 22, Samples: 896/5760, Loss: 0.9550366401672363\n",
      "Epoch: 22, Samples: 928/5760, Loss: 1.6991565227508545\n",
      "Epoch: 22, Samples: 960/5760, Loss: 1.2937874794006348\n",
      "Epoch: 22, Samples: 992/5760, Loss: 1.0902801752090454\n",
      "Epoch: 22, Samples: 1024/5760, Loss: 1.3203238248825073\n",
      "Epoch: 22, Samples: 1056/5760, Loss: 1.445991039276123\n",
      "Epoch: 22, Samples: 1088/5760, Loss: 1.6811059713363647\n",
      "Epoch: 22, Samples: 1120/5760, Loss: 0.9463232755661011\n",
      "Epoch: 22, Samples: 1152/5760, Loss: 1.513002872467041\n",
      "Epoch: 22, Samples: 1184/5760, Loss: 1.0398938655853271\n",
      "Epoch: 22, Samples: 1216/5760, Loss: 1.4298458099365234\n",
      "Epoch: 22, Samples: 1248/5760, Loss: 1.217360019683838\n",
      "Epoch: 22, Samples: 1280/5760, Loss: 1.0861461162567139\n",
      "Epoch: 22, Samples: 1312/5760, Loss: 1.4227279424667358\n",
      "Epoch: 22, Samples: 1344/5760, Loss: 1.2699873447418213\n",
      "Epoch: 22, Samples: 1376/5760, Loss: 1.2381548881530762\n",
      "Epoch: 22, Samples: 1408/5760, Loss: 1.206920862197876\n",
      "Epoch: 22, Samples: 1440/5760, Loss: 1.198185682296753\n",
      "Epoch: 22, Samples: 1472/5760, Loss: 1.3217014074325562\n",
      "Epoch: 22, Samples: 1504/5760, Loss: 1.1255137920379639\n",
      "Epoch: 22, Samples: 1536/5760, Loss: 1.4792513847351074\n",
      "Epoch: 22, Samples: 1568/5760, Loss: 1.3803505897521973\n",
      "Epoch: 22, Samples: 1600/5760, Loss: 1.4548684358596802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Samples: 1632/5760, Loss: 1.3590730428695679\n",
      "Epoch: 22, Samples: 1664/5760, Loss: 1.1223911046981812\n",
      "Epoch: 22, Samples: 1696/5760, Loss: 1.3361971378326416\n",
      "Epoch: 22, Samples: 1728/5760, Loss: 1.0001435279846191\n",
      "Epoch: 22, Samples: 1760/5760, Loss: 1.0469403266906738\n",
      "Epoch: 22, Samples: 1792/5760, Loss: 1.4352959394454956\n",
      "Epoch: 22, Samples: 1824/5760, Loss: 1.4058455228805542\n",
      "Epoch: 22, Samples: 1856/5760, Loss: 1.1371442079544067\n",
      "Epoch: 22, Samples: 1888/5760, Loss: 1.7421777248382568\n",
      "Epoch: 22, Samples: 1920/5760, Loss: 1.2064623832702637\n",
      "Epoch: 22, Samples: 1952/5760, Loss: 1.2822926044464111\n",
      "Epoch: 22, Samples: 1984/5760, Loss: 1.3111183643341064\n",
      "Epoch: 22, Samples: 2016/5760, Loss: 1.1070224046707153\n",
      "Epoch: 22, Samples: 2048/5760, Loss: 1.3656235933303833\n",
      "Epoch: 22, Samples: 2080/5760, Loss: 1.6239166259765625\n",
      "Epoch: 22, Samples: 2112/5760, Loss: 1.5885778665542603\n",
      "Epoch: 22, Samples: 2144/5760, Loss: 1.2015994787216187\n",
      "Epoch: 22, Samples: 2176/5760, Loss: 1.421207070350647\n",
      "Epoch: 22, Samples: 2208/5760, Loss: 1.2706724405288696\n",
      "Epoch: 22, Samples: 2240/5760, Loss: 1.3687596321105957\n",
      "Epoch: 22, Samples: 2272/5760, Loss: 1.7027602195739746\n",
      "Epoch: 22, Samples: 2304/5760, Loss: 1.4730275869369507\n",
      "Epoch: 22, Samples: 2336/5760, Loss: 0.9741382598876953\n",
      "Epoch: 22, Samples: 2368/5760, Loss: 1.2132893800735474\n",
      "Epoch: 22, Samples: 2400/5760, Loss: 1.1511752605438232\n",
      "Epoch: 22, Samples: 2432/5760, Loss: 1.2964160442352295\n",
      "Epoch: 22, Samples: 2464/5760, Loss: 1.7101860046386719\n",
      "Epoch: 22, Samples: 2496/5760, Loss: 1.8180269002914429\n",
      "Epoch: 22, Samples: 2528/5760, Loss: 1.3337128162384033\n",
      "Epoch: 22, Samples: 2560/5760, Loss: 1.2512859106063843\n",
      "Epoch: 22, Samples: 2592/5760, Loss: 1.103804111480713\n",
      "Epoch: 22, Samples: 2624/5760, Loss: 1.462177038192749\n",
      "Epoch: 22, Samples: 2656/5760, Loss: 1.440972089767456\n",
      "Epoch: 22, Samples: 2688/5760, Loss: 1.3344779014587402\n",
      "Epoch: 22, Samples: 2720/5760, Loss: 1.058932900428772\n",
      "Epoch: 22, Samples: 2752/5760, Loss: 1.0247929096221924\n",
      "Epoch: 22, Samples: 2784/5760, Loss: 0.9479058980941772\n",
      "Epoch: 22, Samples: 2816/5760, Loss: 1.4001927375793457\n",
      "Epoch: 22, Samples: 2848/5760, Loss: 1.1811429262161255\n",
      "Epoch: 22, Samples: 2880/5760, Loss: 1.2931002378463745\n",
      "Epoch: 22, Samples: 2912/5760, Loss: 1.2514774799346924\n",
      "Epoch: 22, Samples: 2944/5760, Loss: 1.0938997268676758\n",
      "Epoch: 22, Samples: 2976/5760, Loss: 1.296699047088623\n",
      "Epoch: 22, Samples: 3008/5760, Loss: 1.563488483428955\n",
      "Epoch: 22, Samples: 3040/5760, Loss: 0.9099352359771729\n",
      "Epoch: 22, Samples: 3072/5760, Loss: 1.281071424484253\n",
      "Epoch: 22, Samples: 3104/5760, Loss: 1.2512238025665283\n",
      "Epoch: 22, Samples: 3136/5760, Loss: 1.167752742767334\n",
      "Epoch: 22, Samples: 3168/5760, Loss: 1.5756261348724365\n",
      "Epoch: 22, Samples: 3200/5760, Loss: 1.2230925559997559\n",
      "Epoch: 22, Samples: 3232/5760, Loss: 1.553402304649353\n",
      "Epoch: 22, Samples: 3264/5760, Loss: 1.346592664718628\n",
      "Epoch: 22, Samples: 3296/5760, Loss: 1.315945029258728\n",
      "Epoch: 22, Samples: 3328/5760, Loss: 1.4183863401412964\n",
      "Epoch: 22, Samples: 3360/5760, Loss: 1.4146239757537842\n",
      "Epoch: 22, Samples: 3392/5760, Loss: 1.1725878715515137\n",
      "Epoch: 22, Samples: 3424/5760, Loss: 1.2680879831314087\n",
      "Epoch: 22, Samples: 3456/5760, Loss: 1.5961494445800781\n",
      "Epoch: 22, Samples: 3488/5760, Loss: 1.2100329399108887\n",
      "Epoch: 22, Samples: 3520/5760, Loss: 1.4242905378341675\n",
      "Epoch: 22, Samples: 3552/5760, Loss: 0.9726624488830566\n",
      "Epoch: 22, Samples: 3584/5760, Loss: 1.2493007183074951\n",
      "Epoch: 22, Samples: 3616/5760, Loss: 1.3448890447616577\n",
      "Epoch: 22, Samples: 3648/5760, Loss: 1.4674034118652344\n",
      "Epoch: 22, Samples: 3680/5760, Loss: 1.0515004396438599\n",
      "Epoch: 22, Samples: 3712/5760, Loss: 1.1985859870910645\n",
      "Epoch: 22, Samples: 3744/5760, Loss: 0.847832202911377\n",
      "Epoch: 22, Samples: 3776/5760, Loss: 1.595740556716919\n",
      "Epoch: 22, Samples: 3808/5760, Loss: 1.1449275016784668\n",
      "Epoch: 22, Samples: 3840/5760, Loss: 1.308857798576355\n",
      "Epoch: 22, Samples: 3872/5760, Loss: 1.5811899900436401\n",
      "Epoch: 22, Samples: 3904/5760, Loss: 1.0774402618408203\n",
      "Epoch: 22, Samples: 3936/5760, Loss: 1.3879367113113403\n",
      "Epoch: 22, Samples: 3968/5760, Loss: 1.14689302444458\n",
      "Epoch: 22, Samples: 4000/5760, Loss: 1.5191680192947388\n",
      "Epoch: 22, Samples: 4032/5760, Loss: 1.4126744270324707\n",
      "Epoch: 22, Samples: 4064/5760, Loss: 1.0245482921600342\n",
      "Epoch: 22, Samples: 4096/5760, Loss: 1.3581243753433228\n",
      "Epoch: 22, Samples: 4128/5760, Loss: 1.1258853673934937\n",
      "Epoch: 22, Samples: 4160/5760, Loss: 1.4621919393539429\n",
      "Epoch: 22, Samples: 4192/5760, Loss: 1.2919526100158691\n",
      "Epoch: 22, Samples: 4224/5760, Loss: 1.3908920288085938\n",
      "Epoch: 22, Samples: 4256/5760, Loss: 1.1867058277130127\n",
      "Epoch: 22, Samples: 4288/5760, Loss: 1.4858455657958984\n",
      "Epoch: 22, Samples: 4320/5760, Loss: 1.058478593826294\n",
      "Epoch: 22, Samples: 4352/5760, Loss: 1.2914469242095947\n",
      "Epoch: 22, Samples: 4384/5760, Loss: 1.2217222452163696\n",
      "Epoch: 22, Samples: 4416/5760, Loss: 1.145858645439148\n",
      "Epoch: 22, Samples: 4448/5760, Loss: 1.4896912574768066\n",
      "Epoch: 22, Samples: 4480/5760, Loss: 1.3287012577056885\n",
      "Epoch: 22, Samples: 4512/5760, Loss: 1.3652851581573486\n",
      "Epoch: 22, Samples: 4544/5760, Loss: 1.2465213537216187\n",
      "Epoch: 22, Samples: 4576/5760, Loss: 0.9451648592948914\n",
      "Epoch: 22, Samples: 4608/5760, Loss: 1.1908814907073975\n",
      "Epoch: 22, Samples: 4640/5760, Loss: 1.5657753944396973\n",
      "Epoch: 22, Samples: 4672/5760, Loss: 1.1662894487380981\n",
      "Epoch: 22, Samples: 4704/5760, Loss: 1.4656546115875244\n",
      "Epoch: 22, Samples: 4736/5760, Loss: 1.3241111040115356\n",
      "Epoch: 22, Samples: 4768/5760, Loss: 1.2420729398727417\n",
      "Epoch: 22, Samples: 4800/5760, Loss: 1.0141404867172241\n",
      "Epoch: 22, Samples: 4832/5760, Loss: 1.2969797849655151\n",
      "Epoch: 22, Samples: 4864/5760, Loss: 1.3305490016937256\n",
      "Epoch: 22, Samples: 4896/5760, Loss: 2.130855083465576\n",
      "Epoch: 22, Samples: 4928/5760, Loss: 0.9863888025283813\n",
      "Epoch: 22, Samples: 4960/5760, Loss: 1.315410852432251\n",
      "Epoch: 22, Samples: 4992/5760, Loss: 1.4173500537872314\n",
      "Epoch: 22, Samples: 5024/5760, Loss: 1.5103710889816284\n",
      "Epoch: 22, Samples: 5056/5760, Loss: 1.2744569778442383\n",
      "Epoch: 22, Samples: 5088/5760, Loss: 1.5556721687316895\n",
      "Epoch: 22, Samples: 5120/5760, Loss: 1.2938218116760254\n",
      "Epoch: 22, Samples: 5152/5760, Loss: 1.562847375869751\n",
      "Epoch: 22, Samples: 5184/5760, Loss: 1.354473352432251\n",
      "Epoch: 22, Samples: 5216/5760, Loss: 1.0643153190612793\n",
      "Epoch: 22, Samples: 5248/5760, Loss: 1.3435930013656616\n",
      "Epoch: 22, Samples: 5280/5760, Loss: 1.1273248195648193\n",
      "Epoch: 22, Samples: 5312/5760, Loss: 1.3449327945709229\n",
      "Epoch: 22, Samples: 5344/5760, Loss: 0.9995296001434326\n",
      "Epoch: 22, Samples: 5376/5760, Loss: 1.0344130992889404\n",
      "Epoch: 22, Samples: 5408/5760, Loss: 1.6925394535064697\n",
      "Epoch: 22, Samples: 5440/5760, Loss: 1.0351659059524536\n",
      "Epoch: 22, Samples: 5472/5760, Loss: 1.5451701879501343\n",
      "Epoch: 22, Samples: 5504/5760, Loss: 1.2671499252319336\n",
      "Epoch: 22, Samples: 5536/5760, Loss: 1.0540562868118286\n",
      "Epoch: 22, Samples: 5568/5760, Loss: 1.448561191558838\n",
      "Epoch: 22, Samples: 5600/5760, Loss: 1.3680732250213623\n",
      "Epoch: 22, Samples: 5632/5760, Loss: 1.1945387125015259\n",
      "Epoch: 22, Samples: 5664/5760, Loss: 1.4597785472869873\n",
      "Epoch: 22, Samples: 5696/5760, Loss: 1.680466651916504\n",
      "Epoch: 22, Samples: 5728/5760, Loss: 2.5162034034729004\n",
      "\n",
      "Epoch: 22\n",
      "Training set: Average loss: 1.3122\n",
      "Validation set: Average loss: 1.8447, Accuracy: 421/818 (51%)\n",
      "Saving model (epoch 22) with lowest validation loss: 1.8447161454420824\n",
      "Epoch: 23, Samples: 0/5760, Loss: 1.2375819683074951\n",
      "Epoch: 23, Samples: 32/5760, Loss: 1.1694802045822144\n",
      "Epoch: 23, Samples: 64/5760, Loss: 1.1156545877456665\n",
      "Epoch: 23, Samples: 96/5760, Loss: 1.4387062788009644\n",
      "Epoch: 23, Samples: 128/5760, Loss: 1.345320463180542\n",
      "Epoch: 23, Samples: 160/5760, Loss: 1.3250595331192017\n",
      "Epoch: 23, Samples: 192/5760, Loss: 1.2883069515228271\n",
      "Epoch: 23, Samples: 224/5760, Loss: 1.3570270538330078\n",
      "Epoch: 23, Samples: 256/5760, Loss: 1.1061307191848755\n",
      "Epoch: 23, Samples: 288/5760, Loss: 0.9654105305671692\n",
      "Epoch: 23, Samples: 320/5760, Loss: 1.1965084075927734\n",
      "Epoch: 23, Samples: 352/5760, Loss: 1.3404141664505005\n",
      "Epoch: 23, Samples: 384/5760, Loss: 1.40016508102417\n",
      "Epoch: 23, Samples: 416/5760, Loss: 1.337125539779663\n",
      "Epoch: 23, Samples: 448/5760, Loss: 1.1930508613586426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Samples: 480/5760, Loss: 1.0443400144577026\n",
      "Epoch: 23, Samples: 512/5760, Loss: 1.2114388942718506\n",
      "Epoch: 23, Samples: 544/5760, Loss: 1.4887148141860962\n",
      "Epoch: 23, Samples: 576/5760, Loss: 0.8744761943817139\n",
      "Epoch: 23, Samples: 608/5760, Loss: 1.0896979570388794\n",
      "Epoch: 23, Samples: 640/5760, Loss: 0.8507071137428284\n",
      "Epoch: 23, Samples: 672/5760, Loss: 1.50992751121521\n",
      "Epoch: 23, Samples: 704/5760, Loss: 1.0624914169311523\n",
      "Epoch: 23, Samples: 736/5760, Loss: 1.2382938861846924\n",
      "Epoch: 23, Samples: 768/5760, Loss: 1.2715861797332764\n",
      "Epoch: 23, Samples: 800/5760, Loss: 0.9358011484146118\n",
      "Epoch: 23, Samples: 832/5760, Loss: 1.1468477249145508\n",
      "Epoch: 23, Samples: 864/5760, Loss: 1.4072413444519043\n",
      "Epoch: 23, Samples: 896/5760, Loss: 1.4247206449508667\n",
      "Epoch: 23, Samples: 928/5760, Loss: 1.0491660833358765\n",
      "Epoch: 23, Samples: 960/5760, Loss: 1.5987766981124878\n",
      "Epoch: 23, Samples: 992/5760, Loss: 1.553306221961975\n",
      "Epoch: 23, Samples: 1024/5760, Loss: 1.47781240940094\n",
      "Epoch: 23, Samples: 1056/5760, Loss: 1.4805086851119995\n",
      "Epoch: 23, Samples: 1088/5760, Loss: 1.0849268436431885\n",
      "Epoch: 23, Samples: 1120/5760, Loss: 1.5018079280853271\n",
      "Epoch: 23, Samples: 1152/5760, Loss: 0.6945168972015381\n",
      "Epoch: 23, Samples: 1184/5760, Loss: 1.508606195449829\n",
      "Epoch: 23, Samples: 1216/5760, Loss: 1.2056407928466797\n",
      "Epoch: 23, Samples: 1248/5760, Loss: 1.1859688758850098\n",
      "Epoch: 23, Samples: 1280/5760, Loss: 0.9805642366409302\n",
      "Epoch: 23, Samples: 1312/5760, Loss: 1.2016065120697021\n",
      "Epoch: 23, Samples: 1344/5760, Loss: 1.0386093854904175\n",
      "Epoch: 23, Samples: 1376/5760, Loss: 1.6470837593078613\n",
      "Epoch: 23, Samples: 1408/5760, Loss: 1.1316018104553223\n",
      "Epoch: 23, Samples: 1440/5760, Loss: 1.4911493062973022\n",
      "Epoch: 23, Samples: 1472/5760, Loss: 1.0676544904708862\n",
      "Epoch: 23, Samples: 1504/5760, Loss: 1.2864086627960205\n",
      "Epoch: 23, Samples: 1536/5760, Loss: 1.1944310665130615\n",
      "Epoch: 23, Samples: 1568/5760, Loss: 1.8238263130187988\n",
      "Epoch: 23, Samples: 1600/5760, Loss: 1.3130261898040771\n",
      "Epoch: 23, Samples: 1632/5760, Loss: 1.2239388227462769\n",
      "Epoch: 23, Samples: 1664/5760, Loss: 0.8368903398513794\n",
      "Epoch: 23, Samples: 1696/5760, Loss: 1.2739877700805664\n",
      "Epoch: 23, Samples: 1728/5760, Loss: 1.3497788906097412\n",
      "Epoch: 23, Samples: 1760/5760, Loss: 1.0662801265716553\n",
      "Epoch: 23, Samples: 1792/5760, Loss: 1.7576360702514648\n",
      "Epoch: 23, Samples: 1824/5760, Loss: 1.2676299810409546\n",
      "Epoch: 23, Samples: 1856/5760, Loss: 1.2682526111602783\n",
      "Epoch: 23, Samples: 1888/5760, Loss: 1.4414798021316528\n",
      "Epoch: 23, Samples: 1920/5760, Loss: 1.255418062210083\n",
      "Epoch: 23, Samples: 1952/5760, Loss: 1.1861722469329834\n",
      "Epoch: 23, Samples: 1984/5760, Loss: 0.7396228313446045\n",
      "Epoch: 23, Samples: 2016/5760, Loss: 1.2378871440887451\n",
      "Epoch: 23, Samples: 2048/5760, Loss: 1.3904454708099365\n",
      "Epoch: 23, Samples: 2080/5760, Loss: 1.1684553623199463\n",
      "Epoch: 23, Samples: 2112/5760, Loss: 1.3776063919067383\n",
      "Epoch: 23, Samples: 2144/5760, Loss: 1.1341050863265991\n",
      "Epoch: 23, Samples: 2176/5760, Loss: 1.8780016899108887\n",
      "Epoch: 23, Samples: 2208/5760, Loss: 1.0846381187438965\n",
      "Epoch: 23, Samples: 2240/5760, Loss: 1.0863478183746338\n",
      "Epoch: 23, Samples: 2272/5760, Loss: 1.3215354681015015\n",
      "Epoch: 23, Samples: 2304/5760, Loss: 1.5199404954910278\n",
      "Epoch: 23, Samples: 2336/5760, Loss: 1.3317848443984985\n",
      "Epoch: 23, Samples: 2368/5760, Loss: 1.283748984336853\n",
      "Epoch: 23, Samples: 2400/5760, Loss: 1.2313313484191895\n",
      "Epoch: 23, Samples: 2432/5760, Loss: 1.4719903469085693\n",
      "Epoch: 23, Samples: 2464/5760, Loss: 1.093827247619629\n",
      "Epoch: 23, Samples: 2496/5760, Loss: 1.5690480470657349\n",
      "Epoch: 23, Samples: 2528/5760, Loss: 1.2176322937011719\n",
      "Epoch: 23, Samples: 2560/5760, Loss: 1.5062650442123413\n",
      "Epoch: 23, Samples: 2592/5760, Loss: 1.1731607913970947\n",
      "Epoch: 23, Samples: 2624/5760, Loss: 1.2176785469055176\n",
      "Epoch: 23, Samples: 2656/5760, Loss: 1.1087409257888794\n",
      "Epoch: 23, Samples: 2688/5760, Loss: 1.292349934577942\n",
      "Epoch: 23, Samples: 2720/5760, Loss: 1.6002552509307861\n",
      "Epoch: 23, Samples: 2752/5760, Loss: 1.8217490911483765\n",
      "Epoch: 23, Samples: 2784/5760, Loss: 1.0417454242706299\n",
      "Epoch: 23, Samples: 2816/5760, Loss: 1.413453221321106\n",
      "Epoch: 23, Samples: 2848/5760, Loss: 0.97377610206604\n",
      "Epoch: 23, Samples: 2880/5760, Loss: 1.2483336925506592\n",
      "Epoch: 23, Samples: 2912/5760, Loss: 1.5028934478759766\n",
      "Epoch: 23, Samples: 2944/5760, Loss: 1.2213780879974365\n",
      "Epoch: 23, Samples: 2976/5760, Loss: 1.441622257232666\n",
      "Epoch: 23, Samples: 3008/5760, Loss: 1.525058627128601\n",
      "Epoch: 23, Samples: 3040/5760, Loss: 1.1065621376037598\n",
      "Epoch: 23, Samples: 3072/5760, Loss: 1.0447602272033691\n",
      "Epoch: 23, Samples: 3104/5760, Loss: 1.3458033800125122\n",
      "Epoch: 23, Samples: 3136/5760, Loss: 1.46843421459198\n",
      "Epoch: 23, Samples: 3168/5760, Loss: 1.3234913349151611\n",
      "Epoch: 23, Samples: 3200/5760, Loss: 1.0371952056884766\n",
      "Epoch: 23, Samples: 3232/5760, Loss: 1.184990644454956\n",
      "Epoch: 23, Samples: 3264/5760, Loss: 1.183160662651062\n",
      "Epoch: 23, Samples: 3296/5760, Loss: 1.147587776184082\n",
      "Epoch: 23, Samples: 3328/5760, Loss: 1.0219380855560303\n",
      "Epoch: 23, Samples: 3360/5760, Loss: 1.247540831565857\n",
      "Epoch: 23, Samples: 3392/5760, Loss: 1.127504825592041\n",
      "Epoch: 23, Samples: 3424/5760, Loss: 1.2702856063842773\n",
      "Epoch: 23, Samples: 3456/5760, Loss: 1.264012336730957\n",
      "Epoch: 23, Samples: 3488/5760, Loss: 1.6938464641571045\n",
      "Epoch: 23, Samples: 3520/5760, Loss: 1.030600666999817\n",
      "Epoch: 23, Samples: 3552/5760, Loss: 1.4023029804229736\n",
      "Epoch: 23, Samples: 3584/5760, Loss: 0.8973392248153687\n",
      "Epoch: 23, Samples: 3616/5760, Loss: 1.1100268363952637\n",
      "Epoch: 23, Samples: 3648/5760, Loss: 1.4789220094680786\n",
      "Epoch: 23, Samples: 3680/5760, Loss: 1.0602586269378662\n",
      "Epoch: 23, Samples: 3712/5760, Loss: 1.3595768213272095\n",
      "Epoch: 23, Samples: 3744/5760, Loss: 1.2322261333465576\n",
      "Epoch: 23, Samples: 3776/5760, Loss: 0.5733158588409424\n",
      "Epoch: 23, Samples: 3808/5760, Loss: 1.1814064979553223\n",
      "Epoch: 23, Samples: 3840/5760, Loss: 1.0756808519363403\n",
      "Epoch: 23, Samples: 3872/5760, Loss: 1.0318620204925537\n",
      "Epoch: 23, Samples: 3904/5760, Loss: 0.8004476428031921\n",
      "Epoch: 23, Samples: 3936/5760, Loss: 1.6052544116973877\n",
      "Epoch: 23, Samples: 3968/5760, Loss: 1.2993711233139038\n",
      "Epoch: 23, Samples: 4000/5760, Loss: 1.2333226203918457\n",
      "Epoch: 23, Samples: 4032/5760, Loss: 1.6131230592727661\n",
      "Epoch: 23, Samples: 4064/5760, Loss: 1.1972565650939941\n",
      "Epoch: 23, Samples: 4096/5760, Loss: 1.216217279434204\n",
      "Epoch: 23, Samples: 4128/5760, Loss: 1.0532056093215942\n",
      "Epoch: 23, Samples: 4160/5760, Loss: 0.9465802907943726\n",
      "Epoch: 23, Samples: 4192/5760, Loss: 1.707729697227478\n",
      "Epoch: 23, Samples: 4224/5760, Loss: 1.4458603858947754\n",
      "Epoch: 23, Samples: 4256/5760, Loss: 1.4647578001022339\n",
      "Epoch: 23, Samples: 4288/5760, Loss: 1.0632655620574951\n",
      "Epoch: 23, Samples: 4320/5760, Loss: 1.138774037361145\n",
      "Epoch: 23, Samples: 4352/5760, Loss: 0.6681218147277832\n",
      "Epoch: 23, Samples: 4384/5760, Loss: 0.88667231798172\n",
      "Epoch: 23, Samples: 4416/5760, Loss: 1.424088478088379\n",
      "Epoch: 23, Samples: 4448/5760, Loss: 1.1398262977600098\n",
      "Epoch: 23, Samples: 4480/5760, Loss: 1.0732249021530151\n",
      "Epoch: 23, Samples: 4512/5760, Loss: 1.3891828060150146\n",
      "Epoch: 23, Samples: 4544/5760, Loss: 1.8974279165267944\n",
      "Epoch: 23, Samples: 4576/5760, Loss: 1.1471134424209595\n",
      "Epoch: 23, Samples: 4608/5760, Loss: 1.382481575012207\n",
      "Epoch: 23, Samples: 4640/5760, Loss: 1.3664649724960327\n",
      "Epoch: 23, Samples: 4672/5760, Loss: 1.4143604040145874\n",
      "Epoch: 23, Samples: 4704/5760, Loss: 0.86812824010849\n",
      "Epoch: 23, Samples: 4736/5760, Loss: 1.513033151626587\n",
      "Epoch: 23, Samples: 4768/5760, Loss: 1.4093313217163086\n",
      "Epoch: 23, Samples: 4800/5760, Loss: 1.1899653673171997\n",
      "Epoch: 23, Samples: 4832/5760, Loss: 1.5066900253295898\n",
      "Epoch: 23, Samples: 4864/5760, Loss: 1.7106714248657227\n",
      "Epoch: 23, Samples: 4896/5760, Loss: 1.7014846801757812\n",
      "Epoch: 23, Samples: 4928/5760, Loss: 1.2077000141143799\n",
      "Epoch: 23, Samples: 4960/5760, Loss: 0.9344056844711304\n",
      "Epoch: 23, Samples: 4992/5760, Loss: 1.177450180053711\n",
      "Epoch: 23, Samples: 5024/5760, Loss: 1.2478837966918945\n",
      "Epoch: 23, Samples: 5056/5760, Loss: 1.151350975036621\n",
      "Epoch: 23, Samples: 5088/5760, Loss: 0.8740760087966919\n",
      "Epoch: 23, Samples: 5120/5760, Loss: 1.54142165184021\n",
      "Epoch: 23, Samples: 5152/5760, Loss: 1.409421682357788\n",
      "Epoch: 23, Samples: 5184/5760, Loss: 1.099488615989685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Samples: 5216/5760, Loss: 1.0223308801651\n",
      "Epoch: 23, Samples: 5248/5760, Loss: 1.1127464771270752\n",
      "Epoch: 23, Samples: 5280/5760, Loss: 1.2113243341445923\n",
      "Epoch: 23, Samples: 5312/5760, Loss: 1.5728529691696167\n",
      "Epoch: 23, Samples: 5344/5760, Loss: 1.3054986000061035\n",
      "Epoch: 23, Samples: 5376/5760, Loss: 1.0793697834014893\n",
      "Epoch: 23, Samples: 5408/5760, Loss: 1.2376697063446045\n",
      "Epoch: 23, Samples: 5440/5760, Loss: 0.8878790140151978\n",
      "Epoch: 23, Samples: 5472/5760, Loss: 1.315241813659668\n",
      "Epoch: 23, Samples: 5504/5760, Loss: 0.8317198753356934\n",
      "Epoch: 23, Samples: 5536/5760, Loss: 1.3115992546081543\n",
      "Epoch: 23, Samples: 5568/5760, Loss: 0.9529339075088501\n",
      "Epoch: 23, Samples: 5600/5760, Loss: 1.4778624773025513\n",
      "Epoch: 23, Samples: 5632/5760, Loss: 1.552996039390564\n",
      "Epoch: 23, Samples: 5664/5760, Loss: 1.0708301067352295\n",
      "Epoch: 23, Samples: 5696/5760, Loss: 1.194119930267334\n",
      "Epoch: 23, Samples: 5728/5760, Loss: 1.4396048784255981\n",
      "\n",
      "Epoch: 23\n",
      "Training set: Average loss: 1.2535\n",
      "Validation set: Average loss: 1.8961, Accuracy: 425/818 (52%)\n",
      "Epoch: 24, Samples: 0/5760, Loss: 1.2239537239074707\n",
      "Epoch: 24, Samples: 32/5760, Loss: 0.9964054822921753\n",
      "Epoch: 24, Samples: 64/5760, Loss: 1.5420721769332886\n",
      "Epoch: 24, Samples: 96/5760, Loss: 1.033193826675415\n",
      "Epoch: 24, Samples: 128/5760, Loss: 1.2547574043273926\n",
      "Epoch: 24, Samples: 160/5760, Loss: 1.0992637872695923\n",
      "Epoch: 24, Samples: 192/5760, Loss: 1.1633728742599487\n",
      "Epoch: 24, Samples: 224/5760, Loss: 1.2208307981491089\n",
      "Epoch: 24, Samples: 256/5760, Loss: 1.244524359703064\n",
      "Epoch: 24, Samples: 288/5760, Loss: 0.9635278582572937\n",
      "Epoch: 24, Samples: 320/5760, Loss: 0.9895355701446533\n",
      "Epoch: 24, Samples: 352/5760, Loss: 1.3122224807739258\n",
      "Epoch: 24, Samples: 384/5760, Loss: 0.8232768774032593\n",
      "Epoch: 24, Samples: 416/5760, Loss: 1.4266717433929443\n",
      "Epoch: 24, Samples: 448/5760, Loss: 1.1081311702728271\n",
      "Epoch: 24, Samples: 480/5760, Loss: 1.1589024066925049\n",
      "Epoch: 24, Samples: 512/5760, Loss: 1.1567811965942383\n",
      "Epoch: 24, Samples: 544/5760, Loss: 1.196079134941101\n",
      "Epoch: 24, Samples: 576/5760, Loss: 1.013178825378418\n",
      "Epoch: 24, Samples: 608/5760, Loss: 1.4054694175720215\n",
      "Epoch: 24, Samples: 640/5760, Loss: 0.904154896736145\n",
      "Epoch: 24, Samples: 672/5760, Loss: 1.248009204864502\n",
      "Epoch: 24, Samples: 704/5760, Loss: 1.2933244705200195\n",
      "Epoch: 24, Samples: 736/5760, Loss: 1.1250274181365967\n",
      "Epoch: 24, Samples: 768/5760, Loss: 1.2270236015319824\n",
      "Epoch: 24, Samples: 800/5760, Loss: 1.0936522483825684\n",
      "Epoch: 24, Samples: 832/5760, Loss: 1.1619030237197876\n",
      "Epoch: 24, Samples: 864/5760, Loss: 1.232245683670044\n",
      "Epoch: 24, Samples: 896/5760, Loss: 1.0326271057128906\n",
      "Epoch: 24, Samples: 928/5760, Loss: 1.1411170959472656\n",
      "Epoch: 24, Samples: 960/5760, Loss: 1.210555076599121\n",
      "Epoch: 24, Samples: 992/5760, Loss: 0.7054774761199951\n",
      "Epoch: 24, Samples: 1024/5760, Loss: 1.6692726612091064\n",
      "Epoch: 24, Samples: 1056/5760, Loss: 1.4650123119354248\n",
      "Epoch: 24, Samples: 1088/5760, Loss: 1.3723859786987305\n",
      "Epoch: 24, Samples: 1120/5760, Loss: 1.1313591003417969\n",
      "Epoch: 24, Samples: 1152/5760, Loss: 1.05549955368042\n",
      "Epoch: 24, Samples: 1184/5760, Loss: 0.695623517036438\n",
      "Epoch: 24, Samples: 1216/5760, Loss: 1.0211519002914429\n",
      "Epoch: 24, Samples: 1248/5760, Loss: 1.3617430925369263\n",
      "Epoch: 24, Samples: 1280/5760, Loss: 0.8298321962356567\n",
      "Epoch: 24, Samples: 1312/5760, Loss: 1.3344923257827759\n",
      "Epoch: 24, Samples: 1344/5760, Loss: 1.1126385927200317\n",
      "Epoch: 24, Samples: 1376/5760, Loss: 0.8750727772712708\n",
      "Epoch: 24, Samples: 1408/5760, Loss: 0.968025803565979\n",
      "Epoch: 24, Samples: 1440/5760, Loss: 1.5987247228622437\n",
      "Epoch: 24, Samples: 1472/5760, Loss: 1.5964258909225464\n",
      "Epoch: 24, Samples: 1504/5760, Loss: 1.3661243915557861\n",
      "Epoch: 24, Samples: 1536/5760, Loss: 1.053560733795166\n",
      "Epoch: 24, Samples: 1568/5760, Loss: 0.9392871856689453\n",
      "Epoch: 24, Samples: 1600/5760, Loss: 0.8007140755653381\n",
      "Epoch: 24, Samples: 1632/5760, Loss: 1.4449679851531982\n",
      "Epoch: 24, Samples: 1664/5760, Loss: 0.8946564197540283\n",
      "Epoch: 24, Samples: 1696/5760, Loss: 1.219461441040039\n",
      "Epoch: 24, Samples: 1728/5760, Loss: 1.0457690954208374\n",
      "Epoch: 24, Samples: 1760/5760, Loss: 1.1333509683609009\n",
      "Epoch: 24, Samples: 1792/5760, Loss: 1.31182861328125\n",
      "Epoch: 24, Samples: 1824/5760, Loss: 1.1720616817474365\n",
      "Epoch: 24, Samples: 1856/5760, Loss: 1.624565601348877\n",
      "Epoch: 24, Samples: 1888/5760, Loss: 1.4122766256332397\n",
      "Epoch: 24, Samples: 1920/5760, Loss: 0.8808273077011108\n",
      "Epoch: 24, Samples: 1952/5760, Loss: 1.1042648553848267\n",
      "Epoch: 24, Samples: 1984/5760, Loss: 1.1814396381378174\n",
      "Epoch: 24, Samples: 2016/5760, Loss: 1.414750576019287\n",
      "Epoch: 24, Samples: 2048/5760, Loss: 1.274324893951416\n",
      "Epoch: 24, Samples: 2080/5760, Loss: 0.9844303727149963\n",
      "Epoch: 24, Samples: 2112/5760, Loss: 1.4953951835632324\n",
      "Epoch: 24, Samples: 2144/5760, Loss: 1.2795323133468628\n",
      "Epoch: 24, Samples: 2176/5760, Loss: 0.808445394039154\n",
      "Epoch: 24, Samples: 2208/5760, Loss: 1.4654064178466797\n",
      "Epoch: 24, Samples: 2240/5760, Loss: 1.092146873474121\n",
      "Epoch: 24, Samples: 2272/5760, Loss: 1.2529689073562622\n",
      "Epoch: 24, Samples: 2304/5760, Loss: 1.4478299617767334\n",
      "Epoch: 24, Samples: 2336/5760, Loss: 1.305863857269287\n",
      "Epoch: 24, Samples: 2368/5760, Loss: 1.2683109045028687\n",
      "Epoch: 24, Samples: 2400/5760, Loss: 1.355504035949707\n",
      "Epoch: 24, Samples: 2432/5760, Loss: 1.045769214630127\n",
      "Epoch: 24, Samples: 2464/5760, Loss: 1.1852351427078247\n",
      "Epoch: 24, Samples: 2496/5760, Loss: 1.275019884109497\n",
      "Epoch: 24, Samples: 2528/5760, Loss: 0.7614790201187134\n",
      "Epoch: 24, Samples: 2560/5760, Loss: 1.3359392881393433\n",
      "Epoch: 24, Samples: 2592/5760, Loss: 1.8958134651184082\n",
      "Epoch: 24, Samples: 2624/5760, Loss: 1.4670237302780151\n",
      "Epoch: 24, Samples: 2656/5760, Loss: 1.6202746629714966\n",
      "Epoch: 24, Samples: 2688/5760, Loss: 0.9203136563301086\n",
      "Epoch: 24, Samples: 2720/5760, Loss: 1.0730350017547607\n",
      "Epoch: 24, Samples: 2752/5760, Loss: 1.321805477142334\n",
      "Epoch: 24, Samples: 2784/5760, Loss: 0.9727581739425659\n",
      "Epoch: 24, Samples: 2816/5760, Loss: 0.9689793586730957\n",
      "Epoch: 24, Samples: 2848/5760, Loss: 1.087149739265442\n",
      "Epoch: 24, Samples: 2880/5760, Loss: 1.7516847848892212\n",
      "Epoch: 24, Samples: 2912/5760, Loss: 1.0309593677520752\n",
      "Epoch: 24, Samples: 2944/5760, Loss: 1.1848909854888916\n",
      "Epoch: 24, Samples: 2976/5760, Loss: 1.2295997142791748\n",
      "Epoch: 24, Samples: 3008/5760, Loss: 1.442538857460022\n",
      "Epoch: 24, Samples: 3040/5760, Loss: 1.0440669059753418\n",
      "Epoch: 24, Samples: 3072/5760, Loss: 1.4110466241836548\n",
      "Epoch: 24, Samples: 3104/5760, Loss: 1.2209527492523193\n",
      "Epoch: 24, Samples: 3136/5760, Loss: 1.1572179794311523\n",
      "Epoch: 24, Samples: 3168/5760, Loss: 1.4412531852722168\n",
      "Epoch: 24, Samples: 3200/5760, Loss: 1.0517244338989258\n",
      "Epoch: 24, Samples: 3232/5760, Loss: 1.2072619199752808\n",
      "Epoch: 24, Samples: 3264/5760, Loss: 1.085598111152649\n",
      "Epoch: 24, Samples: 3296/5760, Loss: 1.1909147500991821\n",
      "Epoch: 24, Samples: 3328/5760, Loss: 0.839188814163208\n",
      "Epoch: 24, Samples: 3360/5760, Loss: 0.8064502477645874\n",
      "Epoch: 24, Samples: 3392/5760, Loss: 1.5936007499694824\n",
      "Epoch: 24, Samples: 3424/5760, Loss: 0.9010157585144043\n",
      "Epoch: 24, Samples: 3456/5760, Loss: 0.7602112889289856\n",
      "Epoch: 24, Samples: 3488/5760, Loss: 1.459255576133728\n",
      "Epoch: 24, Samples: 3520/5760, Loss: 1.0618938207626343\n",
      "Epoch: 24, Samples: 3552/5760, Loss: 1.5223636627197266\n",
      "Epoch: 24, Samples: 3584/5760, Loss: 1.2046023607254028\n",
      "Epoch: 24, Samples: 3616/5760, Loss: 0.7857617139816284\n",
      "Epoch: 24, Samples: 3648/5760, Loss: 1.4831160306930542\n",
      "Epoch: 24, Samples: 3680/5760, Loss: 1.176531195640564\n",
      "Epoch: 24, Samples: 3712/5760, Loss: 1.207777976989746\n",
      "Epoch: 24, Samples: 3744/5760, Loss: 1.0426406860351562\n",
      "Epoch: 24, Samples: 3776/5760, Loss: 2.0561747550964355\n",
      "Epoch: 24, Samples: 3808/5760, Loss: 1.4813592433929443\n",
      "Epoch: 24, Samples: 3840/5760, Loss: 0.8388710618019104\n",
      "Epoch: 24, Samples: 3872/5760, Loss: 1.1104042530059814\n",
      "Epoch: 24, Samples: 3904/5760, Loss: 1.1067990064620972\n",
      "Epoch: 24, Samples: 3936/5760, Loss: 0.7397269010543823\n",
      "Epoch: 24, Samples: 3968/5760, Loss: 1.0693600177764893\n",
      "Epoch: 24, Samples: 4000/5760, Loss: 1.0856751203536987\n",
      "Epoch: 24, Samples: 4032/5760, Loss: 1.4009099006652832\n",
      "Epoch: 24, Samples: 4064/5760, Loss: 1.1794553995132446\n",
      "Epoch: 24, Samples: 4096/5760, Loss: 1.3628472089767456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Samples: 4128/5760, Loss: 1.3275361061096191\n",
      "Epoch: 24, Samples: 4160/5760, Loss: 1.0856375694274902\n",
      "Epoch: 24, Samples: 4192/5760, Loss: 1.1463532447814941\n",
      "Epoch: 24, Samples: 4224/5760, Loss: 1.184224247932434\n",
      "Epoch: 24, Samples: 4256/5760, Loss: 0.9454793930053711\n",
      "Epoch: 24, Samples: 4288/5760, Loss: 1.4196821451187134\n",
      "Epoch: 24, Samples: 4320/5760, Loss: 1.2667241096496582\n",
      "Epoch: 24, Samples: 4352/5760, Loss: 1.2395236492156982\n",
      "Epoch: 24, Samples: 4384/5760, Loss: 1.2430051565170288\n",
      "Epoch: 24, Samples: 4416/5760, Loss: 0.8365914821624756\n",
      "Epoch: 24, Samples: 4448/5760, Loss: 1.144883394241333\n",
      "Epoch: 24, Samples: 4480/5760, Loss: 1.3958930969238281\n",
      "Epoch: 24, Samples: 4512/5760, Loss: 0.970901608467102\n",
      "Epoch: 24, Samples: 4544/5760, Loss: 1.111729383468628\n",
      "Epoch: 24, Samples: 4576/5760, Loss: 1.2702604532241821\n",
      "Epoch: 24, Samples: 4608/5760, Loss: 1.1029815673828125\n",
      "Epoch: 24, Samples: 4640/5760, Loss: 1.4515976905822754\n",
      "Epoch: 24, Samples: 4672/5760, Loss: 2.108975410461426\n",
      "Epoch: 24, Samples: 4704/5760, Loss: 1.7038114070892334\n",
      "Epoch: 24, Samples: 4736/5760, Loss: 1.3027331829071045\n",
      "Epoch: 24, Samples: 4768/5760, Loss: 1.5427848100662231\n",
      "Epoch: 24, Samples: 4800/5760, Loss: 1.390112042427063\n",
      "Epoch: 24, Samples: 4832/5760, Loss: 1.053727626800537\n",
      "Epoch: 24, Samples: 4864/5760, Loss: 1.207798719406128\n",
      "Epoch: 24, Samples: 4896/5760, Loss: 1.296393632888794\n",
      "Epoch: 24, Samples: 4928/5760, Loss: 0.9222183227539062\n",
      "Epoch: 24, Samples: 4960/5760, Loss: 1.5992683172225952\n",
      "Epoch: 24, Samples: 4992/5760, Loss: 1.2223886251449585\n",
      "Epoch: 24, Samples: 5024/5760, Loss: 0.8926273584365845\n",
      "Epoch: 24, Samples: 5056/5760, Loss: 1.9347952604293823\n",
      "Epoch: 24, Samples: 5088/5760, Loss: 1.2657402753829956\n",
      "Epoch: 24, Samples: 5120/5760, Loss: 1.41525399684906\n",
      "Epoch: 24, Samples: 5152/5760, Loss: 1.2914317846298218\n",
      "Epoch: 24, Samples: 5184/5760, Loss: 1.5397886037826538\n",
      "Epoch: 24, Samples: 5216/5760, Loss: 1.6329984664916992\n",
      "Epoch: 24, Samples: 5248/5760, Loss: 1.0387020111083984\n",
      "Epoch: 24, Samples: 5280/5760, Loss: 1.1740180253982544\n",
      "Epoch: 24, Samples: 5312/5760, Loss: 1.323978066444397\n",
      "Epoch: 24, Samples: 5344/5760, Loss: 1.283347725868225\n",
      "Epoch: 24, Samples: 5376/5760, Loss: 0.922931969165802\n",
      "Epoch: 24, Samples: 5408/5760, Loss: 1.091296911239624\n",
      "Epoch: 24, Samples: 5440/5760, Loss: 1.273247241973877\n",
      "Epoch: 24, Samples: 5472/5760, Loss: 1.0050022602081299\n",
      "Epoch: 24, Samples: 5504/5760, Loss: 1.2772417068481445\n",
      "Epoch: 24, Samples: 5536/5760, Loss: 0.9013502597808838\n",
      "Epoch: 24, Samples: 5568/5760, Loss: 1.2442426681518555\n",
      "Epoch: 24, Samples: 5600/5760, Loss: 0.7085371017456055\n",
      "Epoch: 24, Samples: 5632/5760, Loss: 0.755943775177002\n",
      "Epoch: 24, Samples: 5664/5760, Loss: 1.0705044269561768\n",
      "Epoch: 24, Samples: 5696/5760, Loss: 1.389505386352539\n",
      "Epoch: 24, Samples: 5728/5760, Loss: 3.2419302463531494\n",
      "\n",
      "Epoch: 24\n",
      "Training set: Average loss: 1.2147\n",
      "Validation set: Average loss: 1.7269, Accuracy: 444/818 (54%)\n",
      "Saving model (epoch 24) with lowest validation loss: 1.7269267164743864\n",
      "Epoch: 25, Samples: 0/5760, Loss: 1.0732637643814087\n",
      "Epoch: 25, Samples: 32/5760, Loss: 0.9932352900505066\n",
      "Epoch: 25, Samples: 64/5760, Loss: 1.2459423542022705\n",
      "Epoch: 25, Samples: 96/5760, Loss: 1.657732605934143\n",
      "Epoch: 25, Samples: 128/5760, Loss: 1.23562490940094\n",
      "Epoch: 25, Samples: 160/5760, Loss: 1.0736771821975708\n",
      "Epoch: 25, Samples: 192/5760, Loss: 1.2629125118255615\n",
      "Epoch: 25, Samples: 224/5760, Loss: 1.280357837677002\n",
      "Epoch: 25, Samples: 256/5760, Loss: 1.2196351289749146\n",
      "Epoch: 25, Samples: 288/5760, Loss: 1.074721097946167\n",
      "Epoch: 25, Samples: 320/5760, Loss: 1.2128702402114868\n",
      "Epoch: 25, Samples: 352/5760, Loss: 2.1246933937072754\n",
      "Epoch: 25, Samples: 384/5760, Loss: 1.214480996131897\n",
      "Epoch: 25, Samples: 416/5760, Loss: 1.2673934698104858\n",
      "Epoch: 25, Samples: 448/5760, Loss: 1.5283139944076538\n",
      "Epoch: 25, Samples: 480/5760, Loss: 1.092245101928711\n",
      "Epoch: 25, Samples: 512/5760, Loss: 0.9873998165130615\n",
      "Epoch: 25, Samples: 544/5760, Loss: 1.4026882648468018\n",
      "Epoch: 25, Samples: 576/5760, Loss: 1.6132761240005493\n",
      "Epoch: 25, Samples: 608/5760, Loss: 1.0049545764923096\n",
      "Epoch: 25, Samples: 640/5760, Loss: 1.2111917734146118\n",
      "Epoch: 25, Samples: 672/5760, Loss: 1.011657953262329\n",
      "Epoch: 25, Samples: 704/5760, Loss: 1.3351671695709229\n",
      "Epoch: 25, Samples: 736/5760, Loss: 0.9406951665878296\n",
      "Epoch: 25, Samples: 768/5760, Loss: 0.9896154999732971\n",
      "Epoch: 25, Samples: 800/5760, Loss: 1.2125327587127686\n",
      "Epoch: 25, Samples: 832/5760, Loss: 1.1462650299072266\n",
      "Epoch: 25, Samples: 864/5760, Loss: 1.3630642890930176\n",
      "Epoch: 25, Samples: 896/5760, Loss: 1.0328068733215332\n",
      "Epoch: 25, Samples: 928/5760, Loss: 1.0077077150344849\n",
      "Epoch: 25, Samples: 960/5760, Loss: 1.2604315280914307\n",
      "Epoch: 25, Samples: 992/5760, Loss: 1.2459304332733154\n",
      "Epoch: 25, Samples: 1024/5760, Loss: 1.1317672729492188\n",
      "Epoch: 25, Samples: 1056/5760, Loss: 1.4603087902069092\n",
      "Epoch: 25, Samples: 1088/5760, Loss: 1.0603504180908203\n",
      "Epoch: 25, Samples: 1120/5760, Loss: 1.38516104221344\n",
      "Epoch: 25, Samples: 1152/5760, Loss: 0.9644523859024048\n",
      "Epoch: 25, Samples: 1184/5760, Loss: 0.747732937335968\n",
      "Epoch: 25, Samples: 1216/5760, Loss: 0.9968779683113098\n",
      "Epoch: 25, Samples: 1248/5760, Loss: 1.4167022705078125\n",
      "Epoch: 25, Samples: 1280/5760, Loss: 1.1626596450805664\n",
      "Epoch: 25, Samples: 1312/5760, Loss: 1.3821600675582886\n",
      "Epoch: 25, Samples: 1344/5760, Loss: 1.0306559801101685\n",
      "Epoch: 25, Samples: 1376/5760, Loss: 0.9101311564445496\n",
      "Epoch: 25, Samples: 1408/5760, Loss: 0.9692341685295105\n",
      "Epoch: 25, Samples: 1440/5760, Loss: 0.9553003907203674\n",
      "Epoch: 25, Samples: 1472/5760, Loss: 1.0366238355636597\n",
      "Epoch: 25, Samples: 1504/5760, Loss: 1.5383328199386597\n",
      "Epoch: 25, Samples: 1536/5760, Loss: 1.9156363010406494\n",
      "Epoch: 25, Samples: 1568/5760, Loss: 1.0191097259521484\n",
      "Epoch: 25, Samples: 1600/5760, Loss: 1.3043699264526367\n",
      "Epoch: 25, Samples: 1632/5760, Loss: 1.104716181755066\n",
      "Epoch: 25, Samples: 1664/5760, Loss: 0.9847107529640198\n",
      "Epoch: 25, Samples: 1696/5760, Loss: 1.3547604084014893\n",
      "Epoch: 25, Samples: 1728/5760, Loss: 1.2765169143676758\n",
      "Epoch: 25, Samples: 1760/5760, Loss: 1.4030215740203857\n",
      "Epoch: 25, Samples: 1792/5760, Loss: 1.3084995746612549\n",
      "Epoch: 25, Samples: 1824/5760, Loss: 0.7071235179901123\n",
      "Epoch: 25, Samples: 1856/5760, Loss: 1.1505838632583618\n",
      "Epoch: 25, Samples: 1888/5760, Loss: 1.2448320388793945\n",
      "Epoch: 25, Samples: 1920/5760, Loss: 1.3355252742767334\n",
      "Epoch: 25, Samples: 1952/5760, Loss: 1.2362782955169678\n",
      "Epoch: 25, Samples: 1984/5760, Loss: 0.7672098875045776\n",
      "Epoch: 25, Samples: 2016/5760, Loss: 0.6755303144454956\n",
      "Epoch: 25, Samples: 2048/5760, Loss: 1.1995207071304321\n",
      "Epoch: 25, Samples: 2080/5760, Loss: 0.9935309886932373\n",
      "Epoch: 25, Samples: 2112/5760, Loss: 1.114016056060791\n",
      "Epoch: 25, Samples: 2144/5760, Loss: 1.3410274982452393\n",
      "Epoch: 25, Samples: 2176/5760, Loss: 1.0513064861297607\n",
      "Epoch: 25, Samples: 2208/5760, Loss: 1.1806585788726807\n",
      "Epoch: 25, Samples: 2240/5760, Loss: 0.9515175223350525\n",
      "Epoch: 25, Samples: 2272/5760, Loss: 1.1772749423980713\n",
      "Epoch: 25, Samples: 2304/5760, Loss: 0.8677281737327576\n",
      "Epoch: 25, Samples: 2336/5760, Loss: 1.2979977130889893\n",
      "Epoch: 25, Samples: 2368/5760, Loss: 1.0065650939941406\n",
      "Epoch: 25, Samples: 2400/5760, Loss: 1.1818805932998657\n",
      "Epoch: 25, Samples: 2432/5760, Loss: 1.5192341804504395\n",
      "Epoch: 25, Samples: 2464/5760, Loss: 1.106985092163086\n",
      "Epoch: 25, Samples: 2496/5760, Loss: 1.0473331212997437\n",
      "Epoch: 25, Samples: 2528/5760, Loss: 1.2220197916030884\n",
      "Epoch: 25, Samples: 2560/5760, Loss: 1.0473864078521729\n",
      "Epoch: 25, Samples: 2592/5760, Loss: 1.353993535041809\n",
      "Epoch: 25, Samples: 2624/5760, Loss: 0.8442409038543701\n",
      "Epoch: 25, Samples: 2656/5760, Loss: 1.0865840911865234\n",
      "Epoch: 25, Samples: 2688/5760, Loss: 1.0316656827926636\n",
      "Epoch: 25, Samples: 2720/5760, Loss: 1.357843279838562\n",
      "Epoch: 25, Samples: 2752/5760, Loss: 1.1655230522155762\n",
      "Epoch: 25, Samples: 2784/5760, Loss: 1.4026317596435547\n",
      "Epoch: 25, Samples: 2816/5760, Loss: 0.7353975772857666\n",
      "Epoch: 25, Samples: 2848/5760, Loss: 1.726861834526062\n",
      "Epoch: 25, Samples: 2880/5760, Loss: 0.8469001650810242\n",
      "Epoch: 25, Samples: 2912/5760, Loss: 1.2366658449172974\n",
      "Epoch: 25, Samples: 2944/5760, Loss: 0.9946576952934265\n",
      "Epoch: 25, Samples: 2976/5760, Loss: 1.1706671714782715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Samples: 3008/5760, Loss: 1.1301592588424683\n",
      "Epoch: 25, Samples: 3040/5760, Loss: 0.8340181708335876\n",
      "Epoch: 25, Samples: 3072/5760, Loss: 1.4247705936431885\n",
      "Epoch: 25, Samples: 3104/5760, Loss: 1.0752153396606445\n",
      "Epoch: 25, Samples: 3136/5760, Loss: 1.5314223766326904\n",
      "Epoch: 25, Samples: 3168/5760, Loss: 1.3250669240951538\n",
      "Epoch: 25, Samples: 3200/5760, Loss: 1.261683702468872\n",
      "Epoch: 25, Samples: 3232/5760, Loss: 0.8424346446990967\n",
      "Epoch: 25, Samples: 3264/5760, Loss: 0.9728208780288696\n",
      "Epoch: 25, Samples: 3296/5760, Loss: 1.3911945819854736\n",
      "Epoch: 25, Samples: 3328/5760, Loss: 1.1813502311706543\n",
      "Epoch: 25, Samples: 3360/5760, Loss: 1.474432110786438\n",
      "Epoch: 25, Samples: 3392/5760, Loss: 1.1084917783737183\n",
      "Epoch: 25, Samples: 3424/5760, Loss: 0.9343304634094238\n",
      "Epoch: 25, Samples: 3456/5760, Loss: 1.436741828918457\n",
      "Epoch: 25, Samples: 3488/5760, Loss: 0.7845075130462646\n",
      "Epoch: 25, Samples: 3520/5760, Loss: 1.0779337882995605\n",
      "Epoch: 25, Samples: 3552/5760, Loss: 1.1601521968841553\n",
      "Epoch: 25, Samples: 3584/5760, Loss: 1.1974518299102783\n",
      "Epoch: 25, Samples: 3616/5760, Loss: 1.0948500633239746\n",
      "Epoch: 25, Samples: 3648/5760, Loss: 1.386478304862976\n",
      "Epoch: 25, Samples: 3680/5760, Loss: 1.4980288743972778\n",
      "Epoch: 25, Samples: 3712/5760, Loss: 1.0991384983062744\n",
      "Epoch: 25, Samples: 3744/5760, Loss: 0.8958082795143127\n",
      "Epoch: 25, Samples: 3776/5760, Loss: 1.1701583862304688\n",
      "Epoch: 25, Samples: 3808/5760, Loss: 1.1756693124771118\n",
      "Epoch: 25, Samples: 3840/5760, Loss: 1.1423048973083496\n",
      "Epoch: 25, Samples: 3872/5760, Loss: 1.1792378425598145\n",
      "Epoch: 25, Samples: 3904/5760, Loss: 1.4043265581130981\n",
      "Epoch: 25, Samples: 3936/5760, Loss: 1.0315033197402954\n",
      "Epoch: 25, Samples: 3968/5760, Loss: 0.8940466642379761\n",
      "Epoch: 25, Samples: 4000/5760, Loss: 1.214255928993225\n",
      "Epoch: 25, Samples: 4032/5760, Loss: 0.8300290107727051\n",
      "Epoch: 25, Samples: 4064/5760, Loss: 1.0345104932785034\n",
      "Epoch: 25, Samples: 4096/5760, Loss: 1.0171737670898438\n",
      "Epoch: 25, Samples: 4128/5760, Loss: 0.9125832915306091\n",
      "Epoch: 25, Samples: 4160/5760, Loss: 0.9855760931968689\n",
      "Epoch: 25, Samples: 4192/5760, Loss: 1.444712519645691\n",
      "Epoch: 25, Samples: 4224/5760, Loss: 1.2079854011535645\n",
      "Epoch: 25, Samples: 4256/5760, Loss: 1.3181337118148804\n",
      "Epoch: 25, Samples: 4288/5760, Loss: 1.1631344556808472\n",
      "Epoch: 25, Samples: 4320/5760, Loss: 1.1481889486312866\n",
      "Epoch: 25, Samples: 4352/5760, Loss: 1.1063507795333862\n",
      "Epoch: 25, Samples: 4384/5760, Loss: 1.2566444873809814\n",
      "Epoch: 25, Samples: 4416/5760, Loss: 1.1513651609420776\n",
      "Epoch: 25, Samples: 4448/5760, Loss: 1.0177656412124634\n",
      "Epoch: 25, Samples: 4480/5760, Loss: 1.0297834873199463\n",
      "Epoch: 25, Samples: 4512/5760, Loss: 1.1116020679473877\n",
      "Epoch: 25, Samples: 4544/5760, Loss: 1.0318197011947632\n",
      "Epoch: 25, Samples: 4576/5760, Loss: 1.240970253944397\n",
      "Epoch: 25, Samples: 4608/5760, Loss: 1.0270133018493652\n",
      "Epoch: 25, Samples: 4640/5760, Loss: 1.2254552841186523\n",
      "Epoch: 25, Samples: 4672/5760, Loss: 0.9293272495269775\n",
      "Epoch: 25, Samples: 4704/5760, Loss: 1.184649109840393\n",
      "Epoch: 25, Samples: 4736/5760, Loss: 1.285598635673523\n",
      "Epoch: 25, Samples: 4768/5760, Loss: 0.9120863080024719\n",
      "Epoch: 25, Samples: 4800/5760, Loss: 0.9785176515579224\n",
      "Epoch: 25, Samples: 4832/5760, Loss: 1.070530891418457\n",
      "Epoch: 25, Samples: 4864/5760, Loss: 0.8234881162643433\n",
      "Epoch: 25, Samples: 4896/5760, Loss: 0.8139821887016296\n",
      "Epoch: 25, Samples: 4928/5760, Loss: 1.34326171875\n",
      "Epoch: 25, Samples: 4960/5760, Loss: 1.15950345993042\n",
      "Epoch: 25, Samples: 4992/5760, Loss: 1.0829206705093384\n",
      "Epoch: 25, Samples: 5024/5760, Loss: 1.392138957977295\n",
      "Epoch: 25, Samples: 5056/5760, Loss: 1.1809660196304321\n",
      "Epoch: 25, Samples: 5088/5760, Loss: 0.6773152351379395\n",
      "Epoch: 25, Samples: 5120/5760, Loss: 1.0998042821884155\n",
      "Epoch: 25, Samples: 5152/5760, Loss: 1.2884281873703003\n",
      "Epoch: 25, Samples: 5184/5760, Loss: 1.4983776807785034\n",
      "Epoch: 25, Samples: 5216/5760, Loss: 1.0040647983551025\n",
      "Epoch: 25, Samples: 5248/5760, Loss: 0.8994852304458618\n",
      "Epoch: 25, Samples: 5280/5760, Loss: 1.0454679727554321\n",
      "Epoch: 25, Samples: 5312/5760, Loss: 1.401371955871582\n",
      "Epoch: 25, Samples: 5344/5760, Loss: 1.472113013267517\n",
      "Epoch: 25, Samples: 5376/5760, Loss: 1.2037626504898071\n",
      "Epoch: 25, Samples: 5408/5760, Loss: 1.2628120183944702\n",
      "Epoch: 25, Samples: 5440/5760, Loss: 1.155547857284546\n",
      "Epoch: 25, Samples: 5472/5760, Loss: 0.9736392498016357\n",
      "Epoch: 25, Samples: 5504/5760, Loss: 1.0169135332107544\n",
      "Epoch: 25, Samples: 5536/5760, Loss: 1.1406581401824951\n",
      "Epoch: 25, Samples: 5568/5760, Loss: 1.010208010673523\n",
      "Epoch: 25, Samples: 5600/5760, Loss: 1.0813349485397339\n",
      "Epoch: 25, Samples: 5632/5760, Loss: 1.3170887231826782\n",
      "Epoch: 25, Samples: 5664/5760, Loss: 0.9982508420944214\n",
      "Epoch: 25, Samples: 5696/5760, Loss: 1.0153682231903076\n",
      "Epoch: 25, Samples: 5728/5760, Loss: 1.745859146118164\n",
      "\n",
      "Epoch: 25\n",
      "Training set: Average loss: 1.1587\n",
      "Validation set: Average loss: 1.5636, Accuracy: 494/818 (60%)\n",
      "Saving model (epoch 25) with lowest validation loss: 1.5635587710600634\n",
      "Epoch: 26, Samples: 0/5760, Loss: 0.7610964775085449\n",
      "Epoch: 26, Samples: 32/5760, Loss: 0.8743717670440674\n",
      "Epoch: 26, Samples: 64/5760, Loss: 0.6974079608917236\n",
      "Epoch: 26, Samples: 96/5760, Loss: 0.9836632013320923\n",
      "Epoch: 26, Samples: 128/5760, Loss: 1.0533182621002197\n",
      "Epoch: 26, Samples: 160/5760, Loss: 1.263559103012085\n",
      "Epoch: 26, Samples: 192/5760, Loss: 0.8980737924575806\n",
      "Epoch: 26, Samples: 224/5760, Loss: 1.019187331199646\n",
      "Epoch: 26, Samples: 256/5760, Loss: 1.1728322505950928\n",
      "Epoch: 26, Samples: 288/5760, Loss: 1.3403716087341309\n",
      "Epoch: 26, Samples: 320/5760, Loss: 0.9831488728523254\n",
      "Epoch: 26, Samples: 352/5760, Loss: 1.0067732334136963\n",
      "Epoch: 26, Samples: 384/5760, Loss: 1.265669345855713\n",
      "Epoch: 26, Samples: 416/5760, Loss: 1.5654596090316772\n",
      "Epoch: 26, Samples: 448/5760, Loss: 1.1454123258590698\n",
      "Epoch: 26, Samples: 480/5760, Loss: 0.46648213267326355\n",
      "Epoch: 26, Samples: 512/5760, Loss: 1.1423224210739136\n",
      "Epoch: 26, Samples: 544/5760, Loss: 1.0623449087142944\n",
      "Epoch: 26, Samples: 576/5760, Loss: 1.4338746070861816\n",
      "Epoch: 26, Samples: 608/5760, Loss: 0.923151969909668\n",
      "Epoch: 26, Samples: 640/5760, Loss: 1.2850669622421265\n",
      "Epoch: 26, Samples: 672/5760, Loss: 1.3058738708496094\n",
      "Epoch: 26, Samples: 704/5760, Loss: 1.2227760553359985\n",
      "Epoch: 26, Samples: 736/5760, Loss: 1.1232777833938599\n",
      "Epoch: 26, Samples: 768/5760, Loss: 1.1757304668426514\n",
      "Epoch: 26, Samples: 800/5760, Loss: 1.2195090055465698\n",
      "Epoch: 26, Samples: 832/5760, Loss: 0.8993285298347473\n",
      "Epoch: 26, Samples: 864/5760, Loss: 0.8704742193222046\n",
      "Epoch: 26, Samples: 896/5760, Loss: 0.7679183483123779\n",
      "Epoch: 26, Samples: 928/5760, Loss: 1.4862689971923828\n",
      "Epoch: 26, Samples: 960/5760, Loss: 1.1538946628570557\n",
      "Epoch: 26, Samples: 992/5760, Loss: 1.1577881574630737\n",
      "Epoch: 26, Samples: 1024/5760, Loss: 1.2601765394210815\n",
      "Epoch: 26, Samples: 1056/5760, Loss: 1.1892681121826172\n",
      "Epoch: 26, Samples: 1088/5760, Loss: 1.1399023532867432\n",
      "Epoch: 26, Samples: 1120/5760, Loss: 1.0938211679458618\n",
      "Epoch: 26, Samples: 1152/5760, Loss: 0.7873387336730957\n",
      "Epoch: 26, Samples: 1184/5760, Loss: 0.8454364538192749\n",
      "Epoch: 26, Samples: 1216/5760, Loss: 1.0683079957962036\n",
      "Epoch: 26, Samples: 1248/5760, Loss: 1.2767348289489746\n",
      "Epoch: 26, Samples: 1280/5760, Loss: 1.4796313047409058\n",
      "Epoch: 26, Samples: 1312/5760, Loss: 1.3673166036605835\n",
      "Epoch: 26, Samples: 1344/5760, Loss: 1.2428092956542969\n",
      "Epoch: 26, Samples: 1376/5760, Loss: 1.246809482574463\n",
      "Epoch: 26, Samples: 1408/5760, Loss: 0.9736916422843933\n",
      "Epoch: 26, Samples: 1440/5760, Loss: 0.9088252782821655\n",
      "Epoch: 26, Samples: 1472/5760, Loss: 1.102494239807129\n",
      "Epoch: 26, Samples: 1504/5760, Loss: 1.1749199628829956\n",
      "Epoch: 26, Samples: 1536/5760, Loss: 1.1015104055404663\n",
      "Epoch: 26, Samples: 1568/5760, Loss: 1.5573222637176514\n",
      "Epoch: 26, Samples: 1600/5760, Loss: 0.9323042631149292\n",
      "Epoch: 26, Samples: 1632/5760, Loss: 1.1450495719909668\n",
      "Epoch: 26, Samples: 1664/5760, Loss: 0.8102072477340698\n",
      "Epoch: 26, Samples: 1696/5760, Loss: 1.1380454301834106\n",
      "Epoch: 26, Samples: 1728/5760, Loss: 0.8125726580619812\n",
      "Epoch: 26, Samples: 1760/5760, Loss: 1.0003025531768799\n",
      "Epoch: 26, Samples: 1792/5760, Loss: 1.0038236379623413\n",
      "Epoch: 26, Samples: 1824/5760, Loss: 0.9221102595329285\n",
      "Epoch: 26, Samples: 1856/5760, Loss: 1.1300510168075562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Samples: 1888/5760, Loss: 0.8312450051307678\n",
      "Epoch: 26, Samples: 1920/5760, Loss: 0.9187774062156677\n",
      "Epoch: 26, Samples: 1952/5760, Loss: 0.9013252258300781\n",
      "Epoch: 26, Samples: 1984/5760, Loss: 0.8516994714736938\n",
      "Epoch: 26, Samples: 2016/5760, Loss: 1.589367151260376\n",
      "Epoch: 26, Samples: 2048/5760, Loss: 0.8542047739028931\n",
      "Epoch: 26, Samples: 2080/5760, Loss: 0.7483576536178589\n",
      "Epoch: 26, Samples: 2112/5760, Loss: 1.4351426362991333\n",
      "Epoch: 26, Samples: 2144/5760, Loss: 1.4313596487045288\n",
      "Epoch: 26, Samples: 2176/5760, Loss: 0.8746873736381531\n",
      "Epoch: 26, Samples: 2208/5760, Loss: 1.254750370979309\n",
      "Epoch: 26, Samples: 2240/5760, Loss: 1.094673991203308\n",
      "Epoch: 26, Samples: 2272/5760, Loss: 0.5700457096099854\n",
      "Epoch: 26, Samples: 2304/5760, Loss: 1.4261118173599243\n",
      "Epoch: 26, Samples: 2336/5760, Loss: 1.0601681470870972\n",
      "Epoch: 26, Samples: 2368/5760, Loss: 1.6712255477905273\n",
      "Epoch: 26, Samples: 2400/5760, Loss: 1.2717247009277344\n",
      "Epoch: 26, Samples: 2432/5760, Loss: 0.9222952127456665\n",
      "Epoch: 26, Samples: 2464/5760, Loss: 1.3736698627471924\n",
      "Epoch: 26, Samples: 2496/5760, Loss: 1.0001049041748047\n",
      "Epoch: 26, Samples: 2528/5760, Loss: 1.0373129844665527\n",
      "Epoch: 26, Samples: 2560/5760, Loss: 0.8615237474441528\n",
      "Epoch: 26, Samples: 2592/5760, Loss: 1.0933459997177124\n",
      "Epoch: 26, Samples: 2624/5760, Loss: 1.0214635133743286\n",
      "Epoch: 26, Samples: 2656/5760, Loss: 0.9441378116607666\n",
      "Epoch: 26, Samples: 2688/5760, Loss: 1.3205252885818481\n",
      "Epoch: 26, Samples: 2720/5760, Loss: 1.0695462226867676\n",
      "Epoch: 26, Samples: 2752/5760, Loss: 1.3045318126678467\n",
      "Epoch: 26, Samples: 2784/5760, Loss: 1.6732163429260254\n",
      "Epoch: 26, Samples: 2816/5760, Loss: 1.4014536142349243\n",
      "Epoch: 26, Samples: 2848/5760, Loss: 0.9241881966590881\n",
      "Epoch: 26, Samples: 2880/5760, Loss: 1.3512991666793823\n",
      "Epoch: 26, Samples: 2912/5760, Loss: 0.8128566741943359\n",
      "Epoch: 26, Samples: 2944/5760, Loss: 1.2096996307373047\n",
      "Epoch: 26, Samples: 2976/5760, Loss: 1.177876591682434\n",
      "Epoch: 26, Samples: 3008/5760, Loss: 1.154270052909851\n",
      "Epoch: 26, Samples: 3040/5760, Loss: 1.496366262435913\n",
      "Epoch: 26, Samples: 3072/5760, Loss: 1.0391331911087036\n",
      "Epoch: 26, Samples: 3104/5760, Loss: 1.226852536201477\n",
      "Epoch: 26, Samples: 3136/5760, Loss: 1.0314172506332397\n",
      "Epoch: 26, Samples: 3168/5760, Loss: 1.4061156511306763\n",
      "Epoch: 26, Samples: 3200/5760, Loss: 0.9496838450431824\n",
      "Epoch: 26, Samples: 3232/5760, Loss: 0.9748845100402832\n",
      "Epoch: 26, Samples: 3264/5760, Loss: 1.3458449840545654\n",
      "Epoch: 26, Samples: 3296/5760, Loss: 1.0304094552993774\n",
      "Epoch: 26, Samples: 3328/5760, Loss: 1.0997512340545654\n",
      "Epoch: 26, Samples: 3360/5760, Loss: 0.8571082353591919\n",
      "Epoch: 26, Samples: 3392/5760, Loss: 1.1246105432510376\n",
      "Epoch: 26, Samples: 3424/5760, Loss: 1.1216140985488892\n",
      "Epoch: 26, Samples: 3456/5760, Loss: 1.4341392517089844\n",
      "Epoch: 26, Samples: 3488/5760, Loss: 1.16837739944458\n",
      "Epoch: 26, Samples: 3520/5760, Loss: 0.9594186544418335\n",
      "Epoch: 26, Samples: 3552/5760, Loss: 0.9852042198181152\n",
      "Epoch: 26, Samples: 3584/5760, Loss: 1.2052689790725708\n",
      "Epoch: 26, Samples: 3616/5760, Loss: 0.9738762974739075\n",
      "Epoch: 26, Samples: 3648/5760, Loss: 1.3646448850631714\n",
      "Epoch: 26, Samples: 3680/5760, Loss: 1.3071041107177734\n",
      "Epoch: 26, Samples: 3712/5760, Loss: 1.2815643548965454\n",
      "Epoch: 26, Samples: 3744/5760, Loss: 1.3598026037216187\n",
      "Epoch: 26, Samples: 3776/5760, Loss: 1.350092887878418\n",
      "Epoch: 26, Samples: 3808/5760, Loss: 0.8831552863121033\n",
      "Epoch: 26, Samples: 3840/5760, Loss: 0.9928205609321594\n",
      "Epoch: 26, Samples: 3872/5760, Loss: 0.6188597679138184\n",
      "Epoch: 26, Samples: 3904/5760, Loss: 1.1702444553375244\n",
      "Epoch: 26, Samples: 3936/5760, Loss: 1.308998942375183\n",
      "Epoch: 26, Samples: 3968/5760, Loss: 1.1741960048675537\n",
      "Epoch: 26, Samples: 4000/5760, Loss: 1.0471608638763428\n",
      "Epoch: 26, Samples: 4032/5760, Loss: 0.8373059034347534\n",
      "Epoch: 26, Samples: 4064/5760, Loss: 0.9100850224494934\n",
      "Epoch: 26, Samples: 4096/5760, Loss: 1.5304937362670898\n",
      "Epoch: 26, Samples: 4128/5760, Loss: 1.118884563446045\n",
      "Epoch: 26, Samples: 4160/5760, Loss: 1.164306402206421\n",
      "Epoch: 26, Samples: 4192/5760, Loss: 1.296534538269043\n",
      "Epoch: 26, Samples: 4224/5760, Loss: 1.0716971158981323\n",
      "Epoch: 26, Samples: 4256/5760, Loss: 1.0847117900848389\n",
      "Epoch: 26, Samples: 4288/5760, Loss: 1.088139295578003\n",
      "Epoch: 26, Samples: 4320/5760, Loss: 1.4031249284744263\n",
      "Epoch: 26, Samples: 4352/5760, Loss: 0.9562935829162598\n",
      "Epoch: 26, Samples: 4384/5760, Loss: 1.3411812782287598\n",
      "Epoch: 26, Samples: 4416/5760, Loss: 0.8480817675590515\n",
      "Epoch: 26, Samples: 4448/5760, Loss: 1.2666571140289307\n",
      "Epoch: 26, Samples: 4480/5760, Loss: 0.7916730046272278\n",
      "Epoch: 26, Samples: 4512/5760, Loss: 1.3449584245681763\n",
      "Epoch: 26, Samples: 4544/5760, Loss: 1.2711368799209595\n",
      "Epoch: 26, Samples: 4576/5760, Loss: 1.2817106246948242\n",
      "Epoch: 26, Samples: 4608/5760, Loss: 1.0434083938598633\n",
      "Epoch: 26, Samples: 4640/5760, Loss: 0.8750843405723572\n",
      "Epoch: 26, Samples: 4672/5760, Loss: 1.4195095300674438\n",
      "Epoch: 26, Samples: 4704/5760, Loss: 0.8093132972717285\n",
      "Epoch: 26, Samples: 4736/5760, Loss: 0.9533334374427795\n",
      "Epoch: 26, Samples: 4768/5760, Loss: 1.202591061592102\n",
      "Epoch: 26, Samples: 4800/5760, Loss: 0.7314649224281311\n",
      "Epoch: 26, Samples: 4832/5760, Loss: 1.018538236618042\n",
      "Epoch: 26, Samples: 4864/5760, Loss: 1.098368763923645\n",
      "Epoch: 26, Samples: 4896/5760, Loss: 1.3024704456329346\n",
      "Epoch: 26, Samples: 4928/5760, Loss: 0.845633864402771\n",
      "Epoch: 26, Samples: 4960/5760, Loss: 1.1051322221755981\n",
      "Epoch: 26, Samples: 4992/5760, Loss: 0.7521747946739197\n",
      "Epoch: 26, Samples: 5024/5760, Loss: 1.1703801155090332\n",
      "Epoch: 26, Samples: 5056/5760, Loss: 1.4397777318954468\n",
      "Epoch: 26, Samples: 5088/5760, Loss: 1.3919456005096436\n",
      "Epoch: 26, Samples: 5120/5760, Loss: 1.151144027709961\n",
      "Epoch: 26, Samples: 5152/5760, Loss: 1.1273682117462158\n",
      "Epoch: 26, Samples: 5184/5760, Loss: 1.0375832319259644\n",
      "Epoch: 26, Samples: 5216/5760, Loss: 1.4067593812942505\n",
      "Epoch: 26, Samples: 5248/5760, Loss: 0.6904923319816589\n",
      "Epoch: 26, Samples: 5280/5760, Loss: 0.9414530992507935\n",
      "Epoch: 26, Samples: 5312/5760, Loss: 1.7269893884658813\n",
      "Epoch: 26, Samples: 5344/5760, Loss: 1.0380220413208008\n",
      "Epoch: 26, Samples: 5376/5760, Loss: 1.127089500427246\n",
      "Epoch: 26, Samples: 5408/5760, Loss: 0.9578182697296143\n",
      "Epoch: 26, Samples: 5440/5760, Loss: 1.0575995445251465\n",
      "Epoch: 26, Samples: 5472/5760, Loss: 1.1540019512176514\n",
      "Epoch: 26, Samples: 5504/5760, Loss: 0.7571846842765808\n",
      "Epoch: 26, Samples: 5536/5760, Loss: 1.2929491996765137\n",
      "Epoch: 26, Samples: 5568/5760, Loss: 0.7151994705200195\n",
      "Epoch: 26, Samples: 5600/5760, Loss: 1.4668192863464355\n",
      "Epoch: 26, Samples: 5632/5760, Loss: 0.9395914673805237\n",
      "Epoch: 26, Samples: 5664/5760, Loss: 0.8589915633201599\n",
      "Epoch: 26, Samples: 5696/5760, Loss: 1.0558404922485352\n",
      "Epoch: 26, Samples: 5728/5760, Loss: 1.914383888244629\n",
      "\n",
      "Epoch: 26\n",
      "Training set: Average loss: 1.1130\n",
      "Validation set: Average loss: 1.7841, Accuracy: 447/818 (55%)\n",
      "Epoch: 27, Samples: 0/5760, Loss: 0.8154715299606323\n",
      "Epoch: 27, Samples: 32/5760, Loss: 1.230615496635437\n",
      "Epoch: 27, Samples: 64/5760, Loss: 1.318353295326233\n",
      "Epoch: 27, Samples: 96/5760, Loss: 1.0307598114013672\n",
      "Epoch: 27, Samples: 128/5760, Loss: 1.209030270576477\n",
      "Epoch: 27, Samples: 160/5760, Loss: 1.1765902042388916\n",
      "Epoch: 27, Samples: 192/5760, Loss: 0.5753438472747803\n",
      "Epoch: 27, Samples: 224/5760, Loss: 1.1329468488693237\n",
      "Epoch: 27, Samples: 256/5760, Loss: 1.3949013948440552\n",
      "Epoch: 27, Samples: 288/5760, Loss: 0.9113547801971436\n",
      "Epoch: 27, Samples: 320/5760, Loss: 1.092883586883545\n",
      "Epoch: 27, Samples: 352/5760, Loss: 1.8776021003723145\n",
      "Epoch: 27, Samples: 384/5760, Loss: 1.1888401508331299\n",
      "Epoch: 27, Samples: 416/5760, Loss: 1.357622742652893\n",
      "Epoch: 27, Samples: 448/5760, Loss: 1.318792462348938\n",
      "Epoch: 27, Samples: 480/5760, Loss: 1.1010105609893799\n",
      "Epoch: 27, Samples: 512/5760, Loss: 1.1496942043304443\n",
      "Epoch: 27, Samples: 544/5760, Loss: 1.06211256980896\n",
      "Epoch: 27, Samples: 576/5760, Loss: 1.117897868156433\n",
      "Epoch: 27, Samples: 608/5760, Loss: 0.9361797571182251\n",
      "Epoch: 27, Samples: 640/5760, Loss: 1.1583670377731323\n",
      "Epoch: 27, Samples: 672/5760, Loss: 0.9452043771743774\n",
      "Epoch: 27, Samples: 704/5760, Loss: 0.8208335041999817\n",
      "Epoch: 27, Samples: 736/5760, Loss: 0.8404237627983093\n",
      "Epoch: 27, Samples: 768/5760, Loss: 1.18267822265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Samples: 800/5760, Loss: 1.1959326267242432\n",
      "Epoch: 27, Samples: 832/5760, Loss: 1.2398018836975098\n",
      "Epoch: 27, Samples: 864/5760, Loss: 1.3487530946731567\n",
      "Epoch: 27, Samples: 896/5760, Loss: 1.139305591583252\n",
      "Epoch: 27, Samples: 928/5760, Loss: 1.0189366340637207\n",
      "Epoch: 27, Samples: 960/5760, Loss: 0.6469734907150269\n",
      "Epoch: 27, Samples: 992/5760, Loss: 1.0446758270263672\n",
      "Epoch: 27, Samples: 1024/5760, Loss: 1.0557111501693726\n",
      "Epoch: 27, Samples: 1056/5760, Loss: 1.0124329328536987\n",
      "Epoch: 27, Samples: 1088/5760, Loss: 1.3642045259475708\n",
      "Epoch: 27, Samples: 1120/5760, Loss: 0.8139588236808777\n",
      "Epoch: 27, Samples: 1152/5760, Loss: 0.9078629612922668\n",
      "Epoch: 27, Samples: 1184/5760, Loss: 1.2421741485595703\n",
      "Epoch: 27, Samples: 1216/5760, Loss: 0.7661166191101074\n",
      "Epoch: 27, Samples: 1248/5760, Loss: 1.208428144454956\n",
      "Epoch: 27, Samples: 1280/5760, Loss: 1.0583081245422363\n",
      "Epoch: 27, Samples: 1312/5760, Loss: 1.3635355234146118\n",
      "Epoch: 27, Samples: 1344/5760, Loss: 1.1179933547973633\n",
      "Epoch: 27, Samples: 1376/5760, Loss: 1.0334904193878174\n",
      "Epoch: 27, Samples: 1408/5760, Loss: 1.3576182126998901\n",
      "Epoch: 27, Samples: 1440/5760, Loss: 1.044379711151123\n",
      "Epoch: 27, Samples: 1472/5760, Loss: 0.9673422574996948\n",
      "Epoch: 27, Samples: 1504/5760, Loss: 0.9676335453987122\n",
      "Epoch: 27, Samples: 1536/5760, Loss: 1.0147165060043335\n",
      "Epoch: 27, Samples: 1568/5760, Loss: 0.7168676853179932\n",
      "Epoch: 27, Samples: 1600/5760, Loss: 1.2580487728118896\n",
      "Epoch: 27, Samples: 1632/5760, Loss: 0.905787467956543\n",
      "Epoch: 27, Samples: 1664/5760, Loss: 0.9935414791107178\n",
      "Epoch: 27, Samples: 1696/5760, Loss: 0.7195440530776978\n",
      "Epoch: 27, Samples: 1728/5760, Loss: 1.0897023677825928\n",
      "Epoch: 27, Samples: 1760/5760, Loss: 0.9704947471618652\n",
      "Epoch: 27, Samples: 1792/5760, Loss: 1.0219459533691406\n",
      "Epoch: 27, Samples: 1824/5760, Loss: 0.9932708740234375\n",
      "Epoch: 27, Samples: 1856/5760, Loss: 0.9762907028198242\n",
      "Epoch: 27, Samples: 1888/5760, Loss: 1.3336422443389893\n",
      "Epoch: 27, Samples: 1920/5760, Loss: 1.0509071350097656\n",
      "Epoch: 27, Samples: 1952/5760, Loss: 0.7306216955184937\n",
      "Epoch: 27, Samples: 1984/5760, Loss: 1.4649713039398193\n",
      "Epoch: 27, Samples: 2016/5760, Loss: 1.133138656616211\n",
      "Epoch: 27, Samples: 2048/5760, Loss: 1.056435227394104\n",
      "Epoch: 27, Samples: 2080/5760, Loss: 0.7982126474380493\n",
      "Epoch: 27, Samples: 2112/5760, Loss: 1.1643368005752563\n",
      "Epoch: 27, Samples: 2144/5760, Loss: 1.1148407459259033\n",
      "Epoch: 27, Samples: 2176/5760, Loss: 0.7821775674819946\n",
      "Epoch: 27, Samples: 2208/5760, Loss: 0.8707214593887329\n",
      "Epoch: 27, Samples: 2240/5760, Loss: 1.0886422395706177\n",
      "Epoch: 27, Samples: 2272/5760, Loss: 0.8886658549308777\n",
      "Epoch: 27, Samples: 2304/5760, Loss: 1.1224567890167236\n",
      "Epoch: 27, Samples: 2336/5760, Loss: 1.0208299160003662\n",
      "Epoch: 27, Samples: 2368/5760, Loss: 1.0443050861358643\n",
      "Epoch: 27, Samples: 2400/5760, Loss: 1.0483593940734863\n",
      "Epoch: 27, Samples: 2432/5760, Loss: 1.0529279708862305\n",
      "Epoch: 27, Samples: 2464/5760, Loss: 1.188570499420166\n",
      "Epoch: 27, Samples: 2496/5760, Loss: 1.2082092761993408\n",
      "Epoch: 27, Samples: 2528/5760, Loss: 1.0638363361358643\n",
      "Epoch: 27, Samples: 2560/5760, Loss: 1.101312518119812\n",
      "Epoch: 27, Samples: 2592/5760, Loss: 0.8434257507324219\n",
      "Epoch: 27, Samples: 2624/5760, Loss: 1.099178433418274\n",
      "Epoch: 27, Samples: 2656/5760, Loss: 1.0214333534240723\n",
      "Epoch: 27, Samples: 2688/5760, Loss: 0.9699255228042603\n",
      "Epoch: 27, Samples: 2720/5760, Loss: 1.1498486995697021\n",
      "Epoch: 27, Samples: 2752/5760, Loss: 0.8237329721450806\n",
      "Epoch: 27, Samples: 2784/5760, Loss: 0.8966764211654663\n",
      "Epoch: 27, Samples: 2816/5760, Loss: 0.8241900205612183\n",
      "Epoch: 27, Samples: 2848/5760, Loss: 0.9766693711280823\n",
      "Epoch: 27, Samples: 2880/5760, Loss: 0.8333369493484497\n",
      "Epoch: 27, Samples: 2912/5760, Loss: 1.0349626541137695\n",
      "Epoch: 27, Samples: 2944/5760, Loss: 0.9104151725769043\n",
      "Epoch: 27, Samples: 2976/5760, Loss: 1.0773288011550903\n",
      "Epoch: 27, Samples: 3008/5760, Loss: 0.8205212354660034\n",
      "Epoch: 27, Samples: 3040/5760, Loss: 0.8355833292007446\n",
      "Epoch: 27, Samples: 3072/5760, Loss: 1.2858331203460693\n",
      "Epoch: 27, Samples: 3104/5760, Loss: 1.256148099899292\n",
      "Epoch: 27, Samples: 3136/5760, Loss: 1.0026034116744995\n",
      "Epoch: 27, Samples: 3168/5760, Loss: 1.0743677616119385\n",
      "Epoch: 27, Samples: 3200/5760, Loss: 1.0650352239608765\n",
      "Epoch: 27, Samples: 3232/5760, Loss: 1.1131339073181152\n",
      "Epoch: 27, Samples: 3264/5760, Loss: 0.9902654886245728\n",
      "Epoch: 27, Samples: 3296/5760, Loss: 1.088491439819336\n",
      "Epoch: 27, Samples: 3328/5760, Loss: 1.0683979988098145\n",
      "Epoch: 27, Samples: 3360/5760, Loss: 1.0507115125656128\n",
      "Epoch: 27, Samples: 3392/5760, Loss: 1.0504995584487915\n",
      "Epoch: 27, Samples: 3424/5760, Loss: 0.7545837163925171\n",
      "Epoch: 27, Samples: 3456/5760, Loss: 0.8163914680480957\n",
      "Epoch: 27, Samples: 3488/5760, Loss: 1.1683106422424316\n",
      "Epoch: 27, Samples: 3520/5760, Loss: 1.0790973901748657\n",
      "Epoch: 27, Samples: 3552/5760, Loss: 1.030437707901001\n",
      "Epoch: 27, Samples: 3584/5760, Loss: 0.9659353494644165\n",
      "Epoch: 27, Samples: 3616/5760, Loss: 0.9399966597557068\n",
      "Epoch: 27, Samples: 3648/5760, Loss: 0.9923776388168335\n",
      "Epoch: 27, Samples: 3680/5760, Loss: 1.026726245880127\n",
      "Epoch: 27, Samples: 3712/5760, Loss: 0.8643876314163208\n",
      "Epoch: 27, Samples: 3744/5760, Loss: 1.2567040920257568\n",
      "Epoch: 27, Samples: 3776/5760, Loss: 0.6139975786209106\n",
      "Epoch: 27, Samples: 3808/5760, Loss: 0.9682726860046387\n",
      "Epoch: 27, Samples: 3840/5760, Loss: 0.906502366065979\n",
      "Epoch: 27, Samples: 3872/5760, Loss: 0.9124668836593628\n",
      "Epoch: 27, Samples: 3904/5760, Loss: 1.2717870473861694\n",
      "Epoch: 27, Samples: 3936/5760, Loss: 1.147796869277954\n",
      "Epoch: 27, Samples: 3968/5760, Loss: 1.1946423053741455\n",
      "Epoch: 27, Samples: 4000/5760, Loss: 1.7365524768829346\n",
      "Epoch: 27, Samples: 4032/5760, Loss: 1.3941105604171753\n",
      "Epoch: 27, Samples: 4064/5760, Loss: 1.0633749961853027\n",
      "Epoch: 27, Samples: 4096/5760, Loss: 0.9799846410751343\n",
      "Epoch: 27, Samples: 4128/5760, Loss: 0.9562860131263733\n",
      "Epoch: 27, Samples: 4160/5760, Loss: 0.6461054086685181\n",
      "Epoch: 27, Samples: 4192/5760, Loss: 1.136006474494934\n",
      "Epoch: 27, Samples: 4224/5760, Loss: 0.9454965591430664\n",
      "Epoch: 27, Samples: 4256/5760, Loss: 0.7610206604003906\n",
      "Epoch: 27, Samples: 4288/5760, Loss: 0.978903591632843\n",
      "Epoch: 27, Samples: 4320/5760, Loss: 0.6533734798431396\n",
      "Epoch: 27, Samples: 4352/5760, Loss: 0.7427714467048645\n",
      "Epoch: 27, Samples: 4384/5760, Loss: 0.8336732983589172\n",
      "Epoch: 27, Samples: 4416/5760, Loss: 0.7621806263923645\n",
      "Epoch: 27, Samples: 4448/5760, Loss: 1.118746042251587\n",
      "Epoch: 27, Samples: 4480/5760, Loss: 0.9678637981414795\n",
      "Epoch: 27, Samples: 4512/5760, Loss: 1.0405505895614624\n",
      "Epoch: 27, Samples: 4544/5760, Loss: 1.643339991569519\n",
      "Epoch: 27, Samples: 4576/5760, Loss: 1.191220760345459\n",
      "Epoch: 27, Samples: 4608/5760, Loss: 0.99701327085495\n",
      "Epoch: 27, Samples: 4640/5760, Loss: 1.3104912042617798\n",
      "Epoch: 27, Samples: 4672/5760, Loss: 0.9450591206550598\n",
      "Epoch: 27, Samples: 4704/5760, Loss: 0.8558403253555298\n",
      "Epoch: 27, Samples: 4736/5760, Loss: 1.0942203998565674\n",
      "Epoch: 27, Samples: 4768/5760, Loss: 1.0311816930770874\n",
      "Epoch: 27, Samples: 4800/5760, Loss: 1.0321362018585205\n",
      "Epoch: 27, Samples: 4832/5760, Loss: 1.0808981657028198\n",
      "Epoch: 27, Samples: 4864/5760, Loss: 0.788863480091095\n",
      "Epoch: 27, Samples: 4896/5760, Loss: 0.753037691116333\n",
      "Epoch: 27, Samples: 4928/5760, Loss: 1.1585495471954346\n",
      "Epoch: 27, Samples: 4960/5760, Loss: 1.393158197402954\n",
      "Epoch: 27, Samples: 4992/5760, Loss: 0.8392008543014526\n",
      "Epoch: 27, Samples: 5024/5760, Loss: 0.8868163228034973\n",
      "Epoch: 27, Samples: 5056/5760, Loss: 1.2307078838348389\n",
      "Epoch: 27, Samples: 5088/5760, Loss: 0.8776257038116455\n",
      "Epoch: 27, Samples: 5120/5760, Loss: 0.6278704404830933\n",
      "Epoch: 27, Samples: 5152/5760, Loss: 0.7123410105705261\n",
      "Epoch: 27, Samples: 5184/5760, Loss: 1.0114378929138184\n",
      "Epoch: 27, Samples: 5216/5760, Loss: 1.2231487035751343\n",
      "Epoch: 27, Samples: 5248/5760, Loss: 0.7607143521308899\n",
      "Epoch: 27, Samples: 5280/5760, Loss: 1.00704824924469\n",
      "Epoch: 27, Samples: 5312/5760, Loss: 0.9445317387580872\n",
      "Epoch: 27, Samples: 5344/5760, Loss: 0.7795581817626953\n",
      "Epoch: 27, Samples: 5376/5760, Loss: 0.8419542908668518\n",
      "Epoch: 27, Samples: 5408/5760, Loss: 1.1407045125961304\n",
      "Epoch: 27, Samples: 5440/5760, Loss: 0.8957626819610596\n",
      "Epoch: 27, Samples: 5472/5760, Loss: 1.1162583827972412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Samples: 5504/5760, Loss: 1.2043850421905518\n",
      "Epoch: 27, Samples: 5536/5760, Loss: 1.220350742340088\n",
      "Epoch: 27, Samples: 5568/5760, Loss: 1.422324776649475\n",
      "Epoch: 27, Samples: 5600/5760, Loss: 0.9348672032356262\n",
      "Epoch: 27, Samples: 5632/5760, Loss: 1.0429584980010986\n",
      "Epoch: 27, Samples: 5664/5760, Loss: 1.4577997922897339\n",
      "Epoch: 27, Samples: 5696/5760, Loss: 1.0038325786590576\n",
      "Epoch: 27, Samples: 5728/5760, Loss: 2.523038864135742\n",
      "\n",
      "Epoch: 27\n",
      "Training set: Average loss: 1.0479\n",
      "Validation set: Average loss: 1.9372, Accuracy: 419/818 (51%)\n",
      "Epoch: 28, Samples: 0/5760, Loss: 0.8927900195121765\n",
      "Epoch: 28, Samples: 32/5760, Loss: 0.6896288394927979\n",
      "Epoch: 28, Samples: 64/5760, Loss: 1.01431143283844\n",
      "Epoch: 28, Samples: 96/5760, Loss: 0.7064847350120544\n",
      "Epoch: 28, Samples: 128/5760, Loss: 1.1706910133361816\n",
      "Epoch: 28, Samples: 160/5760, Loss: 1.2840138673782349\n",
      "Epoch: 28, Samples: 192/5760, Loss: 1.1324743032455444\n",
      "Epoch: 28, Samples: 224/5760, Loss: 1.2961715459823608\n",
      "Epoch: 28, Samples: 256/5760, Loss: 0.964871883392334\n",
      "Epoch: 28, Samples: 288/5760, Loss: 0.7378177046775818\n",
      "Epoch: 28, Samples: 320/5760, Loss: 0.716467559337616\n",
      "Epoch: 28, Samples: 352/5760, Loss: 0.8820086121559143\n",
      "Epoch: 28, Samples: 384/5760, Loss: 1.2064740657806396\n",
      "Epoch: 28, Samples: 416/5760, Loss: 0.8200411796569824\n",
      "Epoch: 28, Samples: 448/5760, Loss: 0.9316250085830688\n",
      "Epoch: 28, Samples: 480/5760, Loss: 1.3005955219268799\n",
      "Epoch: 28, Samples: 512/5760, Loss: 0.695976972579956\n",
      "Epoch: 28, Samples: 544/5760, Loss: 1.415238857269287\n",
      "Epoch: 28, Samples: 576/5760, Loss: 0.7813183069229126\n",
      "Epoch: 28, Samples: 608/5760, Loss: 0.6928184628486633\n",
      "Epoch: 28, Samples: 640/5760, Loss: 1.0006165504455566\n",
      "Epoch: 28, Samples: 672/5760, Loss: 0.7904043197631836\n",
      "Epoch: 28, Samples: 704/5760, Loss: 0.8354155421257019\n",
      "Epoch: 28, Samples: 736/5760, Loss: 1.0338951349258423\n",
      "Epoch: 28, Samples: 768/5760, Loss: 0.818426787853241\n",
      "Epoch: 28, Samples: 800/5760, Loss: 0.9225634932518005\n",
      "Epoch: 28, Samples: 832/5760, Loss: 1.3540183305740356\n",
      "Epoch: 28, Samples: 864/5760, Loss: 0.6790328025817871\n",
      "Epoch: 28, Samples: 896/5760, Loss: 1.2164499759674072\n",
      "Epoch: 28, Samples: 928/5760, Loss: 0.8853421211242676\n",
      "Epoch: 28, Samples: 960/5760, Loss: 0.8700629472732544\n",
      "Epoch: 28, Samples: 992/5760, Loss: 0.7602219581604004\n",
      "Epoch: 28, Samples: 1024/5760, Loss: 0.7617447376251221\n",
      "Epoch: 28, Samples: 1056/5760, Loss: 0.8166780471801758\n",
      "Epoch: 28, Samples: 1088/5760, Loss: 1.1076242923736572\n",
      "Epoch: 28, Samples: 1120/5760, Loss: 0.9005786180496216\n",
      "Epoch: 28, Samples: 1152/5760, Loss: 0.9176349639892578\n",
      "Epoch: 28, Samples: 1184/5760, Loss: 1.252403974533081\n",
      "Epoch: 28, Samples: 1216/5760, Loss: 1.0947856903076172\n",
      "Epoch: 28, Samples: 1248/5760, Loss: 0.9188584089279175\n",
      "Epoch: 28, Samples: 1280/5760, Loss: 1.170668125152588\n",
      "Epoch: 28, Samples: 1312/5760, Loss: 1.1579194068908691\n",
      "Epoch: 28, Samples: 1344/5760, Loss: 0.9389362931251526\n",
      "Epoch: 28, Samples: 1376/5760, Loss: 1.0201319456100464\n",
      "Epoch: 28, Samples: 1408/5760, Loss: 0.8937627077102661\n",
      "Epoch: 28, Samples: 1440/5760, Loss: 1.1443922519683838\n",
      "Epoch: 28, Samples: 1472/5760, Loss: 1.3612245321273804\n",
      "Epoch: 28, Samples: 1504/5760, Loss: 0.7183836102485657\n",
      "Epoch: 28, Samples: 1536/5760, Loss: 1.0187726020812988\n",
      "Epoch: 28, Samples: 1568/5760, Loss: 1.044789433479309\n",
      "Epoch: 28, Samples: 1600/5760, Loss: 0.9130046963691711\n",
      "Epoch: 28, Samples: 1632/5760, Loss: 0.8509142398834229\n",
      "Epoch: 28, Samples: 1664/5760, Loss: 1.1399894952774048\n",
      "Epoch: 28, Samples: 1696/5760, Loss: 0.8375917673110962\n",
      "Epoch: 28, Samples: 1728/5760, Loss: 1.2009538412094116\n",
      "Epoch: 28, Samples: 1760/5760, Loss: 0.8913182020187378\n",
      "Epoch: 28, Samples: 1792/5760, Loss: 1.7220094203948975\n",
      "Epoch: 28, Samples: 1824/5760, Loss: 0.7631726861000061\n",
      "Epoch: 28, Samples: 1856/5760, Loss: 0.882256269454956\n",
      "Epoch: 28, Samples: 1888/5760, Loss: 0.9191780090332031\n",
      "Epoch: 28, Samples: 1920/5760, Loss: 1.3597215414047241\n",
      "Epoch: 28, Samples: 1952/5760, Loss: 1.2769197225570679\n",
      "Epoch: 28, Samples: 1984/5760, Loss: 1.0738446712493896\n",
      "Epoch: 28, Samples: 2016/5760, Loss: 0.8642873764038086\n",
      "Epoch: 28, Samples: 2048/5760, Loss: 1.007947325706482\n",
      "Epoch: 28, Samples: 2080/5760, Loss: 0.9429906010627747\n",
      "Epoch: 28, Samples: 2112/5760, Loss: 1.0769095420837402\n",
      "Epoch: 28, Samples: 2144/5760, Loss: 0.96503746509552\n",
      "Epoch: 28, Samples: 2176/5760, Loss: 0.8698421120643616\n",
      "Epoch: 28, Samples: 2208/5760, Loss: 0.9739465713500977\n",
      "Epoch: 28, Samples: 2240/5760, Loss: 0.8369706869125366\n",
      "Epoch: 28, Samples: 2272/5760, Loss: 1.3118677139282227\n",
      "Epoch: 28, Samples: 2304/5760, Loss: 1.4369257688522339\n",
      "Epoch: 28, Samples: 2336/5760, Loss: 0.6583652496337891\n",
      "Epoch: 28, Samples: 2368/5760, Loss: 1.0685464143753052\n",
      "Epoch: 28, Samples: 2400/5760, Loss: 1.0482984781265259\n",
      "Epoch: 28, Samples: 2432/5760, Loss: 1.0579386949539185\n",
      "Epoch: 28, Samples: 2464/5760, Loss: 1.0430214405059814\n",
      "Epoch: 28, Samples: 2496/5760, Loss: 0.8856931924819946\n",
      "Epoch: 28, Samples: 2528/5760, Loss: 1.3363654613494873\n",
      "Epoch: 28, Samples: 2560/5760, Loss: 0.5238347053527832\n",
      "Epoch: 28, Samples: 2592/5760, Loss: 0.9051258563995361\n",
      "Epoch: 28, Samples: 2624/5760, Loss: 1.1375062465667725\n",
      "Epoch: 28, Samples: 2656/5760, Loss: 0.8574322462081909\n",
      "Epoch: 28, Samples: 2688/5760, Loss: 1.206431269645691\n",
      "Epoch: 28, Samples: 2720/5760, Loss: 1.0185937881469727\n",
      "Epoch: 28, Samples: 2752/5760, Loss: 0.8399795889854431\n",
      "Epoch: 28, Samples: 2784/5760, Loss: 1.4924297332763672\n",
      "Epoch: 28, Samples: 2816/5760, Loss: 0.9506826400756836\n",
      "Epoch: 28, Samples: 2848/5760, Loss: 0.8805406093597412\n",
      "Epoch: 28, Samples: 2880/5760, Loss: 1.1002129316329956\n",
      "Epoch: 28, Samples: 2912/5760, Loss: 0.857079803943634\n",
      "Epoch: 28, Samples: 2944/5760, Loss: 0.8425486087799072\n",
      "Epoch: 28, Samples: 2976/5760, Loss: 1.39139723777771\n",
      "Epoch: 28, Samples: 3008/5760, Loss: 0.9844319820404053\n",
      "Epoch: 28, Samples: 3040/5760, Loss: 1.4796628952026367\n",
      "Epoch: 28, Samples: 3072/5760, Loss: 0.9936341643333435\n",
      "Epoch: 28, Samples: 3104/5760, Loss: 1.1699215173721313\n",
      "Epoch: 28, Samples: 3136/5760, Loss: 1.3114835023880005\n",
      "Epoch: 28, Samples: 3168/5760, Loss: 0.9082818031311035\n",
      "Epoch: 28, Samples: 3200/5760, Loss: 1.232525110244751\n",
      "Epoch: 28, Samples: 3232/5760, Loss: 0.9048303961753845\n",
      "Epoch: 28, Samples: 3264/5760, Loss: 1.213984727859497\n",
      "Epoch: 28, Samples: 3296/5760, Loss: 0.9053120613098145\n",
      "Epoch: 28, Samples: 3328/5760, Loss: 0.8690968751907349\n",
      "Epoch: 28, Samples: 3360/5760, Loss: 0.6781827211380005\n",
      "Epoch: 28, Samples: 3392/5760, Loss: 0.760391354560852\n",
      "Epoch: 28, Samples: 3424/5760, Loss: 1.65296471118927\n",
      "Epoch: 28, Samples: 3456/5760, Loss: 0.9207715392112732\n",
      "Epoch: 28, Samples: 3488/5760, Loss: 1.1226086616516113\n",
      "Epoch: 28, Samples: 3520/5760, Loss: 1.1712312698364258\n",
      "Epoch: 28, Samples: 3552/5760, Loss: 0.9928203821182251\n",
      "Epoch: 28, Samples: 3584/5760, Loss: 1.1071853637695312\n",
      "Epoch: 28, Samples: 3616/5760, Loss: 1.034448504447937\n",
      "Epoch: 28, Samples: 3648/5760, Loss: 1.1324567794799805\n",
      "Epoch: 28, Samples: 3680/5760, Loss: 1.166566252708435\n",
      "Epoch: 28, Samples: 3712/5760, Loss: 0.9189664125442505\n",
      "Epoch: 28, Samples: 3744/5760, Loss: 0.8307566046714783\n",
      "Epoch: 28, Samples: 3776/5760, Loss: 1.1974817514419556\n",
      "Epoch: 28, Samples: 3808/5760, Loss: 1.0496630668640137\n",
      "Epoch: 28, Samples: 3840/5760, Loss: 0.9271159172058105\n",
      "Epoch: 28, Samples: 3872/5760, Loss: 0.7690497636795044\n",
      "Epoch: 28, Samples: 3904/5760, Loss: 1.0959622859954834\n",
      "Epoch: 28, Samples: 3936/5760, Loss: 0.7417234182357788\n",
      "Epoch: 28, Samples: 3968/5760, Loss: 0.9631003141403198\n",
      "Epoch: 28, Samples: 4000/5760, Loss: 0.4112567603588104\n",
      "Epoch: 28, Samples: 4032/5760, Loss: 0.9447715282440186\n",
      "Epoch: 28, Samples: 4064/5760, Loss: 0.7458851933479309\n",
      "Epoch: 28, Samples: 4096/5760, Loss: 0.9150041341781616\n",
      "Epoch: 28, Samples: 4128/5760, Loss: 1.2096537351608276\n",
      "Epoch: 28, Samples: 4160/5760, Loss: 1.1078037023544312\n",
      "Epoch: 28, Samples: 4192/5760, Loss: 1.3770091533660889\n",
      "Epoch: 28, Samples: 4224/5760, Loss: 0.6862552762031555\n",
      "Epoch: 28, Samples: 4256/5760, Loss: 0.8786530494689941\n",
      "Epoch: 28, Samples: 4288/5760, Loss: 0.8474516868591309\n",
      "Epoch: 28, Samples: 4320/5760, Loss: 0.9426546096801758\n",
      "Epoch: 28, Samples: 4352/5760, Loss: 0.7372224926948547\n",
      "Epoch: 28, Samples: 4384/5760, Loss: 0.8864938020706177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Samples: 4416/5760, Loss: 0.8758338689804077\n",
      "Epoch: 28, Samples: 4448/5760, Loss: 1.100440502166748\n",
      "Epoch: 28, Samples: 4480/5760, Loss: 0.9119296669960022\n",
      "Epoch: 28, Samples: 4512/5760, Loss: 0.6093982458114624\n",
      "Epoch: 28, Samples: 4544/5760, Loss: 1.0962663888931274\n",
      "Epoch: 28, Samples: 4576/5760, Loss: 0.9675390720367432\n",
      "Epoch: 28, Samples: 4608/5760, Loss: 1.028297781944275\n",
      "Epoch: 28, Samples: 4640/5760, Loss: 1.244993805885315\n",
      "Epoch: 28, Samples: 4672/5760, Loss: 1.1638987064361572\n",
      "Epoch: 28, Samples: 4704/5760, Loss: 0.7179042100906372\n",
      "Epoch: 28, Samples: 4736/5760, Loss: 1.1425117254257202\n",
      "Epoch: 28, Samples: 4768/5760, Loss: 1.0329874753952026\n",
      "Epoch: 28, Samples: 4800/5760, Loss: 0.8859154582023621\n",
      "Epoch: 28, Samples: 4832/5760, Loss: 0.7739454507827759\n",
      "Epoch: 28, Samples: 4864/5760, Loss: 0.9621437191963196\n",
      "Epoch: 28, Samples: 4896/5760, Loss: 0.6437938213348389\n",
      "Epoch: 28, Samples: 4928/5760, Loss: 0.9840832948684692\n",
      "Epoch: 28, Samples: 4960/5760, Loss: 0.560289740562439\n",
      "Epoch: 28, Samples: 4992/5760, Loss: 1.0685979127883911\n",
      "Epoch: 28, Samples: 5024/5760, Loss: 1.3830020427703857\n",
      "Epoch: 28, Samples: 5056/5760, Loss: 0.8949500322341919\n",
      "Epoch: 28, Samples: 5088/5760, Loss: 0.5180971026420593\n",
      "Epoch: 28, Samples: 5120/5760, Loss: 0.7753560543060303\n",
      "Epoch: 28, Samples: 5152/5760, Loss: 1.532942533493042\n",
      "Epoch: 28, Samples: 5184/5760, Loss: 0.8687440156936646\n",
      "Epoch: 28, Samples: 5216/5760, Loss: 1.194315791130066\n",
      "Epoch: 28, Samples: 5248/5760, Loss: 0.7723497152328491\n",
      "Epoch: 28, Samples: 5280/5760, Loss: 1.0211189985275269\n",
      "Epoch: 28, Samples: 5312/5760, Loss: 1.0254998207092285\n",
      "Epoch: 28, Samples: 5344/5760, Loss: 1.174665093421936\n",
      "Epoch: 28, Samples: 5376/5760, Loss: 0.9956768751144409\n",
      "Epoch: 28, Samples: 5408/5760, Loss: 0.6767981052398682\n",
      "Epoch: 28, Samples: 5440/5760, Loss: 0.9973071813583374\n",
      "Epoch: 28, Samples: 5472/5760, Loss: 0.8749794363975525\n",
      "Epoch: 28, Samples: 5504/5760, Loss: 0.7908119559288025\n",
      "Epoch: 28, Samples: 5536/5760, Loss: 1.213627815246582\n",
      "Epoch: 28, Samples: 5568/5760, Loss: 0.8794926404953003\n",
      "Epoch: 28, Samples: 5600/5760, Loss: 0.8792136311531067\n",
      "Epoch: 28, Samples: 5632/5760, Loss: 1.0460947751998901\n",
      "Epoch: 28, Samples: 5664/5760, Loss: 0.8844887018203735\n",
      "Epoch: 28, Samples: 5696/5760, Loss: 0.7623794674873352\n",
      "Epoch: 28, Samples: 5728/5760, Loss: 2.1208291053771973\n",
      "\n",
      "Epoch: 28\n",
      "Training set: Average loss: 0.9939\n",
      "Validation set: Average loss: 1.5736, Accuracy: 487/818 (60%)\n",
      "Epoch: 29, Samples: 0/5760, Loss: 0.6465833783149719\n",
      "Epoch: 29, Samples: 32/5760, Loss: 1.140777349472046\n",
      "Epoch: 29, Samples: 64/5760, Loss: 0.8437622785568237\n",
      "Epoch: 29, Samples: 96/5760, Loss: 0.8356984257698059\n",
      "Epoch: 29, Samples: 128/5760, Loss: 0.9560274481773376\n",
      "Epoch: 29, Samples: 160/5760, Loss: 1.0069897174835205\n",
      "Epoch: 29, Samples: 192/5760, Loss: 0.820441484451294\n",
      "Epoch: 29, Samples: 224/5760, Loss: 0.8462918996810913\n",
      "Epoch: 29, Samples: 256/5760, Loss: 1.0587196350097656\n",
      "Epoch: 29, Samples: 288/5760, Loss: 1.2801427841186523\n",
      "Epoch: 29, Samples: 320/5760, Loss: 1.0894896984100342\n",
      "Epoch: 29, Samples: 352/5760, Loss: 1.0429152250289917\n",
      "Epoch: 29, Samples: 384/5760, Loss: 1.0482816696166992\n",
      "Epoch: 29, Samples: 416/5760, Loss: 1.1703063249588013\n",
      "Epoch: 29, Samples: 448/5760, Loss: 1.4188493490219116\n",
      "Epoch: 29, Samples: 480/5760, Loss: 0.9753561019897461\n",
      "Epoch: 29, Samples: 512/5760, Loss: 0.7838149666786194\n",
      "Epoch: 29, Samples: 544/5760, Loss: 1.341245412826538\n",
      "Epoch: 29, Samples: 576/5760, Loss: 0.6705506443977356\n",
      "Epoch: 29, Samples: 608/5760, Loss: 0.8888805508613586\n",
      "Epoch: 29, Samples: 640/5760, Loss: 0.9144729971885681\n",
      "Epoch: 29, Samples: 672/5760, Loss: 1.0924705266952515\n",
      "Epoch: 29, Samples: 704/5760, Loss: 0.8589593768119812\n",
      "Epoch: 29, Samples: 736/5760, Loss: 0.9925664663314819\n",
      "Epoch: 29, Samples: 768/5760, Loss: 1.0736324787139893\n",
      "Epoch: 29, Samples: 800/5760, Loss: 0.9946429133415222\n",
      "Epoch: 29, Samples: 832/5760, Loss: 0.8744863867759705\n",
      "Epoch: 29, Samples: 864/5760, Loss: 1.057255506515503\n",
      "Epoch: 29, Samples: 896/5760, Loss: 0.6494030952453613\n",
      "Epoch: 29, Samples: 928/5760, Loss: 0.8338435888290405\n",
      "Epoch: 29, Samples: 960/5760, Loss: 0.9846149682998657\n",
      "Epoch: 29, Samples: 992/5760, Loss: 1.2386409044265747\n",
      "Epoch: 29, Samples: 1024/5760, Loss: 0.8658454418182373\n",
      "Epoch: 29, Samples: 1056/5760, Loss: 0.923271656036377\n",
      "Epoch: 29, Samples: 1088/5760, Loss: 1.0729026794433594\n",
      "Epoch: 29, Samples: 1120/5760, Loss: 1.1336979866027832\n",
      "Epoch: 29, Samples: 1152/5760, Loss: 0.9037612676620483\n",
      "Epoch: 29, Samples: 1184/5760, Loss: 0.8261308670043945\n",
      "Epoch: 29, Samples: 1216/5760, Loss: 1.0018019676208496\n",
      "Epoch: 29, Samples: 1248/5760, Loss: 1.1194288730621338\n",
      "Epoch: 29, Samples: 1280/5760, Loss: 1.2678955793380737\n",
      "Epoch: 29, Samples: 1312/5760, Loss: 0.6377396583557129\n",
      "Epoch: 29, Samples: 1344/5760, Loss: 0.8586280345916748\n",
      "Epoch: 29, Samples: 1376/5760, Loss: 0.7946052551269531\n",
      "Epoch: 29, Samples: 1408/5760, Loss: 1.0802806615829468\n",
      "Epoch: 29, Samples: 1440/5760, Loss: 0.8824899196624756\n",
      "Epoch: 29, Samples: 1472/5760, Loss: 1.0942577123641968\n",
      "Epoch: 29, Samples: 1504/5760, Loss: 1.0922518968582153\n",
      "Epoch: 29, Samples: 1536/5760, Loss: 1.0108871459960938\n",
      "Epoch: 29, Samples: 1568/5760, Loss: 0.8815827965736389\n",
      "Epoch: 29, Samples: 1600/5760, Loss: 0.932881236076355\n",
      "Epoch: 29, Samples: 1632/5760, Loss: 0.6038064956665039\n",
      "Epoch: 29, Samples: 1664/5760, Loss: 0.8181281089782715\n",
      "Epoch: 29, Samples: 1696/5760, Loss: 0.8057085275650024\n",
      "Epoch: 29, Samples: 1728/5760, Loss: 0.8718406558036804\n",
      "Epoch: 29, Samples: 1760/5760, Loss: 1.2742701768875122\n",
      "Epoch: 29, Samples: 1792/5760, Loss: 1.1658971309661865\n",
      "Epoch: 29, Samples: 1824/5760, Loss: 0.8117837905883789\n",
      "Epoch: 29, Samples: 1856/5760, Loss: 0.636864423751831\n",
      "Epoch: 29, Samples: 1888/5760, Loss: 0.6667975187301636\n",
      "Epoch: 29, Samples: 1920/5760, Loss: 1.0127323865890503\n",
      "Epoch: 29, Samples: 1952/5760, Loss: 0.7545642852783203\n",
      "Epoch: 29, Samples: 1984/5760, Loss: 1.3402011394500732\n",
      "Epoch: 29, Samples: 2016/5760, Loss: 0.9542391300201416\n",
      "Epoch: 29, Samples: 2048/5760, Loss: 1.3957922458648682\n",
      "Epoch: 29, Samples: 2080/5760, Loss: 1.1062524318695068\n",
      "Epoch: 29, Samples: 2112/5760, Loss: 1.119859218597412\n",
      "Epoch: 29, Samples: 2144/5760, Loss: 0.7794804573059082\n",
      "Epoch: 29, Samples: 2176/5760, Loss: 0.8261961936950684\n",
      "Epoch: 29, Samples: 2208/5760, Loss: 0.7130746841430664\n",
      "Epoch: 29, Samples: 2240/5760, Loss: 1.2419670820236206\n",
      "Epoch: 29, Samples: 2272/5760, Loss: 0.8778375387191772\n",
      "Epoch: 29, Samples: 2304/5760, Loss: 0.8708977699279785\n",
      "Epoch: 29, Samples: 2336/5760, Loss: 1.1539373397827148\n",
      "Epoch: 29, Samples: 2368/5760, Loss: 0.9106367230415344\n",
      "Epoch: 29, Samples: 2400/5760, Loss: 0.9311702251434326\n",
      "Epoch: 29, Samples: 2432/5760, Loss: 0.8961875438690186\n",
      "Epoch: 29, Samples: 2464/5760, Loss: 0.9450464248657227\n",
      "Epoch: 29, Samples: 2496/5760, Loss: 1.332495093345642\n",
      "Epoch: 29, Samples: 2528/5760, Loss: 1.1779509782791138\n",
      "Epoch: 29, Samples: 2560/5760, Loss: 0.8393197059631348\n",
      "Epoch: 29, Samples: 2592/5760, Loss: 1.0059281587600708\n",
      "Epoch: 29, Samples: 2624/5760, Loss: 1.0105335712432861\n",
      "Epoch: 29, Samples: 2656/5760, Loss: 0.9494799375534058\n",
      "Epoch: 29, Samples: 2688/5760, Loss: 1.0777548551559448\n",
      "Epoch: 29, Samples: 2720/5760, Loss: 0.8430358171463013\n",
      "Epoch: 29, Samples: 2752/5760, Loss: 1.1321208477020264\n",
      "Epoch: 29, Samples: 2784/5760, Loss: 1.0280983448028564\n",
      "Epoch: 29, Samples: 2816/5760, Loss: 0.6681843996047974\n",
      "Epoch: 29, Samples: 2848/5760, Loss: 0.7460536956787109\n",
      "Epoch: 29, Samples: 2880/5760, Loss: 0.9657621383666992\n",
      "Epoch: 29, Samples: 2912/5760, Loss: 1.2542260885238647\n",
      "Epoch: 29, Samples: 2944/5760, Loss: 0.931576669216156\n",
      "Epoch: 29, Samples: 2976/5760, Loss: 1.4688397645950317\n",
      "Epoch: 29, Samples: 3008/5760, Loss: 1.0431456565856934\n",
      "Epoch: 29, Samples: 3040/5760, Loss: 1.1876300573349\n",
      "Epoch: 29, Samples: 3072/5760, Loss: 0.7663918733596802\n",
      "Epoch: 29, Samples: 3104/5760, Loss: 0.5445288419723511\n",
      "Epoch: 29, Samples: 3136/5760, Loss: 0.9590916633605957\n",
      "Epoch: 29, Samples: 3168/5760, Loss: 1.0333967208862305\n",
      "Epoch: 29, Samples: 3200/5760, Loss: 0.9087837338447571\n",
      "Epoch: 29, Samples: 3232/5760, Loss: 1.461911678314209\n",
      "Epoch: 29, Samples: 3264/5760, Loss: 0.8242490291595459\n",
      "Epoch: 29, Samples: 3296/5760, Loss: 0.7388650178909302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Samples: 3328/5760, Loss: 0.9520066976547241\n",
      "Epoch: 29, Samples: 3360/5760, Loss: 1.0055092573165894\n",
      "Epoch: 29, Samples: 3392/5760, Loss: 0.8207269906997681\n",
      "Epoch: 29, Samples: 3424/5760, Loss: 1.2252066135406494\n",
      "Epoch: 29, Samples: 3456/5760, Loss: 0.8301897644996643\n",
      "Epoch: 29, Samples: 3488/5760, Loss: 1.3526852130889893\n",
      "Epoch: 29, Samples: 3520/5760, Loss: 0.5385308265686035\n",
      "Epoch: 29, Samples: 3552/5760, Loss: 1.1114344596862793\n",
      "Epoch: 29, Samples: 3584/5760, Loss: 1.1754119396209717\n",
      "Epoch: 29, Samples: 3616/5760, Loss: 0.9938480854034424\n",
      "Epoch: 29, Samples: 3648/5760, Loss: 0.8637271523475647\n",
      "Epoch: 29, Samples: 3680/5760, Loss: 1.1895893812179565\n",
      "Epoch: 29, Samples: 3712/5760, Loss: 0.853185772895813\n",
      "Epoch: 29, Samples: 3744/5760, Loss: 1.016788363456726\n",
      "Epoch: 29, Samples: 3776/5760, Loss: 0.6902605295181274\n",
      "Epoch: 29, Samples: 3808/5760, Loss: 1.5134443044662476\n",
      "Epoch: 29, Samples: 3840/5760, Loss: 0.5790214538574219\n",
      "Epoch: 29, Samples: 3872/5760, Loss: 0.7904818058013916\n",
      "Epoch: 29, Samples: 3904/5760, Loss: 1.0794038772583008\n",
      "Epoch: 29, Samples: 3936/5760, Loss: 1.1931381225585938\n",
      "Epoch: 29, Samples: 3968/5760, Loss: 1.0411757230758667\n",
      "Epoch: 29, Samples: 4000/5760, Loss: 1.3828743696212769\n",
      "Epoch: 29, Samples: 4032/5760, Loss: 0.9104896783828735\n",
      "Epoch: 29, Samples: 4064/5760, Loss: 1.0566664934158325\n",
      "Epoch: 29, Samples: 4096/5760, Loss: 0.7038428783416748\n",
      "Epoch: 29, Samples: 4128/5760, Loss: 0.7186436653137207\n",
      "Epoch: 29, Samples: 4160/5760, Loss: 0.8413318395614624\n",
      "Epoch: 29, Samples: 4192/5760, Loss: 1.1479556560516357\n",
      "Epoch: 29, Samples: 4224/5760, Loss: 1.1000564098358154\n",
      "Epoch: 29, Samples: 4256/5760, Loss: 1.1911991834640503\n",
      "Epoch: 29, Samples: 4288/5760, Loss: 0.5785257816314697\n",
      "Epoch: 29, Samples: 4320/5760, Loss: 0.880930483341217\n",
      "Epoch: 29, Samples: 4352/5760, Loss: 0.759231448173523\n",
      "Epoch: 29, Samples: 4384/5760, Loss: 0.7947155833244324\n",
      "Epoch: 29, Samples: 4416/5760, Loss: 0.6775048971176147\n",
      "Epoch: 29, Samples: 4448/5760, Loss: 1.0281661748886108\n",
      "Epoch: 29, Samples: 4480/5760, Loss: 0.7880126237869263\n",
      "Epoch: 29, Samples: 4512/5760, Loss: 1.5775375366210938\n",
      "Epoch: 29, Samples: 4544/5760, Loss: 0.7633256912231445\n",
      "Epoch: 29, Samples: 4576/5760, Loss: 0.9582273960113525\n",
      "Epoch: 29, Samples: 4608/5760, Loss: 1.0082886219024658\n",
      "Epoch: 29, Samples: 4640/5760, Loss: 1.0998680591583252\n",
      "Epoch: 29, Samples: 4672/5760, Loss: 0.7344458699226379\n",
      "Epoch: 29, Samples: 4704/5760, Loss: 1.1060845851898193\n",
      "Epoch: 29, Samples: 4736/5760, Loss: 1.0960580110549927\n",
      "Epoch: 29, Samples: 4768/5760, Loss: 1.134563684463501\n",
      "Epoch: 29, Samples: 4800/5760, Loss: 0.8996067643165588\n",
      "Epoch: 29, Samples: 4832/5760, Loss: 0.7898606061935425\n",
      "Epoch: 29, Samples: 4864/5760, Loss: 1.1860591173171997\n",
      "Epoch: 29, Samples: 4896/5760, Loss: 1.0911283493041992\n",
      "Epoch: 29, Samples: 4928/5760, Loss: 0.7322508096694946\n",
      "Epoch: 29, Samples: 4960/5760, Loss: 1.4320013523101807\n",
      "Epoch: 29, Samples: 4992/5760, Loss: 0.6597146391868591\n",
      "Epoch: 29, Samples: 5024/5760, Loss: 0.8441749811172485\n",
      "Epoch: 29, Samples: 5056/5760, Loss: 0.9438751339912415\n",
      "Epoch: 29, Samples: 5088/5760, Loss: 0.641852617263794\n",
      "Epoch: 29, Samples: 5120/5760, Loss: 0.6212959885597229\n",
      "Epoch: 29, Samples: 5152/5760, Loss: 0.6286478042602539\n",
      "Epoch: 29, Samples: 5184/5760, Loss: 0.9126267433166504\n",
      "Epoch: 29, Samples: 5216/5760, Loss: 0.7123380899429321\n",
      "Epoch: 29, Samples: 5248/5760, Loss: 1.041272521018982\n",
      "Epoch: 29, Samples: 5280/5760, Loss: 1.1394188404083252\n",
      "Epoch: 29, Samples: 5312/5760, Loss: 0.9240989685058594\n",
      "Epoch: 29, Samples: 5344/5760, Loss: 1.0701754093170166\n",
      "Epoch: 29, Samples: 5376/5760, Loss: 1.1104243993759155\n",
      "Epoch: 29, Samples: 5408/5760, Loss: 0.6986408829689026\n",
      "Epoch: 29, Samples: 5440/5760, Loss: 1.1894934177398682\n",
      "Epoch: 29, Samples: 5472/5760, Loss: 1.2432081699371338\n",
      "Epoch: 29, Samples: 5504/5760, Loss: 0.642609715461731\n",
      "Epoch: 29, Samples: 5536/5760, Loss: 0.9830950498580933\n",
      "Epoch: 29, Samples: 5568/5760, Loss: 1.2265974283218384\n",
      "Epoch: 29, Samples: 5600/5760, Loss: 0.7691336870193481\n",
      "Epoch: 29, Samples: 5632/5760, Loss: 0.8390432596206665\n",
      "Epoch: 29, Samples: 5664/5760, Loss: 1.011628270149231\n",
      "Epoch: 29, Samples: 5696/5760, Loss: 1.0569692850112915\n",
      "Epoch: 29, Samples: 5728/5760, Loss: 1.195054292678833\n",
      "\n",
      "Epoch: 29\n",
      "Training set: Average loss: 0.9683\n",
      "Validation set: Average loss: 1.4530, Accuracy: 531/818 (65%)\n",
      "Saving model (epoch 29) with lowest validation loss: 1.4529527975962713\n",
      "Epoch: 30, Samples: 0/5760, Loss: 0.6365469694137573\n",
      "Epoch: 30, Samples: 32/5760, Loss: 0.7752604484558105\n",
      "Epoch: 30, Samples: 64/5760, Loss: 0.7029819488525391\n",
      "Epoch: 30, Samples: 96/5760, Loss: 0.8444764614105225\n",
      "Epoch: 30, Samples: 128/5760, Loss: 0.8214561343193054\n",
      "Epoch: 30, Samples: 160/5760, Loss: 0.9271520376205444\n",
      "Epoch: 30, Samples: 192/5760, Loss: 0.6497331857681274\n",
      "Epoch: 30, Samples: 224/5760, Loss: 0.8421505093574524\n",
      "Epoch: 30, Samples: 256/5760, Loss: 0.8494537472724915\n",
      "Epoch: 30, Samples: 288/5760, Loss: 0.9338715672492981\n",
      "Epoch: 30, Samples: 320/5760, Loss: 0.5833525061607361\n",
      "Epoch: 30, Samples: 352/5760, Loss: 0.7786557674407959\n",
      "Epoch: 30, Samples: 384/5760, Loss: 0.45256808400154114\n",
      "Epoch: 30, Samples: 416/5760, Loss: 1.4025746583938599\n",
      "Epoch: 30, Samples: 448/5760, Loss: 0.9766741991043091\n",
      "Epoch: 30, Samples: 480/5760, Loss: 1.241481065750122\n",
      "Epoch: 30, Samples: 512/5760, Loss: 0.9686994552612305\n",
      "Epoch: 30, Samples: 544/5760, Loss: 0.817078173160553\n",
      "Epoch: 30, Samples: 576/5760, Loss: 1.0483959913253784\n",
      "Epoch: 30, Samples: 608/5760, Loss: 1.0672037601470947\n",
      "Epoch: 30, Samples: 640/5760, Loss: 0.5727579593658447\n",
      "Epoch: 30, Samples: 672/5760, Loss: 0.9937200546264648\n",
      "Epoch: 30, Samples: 704/5760, Loss: 0.9101120829582214\n",
      "Epoch: 30, Samples: 736/5760, Loss: 1.1715108156204224\n",
      "Epoch: 30, Samples: 768/5760, Loss: 0.8360791206359863\n",
      "Epoch: 30, Samples: 800/5760, Loss: 1.0705206394195557\n",
      "Epoch: 30, Samples: 832/5760, Loss: 0.47562742233276367\n",
      "Epoch: 30, Samples: 864/5760, Loss: 0.6301195621490479\n",
      "Epoch: 30, Samples: 896/5760, Loss: 0.7283312082290649\n",
      "Epoch: 30, Samples: 928/5760, Loss: 0.538672685623169\n",
      "Epoch: 30, Samples: 960/5760, Loss: 1.0779916048049927\n",
      "Epoch: 30, Samples: 992/5760, Loss: 0.9471763372421265\n",
      "Epoch: 30, Samples: 1024/5760, Loss: 0.9804638624191284\n",
      "Epoch: 30, Samples: 1056/5760, Loss: 1.1281812191009521\n",
      "Epoch: 30, Samples: 1088/5760, Loss: 0.6386228799819946\n",
      "Epoch: 30, Samples: 1120/5760, Loss: 0.9034302830696106\n",
      "Epoch: 30, Samples: 1152/5760, Loss: 0.8369475603103638\n",
      "Epoch: 30, Samples: 1184/5760, Loss: 1.0300958156585693\n",
      "Epoch: 30, Samples: 1216/5760, Loss: 0.7889798879623413\n",
      "Epoch: 30, Samples: 1248/5760, Loss: 0.9498010873794556\n",
      "Epoch: 30, Samples: 1280/5760, Loss: 0.9309003353118896\n",
      "Epoch: 30, Samples: 1312/5760, Loss: 0.8544861674308777\n",
      "Epoch: 30, Samples: 1344/5760, Loss: 0.6312872767448425\n",
      "Epoch: 30, Samples: 1376/5760, Loss: 0.8618193864822388\n",
      "Epoch: 30, Samples: 1408/5760, Loss: 1.0185878276824951\n",
      "Epoch: 30, Samples: 1440/5760, Loss: 0.857509970664978\n",
      "Epoch: 30, Samples: 1472/5760, Loss: 1.1322226524353027\n",
      "Epoch: 30, Samples: 1504/5760, Loss: 0.6930884122848511\n",
      "Epoch: 30, Samples: 1536/5760, Loss: 1.2187350988388062\n",
      "Epoch: 30, Samples: 1568/5760, Loss: 1.0640968084335327\n",
      "Epoch: 30, Samples: 1600/5760, Loss: 0.8027753829956055\n",
      "Epoch: 30, Samples: 1632/5760, Loss: 0.880641758441925\n",
      "Epoch: 30, Samples: 1664/5760, Loss: 1.1159800291061401\n",
      "Epoch: 30, Samples: 1696/5760, Loss: 0.9208227396011353\n",
      "Epoch: 30, Samples: 1728/5760, Loss: 0.8678156733512878\n",
      "Epoch: 30, Samples: 1760/5760, Loss: 0.5318963527679443\n",
      "Epoch: 30, Samples: 1792/5760, Loss: 0.834101676940918\n",
      "Epoch: 30, Samples: 1824/5760, Loss: 0.6759738922119141\n",
      "Epoch: 30, Samples: 1856/5760, Loss: 1.0974634885787964\n",
      "Epoch: 30, Samples: 1888/5760, Loss: 0.8331606388092041\n",
      "Epoch: 30, Samples: 1920/5760, Loss: 0.9275190830230713\n",
      "Epoch: 30, Samples: 1952/5760, Loss: 0.7106099724769592\n",
      "Epoch: 30, Samples: 1984/5760, Loss: 0.5987364053726196\n",
      "Epoch: 30, Samples: 2016/5760, Loss: 0.7438914775848389\n",
      "Epoch: 30, Samples: 2048/5760, Loss: 0.6153412461280823\n",
      "Epoch: 30, Samples: 2080/5760, Loss: 0.7733500003814697\n",
      "Epoch: 30, Samples: 2112/5760, Loss: 1.0660479068756104\n",
      "Epoch: 30, Samples: 2144/5760, Loss: 0.7928290367126465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Samples: 2176/5760, Loss: 0.7810095548629761\n",
      "Epoch: 30, Samples: 2208/5760, Loss: 0.8470879793167114\n",
      "Epoch: 30, Samples: 2240/5760, Loss: 1.0805444717407227\n",
      "Epoch: 30, Samples: 2272/5760, Loss: 0.8089274168014526\n",
      "Epoch: 30, Samples: 2304/5760, Loss: 0.9977846145629883\n",
      "Epoch: 30, Samples: 2336/5760, Loss: 0.6900895833969116\n",
      "Epoch: 30, Samples: 2368/5760, Loss: 1.0382376909255981\n",
      "Epoch: 30, Samples: 2400/5760, Loss: 0.9396708011627197\n",
      "Epoch: 30, Samples: 2432/5760, Loss: 1.235216736793518\n",
      "Epoch: 30, Samples: 2464/5760, Loss: 1.3271899223327637\n",
      "Epoch: 30, Samples: 2496/5760, Loss: 0.776465892791748\n",
      "Epoch: 30, Samples: 2528/5760, Loss: 1.434032917022705\n",
      "Epoch: 30, Samples: 2560/5760, Loss: 0.936100959777832\n",
      "Epoch: 30, Samples: 2592/5760, Loss: 0.9892551898956299\n",
      "Epoch: 30, Samples: 2624/5760, Loss: 0.680260181427002\n",
      "Epoch: 30, Samples: 2656/5760, Loss: 0.8854727149009705\n",
      "Epoch: 30, Samples: 2688/5760, Loss: 0.9363566040992737\n",
      "Epoch: 30, Samples: 2720/5760, Loss: 0.8256524801254272\n",
      "Epoch: 30, Samples: 2752/5760, Loss: 0.6806584596633911\n",
      "Epoch: 30, Samples: 2784/5760, Loss: 0.8863799571990967\n",
      "Epoch: 30, Samples: 2816/5760, Loss: 0.7789148092269897\n",
      "Epoch: 30, Samples: 2848/5760, Loss: 1.0510516166687012\n",
      "Epoch: 30, Samples: 2880/5760, Loss: 0.5576615333557129\n",
      "Epoch: 30, Samples: 2912/5760, Loss: 0.9296398162841797\n",
      "Epoch: 30, Samples: 2944/5760, Loss: 0.9012916088104248\n",
      "Epoch: 30, Samples: 2976/5760, Loss: 0.7655799984931946\n",
      "Epoch: 30, Samples: 3008/5760, Loss: 0.8092824816703796\n",
      "Epoch: 30, Samples: 3040/5760, Loss: 0.8446778059005737\n",
      "Epoch: 30, Samples: 3072/5760, Loss: 0.8795356154441833\n",
      "Epoch: 30, Samples: 3104/5760, Loss: 0.6830198764801025\n",
      "Epoch: 30, Samples: 3136/5760, Loss: 1.0621753931045532\n",
      "Epoch: 30, Samples: 3168/5760, Loss: 0.8966546058654785\n",
      "Epoch: 30, Samples: 3200/5760, Loss: 1.1882137060165405\n",
      "Epoch: 30, Samples: 3232/5760, Loss: 0.5666964054107666\n",
      "Epoch: 30, Samples: 3264/5760, Loss: 1.1506277322769165\n",
      "Epoch: 30, Samples: 3296/5760, Loss: 1.3587956428527832\n",
      "Epoch: 30, Samples: 3328/5760, Loss: 0.9132668972015381\n",
      "Epoch: 30, Samples: 3360/5760, Loss: 1.0708969831466675\n",
      "Epoch: 30, Samples: 3392/5760, Loss: 0.8235126733779907\n",
      "Epoch: 30, Samples: 3424/5760, Loss: 1.1601845026016235\n",
      "Epoch: 30, Samples: 3456/5760, Loss: 0.9366987943649292\n",
      "Epoch: 30, Samples: 3488/5760, Loss: 1.1167784929275513\n",
      "Epoch: 30, Samples: 3520/5760, Loss: 0.730279803276062\n",
      "Epoch: 30, Samples: 3552/5760, Loss: 0.9633283019065857\n",
      "Epoch: 30, Samples: 3584/5760, Loss: 1.4219871759414673\n",
      "Epoch: 30, Samples: 3616/5760, Loss: 0.6734656095504761\n",
      "Epoch: 30, Samples: 3648/5760, Loss: 0.9207471013069153\n",
      "Epoch: 30, Samples: 3680/5760, Loss: 0.7286640405654907\n",
      "Epoch: 30, Samples: 3712/5760, Loss: 0.93572998046875\n",
      "Epoch: 30, Samples: 3744/5760, Loss: 0.8439300060272217\n",
      "Epoch: 30, Samples: 3776/5760, Loss: 0.9436590671539307\n",
      "Epoch: 30, Samples: 3808/5760, Loss: 1.0741084814071655\n",
      "Epoch: 30, Samples: 3840/5760, Loss: 0.7253797054290771\n",
      "Epoch: 30, Samples: 3872/5760, Loss: 1.1779608726501465\n",
      "Epoch: 30, Samples: 3904/5760, Loss: 1.0249507427215576\n",
      "Epoch: 30, Samples: 3936/5760, Loss: 0.9340507984161377\n",
      "Epoch: 30, Samples: 3968/5760, Loss: 1.0224623680114746\n",
      "Epoch: 30, Samples: 4000/5760, Loss: 0.7533974647521973\n",
      "Epoch: 30, Samples: 4032/5760, Loss: 0.6848986744880676\n",
      "Epoch: 30, Samples: 4064/5760, Loss: 1.0319889783859253\n",
      "Epoch: 30, Samples: 4096/5760, Loss: 0.8334594368934631\n",
      "Epoch: 30, Samples: 4128/5760, Loss: 0.7906807661056519\n",
      "Epoch: 30, Samples: 4160/5760, Loss: 1.1319855451583862\n",
      "Epoch: 30, Samples: 4192/5760, Loss: 0.9995219707489014\n",
      "Epoch: 30, Samples: 4224/5760, Loss: 0.7294424772262573\n",
      "Epoch: 30, Samples: 4256/5760, Loss: 0.7361584305763245\n",
      "Epoch: 30, Samples: 4288/5760, Loss: 0.6921897530555725\n",
      "Epoch: 30, Samples: 4320/5760, Loss: 0.7536343932151794\n",
      "Epoch: 30, Samples: 4352/5760, Loss: 0.9579235315322876\n",
      "Epoch: 30, Samples: 4384/5760, Loss: 1.0114190578460693\n",
      "Epoch: 30, Samples: 4416/5760, Loss: 0.7555146813392639\n",
      "Epoch: 30, Samples: 4448/5760, Loss: 0.7308012843132019\n",
      "Epoch: 30, Samples: 4480/5760, Loss: 1.0362110137939453\n",
      "Epoch: 30, Samples: 4512/5760, Loss: 0.5637975931167603\n",
      "Epoch: 30, Samples: 4544/5760, Loss: 1.0838977098464966\n",
      "Epoch: 30, Samples: 4576/5760, Loss: 1.247310757637024\n",
      "Epoch: 30, Samples: 4608/5760, Loss: 0.8291634917259216\n",
      "Epoch: 30, Samples: 4640/5760, Loss: 0.7995308041572571\n",
      "Epoch: 30, Samples: 4672/5760, Loss: 0.657674252986908\n",
      "Epoch: 30, Samples: 4704/5760, Loss: 0.7254022359848022\n",
      "Epoch: 30, Samples: 4736/5760, Loss: 0.8779348731040955\n",
      "Epoch: 30, Samples: 4768/5760, Loss: 0.5208474397659302\n",
      "Epoch: 30, Samples: 4800/5760, Loss: 1.1375476121902466\n",
      "Epoch: 30, Samples: 4832/5760, Loss: 0.8891456127166748\n",
      "Epoch: 30, Samples: 4864/5760, Loss: 0.9110631942749023\n",
      "Epoch: 30, Samples: 4896/5760, Loss: 0.9240856170654297\n",
      "Epoch: 30, Samples: 4928/5760, Loss: 1.440312147140503\n",
      "Epoch: 30, Samples: 4960/5760, Loss: 0.5096085071563721\n",
      "Epoch: 30, Samples: 4992/5760, Loss: 0.7625312805175781\n",
      "Epoch: 30, Samples: 5024/5760, Loss: 1.1930732727050781\n",
      "Epoch: 30, Samples: 5056/5760, Loss: 1.1032696962356567\n",
      "Epoch: 30, Samples: 5088/5760, Loss: 1.218368649482727\n",
      "Epoch: 30, Samples: 5120/5760, Loss: 0.8528698086738586\n",
      "Epoch: 30, Samples: 5152/5760, Loss: 1.262425184249878\n",
      "Epoch: 30, Samples: 5184/5760, Loss: 0.9397310614585876\n",
      "Epoch: 30, Samples: 5216/5760, Loss: 0.7283523082733154\n",
      "Epoch: 30, Samples: 5248/5760, Loss: 0.9860066771507263\n",
      "Epoch: 30, Samples: 5280/5760, Loss: 0.8525816798210144\n",
      "Epoch: 30, Samples: 5312/5760, Loss: 0.9249333739280701\n",
      "Epoch: 30, Samples: 5344/5760, Loss: 0.7018591165542603\n",
      "Epoch: 30, Samples: 5376/5760, Loss: 0.5393878221511841\n",
      "Epoch: 30, Samples: 5408/5760, Loss: 0.9938254356384277\n",
      "Epoch: 30, Samples: 5440/5760, Loss: 0.5191352367401123\n",
      "Epoch: 30, Samples: 5472/5760, Loss: 1.1090483665466309\n",
      "Epoch: 30, Samples: 5504/5760, Loss: 1.2032124996185303\n",
      "Epoch: 30, Samples: 5536/5760, Loss: 0.9175772666931152\n",
      "Epoch: 30, Samples: 5568/5760, Loss: 0.9962131381034851\n",
      "Epoch: 30, Samples: 5600/5760, Loss: 1.109542965888977\n",
      "Epoch: 30, Samples: 5632/5760, Loss: 0.9483282566070557\n",
      "Epoch: 30, Samples: 5664/5760, Loss: 0.7866766452789307\n",
      "Epoch: 30, Samples: 5696/5760, Loss: 0.5958135724067688\n",
      "Epoch: 30, Samples: 5728/5760, Loss: 2.0479257106781006\n",
      "\n",
      "Epoch: 30\n",
      "Training set: Average loss: 0.8998\n",
      "Validation set: Average loss: 1.5333, Accuracy: 496/818 (61%)\n",
      "Epoch: 31, Samples: 0/5760, Loss: 0.755018949508667\n",
      "Epoch: 31, Samples: 32/5760, Loss: 0.7779442667961121\n",
      "Epoch: 31, Samples: 64/5760, Loss: 1.0557526350021362\n",
      "Epoch: 31, Samples: 96/5760, Loss: 1.047816276550293\n",
      "Epoch: 31, Samples: 128/5760, Loss: 0.5655226111412048\n",
      "Epoch: 31, Samples: 160/5760, Loss: 0.6767662763595581\n",
      "Epoch: 31, Samples: 192/5760, Loss: 0.8198920488357544\n",
      "Epoch: 31, Samples: 224/5760, Loss: 0.7980839610099792\n",
      "Epoch: 31, Samples: 256/5760, Loss: 0.893603503704071\n",
      "Epoch: 31, Samples: 288/5760, Loss: 1.1974942684173584\n",
      "Epoch: 31, Samples: 320/5760, Loss: 0.6061834096908569\n",
      "Epoch: 31, Samples: 352/5760, Loss: 0.9401249885559082\n",
      "Epoch: 31, Samples: 384/5760, Loss: 0.9367505311965942\n",
      "Epoch: 31, Samples: 416/5760, Loss: 0.8973953723907471\n",
      "Epoch: 31, Samples: 448/5760, Loss: 1.1940480470657349\n",
      "Epoch: 31, Samples: 480/5760, Loss: 0.7463750839233398\n",
      "Epoch: 31, Samples: 512/5760, Loss: 1.1067583560943604\n",
      "Epoch: 31, Samples: 544/5760, Loss: 0.7531758546829224\n",
      "Epoch: 31, Samples: 576/5760, Loss: 0.7647587060928345\n",
      "Epoch: 31, Samples: 608/5760, Loss: 1.0753427743911743\n",
      "Epoch: 31, Samples: 640/5760, Loss: 0.8993254899978638\n",
      "Epoch: 31, Samples: 672/5760, Loss: 0.8619863390922546\n",
      "Epoch: 31, Samples: 704/5760, Loss: 0.5326410531997681\n",
      "Epoch: 31, Samples: 736/5760, Loss: 0.6897450685501099\n",
      "Epoch: 31, Samples: 768/5760, Loss: 0.846318244934082\n",
      "Epoch: 31, Samples: 800/5760, Loss: 1.099134922027588\n",
      "Epoch: 31, Samples: 832/5760, Loss: 0.5636711120605469\n",
      "Epoch: 31, Samples: 864/5760, Loss: 0.9158368706703186\n",
      "Epoch: 31, Samples: 896/5760, Loss: 1.0817949771881104\n",
      "Epoch: 31, Samples: 928/5760, Loss: 0.858373761177063\n",
      "Epoch: 31, Samples: 960/5760, Loss: 0.9162190556526184\n",
      "Epoch: 31, Samples: 992/5760, Loss: 0.8697400093078613\n",
      "Epoch: 31, Samples: 1024/5760, Loss: 0.7818963527679443\n",
      "Epoch: 31, Samples: 1056/5760, Loss: 0.9296395182609558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31, Samples: 1088/5760, Loss: 0.8846619725227356\n",
      "Epoch: 31, Samples: 1120/5760, Loss: 0.6200534701347351\n",
      "Epoch: 31, Samples: 1152/5760, Loss: 0.5576440095901489\n",
      "Epoch: 31, Samples: 1184/5760, Loss: 0.7194252014160156\n",
      "Epoch: 31, Samples: 1216/5760, Loss: 0.8875486850738525\n",
      "Epoch: 31, Samples: 1248/5760, Loss: 0.8829424977302551\n",
      "Epoch: 31, Samples: 1280/5760, Loss: 0.7867785692214966\n",
      "Epoch: 31, Samples: 1312/5760, Loss: 0.9054948091506958\n",
      "Epoch: 31, Samples: 1344/5760, Loss: 0.8218565583229065\n",
      "Epoch: 31, Samples: 1376/5760, Loss: 0.7227864861488342\n",
      "Epoch: 31, Samples: 1408/5760, Loss: 0.5325942039489746\n",
      "Epoch: 31, Samples: 1440/5760, Loss: 0.6928035020828247\n",
      "Epoch: 31, Samples: 1472/5760, Loss: 0.6308128237724304\n",
      "Epoch: 31, Samples: 1504/5760, Loss: 0.7229629755020142\n",
      "Epoch: 31, Samples: 1536/5760, Loss: 0.9516793489456177\n",
      "Epoch: 31, Samples: 1568/5760, Loss: 0.4856967031955719\n",
      "Epoch: 31, Samples: 1600/5760, Loss: 0.6806660890579224\n",
      "Epoch: 31, Samples: 1632/5760, Loss: 0.5413874387741089\n",
      "Epoch: 31, Samples: 1664/5760, Loss: 0.9345848560333252\n",
      "Epoch: 31, Samples: 1696/5760, Loss: 0.5502114295959473\n",
      "Epoch: 31, Samples: 1728/5760, Loss: 0.8008296489715576\n",
      "Epoch: 31, Samples: 1760/5760, Loss: 0.7958431243896484\n",
      "Epoch: 31, Samples: 1792/5760, Loss: 0.9670068621635437\n",
      "Epoch: 31, Samples: 1824/5760, Loss: 1.3479117155075073\n",
      "Epoch: 31, Samples: 1856/5760, Loss: 0.6120650768280029\n",
      "Epoch: 31, Samples: 1888/5760, Loss: 1.0517698526382446\n",
      "Epoch: 31, Samples: 1920/5760, Loss: 0.481946736574173\n",
      "Epoch: 31, Samples: 1952/5760, Loss: 0.5533171892166138\n",
      "Epoch: 31, Samples: 1984/5760, Loss: 0.7298018932342529\n",
      "Epoch: 31, Samples: 2016/5760, Loss: 0.7065479755401611\n",
      "Epoch: 31, Samples: 2048/5760, Loss: 0.8872295618057251\n",
      "Epoch: 31, Samples: 2080/5760, Loss: 0.9355267882347107\n",
      "Epoch: 31, Samples: 2112/5760, Loss: 0.7067275047302246\n",
      "Epoch: 31, Samples: 2144/5760, Loss: 0.7315073609352112\n",
      "Epoch: 31, Samples: 2176/5760, Loss: 0.9415961503982544\n",
      "Epoch: 31, Samples: 2208/5760, Loss: 0.6538511514663696\n",
      "Epoch: 31, Samples: 2240/5760, Loss: 0.9325114488601685\n",
      "Epoch: 31, Samples: 2272/5760, Loss: 0.6262916922569275\n",
      "Epoch: 31, Samples: 2304/5760, Loss: 0.5498608350753784\n",
      "Epoch: 31, Samples: 2336/5760, Loss: 0.7013412117958069\n",
      "Epoch: 31, Samples: 2368/5760, Loss: 0.6349248886108398\n",
      "Epoch: 31, Samples: 2400/5760, Loss: 0.8418282270431519\n",
      "Epoch: 31, Samples: 2432/5760, Loss: 0.6810879707336426\n",
      "Epoch: 31, Samples: 2464/5760, Loss: 1.0386216640472412\n",
      "Epoch: 31, Samples: 2496/5760, Loss: 0.8093208074569702\n",
      "Epoch: 31, Samples: 2528/5760, Loss: 1.0020544528961182\n",
      "Epoch: 31, Samples: 2560/5760, Loss: 0.7829502820968628\n",
      "Epoch: 31, Samples: 2592/5760, Loss: 0.88374924659729\n",
      "Epoch: 31, Samples: 2624/5760, Loss: 1.510077714920044\n",
      "Epoch: 31, Samples: 2656/5760, Loss: 0.8551061153411865\n",
      "Epoch: 31, Samples: 2688/5760, Loss: 0.9838890433311462\n",
      "Epoch: 31, Samples: 2720/5760, Loss: 0.7319880127906799\n",
      "Epoch: 31, Samples: 2752/5760, Loss: 0.8660534620285034\n",
      "Epoch: 31, Samples: 2784/5760, Loss: 0.7081457376480103\n",
      "Epoch: 31, Samples: 2816/5760, Loss: 0.6548924446105957\n",
      "Epoch: 31, Samples: 2848/5760, Loss: 0.6888133883476257\n",
      "Epoch: 31, Samples: 2880/5760, Loss: 0.8671349287033081\n",
      "Epoch: 31, Samples: 2912/5760, Loss: 0.7258653044700623\n",
      "Epoch: 31, Samples: 2944/5760, Loss: 0.7170672416687012\n",
      "Epoch: 31, Samples: 2976/5760, Loss: 0.5694284439086914\n",
      "Epoch: 31, Samples: 3008/5760, Loss: 0.6738706827163696\n",
      "Epoch: 31, Samples: 3040/5760, Loss: 0.9229594469070435\n",
      "Epoch: 31, Samples: 3072/5760, Loss: 0.5520510673522949\n",
      "Epoch: 31, Samples: 3104/5760, Loss: 0.9477276802062988\n",
      "Epoch: 31, Samples: 3136/5760, Loss: 0.9993593096733093\n",
      "Epoch: 31, Samples: 3168/5760, Loss: 0.6702052354812622\n",
      "Epoch: 31, Samples: 3200/5760, Loss: 1.4303919076919556\n",
      "Epoch: 31, Samples: 3232/5760, Loss: 0.6418266892433167\n",
      "Epoch: 31, Samples: 3264/5760, Loss: 0.7846829891204834\n",
      "Epoch: 31, Samples: 3296/5760, Loss: 0.9100030660629272\n",
      "Epoch: 31, Samples: 3328/5760, Loss: 1.0540740489959717\n",
      "Epoch: 31, Samples: 3360/5760, Loss: 0.7694854140281677\n",
      "Epoch: 31, Samples: 3392/5760, Loss: 1.1083958148956299\n",
      "Epoch: 31, Samples: 3424/5760, Loss: 0.7772738933563232\n",
      "Epoch: 31, Samples: 3456/5760, Loss: 0.8868639469146729\n",
      "Epoch: 31, Samples: 3488/5760, Loss: 0.7458482980728149\n",
      "Epoch: 31, Samples: 3520/5760, Loss: 0.9151327610015869\n",
      "Epoch: 31, Samples: 3552/5760, Loss: 1.1663590669631958\n",
      "Epoch: 31, Samples: 3584/5760, Loss: 0.7106675505638123\n",
      "Epoch: 31, Samples: 3616/5760, Loss: 0.6901172995567322\n",
      "Epoch: 31, Samples: 3648/5760, Loss: 1.0191500186920166\n",
      "Epoch: 31, Samples: 3680/5760, Loss: 1.1312299966812134\n",
      "Epoch: 31, Samples: 3712/5760, Loss: 0.6263813376426697\n",
      "Epoch: 31, Samples: 3744/5760, Loss: 1.0920355319976807\n",
      "Epoch: 31, Samples: 3776/5760, Loss: 0.9023149013519287\n",
      "Epoch: 31, Samples: 3808/5760, Loss: 0.6191943883895874\n",
      "Epoch: 31, Samples: 3840/5760, Loss: 0.5540379881858826\n",
      "Epoch: 31, Samples: 3872/5760, Loss: 0.8168834447860718\n",
      "Epoch: 31, Samples: 3904/5760, Loss: 0.8529636859893799\n",
      "Epoch: 31, Samples: 3936/5760, Loss: 0.8267470598220825\n",
      "Epoch: 31, Samples: 3968/5760, Loss: 1.0954854488372803\n",
      "Epoch: 31, Samples: 4000/5760, Loss: 0.5046144723892212\n",
      "Epoch: 31, Samples: 4032/5760, Loss: 0.8074915409088135\n",
      "Epoch: 31, Samples: 4064/5760, Loss: 0.7019028067588806\n",
      "Epoch: 31, Samples: 4096/5760, Loss: 0.5839959979057312\n",
      "Epoch: 31, Samples: 4128/5760, Loss: 0.8380692005157471\n",
      "Epoch: 31, Samples: 4160/5760, Loss: 0.9296135902404785\n",
      "Epoch: 31, Samples: 4192/5760, Loss: 0.5870645046234131\n",
      "Epoch: 31, Samples: 4224/5760, Loss: 1.0456743240356445\n",
      "Epoch: 31, Samples: 4256/5760, Loss: 0.7595397233963013\n",
      "Epoch: 31, Samples: 4288/5760, Loss: 1.1071866750717163\n",
      "Epoch: 31, Samples: 4320/5760, Loss: 0.8251808881759644\n",
      "Epoch: 31, Samples: 4352/5760, Loss: 0.9901071786880493\n",
      "Epoch: 31, Samples: 4384/5760, Loss: 1.0468201637268066\n",
      "Epoch: 31, Samples: 4416/5760, Loss: 1.1433866024017334\n",
      "Epoch: 31, Samples: 4448/5760, Loss: 0.6536595821380615\n",
      "Epoch: 31, Samples: 4480/5760, Loss: 0.6355150938034058\n",
      "Epoch: 31, Samples: 4512/5760, Loss: 0.9020003080368042\n",
      "Epoch: 31, Samples: 4544/5760, Loss: 0.6773996353149414\n",
      "Epoch: 31, Samples: 4576/5760, Loss: 0.8365616202354431\n",
      "Epoch: 31, Samples: 4608/5760, Loss: 0.5893568992614746\n",
      "Epoch: 31, Samples: 4640/5760, Loss: 0.7172446250915527\n",
      "Epoch: 31, Samples: 4672/5760, Loss: 0.9645620584487915\n",
      "Epoch: 31, Samples: 4704/5760, Loss: 0.880841076374054\n",
      "Epoch: 31, Samples: 4736/5760, Loss: 0.7171400785446167\n",
      "Epoch: 31, Samples: 4768/5760, Loss: 0.6255652904510498\n",
      "Epoch: 31, Samples: 4800/5760, Loss: 0.9560030698776245\n",
      "Epoch: 31, Samples: 4832/5760, Loss: 0.8102316856384277\n",
      "Epoch: 31, Samples: 4864/5760, Loss: 1.0218262672424316\n",
      "Epoch: 31, Samples: 4896/5760, Loss: 0.9992372989654541\n",
      "Epoch: 31, Samples: 4928/5760, Loss: 0.7801263332366943\n",
      "Epoch: 31, Samples: 4960/5760, Loss: 0.892364501953125\n",
      "Epoch: 31, Samples: 4992/5760, Loss: 0.8485606908798218\n",
      "Epoch: 31, Samples: 5024/5760, Loss: 0.6204493045806885\n",
      "Epoch: 31, Samples: 5056/5760, Loss: 0.7997103929519653\n",
      "Epoch: 31, Samples: 5088/5760, Loss: 0.665462076663971\n",
      "Epoch: 31, Samples: 5120/5760, Loss: 0.9465257525444031\n",
      "Epoch: 31, Samples: 5152/5760, Loss: 0.7431607246398926\n",
      "Epoch: 31, Samples: 5184/5760, Loss: 1.09847891330719\n",
      "Epoch: 31, Samples: 5216/5760, Loss: 1.443120002746582\n",
      "Epoch: 31, Samples: 5248/5760, Loss: 0.7565072774887085\n",
      "Epoch: 31, Samples: 5280/5760, Loss: 0.9041495323181152\n",
      "Epoch: 31, Samples: 5312/5760, Loss: 0.8105732202529907\n",
      "Epoch: 31, Samples: 5344/5760, Loss: 0.7525037527084351\n",
      "Epoch: 31, Samples: 5376/5760, Loss: 0.714516282081604\n",
      "Epoch: 31, Samples: 5408/5760, Loss: 0.652153491973877\n",
      "Epoch: 31, Samples: 5440/5760, Loss: 1.001983642578125\n",
      "Epoch: 31, Samples: 5472/5760, Loss: 1.107392430305481\n",
      "Epoch: 31, Samples: 5504/5760, Loss: 0.9094163179397583\n",
      "Epoch: 31, Samples: 5536/5760, Loss: 0.8119237422943115\n",
      "Epoch: 31, Samples: 5568/5760, Loss: 1.1968109607696533\n",
      "Epoch: 31, Samples: 5600/5760, Loss: 0.8546352982521057\n",
      "Epoch: 31, Samples: 5632/5760, Loss: 1.2027482986450195\n",
      "Epoch: 31, Samples: 5664/5760, Loss: 1.8246722221374512\n",
      "Epoch: 31, Samples: 5696/5760, Loss: 1.0975052118301392\n",
      "Epoch: 31, Samples: 5728/5760, Loss: 2.5344951152801514\n",
      "\n",
      "Epoch: 31\n",
      "Training set: Average loss: 0.8498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss: 1.4849, Accuracy: 522/818 (64%)\n",
      "Epoch: 32, Samples: 0/5760, Loss: 0.6703985929489136\n",
      "Epoch: 32, Samples: 32/5760, Loss: 0.7413330674171448\n",
      "Epoch: 32, Samples: 64/5760, Loss: 0.9824286103248596\n",
      "Epoch: 32, Samples: 96/5760, Loss: 0.528044581413269\n",
      "Epoch: 32, Samples: 128/5760, Loss: 1.0860546827316284\n",
      "Epoch: 32, Samples: 160/5760, Loss: 0.5797615051269531\n",
      "Epoch: 32, Samples: 192/5760, Loss: 0.8759117126464844\n",
      "Epoch: 32, Samples: 224/5760, Loss: 1.2999145984649658\n",
      "Epoch: 32, Samples: 256/5760, Loss: 0.6766732931137085\n",
      "Epoch: 32, Samples: 288/5760, Loss: 0.6007616519927979\n",
      "Epoch: 32, Samples: 320/5760, Loss: 0.2979416251182556\n",
      "Epoch: 32, Samples: 352/5760, Loss: 1.0436265468597412\n",
      "Epoch: 32, Samples: 384/5760, Loss: 0.8528933525085449\n",
      "Epoch: 32, Samples: 416/5760, Loss: 0.657089114189148\n",
      "Epoch: 32, Samples: 448/5760, Loss: 0.7907335758209229\n",
      "Epoch: 32, Samples: 480/5760, Loss: 0.9849618673324585\n",
      "Epoch: 32, Samples: 512/5760, Loss: 1.0850247144699097\n",
      "Epoch: 32, Samples: 544/5760, Loss: 0.5376698970794678\n",
      "Epoch: 32, Samples: 576/5760, Loss: 0.9361252784729004\n",
      "Epoch: 32, Samples: 608/5760, Loss: 0.7925760746002197\n",
      "Epoch: 32, Samples: 640/5760, Loss: 0.7892420291900635\n",
      "Epoch: 32, Samples: 672/5760, Loss: 0.9199614524841309\n",
      "Epoch: 32, Samples: 704/5760, Loss: 0.7327472567558289\n",
      "Epoch: 32, Samples: 736/5760, Loss: 0.8936406373977661\n",
      "Epoch: 32, Samples: 768/5760, Loss: 1.0033681392669678\n",
      "Epoch: 32, Samples: 800/5760, Loss: 0.6640346050262451\n",
      "Epoch: 32, Samples: 832/5760, Loss: 0.6507546901702881\n",
      "Epoch: 32, Samples: 864/5760, Loss: 0.544029712677002\n",
      "Epoch: 32, Samples: 896/5760, Loss: 0.5198043584823608\n",
      "Epoch: 32, Samples: 928/5760, Loss: 0.7158585786819458\n",
      "Epoch: 32, Samples: 960/5760, Loss: 0.8516178131103516\n",
      "Epoch: 32, Samples: 992/5760, Loss: 0.6286097764968872\n",
      "Epoch: 32, Samples: 1024/5760, Loss: 0.49017056822776794\n",
      "Epoch: 32, Samples: 1056/5760, Loss: 0.7028994560241699\n",
      "Epoch: 32, Samples: 1088/5760, Loss: 0.6552965641021729\n",
      "Epoch: 32, Samples: 1120/5760, Loss: 0.9487940073013306\n",
      "Epoch: 32, Samples: 1152/5760, Loss: 0.8259410262107849\n",
      "Epoch: 32, Samples: 1184/5760, Loss: 0.9324158430099487\n",
      "Epoch: 32, Samples: 1216/5760, Loss: 0.8041768074035645\n",
      "Epoch: 32, Samples: 1248/5760, Loss: 0.5992549657821655\n",
      "Epoch: 32, Samples: 1280/5760, Loss: 0.40296250581741333\n",
      "Epoch: 32, Samples: 1312/5760, Loss: 0.6511394381523132\n",
      "Epoch: 32, Samples: 1344/5760, Loss: 0.953108012676239\n",
      "Epoch: 32, Samples: 1376/5760, Loss: 0.6859942674636841\n",
      "Epoch: 32, Samples: 1408/5760, Loss: 0.9905846118927002\n",
      "Epoch: 32, Samples: 1440/5760, Loss: 0.6938832998275757\n",
      "Epoch: 32, Samples: 1472/5760, Loss: 0.49502480030059814\n",
      "Epoch: 32, Samples: 1504/5760, Loss: 0.7705391645431519\n",
      "Epoch: 32, Samples: 1536/5760, Loss: 0.4942813217639923\n",
      "Epoch: 32, Samples: 1568/5760, Loss: 0.8378883600234985\n",
      "Epoch: 32, Samples: 1600/5760, Loss: 0.3865305781364441\n",
      "Epoch: 32, Samples: 1632/5760, Loss: 0.8587672114372253\n",
      "Epoch: 32, Samples: 1664/5760, Loss: 0.8333950042724609\n",
      "Epoch: 32, Samples: 1696/5760, Loss: 0.5811220407485962\n",
      "Epoch: 32, Samples: 1728/5760, Loss: 0.9544227123260498\n",
      "Epoch: 32, Samples: 1760/5760, Loss: 0.8549623489379883\n",
      "Epoch: 32, Samples: 1792/5760, Loss: 1.5069122314453125\n",
      "Epoch: 32, Samples: 1824/5760, Loss: 0.9007359743118286\n",
      "Epoch: 32, Samples: 1856/5760, Loss: 0.7036929130554199\n",
      "Epoch: 32, Samples: 1888/5760, Loss: 0.6229552030563354\n",
      "Epoch: 32, Samples: 1920/5760, Loss: 0.5726677179336548\n",
      "Epoch: 32, Samples: 1952/5760, Loss: 0.5561764240264893\n",
      "Epoch: 32, Samples: 1984/5760, Loss: 1.2344021797180176\n",
      "Epoch: 32, Samples: 2016/5760, Loss: 0.8567301630973816\n",
      "Epoch: 32, Samples: 2048/5760, Loss: 0.9215714335441589\n",
      "Epoch: 32, Samples: 2080/5760, Loss: 0.7150963544845581\n",
      "Epoch: 32, Samples: 2112/5760, Loss: 1.0914909839630127\n",
      "Epoch: 32, Samples: 2144/5760, Loss: 0.7953734397888184\n",
      "Epoch: 32, Samples: 2176/5760, Loss: 0.8096322417259216\n",
      "Epoch: 32, Samples: 2208/5760, Loss: 0.9882222414016724\n",
      "Epoch: 32, Samples: 2240/5760, Loss: 0.6107069849967957\n",
      "Epoch: 32, Samples: 2272/5760, Loss: 0.8775504231452942\n",
      "Epoch: 32, Samples: 2304/5760, Loss: 0.6637433767318726\n",
      "Epoch: 32, Samples: 2336/5760, Loss: 1.0317635536193848\n",
      "Epoch: 32, Samples: 2368/5760, Loss: 0.7083023190498352\n",
      "Epoch: 32, Samples: 2400/5760, Loss: 0.5895311236381531\n",
      "Epoch: 32, Samples: 2432/5760, Loss: 1.1607811450958252\n",
      "Epoch: 32, Samples: 2464/5760, Loss: 0.7888038158416748\n",
      "Epoch: 32, Samples: 2496/5760, Loss: 0.6841710805892944\n",
      "Epoch: 32, Samples: 2528/5760, Loss: 0.6928208470344543\n",
      "Epoch: 32, Samples: 2560/5760, Loss: 0.8947370648384094\n",
      "Epoch: 32, Samples: 2592/5760, Loss: 0.6921722292900085\n",
      "Epoch: 32, Samples: 2624/5760, Loss: 0.42958658933639526\n",
      "Epoch: 32, Samples: 2656/5760, Loss: 0.44048506021499634\n",
      "Epoch: 32, Samples: 2688/5760, Loss: 0.7029469013214111\n",
      "Epoch: 32, Samples: 2720/5760, Loss: 0.7438240051269531\n",
      "Epoch: 32, Samples: 2752/5760, Loss: 1.2344859838485718\n",
      "Epoch: 32, Samples: 2784/5760, Loss: 0.6820199489593506\n",
      "Epoch: 32, Samples: 2816/5760, Loss: 1.0187073945999146\n",
      "Epoch: 32, Samples: 2848/5760, Loss: 0.5185195803642273\n",
      "Epoch: 32, Samples: 2880/5760, Loss: 0.7699180245399475\n",
      "Epoch: 32, Samples: 2912/5760, Loss: 0.9447606801986694\n",
      "Epoch: 32, Samples: 2944/5760, Loss: 1.1613999605178833\n",
      "Epoch: 32, Samples: 2976/5760, Loss: 0.6813979744911194\n",
      "Epoch: 32, Samples: 3008/5760, Loss: 0.5088253021240234\n",
      "Epoch: 32, Samples: 3040/5760, Loss: 0.5423735976219177\n",
      "Epoch: 32, Samples: 3072/5760, Loss: 1.1593358516693115\n",
      "Epoch: 32, Samples: 3104/5760, Loss: 0.6547665596008301\n",
      "Epoch: 32, Samples: 3136/5760, Loss: 0.6075434684753418\n",
      "Epoch: 32, Samples: 3168/5760, Loss: 0.7413858771324158\n",
      "Epoch: 32, Samples: 3200/5760, Loss: 1.1915472745895386\n",
      "Epoch: 32, Samples: 3232/5760, Loss: 0.8233274817466736\n",
      "Epoch: 32, Samples: 3264/5760, Loss: 0.7223631739616394\n",
      "Epoch: 32, Samples: 3296/5760, Loss: 0.691344141960144\n",
      "Epoch: 32, Samples: 3328/5760, Loss: 0.6775152683258057\n",
      "Epoch: 32, Samples: 3360/5760, Loss: 0.6160484552383423\n",
      "Epoch: 32, Samples: 3392/5760, Loss: 1.1350146532058716\n",
      "Epoch: 32, Samples: 3424/5760, Loss: 0.6976840496063232\n",
      "Epoch: 32, Samples: 3456/5760, Loss: 0.8607319593429565\n",
      "Epoch: 32, Samples: 3488/5760, Loss: 0.9433990716934204\n",
      "Epoch: 32, Samples: 3520/5760, Loss: 0.6148650646209717\n",
      "Epoch: 32, Samples: 3552/5760, Loss: 1.1365420818328857\n",
      "Epoch: 32, Samples: 3584/5760, Loss: 1.0721118450164795\n",
      "Epoch: 32, Samples: 3616/5760, Loss: 0.8122712969779968\n",
      "Epoch: 32, Samples: 3648/5760, Loss: 0.8901480436325073\n",
      "Epoch: 32, Samples: 3680/5760, Loss: 1.0364820957183838\n",
      "Epoch: 32, Samples: 3712/5760, Loss: 0.7233811616897583\n",
      "Epoch: 32, Samples: 3744/5760, Loss: 1.0659890174865723\n",
      "Epoch: 32, Samples: 3776/5760, Loss: 0.7412781715393066\n",
      "Epoch: 32, Samples: 3808/5760, Loss: 0.8657019138336182\n",
      "Epoch: 32, Samples: 3840/5760, Loss: 0.8300091028213501\n",
      "Epoch: 32, Samples: 3872/5760, Loss: 0.6366976499557495\n",
      "Epoch: 32, Samples: 3904/5760, Loss: 0.672318696975708\n",
      "Epoch: 32, Samples: 3936/5760, Loss: 0.6302390098571777\n",
      "Epoch: 32, Samples: 3968/5760, Loss: 0.5036806464195251\n",
      "Epoch: 32, Samples: 4000/5760, Loss: 0.49762699007987976\n",
      "Epoch: 32, Samples: 4032/5760, Loss: 0.6295794248580933\n",
      "Epoch: 32, Samples: 4064/5760, Loss: 0.7296826839447021\n",
      "Epoch: 32, Samples: 4096/5760, Loss: 0.6793903112411499\n",
      "Epoch: 32, Samples: 4128/5760, Loss: 0.8992165923118591\n",
      "Epoch: 32, Samples: 4160/5760, Loss: 0.6425141096115112\n",
      "Epoch: 32, Samples: 4192/5760, Loss: 0.7959539294242859\n",
      "Epoch: 32, Samples: 4224/5760, Loss: 0.8274984359741211\n",
      "Epoch: 32, Samples: 4256/5760, Loss: 0.9346237778663635\n",
      "Epoch: 32, Samples: 4288/5760, Loss: 0.8758929967880249\n",
      "Epoch: 32, Samples: 4320/5760, Loss: 0.5931575298309326\n",
      "Epoch: 32, Samples: 4352/5760, Loss: 0.6209210753440857\n",
      "Epoch: 32, Samples: 4384/5760, Loss: 0.9735372066497803\n",
      "Epoch: 32, Samples: 4416/5760, Loss: 0.5806896686553955\n",
      "Epoch: 32, Samples: 4448/5760, Loss: 0.8452381491661072\n",
      "Epoch: 32, Samples: 4480/5760, Loss: 0.5991814136505127\n",
      "Epoch: 32, Samples: 4512/5760, Loss: 0.7576037645339966\n",
      "Epoch: 32, Samples: 4544/5760, Loss: 0.6125982999801636\n",
      "Epoch: 32, Samples: 4576/5760, Loss: 0.5979153513908386\n",
      "Epoch: 32, Samples: 4608/5760, Loss: 0.632063627243042\n",
      "Epoch: 32, Samples: 4640/5760, Loss: 1.005009651184082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Samples: 4672/5760, Loss: 0.9280288219451904\n",
      "Epoch: 32, Samples: 4704/5760, Loss: 0.7905575037002563\n",
      "Epoch: 32, Samples: 4736/5760, Loss: 0.8377131819725037\n",
      "Epoch: 32, Samples: 4768/5760, Loss: 0.7122634649276733\n",
      "Epoch: 32, Samples: 4800/5760, Loss: 0.901917576789856\n",
      "Epoch: 32, Samples: 4832/5760, Loss: 0.48528993129730225\n",
      "Epoch: 32, Samples: 4864/5760, Loss: 1.1571319103240967\n",
      "Epoch: 32, Samples: 4896/5760, Loss: 0.8727957606315613\n",
      "Epoch: 32, Samples: 4928/5760, Loss: 0.6914643049240112\n",
      "Epoch: 32, Samples: 4960/5760, Loss: 0.5859410762786865\n",
      "Epoch: 32, Samples: 4992/5760, Loss: 0.506558895111084\n",
      "Epoch: 32, Samples: 5024/5760, Loss: 0.5949559211730957\n",
      "Epoch: 32, Samples: 5056/5760, Loss: 0.7340037822723389\n",
      "Epoch: 32, Samples: 5088/5760, Loss: 0.9825509786605835\n",
      "Epoch: 32, Samples: 5120/5760, Loss: 0.6012896299362183\n",
      "Epoch: 32, Samples: 5152/5760, Loss: 1.0060747861862183\n",
      "Epoch: 32, Samples: 5184/5760, Loss: 0.7408699989318848\n",
      "Epoch: 32, Samples: 5216/5760, Loss: 1.2081528902053833\n",
      "Epoch: 32, Samples: 5248/5760, Loss: 0.8640532493591309\n",
      "Epoch: 32, Samples: 5280/5760, Loss: 0.6231213808059692\n",
      "Epoch: 32, Samples: 5312/5760, Loss: 0.958592414855957\n",
      "Epoch: 32, Samples: 5344/5760, Loss: 0.9061737656593323\n",
      "Epoch: 32, Samples: 5376/5760, Loss: 0.5922064781188965\n",
      "Epoch: 32, Samples: 5408/5760, Loss: 0.817622721195221\n",
      "Epoch: 32, Samples: 5440/5760, Loss: 0.5087960362434387\n",
      "Epoch: 32, Samples: 5472/5760, Loss: 0.8692737221717834\n",
      "Epoch: 32, Samples: 5504/5760, Loss: 0.5571734309196472\n",
      "Epoch: 32, Samples: 5536/5760, Loss: 0.64260333776474\n",
      "Epoch: 32, Samples: 5568/5760, Loss: 0.6573532819747925\n",
      "Epoch: 32, Samples: 5600/5760, Loss: 0.9307447671890259\n",
      "Epoch: 32, Samples: 5632/5760, Loss: 1.0604681968688965\n",
      "Epoch: 32, Samples: 5664/5760, Loss: 0.5849555730819702\n",
      "Epoch: 32, Samples: 5696/5760, Loss: 0.9567043781280518\n",
      "Epoch: 32, Samples: 5728/5760, Loss: 1.7036343812942505\n",
      "\n",
      "Epoch: 32\n",
      "Training set: Average loss: 0.7836\n",
      "Validation set: Average loss: 1.6234, Accuracy: 457/818 (56%)\n",
      "Epoch: 33, Samples: 0/5760, Loss: 0.710899829864502\n",
      "Epoch: 33, Samples: 32/5760, Loss: 0.8731001615524292\n",
      "Epoch: 33, Samples: 64/5760, Loss: 0.5385328531265259\n",
      "Epoch: 33, Samples: 96/5760, Loss: 0.6566962003707886\n",
      "Epoch: 33, Samples: 128/5760, Loss: 0.9940402507781982\n",
      "Epoch: 33, Samples: 160/5760, Loss: 0.7880373597145081\n",
      "Epoch: 33, Samples: 192/5760, Loss: 0.9707200527191162\n",
      "Epoch: 33, Samples: 224/5760, Loss: 0.6913654208183289\n",
      "Epoch: 33, Samples: 256/5760, Loss: 0.5355923175811768\n",
      "Epoch: 33, Samples: 288/5760, Loss: 0.5454300045967102\n",
      "Epoch: 33, Samples: 320/5760, Loss: 0.402374267578125\n",
      "Epoch: 33, Samples: 352/5760, Loss: 0.5043545961380005\n",
      "Epoch: 33, Samples: 384/5760, Loss: 0.7386081218719482\n",
      "Epoch: 33, Samples: 416/5760, Loss: 0.6295592188835144\n",
      "Epoch: 33, Samples: 448/5760, Loss: 0.9215395450592041\n",
      "Epoch: 33, Samples: 480/5760, Loss: 0.8176891803741455\n",
      "Epoch: 33, Samples: 512/5760, Loss: 0.70497727394104\n",
      "Epoch: 33, Samples: 544/5760, Loss: 0.6897958517074585\n",
      "Epoch: 33, Samples: 576/5760, Loss: 1.4438095092773438\n",
      "Epoch: 33, Samples: 608/5760, Loss: 0.8608506917953491\n",
      "Epoch: 33, Samples: 640/5760, Loss: 0.6863361597061157\n",
      "Epoch: 33, Samples: 672/5760, Loss: 0.8429829478263855\n",
      "Epoch: 33, Samples: 704/5760, Loss: 0.8718047738075256\n",
      "Epoch: 33, Samples: 736/5760, Loss: 0.7583006620407104\n",
      "Epoch: 33, Samples: 768/5760, Loss: 0.8274312615394592\n",
      "Epoch: 33, Samples: 800/5760, Loss: 0.7446215152740479\n",
      "Epoch: 33, Samples: 832/5760, Loss: 0.9111956357955933\n",
      "Epoch: 33, Samples: 864/5760, Loss: 0.711592435836792\n",
      "Epoch: 33, Samples: 896/5760, Loss: 0.9438422918319702\n",
      "Epoch: 33, Samples: 928/5760, Loss: 0.6191120147705078\n",
      "Epoch: 33, Samples: 960/5760, Loss: 0.7713922262191772\n",
      "Epoch: 33, Samples: 992/5760, Loss: 0.6927545070648193\n",
      "Epoch: 33, Samples: 1024/5760, Loss: 1.0322809219360352\n",
      "Epoch: 33, Samples: 1056/5760, Loss: 0.8863368034362793\n",
      "Epoch: 33, Samples: 1088/5760, Loss: 0.680915117263794\n",
      "Epoch: 33, Samples: 1120/5760, Loss: 1.0154047012329102\n",
      "Epoch: 33, Samples: 1152/5760, Loss: 0.6157803535461426\n",
      "Epoch: 33, Samples: 1184/5760, Loss: 0.5907382965087891\n",
      "Epoch: 33, Samples: 1216/5760, Loss: 0.8140563368797302\n",
      "Epoch: 33, Samples: 1248/5760, Loss: 0.5824576616287231\n",
      "Epoch: 33, Samples: 1280/5760, Loss: 0.6499750018119812\n",
      "Epoch: 33, Samples: 1312/5760, Loss: 1.0240049362182617\n",
      "Epoch: 33, Samples: 1344/5760, Loss: 0.8352186679840088\n",
      "Epoch: 33, Samples: 1376/5760, Loss: 0.6111924648284912\n",
      "Epoch: 33, Samples: 1408/5760, Loss: 0.5124121308326721\n",
      "Epoch: 33, Samples: 1440/5760, Loss: 1.0939280986785889\n",
      "Epoch: 33, Samples: 1472/5760, Loss: 0.7146012783050537\n",
      "Epoch: 33, Samples: 1504/5760, Loss: 0.812215268611908\n",
      "Epoch: 33, Samples: 1536/5760, Loss: 0.47173330187797546\n",
      "Epoch: 33, Samples: 1568/5760, Loss: 0.678650438785553\n",
      "Epoch: 33, Samples: 1600/5760, Loss: 0.60703045129776\n",
      "Epoch: 33, Samples: 1632/5760, Loss: 0.7663334012031555\n",
      "Epoch: 33, Samples: 1664/5760, Loss: 0.9245041608810425\n",
      "Epoch: 33, Samples: 1696/5760, Loss: 1.1103854179382324\n",
      "Epoch: 33, Samples: 1728/5760, Loss: 0.742701530456543\n",
      "Epoch: 33, Samples: 1760/5760, Loss: 0.6906114220619202\n",
      "Epoch: 33, Samples: 1792/5760, Loss: 0.7267264723777771\n",
      "Epoch: 33, Samples: 1824/5760, Loss: 0.6886410713195801\n",
      "Epoch: 33, Samples: 1856/5760, Loss: 0.4883120656013489\n",
      "Epoch: 33, Samples: 1888/5760, Loss: 0.7053972482681274\n",
      "Epoch: 33, Samples: 1920/5760, Loss: 0.9805535078048706\n",
      "Epoch: 33, Samples: 1952/5760, Loss: 0.5012192726135254\n",
      "Epoch: 33, Samples: 1984/5760, Loss: 0.8252543210983276\n",
      "Epoch: 33, Samples: 2016/5760, Loss: 0.7294042706489563\n",
      "Epoch: 33, Samples: 2048/5760, Loss: 0.6547377109527588\n",
      "Epoch: 33, Samples: 2080/5760, Loss: 0.8901293277740479\n",
      "Epoch: 33, Samples: 2112/5760, Loss: 0.4937182068824768\n",
      "Epoch: 33, Samples: 2144/5760, Loss: 1.0063412189483643\n",
      "Epoch: 33, Samples: 2176/5760, Loss: 0.744047999382019\n",
      "Epoch: 33, Samples: 2208/5760, Loss: 0.6271106600761414\n",
      "Epoch: 33, Samples: 2240/5760, Loss: 0.7558210492134094\n",
      "Epoch: 33, Samples: 2272/5760, Loss: 0.7856673002243042\n",
      "Epoch: 33, Samples: 2304/5760, Loss: 0.7384017705917358\n",
      "Epoch: 33, Samples: 2336/5760, Loss: 0.7511917352676392\n",
      "Epoch: 33, Samples: 2368/5760, Loss: 0.8133031129837036\n",
      "Epoch: 33, Samples: 2400/5760, Loss: 0.82415771484375\n",
      "Epoch: 33, Samples: 2432/5760, Loss: 0.9588121175765991\n",
      "Epoch: 33, Samples: 2464/5760, Loss: 0.41721996665000916\n",
      "Epoch: 33, Samples: 2496/5760, Loss: 0.7803105115890503\n",
      "Epoch: 33, Samples: 2528/5760, Loss: 0.8671060800552368\n",
      "Epoch: 33, Samples: 2560/5760, Loss: 0.8260492086410522\n",
      "Epoch: 33, Samples: 2592/5760, Loss: 0.7802921533584595\n",
      "Epoch: 33, Samples: 2624/5760, Loss: 0.6203476786613464\n",
      "Epoch: 33, Samples: 2656/5760, Loss: 1.0100946426391602\n",
      "Epoch: 33, Samples: 2688/5760, Loss: 0.6322946548461914\n",
      "Epoch: 33, Samples: 2720/5760, Loss: 0.6045312881469727\n",
      "Epoch: 33, Samples: 2752/5760, Loss: 0.946186900138855\n",
      "Epoch: 33, Samples: 2784/5760, Loss: 0.6882070302963257\n",
      "Epoch: 33, Samples: 2816/5760, Loss: 0.5386736392974854\n",
      "Epoch: 33, Samples: 2848/5760, Loss: 1.0423496961593628\n",
      "Epoch: 33, Samples: 2880/5760, Loss: 0.85828697681427\n",
      "Epoch: 33, Samples: 2912/5760, Loss: 0.897918701171875\n",
      "Epoch: 33, Samples: 2944/5760, Loss: 0.6040037274360657\n",
      "Epoch: 33, Samples: 2976/5760, Loss: 0.7546172142028809\n",
      "Epoch: 33, Samples: 3008/5760, Loss: 1.0902491807937622\n",
      "Epoch: 33, Samples: 3040/5760, Loss: 0.8386563062667847\n",
      "Epoch: 33, Samples: 3072/5760, Loss: 0.7425034046173096\n",
      "Epoch: 33, Samples: 3104/5760, Loss: 0.8166491985321045\n",
      "Epoch: 33, Samples: 3136/5760, Loss: 1.1115310192108154\n",
      "Epoch: 33, Samples: 3168/5760, Loss: 0.6656074523925781\n",
      "Epoch: 33, Samples: 3200/5760, Loss: 0.6504218578338623\n",
      "Epoch: 33, Samples: 3232/5760, Loss: 0.6201798915863037\n",
      "Epoch: 33, Samples: 3264/5760, Loss: 0.7684017419815063\n",
      "Epoch: 33, Samples: 3296/5760, Loss: 0.6586977243423462\n",
      "Epoch: 33, Samples: 3328/5760, Loss: 0.5197237730026245\n",
      "Epoch: 33, Samples: 3360/5760, Loss: 0.8339750170707703\n",
      "Epoch: 33, Samples: 3392/5760, Loss: 0.8364531993865967\n",
      "Epoch: 33, Samples: 3424/5760, Loss: 1.0924268960952759\n",
      "Epoch: 33, Samples: 3456/5760, Loss: 1.1049253940582275\n",
      "Epoch: 33, Samples: 3488/5760, Loss: 0.7984898090362549\n",
      "Epoch: 33, Samples: 3520/5760, Loss: 0.5298130512237549\n",
      "Epoch: 33, Samples: 3552/5760, Loss: 1.1351807117462158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33, Samples: 3584/5760, Loss: 0.6714683771133423\n",
      "Epoch: 33, Samples: 3616/5760, Loss: 0.9594224691390991\n",
      "Epoch: 33, Samples: 3648/5760, Loss: 0.9525201320648193\n",
      "Epoch: 33, Samples: 3680/5760, Loss: 0.6653741598129272\n",
      "Epoch: 33, Samples: 3712/5760, Loss: 0.7071874141693115\n",
      "Epoch: 33, Samples: 3744/5760, Loss: 0.31263357400894165\n",
      "Epoch: 33, Samples: 3776/5760, Loss: 0.7961517572402954\n",
      "Epoch: 33, Samples: 3808/5760, Loss: 0.7364271879196167\n",
      "Epoch: 33, Samples: 3840/5760, Loss: 1.1127301454544067\n",
      "Epoch: 33, Samples: 3872/5760, Loss: 0.8537251353263855\n",
      "Epoch: 33, Samples: 3904/5760, Loss: 0.8033427000045776\n",
      "Epoch: 33, Samples: 3936/5760, Loss: 0.5323142409324646\n",
      "Epoch: 33, Samples: 3968/5760, Loss: 0.7254701852798462\n",
      "Epoch: 33, Samples: 4000/5760, Loss: 0.642081618309021\n",
      "Epoch: 33, Samples: 4032/5760, Loss: 0.8439370393753052\n",
      "Epoch: 33, Samples: 4064/5760, Loss: 0.5953338146209717\n",
      "Epoch: 33, Samples: 4096/5760, Loss: 0.6332342028617859\n",
      "Epoch: 33, Samples: 4128/5760, Loss: 0.7014710307121277\n",
      "Epoch: 33, Samples: 4160/5760, Loss: 0.8742992281913757\n",
      "Epoch: 33, Samples: 4192/5760, Loss: 1.118772268295288\n",
      "Epoch: 33, Samples: 4224/5760, Loss: 0.5382298231124878\n",
      "Epoch: 33, Samples: 4256/5760, Loss: 0.484433650970459\n",
      "Epoch: 33, Samples: 4288/5760, Loss: 0.4278099834918976\n",
      "Epoch: 33, Samples: 4320/5760, Loss: 0.4607774615287781\n",
      "Epoch: 33, Samples: 4352/5760, Loss: 0.7666122913360596\n",
      "Epoch: 33, Samples: 4384/5760, Loss: 0.41090646386146545\n",
      "Epoch: 33, Samples: 4416/5760, Loss: 0.8448846340179443\n",
      "Epoch: 33, Samples: 4448/5760, Loss: 0.8179795145988464\n",
      "Epoch: 33, Samples: 4480/5760, Loss: 0.829443097114563\n",
      "Epoch: 33, Samples: 4512/5760, Loss: 0.8908156156539917\n",
      "Epoch: 33, Samples: 4544/5760, Loss: 1.194511890411377\n",
      "Epoch: 33, Samples: 4576/5760, Loss: 0.7614656686782837\n",
      "Epoch: 33, Samples: 4608/5760, Loss: 0.7116433382034302\n",
      "Epoch: 33, Samples: 4640/5760, Loss: 0.8525251150131226\n",
      "Epoch: 33, Samples: 4672/5760, Loss: 0.44171416759490967\n",
      "Epoch: 33, Samples: 4704/5760, Loss: 0.8521604537963867\n",
      "Epoch: 33, Samples: 4736/5760, Loss: 0.4679957330226898\n",
      "Epoch: 33, Samples: 4768/5760, Loss: 0.900981068611145\n",
      "Epoch: 33, Samples: 4800/5760, Loss: 0.7600788474082947\n",
      "Epoch: 33, Samples: 4832/5760, Loss: 0.5652268528938293\n",
      "Epoch: 33, Samples: 4864/5760, Loss: 1.0356385707855225\n",
      "Epoch: 33, Samples: 4896/5760, Loss: 0.630722165107727\n",
      "Epoch: 33, Samples: 4928/5760, Loss: 0.5340766906738281\n",
      "Epoch: 33, Samples: 4960/5760, Loss: 0.6927642226219177\n",
      "Epoch: 33, Samples: 4992/5760, Loss: 0.8432031869888306\n",
      "Epoch: 33, Samples: 5024/5760, Loss: 0.655501663684845\n",
      "Epoch: 33, Samples: 5056/5760, Loss: 0.704255223274231\n",
      "Epoch: 33, Samples: 5088/5760, Loss: 0.8626142144203186\n",
      "Epoch: 33, Samples: 5120/5760, Loss: 0.8942688703536987\n",
      "Epoch: 33, Samples: 5152/5760, Loss: 0.5787737369537354\n",
      "Epoch: 33, Samples: 5184/5760, Loss: 0.713620662689209\n",
      "Epoch: 33, Samples: 5216/5760, Loss: 0.8240058422088623\n",
      "Epoch: 33, Samples: 5248/5760, Loss: 0.6264182925224304\n",
      "Epoch: 33, Samples: 5280/5760, Loss: 0.8010613918304443\n",
      "Epoch: 33, Samples: 5312/5760, Loss: 0.5455213785171509\n",
      "Epoch: 33, Samples: 5344/5760, Loss: 0.6024665236473083\n",
      "Epoch: 33, Samples: 5376/5760, Loss: 0.5748412013053894\n",
      "Epoch: 33, Samples: 5408/5760, Loss: 0.884096622467041\n",
      "Epoch: 33, Samples: 5440/5760, Loss: 0.8020881414413452\n",
      "Epoch: 33, Samples: 5472/5760, Loss: 0.9625280499458313\n",
      "Epoch: 33, Samples: 5504/5760, Loss: 0.6965146660804749\n",
      "Epoch: 33, Samples: 5536/5760, Loss: 1.0326969623565674\n",
      "Epoch: 33, Samples: 5568/5760, Loss: 0.5245457887649536\n",
      "Epoch: 33, Samples: 5600/5760, Loss: 0.991197943687439\n",
      "Epoch: 33, Samples: 5632/5760, Loss: 0.5620405673980713\n",
      "Epoch: 33, Samples: 5664/5760, Loss: 0.7328495979309082\n",
      "Epoch: 33, Samples: 5696/5760, Loss: 0.9708110094070435\n",
      "Epoch: 33, Samples: 5728/5760, Loss: 1.694756031036377\n",
      "\n",
      "Epoch: 33\n",
      "Training set: Average loss: 0.7635\n",
      "Validation set: Average loss: 1.4696, Accuracy: 527/818 (64%)\n",
      "Epoch: 34, Samples: 0/5760, Loss: 0.425068736076355\n",
      "Epoch: 34, Samples: 32/5760, Loss: 0.559912383556366\n",
      "Epoch: 34, Samples: 64/5760, Loss: 0.581508994102478\n",
      "Epoch: 34, Samples: 96/5760, Loss: 0.5729461312294006\n",
      "Epoch: 34, Samples: 128/5760, Loss: 0.5703732967376709\n",
      "Epoch: 34, Samples: 160/5760, Loss: 0.595960259437561\n",
      "Epoch: 34, Samples: 192/5760, Loss: 0.6608432531356812\n",
      "Epoch: 34, Samples: 224/5760, Loss: 0.6914375424385071\n",
      "Epoch: 34, Samples: 256/5760, Loss: 0.7601621150970459\n",
      "Epoch: 34, Samples: 288/5760, Loss: 0.7600141763687134\n",
      "Epoch: 34, Samples: 320/5760, Loss: 0.7617264986038208\n",
      "Epoch: 34, Samples: 352/5760, Loss: 0.6680517196655273\n",
      "Epoch: 34, Samples: 384/5760, Loss: 0.6605026721954346\n",
      "Epoch: 34, Samples: 416/5760, Loss: 0.5532160997390747\n",
      "Epoch: 34, Samples: 448/5760, Loss: 0.8234274983406067\n",
      "Epoch: 34, Samples: 480/5760, Loss: 1.1215194463729858\n",
      "Epoch: 34, Samples: 512/5760, Loss: 0.900206983089447\n",
      "Epoch: 34, Samples: 544/5760, Loss: 0.7555988430976868\n",
      "Epoch: 34, Samples: 576/5760, Loss: 0.834939181804657\n",
      "Epoch: 34, Samples: 608/5760, Loss: 0.4369722306728363\n",
      "Epoch: 34, Samples: 640/5760, Loss: 0.500025749206543\n",
      "Epoch: 34, Samples: 672/5760, Loss: 0.7055375576019287\n",
      "Epoch: 34, Samples: 704/5760, Loss: 0.6902015805244446\n",
      "Epoch: 34, Samples: 736/5760, Loss: 0.4295389652252197\n",
      "Epoch: 34, Samples: 768/5760, Loss: 0.46001186966896057\n",
      "Epoch: 34, Samples: 800/5760, Loss: 0.8310902118682861\n",
      "Epoch: 34, Samples: 832/5760, Loss: 1.103093147277832\n",
      "Epoch: 34, Samples: 864/5760, Loss: 0.660173773765564\n",
      "Epoch: 34, Samples: 896/5760, Loss: 0.7341207265853882\n",
      "Epoch: 34, Samples: 928/5760, Loss: 0.871148943901062\n",
      "Epoch: 34, Samples: 960/5760, Loss: 0.604353666305542\n",
      "Epoch: 34, Samples: 992/5760, Loss: 0.7099953293800354\n",
      "Epoch: 34, Samples: 1024/5760, Loss: 0.47492700815200806\n",
      "Epoch: 34, Samples: 1056/5760, Loss: 1.0965996980667114\n",
      "Epoch: 34, Samples: 1088/5760, Loss: 0.49411365389823914\n",
      "Epoch: 34, Samples: 1120/5760, Loss: 0.6411692500114441\n",
      "Epoch: 34, Samples: 1152/5760, Loss: 0.5788826942443848\n",
      "Epoch: 34, Samples: 1184/5760, Loss: 0.533739447593689\n",
      "Epoch: 34, Samples: 1216/5760, Loss: 0.5867738723754883\n",
      "Epoch: 34, Samples: 1248/5760, Loss: 0.5942763686180115\n",
      "Epoch: 34, Samples: 1280/5760, Loss: 0.8189200162887573\n",
      "Epoch: 34, Samples: 1312/5760, Loss: 0.44336989521980286\n",
      "Epoch: 34, Samples: 1344/5760, Loss: 0.7431833148002625\n",
      "Epoch: 34, Samples: 1376/5760, Loss: 0.5220822691917419\n",
      "Epoch: 34, Samples: 1408/5760, Loss: 0.6953824758529663\n",
      "Epoch: 34, Samples: 1440/5760, Loss: 0.5150155425071716\n",
      "Epoch: 34, Samples: 1472/5760, Loss: 0.6856107711791992\n",
      "Epoch: 34, Samples: 1504/5760, Loss: 0.5965878367424011\n",
      "Epoch: 34, Samples: 1536/5760, Loss: 0.8930035829544067\n",
      "Epoch: 34, Samples: 1568/5760, Loss: 0.4785032272338867\n",
      "Epoch: 34, Samples: 1600/5760, Loss: 0.7337419390678406\n",
      "Epoch: 34, Samples: 1632/5760, Loss: 0.5234338641166687\n",
      "Epoch: 34, Samples: 1664/5760, Loss: 0.7349905371665955\n",
      "Epoch: 34, Samples: 1696/5760, Loss: 0.8311777710914612\n",
      "Epoch: 34, Samples: 1728/5760, Loss: 0.3504880368709564\n",
      "Epoch: 34, Samples: 1760/5760, Loss: 0.5695431232452393\n",
      "Epoch: 34, Samples: 1792/5760, Loss: 0.49231579899787903\n",
      "Epoch: 34, Samples: 1824/5760, Loss: 0.7978643178939819\n",
      "Epoch: 34, Samples: 1856/5760, Loss: 0.6735458970069885\n",
      "Epoch: 34, Samples: 1888/5760, Loss: 0.5167747139930725\n",
      "Epoch: 34, Samples: 1920/5760, Loss: 0.5116406679153442\n",
      "Epoch: 34, Samples: 1952/5760, Loss: 0.5385771989822388\n",
      "Epoch: 34, Samples: 1984/5760, Loss: 0.46453166007995605\n",
      "Epoch: 34, Samples: 2016/5760, Loss: 0.8299908638000488\n",
      "Epoch: 34, Samples: 2048/5760, Loss: 0.7295528650283813\n",
      "Epoch: 34, Samples: 2080/5760, Loss: 0.5255241990089417\n",
      "Epoch: 34, Samples: 2112/5760, Loss: 0.6851992011070251\n",
      "Epoch: 34, Samples: 2144/5760, Loss: 0.7243734002113342\n",
      "Epoch: 34, Samples: 2176/5760, Loss: 0.5446978211402893\n",
      "Epoch: 34, Samples: 2208/5760, Loss: 0.761056125164032\n",
      "Epoch: 34, Samples: 2240/5760, Loss: 0.6013714075088501\n",
      "Epoch: 34, Samples: 2272/5760, Loss: 0.6107827425003052\n",
      "Epoch: 34, Samples: 2304/5760, Loss: 0.7093300819396973\n",
      "Epoch: 34, Samples: 2336/5760, Loss: 0.7688829302787781\n",
      "Epoch: 34, Samples: 2368/5760, Loss: 0.7019595503807068\n",
      "Epoch: 34, Samples: 2400/5760, Loss: 0.6716248989105225\n",
      "Epoch: 34, Samples: 2432/5760, Loss: 0.8775831460952759\n",
      "Epoch: 34, Samples: 2464/5760, Loss: 0.7090259194374084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Samples: 2496/5760, Loss: 0.8997610211372375\n",
      "Epoch: 34, Samples: 2528/5760, Loss: 0.6366734504699707\n",
      "Epoch: 34, Samples: 2560/5760, Loss: 0.659277081489563\n",
      "Epoch: 34, Samples: 2592/5760, Loss: 0.6338505744934082\n",
      "Epoch: 34, Samples: 2624/5760, Loss: 0.6958134770393372\n",
      "Epoch: 34, Samples: 2656/5760, Loss: 0.9231173992156982\n",
      "Epoch: 34, Samples: 2688/5760, Loss: 0.48773160576820374\n",
      "Epoch: 34, Samples: 2720/5760, Loss: 0.7644001245498657\n",
      "Epoch: 34, Samples: 2752/5760, Loss: 0.6190735101699829\n",
      "Epoch: 34, Samples: 2784/5760, Loss: 0.8575273752212524\n",
      "Epoch: 34, Samples: 2816/5760, Loss: 0.608710527420044\n",
      "Epoch: 34, Samples: 2848/5760, Loss: 0.8403401374816895\n",
      "Epoch: 34, Samples: 2880/5760, Loss: 1.0160481929779053\n",
      "Epoch: 34, Samples: 2912/5760, Loss: 0.5421472191810608\n",
      "Epoch: 34, Samples: 2944/5760, Loss: 0.5155965089797974\n",
      "Epoch: 34, Samples: 2976/5760, Loss: 1.2299152612686157\n",
      "Epoch: 34, Samples: 3008/5760, Loss: 0.5482853651046753\n",
      "Epoch: 34, Samples: 3040/5760, Loss: 0.7612299919128418\n",
      "Epoch: 34, Samples: 3072/5760, Loss: 0.7745108604431152\n",
      "Epoch: 34, Samples: 3104/5760, Loss: 1.149251937866211\n",
      "Epoch: 34, Samples: 3136/5760, Loss: 0.813104510307312\n",
      "Epoch: 34, Samples: 3168/5760, Loss: 0.8164781332015991\n",
      "Epoch: 34, Samples: 3200/5760, Loss: 0.8274998664855957\n",
      "Epoch: 34, Samples: 3232/5760, Loss: 0.8475632071495056\n",
      "Epoch: 34, Samples: 3264/5760, Loss: 0.807538628578186\n",
      "Epoch: 34, Samples: 3296/5760, Loss: 1.0480196475982666\n",
      "Epoch: 34, Samples: 3328/5760, Loss: 0.6917115449905396\n",
      "Epoch: 34, Samples: 3360/5760, Loss: 0.352754682302475\n",
      "Epoch: 34, Samples: 3392/5760, Loss: 0.5623685121536255\n",
      "Epoch: 34, Samples: 3424/5760, Loss: 0.9333347082138062\n",
      "Epoch: 34, Samples: 3456/5760, Loss: 0.6023359894752502\n",
      "Epoch: 34, Samples: 3488/5760, Loss: 0.8707178831100464\n",
      "Epoch: 34, Samples: 3520/5760, Loss: 0.5698672533035278\n",
      "Epoch: 34, Samples: 3552/5760, Loss: 0.8279780149459839\n",
      "Epoch: 34, Samples: 3584/5760, Loss: 0.4782058894634247\n",
      "Epoch: 34, Samples: 3616/5760, Loss: 0.7394691705703735\n",
      "Epoch: 34, Samples: 3648/5760, Loss: 0.7372802495956421\n",
      "Epoch: 34, Samples: 3680/5760, Loss: 0.6747762560844421\n",
      "Epoch: 34, Samples: 3712/5760, Loss: 1.1578631401062012\n",
      "Epoch: 34, Samples: 3744/5760, Loss: 0.4794277548789978\n",
      "Epoch: 34, Samples: 3776/5760, Loss: 0.6610827445983887\n",
      "Epoch: 34, Samples: 3808/5760, Loss: 0.6211926341056824\n",
      "Epoch: 34, Samples: 3840/5760, Loss: 0.6952629685401917\n",
      "Epoch: 34, Samples: 3872/5760, Loss: 0.8558143377304077\n",
      "Epoch: 34, Samples: 3904/5760, Loss: 0.4643095135688782\n",
      "Epoch: 34, Samples: 3936/5760, Loss: 0.6822637915611267\n",
      "Epoch: 34, Samples: 3968/5760, Loss: 0.7197690606117249\n",
      "Epoch: 34, Samples: 4000/5760, Loss: 0.6858859062194824\n",
      "Epoch: 34, Samples: 4032/5760, Loss: 0.8471591472625732\n",
      "Epoch: 34, Samples: 4064/5760, Loss: 0.6795878410339355\n",
      "Epoch: 34, Samples: 4096/5760, Loss: 0.9640502333641052\n",
      "Epoch: 34, Samples: 4128/5760, Loss: 0.5981714725494385\n",
      "Epoch: 34, Samples: 4160/5760, Loss: 0.7174732685089111\n",
      "Epoch: 34, Samples: 4192/5760, Loss: 0.863107442855835\n",
      "Epoch: 34, Samples: 4224/5760, Loss: 0.7971920967102051\n",
      "Epoch: 34, Samples: 4256/5760, Loss: 0.7238669395446777\n",
      "Epoch: 34, Samples: 4288/5760, Loss: 0.651308536529541\n",
      "Epoch: 34, Samples: 4320/5760, Loss: 1.0794554948806763\n",
      "Epoch: 34, Samples: 4352/5760, Loss: 0.5788617730140686\n",
      "Epoch: 34, Samples: 4384/5760, Loss: 0.9859774112701416\n",
      "Epoch: 34, Samples: 4416/5760, Loss: 0.816220223903656\n",
      "Epoch: 34, Samples: 4448/5760, Loss: 0.5513761639595032\n",
      "Epoch: 34, Samples: 4480/5760, Loss: 0.7597192525863647\n",
      "Epoch: 34, Samples: 4512/5760, Loss: 0.5628563165664673\n",
      "Epoch: 34, Samples: 4544/5760, Loss: 0.705865204334259\n",
      "Epoch: 34, Samples: 4576/5760, Loss: 0.6330997943878174\n",
      "Epoch: 34, Samples: 4608/5760, Loss: 0.620820939540863\n",
      "Epoch: 34, Samples: 4640/5760, Loss: 0.9996005296707153\n",
      "Epoch: 34, Samples: 4672/5760, Loss: 0.3983498811721802\n",
      "Epoch: 34, Samples: 4704/5760, Loss: 0.7930153012275696\n",
      "Epoch: 34, Samples: 4736/5760, Loss: 0.5063169002532959\n",
      "Epoch: 34, Samples: 4768/5760, Loss: 0.625264048576355\n",
      "Epoch: 34, Samples: 4800/5760, Loss: 0.7333461046218872\n",
      "Epoch: 34, Samples: 4832/5760, Loss: 0.5640224814414978\n",
      "Epoch: 34, Samples: 4864/5760, Loss: 0.7515146732330322\n",
      "Epoch: 34, Samples: 4896/5760, Loss: 0.7469207048416138\n",
      "Epoch: 34, Samples: 4928/5760, Loss: 0.7556003332138062\n",
      "Epoch: 34, Samples: 4960/5760, Loss: 0.7957500219345093\n",
      "Epoch: 34, Samples: 4992/5760, Loss: 0.7509714365005493\n",
      "Epoch: 34, Samples: 5024/5760, Loss: 0.5581812858581543\n",
      "Epoch: 34, Samples: 5056/5760, Loss: 0.7461022138595581\n",
      "Epoch: 34, Samples: 5088/5760, Loss: 0.843195378780365\n",
      "Epoch: 34, Samples: 5120/5760, Loss: 0.6045620441436768\n",
      "Epoch: 34, Samples: 5152/5760, Loss: 0.5361964702606201\n",
      "Epoch: 34, Samples: 5184/5760, Loss: 0.6540445685386658\n",
      "Epoch: 34, Samples: 5216/5760, Loss: 0.461286723613739\n",
      "Epoch: 34, Samples: 5248/5760, Loss: 0.7838507890701294\n",
      "Epoch: 34, Samples: 5280/5760, Loss: 0.788743257522583\n",
      "Epoch: 34, Samples: 5312/5760, Loss: 0.690576434135437\n",
      "Epoch: 34, Samples: 5344/5760, Loss: 0.6755546927452087\n",
      "Epoch: 34, Samples: 5376/5760, Loss: 0.4555116891860962\n",
      "Epoch: 34, Samples: 5408/5760, Loss: 0.6951667666435242\n",
      "Epoch: 34, Samples: 5440/5760, Loss: 0.9028967618942261\n",
      "Epoch: 34, Samples: 5472/5760, Loss: 0.8281349539756775\n",
      "Epoch: 34, Samples: 5504/5760, Loss: 0.5697979927062988\n",
      "Epoch: 34, Samples: 5536/5760, Loss: 0.6088879108428955\n",
      "Epoch: 34, Samples: 5568/5760, Loss: 0.815644383430481\n",
      "Epoch: 34, Samples: 5600/5760, Loss: 0.8805029392242432\n",
      "Epoch: 34, Samples: 5632/5760, Loss: 0.6633433103561401\n",
      "Epoch: 34, Samples: 5664/5760, Loss: 0.7750289440155029\n",
      "Epoch: 34, Samples: 5696/5760, Loss: 0.5254511833190918\n",
      "Epoch: 34, Samples: 5728/5760, Loss: 2.3725461959838867\n",
      "\n",
      "Epoch: 34\n",
      "Training set: Average loss: 0.7053\n",
      "Validation set: Average loss: 1.5556, Accuracy: 490/818 (60%)\n",
      "Epoch: 35, Samples: 0/5760, Loss: 0.4767782986164093\n",
      "Epoch: 35, Samples: 32/5760, Loss: 0.5889220237731934\n",
      "Epoch: 35, Samples: 64/5760, Loss: 0.6874600052833557\n",
      "Epoch: 35, Samples: 96/5760, Loss: 0.6046987175941467\n",
      "Epoch: 35, Samples: 128/5760, Loss: 0.5102626085281372\n",
      "Epoch: 35, Samples: 160/5760, Loss: 0.8676401376724243\n",
      "Epoch: 35, Samples: 192/5760, Loss: 0.6781877279281616\n",
      "Epoch: 35, Samples: 224/5760, Loss: 0.7585288882255554\n",
      "Epoch: 35, Samples: 256/5760, Loss: 0.3617030084133148\n",
      "Epoch: 35, Samples: 288/5760, Loss: 0.4881068468093872\n",
      "Epoch: 35, Samples: 320/5760, Loss: 0.6526690721511841\n",
      "Epoch: 35, Samples: 352/5760, Loss: 0.8893445134162903\n",
      "Epoch: 35, Samples: 384/5760, Loss: 0.6302412748336792\n",
      "Epoch: 35, Samples: 416/5760, Loss: 0.7755324840545654\n",
      "Epoch: 35, Samples: 448/5760, Loss: 0.6355222463607788\n",
      "Epoch: 35, Samples: 480/5760, Loss: 0.8129159808158875\n",
      "Epoch: 35, Samples: 512/5760, Loss: 0.7042797803878784\n",
      "Epoch: 35, Samples: 544/5760, Loss: 1.050215721130371\n",
      "Epoch: 35, Samples: 576/5760, Loss: 0.5326091051101685\n",
      "Epoch: 35, Samples: 608/5760, Loss: 0.5828967094421387\n",
      "Epoch: 35, Samples: 640/5760, Loss: 0.4542582035064697\n",
      "Epoch: 35, Samples: 672/5760, Loss: 0.47582948207855225\n",
      "Epoch: 35, Samples: 704/5760, Loss: 0.7441186308860779\n",
      "Epoch: 35, Samples: 736/5760, Loss: 0.5363662838935852\n",
      "Epoch: 35, Samples: 768/5760, Loss: 0.5231205224990845\n",
      "Epoch: 35, Samples: 800/5760, Loss: 0.5357711315155029\n",
      "Epoch: 35, Samples: 832/5760, Loss: 1.092801809310913\n",
      "Epoch: 35, Samples: 864/5760, Loss: 0.7506412267684937\n",
      "Epoch: 35, Samples: 896/5760, Loss: 0.950204610824585\n",
      "Epoch: 35, Samples: 928/5760, Loss: 0.6552325487136841\n",
      "Epoch: 35, Samples: 960/5760, Loss: 0.5262081623077393\n",
      "Epoch: 35, Samples: 992/5760, Loss: 0.5037305355072021\n",
      "Epoch: 35, Samples: 1024/5760, Loss: 0.6145694255828857\n",
      "Epoch: 35, Samples: 1056/5760, Loss: 0.47261127829551697\n",
      "Epoch: 35, Samples: 1088/5760, Loss: 0.6242486238479614\n",
      "Epoch: 35, Samples: 1120/5760, Loss: 0.7291592359542847\n",
      "Epoch: 35, Samples: 1152/5760, Loss: 0.448795884847641\n",
      "Epoch: 35, Samples: 1184/5760, Loss: 0.6722908020019531\n",
      "Epoch: 35, Samples: 1216/5760, Loss: 0.6722629070281982\n",
      "Epoch: 35, Samples: 1248/5760, Loss: 0.5362213850021362\n",
      "Epoch: 35, Samples: 1280/5760, Loss: 0.6728118658065796\n",
      "Epoch: 35, Samples: 1312/5760, Loss: 0.7722429037094116\n",
      "Epoch: 35, Samples: 1344/5760, Loss: 0.4508786201477051\n",
      "Epoch: 35, Samples: 1376/5760, Loss: 0.8350856304168701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, Samples: 1408/5760, Loss: 0.7484443187713623\n",
      "Epoch: 35, Samples: 1440/5760, Loss: 0.6086426973342896\n",
      "Epoch: 35, Samples: 1472/5760, Loss: 0.6079604625701904\n",
      "Epoch: 35, Samples: 1504/5760, Loss: 0.926723837852478\n",
      "Epoch: 35, Samples: 1536/5760, Loss: 0.6163055896759033\n",
      "Epoch: 35, Samples: 1568/5760, Loss: 0.6994073390960693\n",
      "Epoch: 35, Samples: 1600/5760, Loss: 0.6025810241699219\n",
      "Epoch: 35, Samples: 1632/5760, Loss: 0.5281963348388672\n",
      "Epoch: 35, Samples: 1664/5760, Loss: 0.6051758527755737\n",
      "Epoch: 35, Samples: 1696/5760, Loss: 0.6084346771240234\n",
      "Epoch: 35, Samples: 1728/5760, Loss: 1.0297188758850098\n",
      "Epoch: 35, Samples: 1760/5760, Loss: 0.8975057005882263\n",
      "Epoch: 35, Samples: 1792/5760, Loss: 0.7517726421356201\n",
      "Epoch: 35, Samples: 1824/5760, Loss: 0.3762572407722473\n",
      "Epoch: 35, Samples: 1856/5760, Loss: 0.4824526906013489\n",
      "Epoch: 35, Samples: 1888/5760, Loss: 0.4795476794242859\n",
      "Epoch: 35, Samples: 1920/5760, Loss: 0.5853683352470398\n",
      "Epoch: 35, Samples: 1952/5760, Loss: 0.8334972262382507\n",
      "Epoch: 35, Samples: 1984/5760, Loss: 0.5692018270492554\n",
      "Epoch: 35, Samples: 2016/5760, Loss: 0.6103384494781494\n",
      "Epoch: 35, Samples: 2048/5760, Loss: 0.6753891706466675\n",
      "Epoch: 35, Samples: 2080/5760, Loss: 0.3585996627807617\n",
      "Epoch: 35, Samples: 2112/5760, Loss: 0.9061839580535889\n",
      "Epoch: 35, Samples: 2144/5760, Loss: 0.6614267826080322\n",
      "Epoch: 35, Samples: 2176/5760, Loss: 0.845136284828186\n",
      "Epoch: 35, Samples: 2208/5760, Loss: 0.6019123792648315\n",
      "Epoch: 35, Samples: 2240/5760, Loss: 0.7980756759643555\n",
      "Epoch: 35, Samples: 2272/5760, Loss: 0.43362048268318176\n",
      "Epoch: 35, Samples: 2304/5760, Loss: 0.6446208953857422\n",
      "Epoch: 35, Samples: 2336/5760, Loss: 0.6073840260505676\n",
      "Epoch: 35, Samples: 2368/5760, Loss: 0.6192349195480347\n",
      "Epoch: 35, Samples: 2400/5760, Loss: 0.6523622274398804\n",
      "Epoch: 35, Samples: 2432/5760, Loss: 0.7047266960144043\n",
      "Epoch: 35, Samples: 2464/5760, Loss: 0.6471772789955139\n",
      "Epoch: 35, Samples: 2496/5760, Loss: 0.7803828716278076\n",
      "Epoch: 35, Samples: 2528/5760, Loss: 0.9402744770050049\n",
      "Epoch: 35, Samples: 2560/5760, Loss: 0.7404628992080688\n",
      "Epoch: 35, Samples: 2592/5760, Loss: 0.4959403872489929\n",
      "Epoch: 35, Samples: 2624/5760, Loss: 0.9070411920547485\n",
      "Epoch: 35, Samples: 2656/5760, Loss: 0.9413410425186157\n",
      "Epoch: 35, Samples: 2688/5760, Loss: 0.3787582218647003\n",
      "Epoch: 35, Samples: 2720/5760, Loss: 0.4074154496192932\n",
      "Epoch: 35, Samples: 2752/5760, Loss: 0.6864679455757141\n",
      "Epoch: 35, Samples: 2784/5760, Loss: 0.4542558193206787\n",
      "Epoch: 35, Samples: 2816/5760, Loss: 0.6917811632156372\n",
      "Epoch: 35, Samples: 2848/5760, Loss: 0.7491928339004517\n",
      "Epoch: 35, Samples: 2880/5760, Loss: 0.6759915351867676\n",
      "Epoch: 35, Samples: 2912/5760, Loss: 0.5797650218009949\n",
      "Epoch: 35, Samples: 2944/5760, Loss: 0.5859460830688477\n",
      "Epoch: 35, Samples: 2976/5760, Loss: 0.7178326845169067\n",
      "Epoch: 35, Samples: 3008/5760, Loss: 0.5486425757408142\n",
      "Epoch: 35, Samples: 3040/5760, Loss: 0.8481718897819519\n",
      "Epoch: 35, Samples: 3072/5760, Loss: 0.3627432584762573\n",
      "Epoch: 35, Samples: 3104/5760, Loss: 0.41667014360427856\n",
      "Epoch: 35, Samples: 3136/5760, Loss: 0.4816610813140869\n",
      "Epoch: 35, Samples: 3168/5760, Loss: 0.6119928956031799\n",
      "Epoch: 35, Samples: 3200/5760, Loss: 0.4382520914077759\n",
      "Epoch: 35, Samples: 3232/5760, Loss: 0.49529945850372314\n",
      "Epoch: 35, Samples: 3264/5760, Loss: 0.6630861759185791\n",
      "Epoch: 35, Samples: 3296/5760, Loss: 0.5883848667144775\n",
      "Epoch: 35, Samples: 3328/5760, Loss: 0.7983657717704773\n",
      "Epoch: 35, Samples: 3360/5760, Loss: 0.6382876038551331\n",
      "Epoch: 35, Samples: 3392/5760, Loss: 0.6073545217514038\n",
      "Epoch: 35, Samples: 3424/5760, Loss: 0.6355486512184143\n",
      "Epoch: 35, Samples: 3456/5760, Loss: 0.651382565498352\n",
      "Epoch: 35, Samples: 3488/5760, Loss: 0.8423677682876587\n",
      "Epoch: 35, Samples: 3520/5760, Loss: 0.5620812773704529\n",
      "Epoch: 35, Samples: 3552/5760, Loss: 0.875091016292572\n",
      "Epoch: 35, Samples: 3584/5760, Loss: 0.7848849892616272\n",
      "Epoch: 35, Samples: 3616/5760, Loss: 0.7597995400428772\n",
      "Epoch: 35, Samples: 3648/5760, Loss: 1.1469954252243042\n",
      "Epoch: 35, Samples: 3680/5760, Loss: 0.6902211904525757\n",
      "Epoch: 35, Samples: 3712/5760, Loss: 0.5413554906845093\n",
      "Epoch: 35, Samples: 3744/5760, Loss: 0.7175669074058533\n",
      "Epoch: 35, Samples: 3776/5760, Loss: 0.8749886751174927\n",
      "Epoch: 35, Samples: 3808/5760, Loss: 0.6193997263908386\n",
      "Epoch: 35, Samples: 3840/5760, Loss: 0.6793094873428345\n",
      "Epoch: 35, Samples: 3872/5760, Loss: 0.8313952684402466\n",
      "Epoch: 35, Samples: 3904/5760, Loss: 0.45210978388786316\n",
      "Epoch: 35, Samples: 3936/5760, Loss: 1.1410492658615112\n",
      "Epoch: 35, Samples: 3968/5760, Loss: 0.8218836784362793\n",
      "Epoch: 35, Samples: 4000/5760, Loss: 0.9800533652305603\n",
      "Epoch: 35, Samples: 4032/5760, Loss: 0.7012966871261597\n",
      "Epoch: 35, Samples: 4064/5760, Loss: 0.6591556668281555\n",
      "Epoch: 35, Samples: 4096/5760, Loss: 0.7327657341957092\n",
      "Epoch: 35, Samples: 4128/5760, Loss: 0.5887994766235352\n",
      "Epoch: 35, Samples: 4160/5760, Loss: 0.9370923042297363\n",
      "Epoch: 35, Samples: 4192/5760, Loss: 0.7110352516174316\n",
      "Epoch: 35, Samples: 4224/5760, Loss: 0.7301158905029297\n",
      "Epoch: 35, Samples: 4256/5760, Loss: 0.6358814835548401\n",
      "Epoch: 35, Samples: 4288/5760, Loss: 0.4245293140411377\n",
      "Epoch: 35, Samples: 4320/5760, Loss: 0.7446287274360657\n",
      "Epoch: 35, Samples: 4352/5760, Loss: 0.999884843826294\n",
      "Epoch: 35, Samples: 4384/5760, Loss: 0.5618690252304077\n",
      "Epoch: 35, Samples: 4416/5760, Loss: 0.6174888610839844\n",
      "Epoch: 35, Samples: 4448/5760, Loss: 0.29450589418411255\n",
      "Epoch: 35, Samples: 4480/5760, Loss: 1.0746954679489136\n",
      "Epoch: 35, Samples: 4512/5760, Loss: 0.5586245059967041\n",
      "Epoch: 35, Samples: 4544/5760, Loss: 0.5576294660568237\n",
      "Epoch: 35, Samples: 4576/5760, Loss: 0.6066305041313171\n",
      "Epoch: 35, Samples: 4608/5760, Loss: 0.6652007699012756\n",
      "Epoch: 35, Samples: 4640/5760, Loss: 0.8452655076980591\n",
      "Epoch: 35, Samples: 4672/5760, Loss: 0.538347601890564\n",
      "Epoch: 35, Samples: 4704/5760, Loss: 0.7081242799758911\n",
      "Epoch: 35, Samples: 4736/5760, Loss: 0.7567470669746399\n",
      "Epoch: 35, Samples: 4768/5760, Loss: 0.6728783249855042\n",
      "Epoch: 35, Samples: 4800/5760, Loss: 0.621474027633667\n",
      "Epoch: 35, Samples: 4832/5760, Loss: 0.6406364440917969\n",
      "Epoch: 35, Samples: 4864/5760, Loss: 0.8009923696517944\n",
      "Epoch: 35, Samples: 4896/5760, Loss: 0.8161618709564209\n",
      "Epoch: 35, Samples: 4928/5760, Loss: 0.6293640732765198\n",
      "Epoch: 35, Samples: 4960/5760, Loss: 0.8745213150978088\n",
      "Epoch: 35, Samples: 4992/5760, Loss: 0.7780355215072632\n",
      "Epoch: 35, Samples: 5024/5760, Loss: 0.7732100486755371\n",
      "Epoch: 35, Samples: 5056/5760, Loss: 0.5051044225692749\n",
      "Epoch: 35, Samples: 5088/5760, Loss: 0.6737592220306396\n",
      "Epoch: 35, Samples: 5120/5760, Loss: 1.2772016525268555\n",
      "Epoch: 35, Samples: 5152/5760, Loss: 0.9676059484481812\n",
      "Epoch: 35, Samples: 5184/5760, Loss: 0.597303569316864\n",
      "Epoch: 35, Samples: 5216/5760, Loss: 0.909703254699707\n",
      "Epoch: 35, Samples: 5248/5760, Loss: 0.9689055681228638\n",
      "Epoch: 35, Samples: 5280/5760, Loss: 1.0347697734832764\n",
      "Epoch: 35, Samples: 5312/5760, Loss: 0.5179561972618103\n",
      "Epoch: 35, Samples: 5344/5760, Loss: 0.5201092958450317\n",
      "Epoch: 35, Samples: 5376/5760, Loss: 0.9834229946136475\n",
      "Epoch: 35, Samples: 5408/5760, Loss: 0.8434463739395142\n",
      "Epoch: 35, Samples: 5440/5760, Loss: 0.6476167440414429\n",
      "Epoch: 35, Samples: 5472/5760, Loss: 0.512942910194397\n",
      "Epoch: 35, Samples: 5504/5760, Loss: 0.6477497816085815\n",
      "Epoch: 35, Samples: 5536/5760, Loss: 0.7149097919464111\n",
      "Epoch: 35, Samples: 5568/5760, Loss: 0.7308652400970459\n",
      "Epoch: 35, Samples: 5600/5760, Loss: 0.5664579272270203\n",
      "Epoch: 35, Samples: 5632/5760, Loss: 0.5803812742233276\n",
      "Epoch: 35, Samples: 5664/5760, Loss: 0.6788758039474487\n",
      "Epoch: 35, Samples: 5696/5760, Loss: 0.7495306730270386\n",
      "Epoch: 35, Samples: 5728/5760, Loss: 0.7178059816360474\n",
      "\n",
      "Epoch: 35\n",
      "Training set: Average loss: 0.6795\n",
      "Validation set: Average loss: 1.9028, Accuracy: 445/818 (54%)\n",
      "Epoch: 36, Samples: 0/5760, Loss: 0.6900423765182495\n",
      "Epoch: 36, Samples: 32/5760, Loss: 0.4554511308670044\n",
      "Epoch: 36, Samples: 64/5760, Loss: 0.6899353265762329\n",
      "Epoch: 36, Samples: 96/5760, Loss: 0.7736386656761169\n",
      "Epoch: 36, Samples: 128/5760, Loss: 0.9185003638267517\n",
      "Epoch: 36, Samples: 160/5760, Loss: 0.5770633816719055\n",
      "Epoch: 36, Samples: 192/5760, Loss: 0.8984299302101135\n",
      "Epoch: 36, Samples: 224/5760, Loss: 0.8356430530548096\n",
      "Epoch: 36, Samples: 256/5760, Loss: 0.6158018112182617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Samples: 288/5760, Loss: 0.35999423265457153\n",
      "Epoch: 36, Samples: 320/5760, Loss: 0.7048910856246948\n",
      "Epoch: 36, Samples: 352/5760, Loss: 0.5246395468711853\n",
      "Epoch: 36, Samples: 384/5760, Loss: 0.3887442350387573\n",
      "Epoch: 36, Samples: 416/5760, Loss: 0.6089359521865845\n",
      "Epoch: 36, Samples: 448/5760, Loss: 0.8249750137329102\n",
      "Epoch: 36, Samples: 480/5760, Loss: 0.49460747838020325\n",
      "Epoch: 36, Samples: 512/5760, Loss: 0.57329261302948\n",
      "Epoch: 36, Samples: 544/5760, Loss: 0.6934134364128113\n",
      "Epoch: 36, Samples: 576/5760, Loss: 0.7999531030654907\n",
      "Epoch: 36, Samples: 608/5760, Loss: 0.6109426021575928\n",
      "Epoch: 36, Samples: 640/5760, Loss: 0.4745103418827057\n",
      "Epoch: 36, Samples: 672/5760, Loss: 0.5178096294403076\n",
      "Epoch: 36, Samples: 704/5760, Loss: 0.563604474067688\n",
      "Epoch: 36, Samples: 736/5760, Loss: 0.5806522369384766\n",
      "Epoch: 36, Samples: 768/5760, Loss: 0.7459466457366943\n",
      "Epoch: 36, Samples: 800/5760, Loss: 0.4860488474369049\n",
      "Epoch: 36, Samples: 832/5760, Loss: 0.8107069730758667\n",
      "Epoch: 36, Samples: 864/5760, Loss: 0.7731190323829651\n",
      "Epoch: 36, Samples: 896/5760, Loss: 0.8138025999069214\n",
      "Epoch: 36, Samples: 928/5760, Loss: 0.647507905960083\n",
      "Epoch: 36, Samples: 960/5760, Loss: 0.6166379451751709\n",
      "Epoch: 36, Samples: 992/5760, Loss: 0.6561378240585327\n",
      "Epoch: 36, Samples: 1024/5760, Loss: 0.5708620548248291\n",
      "Epoch: 36, Samples: 1056/5760, Loss: 0.6280504465103149\n",
      "Epoch: 36, Samples: 1088/5760, Loss: 1.0580497980117798\n",
      "Epoch: 36, Samples: 1120/5760, Loss: 0.6477900147438049\n",
      "Epoch: 36, Samples: 1152/5760, Loss: 0.6566393971443176\n",
      "Epoch: 36, Samples: 1184/5760, Loss: 0.3723717927932739\n",
      "Epoch: 36, Samples: 1216/5760, Loss: 0.6632013320922852\n",
      "Epoch: 36, Samples: 1248/5760, Loss: 0.8149916529655457\n",
      "Epoch: 36, Samples: 1280/5760, Loss: 0.6157193779945374\n",
      "Epoch: 36, Samples: 1312/5760, Loss: 0.7847560048103333\n",
      "Epoch: 36, Samples: 1344/5760, Loss: 0.8801878690719604\n",
      "Epoch: 36, Samples: 1376/5760, Loss: 0.398123174905777\n",
      "Epoch: 36, Samples: 1408/5760, Loss: 0.6600314974784851\n",
      "Epoch: 36, Samples: 1440/5760, Loss: 0.6772088408470154\n",
      "Epoch: 36, Samples: 1472/5760, Loss: 0.5349056720733643\n",
      "Epoch: 36, Samples: 1504/5760, Loss: 0.5523645877838135\n",
      "Epoch: 36, Samples: 1536/5760, Loss: 0.7813684940338135\n",
      "Epoch: 36, Samples: 1568/5760, Loss: 0.3363949656486511\n",
      "Epoch: 36, Samples: 1600/5760, Loss: 0.4172378480434418\n",
      "Epoch: 36, Samples: 1632/5760, Loss: 0.9557036757469177\n",
      "Epoch: 36, Samples: 1664/5760, Loss: 0.7237961292266846\n",
      "Epoch: 36, Samples: 1696/5760, Loss: 0.5451508164405823\n",
      "Epoch: 36, Samples: 1728/5760, Loss: 0.9965029954910278\n",
      "Epoch: 36, Samples: 1760/5760, Loss: 0.47852203249931335\n",
      "Epoch: 36, Samples: 1792/5760, Loss: 0.46028175950050354\n",
      "Epoch: 36, Samples: 1824/5760, Loss: 0.6008719205856323\n",
      "Epoch: 36, Samples: 1856/5760, Loss: 0.5566753149032593\n",
      "Epoch: 36, Samples: 1888/5760, Loss: 0.8628158569335938\n",
      "Epoch: 36, Samples: 1920/5760, Loss: 0.45126229524612427\n",
      "Epoch: 36, Samples: 1952/5760, Loss: 0.5749859809875488\n",
      "Epoch: 36, Samples: 1984/5760, Loss: 0.44117364287376404\n",
      "Epoch: 36, Samples: 2016/5760, Loss: 0.47509509325027466\n",
      "Epoch: 36, Samples: 2048/5760, Loss: 0.7969547510147095\n",
      "Epoch: 36, Samples: 2080/5760, Loss: 0.5274450778961182\n",
      "Epoch: 36, Samples: 2112/5760, Loss: 0.6772738695144653\n",
      "Epoch: 36, Samples: 2144/5760, Loss: 0.5155206918716431\n",
      "Epoch: 36, Samples: 2176/5760, Loss: 0.5240018963813782\n",
      "Epoch: 36, Samples: 2208/5760, Loss: 0.8656448721885681\n",
      "Epoch: 36, Samples: 2240/5760, Loss: 0.6386666893959045\n",
      "Epoch: 36, Samples: 2272/5760, Loss: 0.5491563081741333\n",
      "Epoch: 36, Samples: 2304/5760, Loss: 0.7996793985366821\n",
      "Epoch: 36, Samples: 2336/5760, Loss: 0.524837851524353\n",
      "Epoch: 36, Samples: 2368/5760, Loss: 0.6959128975868225\n",
      "Epoch: 36, Samples: 2400/5760, Loss: 0.6451404094696045\n",
      "Epoch: 36, Samples: 2432/5760, Loss: 0.7374584674835205\n",
      "Epoch: 36, Samples: 2464/5760, Loss: 0.6614524722099304\n",
      "Epoch: 36, Samples: 2496/5760, Loss: 0.4950493574142456\n",
      "Epoch: 36, Samples: 2528/5760, Loss: 0.9202063083648682\n",
      "Epoch: 36, Samples: 2560/5760, Loss: 0.617807149887085\n",
      "Epoch: 36, Samples: 2592/5760, Loss: 0.38359662890434265\n",
      "Epoch: 36, Samples: 2624/5760, Loss: 0.890626072883606\n",
      "Epoch: 36, Samples: 2656/5760, Loss: 0.40677347779273987\n",
      "Epoch: 36, Samples: 2688/5760, Loss: 0.20806114375591278\n",
      "Epoch: 36, Samples: 2720/5760, Loss: 0.5821326375007629\n",
      "Epoch: 36, Samples: 2752/5760, Loss: 0.5054374933242798\n",
      "Epoch: 36, Samples: 2784/5760, Loss: 1.11924147605896\n",
      "Epoch: 36, Samples: 2816/5760, Loss: 0.6511285901069641\n",
      "Epoch: 36, Samples: 2848/5760, Loss: 0.5194423198699951\n",
      "Epoch: 36, Samples: 2880/5760, Loss: 0.8033841252326965\n",
      "Epoch: 36, Samples: 2912/5760, Loss: 0.4247300624847412\n",
      "Epoch: 36, Samples: 2944/5760, Loss: 0.5089418888092041\n",
      "Epoch: 36, Samples: 2976/5760, Loss: 0.7182706594467163\n",
      "Epoch: 36, Samples: 3008/5760, Loss: 0.5413583517074585\n",
      "Epoch: 36, Samples: 3040/5760, Loss: 0.40605348348617554\n",
      "Epoch: 36, Samples: 3072/5760, Loss: 0.6370712518692017\n",
      "Epoch: 36, Samples: 3104/5760, Loss: 0.5128390789031982\n",
      "Epoch: 36, Samples: 3136/5760, Loss: 0.8585768938064575\n",
      "Epoch: 36, Samples: 3168/5760, Loss: 0.54163658618927\n",
      "Epoch: 36, Samples: 3200/5760, Loss: 0.7261224985122681\n",
      "Epoch: 36, Samples: 3232/5760, Loss: 0.8678443431854248\n",
      "Epoch: 36, Samples: 3264/5760, Loss: 0.5098164081573486\n",
      "Epoch: 36, Samples: 3296/5760, Loss: 0.41784054040908813\n",
      "Epoch: 36, Samples: 3328/5760, Loss: 0.48015984892845154\n",
      "Epoch: 36, Samples: 3360/5760, Loss: 0.2797570526599884\n",
      "Epoch: 36, Samples: 3392/5760, Loss: 0.43971580266952515\n",
      "Epoch: 36, Samples: 3424/5760, Loss: 0.667509913444519\n",
      "Epoch: 36, Samples: 3456/5760, Loss: 0.5671138167381287\n",
      "Epoch: 36, Samples: 3488/5760, Loss: 0.44854384660720825\n",
      "Epoch: 36, Samples: 3520/5760, Loss: 0.9422028660774231\n",
      "Epoch: 36, Samples: 3552/5760, Loss: 0.5583125352859497\n",
      "Epoch: 36, Samples: 3584/5760, Loss: 0.681133508682251\n",
      "Epoch: 36, Samples: 3616/5760, Loss: 0.5276841521263123\n",
      "Epoch: 36, Samples: 3648/5760, Loss: 0.636025071144104\n",
      "Epoch: 36, Samples: 3680/5760, Loss: 0.7663223743438721\n",
      "Epoch: 36, Samples: 3712/5760, Loss: 0.29210537672042847\n",
      "Epoch: 36, Samples: 3744/5760, Loss: 0.5865338444709778\n",
      "Epoch: 36, Samples: 3776/5760, Loss: 0.40896499156951904\n",
      "Epoch: 36, Samples: 3808/5760, Loss: 0.46166476607322693\n",
      "Epoch: 36, Samples: 3840/5760, Loss: 0.8538482785224915\n",
      "Epoch: 36, Samples: 3872/5760, Loss: 0.6285066604614258\n",
      "Epoch: 36, Samples: 3904/5760, Loss: 0.5924286842346191\n",
      "Epoch: 36, Samples: 3936/5760, Loss: 0.8754318952560425\n",
      "Epoch: 36, Samples: 3968/5760, Loss: 0.46679678559303284\n",
      "Epoch: 36, Samples: 4000/5760, Loss: 0.44784680008888245\n",
      "Epoch: 36, Samples: 4032/5760, Loss: 0.7713748216629028\n",
      "Epoch: 36, Samples: 4064/5760, Loss: 0.6324276924133301\n",
      "Epoch: 36, Samples: 4096/5760, Loss: 0.5389339327812195\n",
      "Epoch: 36, Samples: 4128/5760, Loss: 0.6963270902633667\n",
      "Epoch: 36, Samples: 4160/5760, Loss: 0.6185504198074341\n",
      "Epoch: 36, Samples: 4192/5760, Loss: 0.5290942192077637\n",
      "Epoch: 36, Samples: 4224/5760, Loss: 0.4671003520488739\n",
      "Epoch: 36, Samples: 4256/5760, Loss: 0.5840435028076172\n",
      "Epoch: 36, Samples: 4288/5760, Loss: 0.7797396183013916\n",
      "Epoch: 36, Samples: 4320/5760, Loss: 1.0257740020751953\n",
      "Epoch: 36, Samples: 4352/5760, Loss: 0.8738224506378174\n",
      "Epoch: 36, Samples: 4384/5760, Loss: 0.9430812001228333\n",
      "Epoch: 36, Samples: 4416/5760, Loss: 0.7842618227005005\n",
      "Epoch: 36, Samples: 4448/5760, Loss: 0.5694634914398193\n",
      "Epoch: 36, Samples: 4480/5760, Loss: 0.5576375722885132\n",
      "Epoch: 36, Samples: 4512/5760, Loss: 0.7388850450515747\n",
      "Epoch: 36, Samples: 4544/5760, Loss: 0.6152292490005493\n",
      "Epoch: 36, Samples: 4576/5760, Loss: 0.44804900884628296\n",
      "Epoch: 36, Samples: 4608/5760, Loss: 0.6309293508529663\n",
      "Epoch: 36, Samples: 4640/5760, Loss: 0.7331955432891846\n",
      "Epoch: 36, Samples: 4672/5760, Loss: 0.9905924201011658\n",
      "Epoch: 36, Samples: 4704/5760, Loss: 0.681606650352478\n",
      "Epoch: 36, Samples: 4736/5760, Loss: 0.5851660966873169\n",
      "Epoch: 36, Samples: 4768/5760, Loss: 0.6107020378112793\n",
      "Epoch: 36, Samples: 4800/5760, Loss: 0.4214586615562439\n",
      "Epoch: 36, Samples: 4832/5760, Loss: 0.8572567701339722\n",
      "Epoch: 36, Samples: 4864/5760, Loss: 0.7511193156242371\n",
      "Epoch: 36, Samples: 4896/5760, Loss: 0.371565580368042\n",
      "Epoch: 36, Samples: 4928/5760, Loss: 0.7369869351387024\n",
      "Epoch: 36, Samples: 4960/5760, Loss: 0.43057993054389954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Samples: 4992/5760, Loss: 0.29697516560554504\n",
      "Epoch: 36, Samples: 5024/5760, Loss: 0.6122599244117737\n",
      "Epoch: 36, Samples: 5056/5760, Loss: 0.5407215356826782\n",
      "Epoch: 36, Samples: 5088/5760, Loss: 0.5018330812454224\n",
      "Epoch: 36, Samples: 5120/5760, Loss: 0.6966304779052734\n",
      "Epoch: 36, Samples: 5152/5760, Loss: 0.6554470062255859\n",
      "Epoch: 36, Samples: 5184/5760, Loss: 0.5294550657272339\n",
      "Epoch: 36, Samples: 5216/5760, Loss: 0.6642077565193176\n",
      "Epoch: 36, Samples: 5248/5760, Loss: 0.8179638385772705\n",
      "Epoch: 36, Samples: 5280/5760, Loss: 0.4585287868976593\n",
      "Epoch: 36, Samples: 5312/5760, Loss: 1.1312460899353027\n",
      "Epoch: 36, Samples: 5344/5760, Loss: 0.7113919258117676\n",
      "Epoch: 36, Samples: 5376/5760, Loss: 0.7108756303787231\n",
      "Epoch: 36, Samples: 5408/5760, Loss: 1.0811471939086914\n",
      "Epoch: 36, Samples: 5440/5760, Loss: 0.7732088565826416\n",
      "Epoch: 36, Samples: 5472/5760, Loss: 0.598014235496521\n",
      "Epoch: 36, Samples: 5504/5760, Loss: 0.6480883359909058\n",
      "Epoch: 36, Samples: 5536/5760, Loss: 0.6684098243713379\n",
      "Epoch: 36, Samples: 5568/5760, Loss: 1.0213693380355835\n",
      "Epoch: 36, Samples: 5600/5760, Loss: 0.6561980247497559\n",
      "Epoch: 36, Samples: 5632/5760, Loss: 0.6165645718574524\n",
      "Epoch: 36, Samples: 5664/5760, Loss: 0.5569149255752563\n",
      "Epoch: 36, Samples: 5696/5760, Loss: 0.8349389433860779\n",
      "Epoch: 36, Samples: 5728/5760, Loss: 1.222877860069275\n",
      "\n",
      "Epoch: 36\n",
      "Training set: Average loss: 0.6412\n",
      "Validation set: Average loss: 1.5182, Accuracy: 509/818 (62%)\n",
      "Epoch: 37, Samples: 0/5760, Loss: 0.5163770318031311\n",
      "Epoch: 37, Samples: 32/5760, Loss: 0.3952587842941284\n",
      "Epoch: 37, Samples: 64/5760, Loss: 0.5093584060668945\n",
      "Epoch: 37, Samples: 96/5760, Loss: 0.5089129209518433\n",
      "Epoch: 37, Samples: 128/5760, Loss: 0.5001649856567383\n",
      "Epoch: 37, Samples: 160/5760, Loss: 0.4783480763435364\n",
      "Epoch: 37, Samples: 192/5760, Loss: 0.6963305473327637\n",
      "Epoch: 37, Samples: 224/5760, Loss: 0.44476306438446045\n",
      "Epoch: 37, Samples: 256/5760, Loss: 0.5700978636741638\n",
      "Epoch: 37, Samples: 288/5760, Loss: 0.35616230964660645\n",
      "Epoch: 37, Samples: 320/5760, Loss: 0.505029559135437\n",
      "Epoch: 37, Samples: 352/5760, Loss: 0.45960533618927\n",
      "Epoch: 37, Samples: 384/5760, Loss: 0.5446441769599915\n",
      "Epoch: 37, Samples: 416/5760, Loss: 0.5357614755630493\n",
      "Epoch: 37, Samples: 448/5760, Loss: 0.8479379415512085\n",
      "Epoch: 37, Samples: 480/5760, Loss: 0.5292935967445374\n",
      "Epoch: 37, Samples: 512/5760, Loss: 0.8019843697547913\n",
      "Epoch: 37, Samples: 544/5760, Loss: 0.6434938907623291\n",
      "Epoch: 37, Samples: 576/5760, Loss: 0.4736226499080658\n",
      "Epoch: 37, Samples: 608/5760, Loss: 0.6584547758102417\n",
      "Epoch: 37, Samples: 640/5760, Loss: 0.5596638917922974\n",
      "Epoch: 37, Samples: 672/5760, Loss: 0.6771976947784424\n",
      "Epoch: 37, Samples: 704/5760, Loss: 0.8547157645225525\n",
      "Epoch: 37, Samples: 736/5760, Loss: 0.6607409715652466\n",
      "Epoch: 37, Samples: 768/5760, Loss: 0.4668617248535156\n",
      "Epoch: 37, Samples: 800/5760, Loss: 0.6051201820373535\n",
      "Epoch: 37, Samples: 832/5760, Loss: 0.8539934158325195\n",
      "Epoch: 37, Samples: 864/5760, Loss: 0.40804165601730347\n",
      "Epoch: 37, Samples: 896/5760, Loss: 0.48728230595588684\n",
      "Epoch: 37, Samples: 928/5760, Loss: 0.46488529443740845\n",
      "Epoch: 37, Samples: 960/5760, Loss: 0.5663994550704956\n",
      "Epoch: 37, Samples: 992/5760, Loss: 0.5926650762557983\n",
      "Epoch: 37, Samples: 1024/5760, Loss: 0.5674868226051331\n",
      "Epoch: 37, Samples: 1056/5760, Loss: 0.5226327776908875\n",
      "Epoch: 37, Samples: 1088/5760, Loss: 0.8865365982055664\n",
      "Epoch: 37, Samples: 1120/5760, Loss: 0.658261239528656\n",
      "Epoch: 37, Samples: 1152/5760, Loss: 0.4858187139034271\n",
      "Epoch: 37, Samples: 1184/5760, Loss: 0.6715989112854004\n",
      "Epoch: 37, Samples: 1216/5760, Loss: 0.45403075218200684\n",
      "Epoch: 37, Samples: 1248/5760, Loss: 0.5437526106834412\n",
      "Epoch: 37, Samples: 1280/5760, Loss: 0.4635321795940399\n",
      "Epoch: 37, Samples: 1312/5760, Loss: 0.7622649073600769\n",
      "Epoch: 37, Samples: 1344/5760, Loss: 0.5528748035430908\n",
      "Epoch: 37, Samples: 1376/5760, Loss: 0.7025284767150879\n",
      "Epoch: 37, Samples: 1408/5760, Loss: 0.8075644969940186\n",
      "Epoch: 37, Samples: 1440/5760, Loss: 0.44114574790000916\n",
      "Epoch: 37, Samples: 1472/5760, Loss: 0.3259449005126953\n",
      "Epoch: 37, Samples: 1504/5760, Loss: 0.5517600178718567\n",
      "Epoch: 37, Samples: 1536/5760, Loss: 0.5653905272483826\n",
      "Epoch: 37, Samples: 1568/5760, Loss: 0.5866132378578186\n",
      "Epoch: 37, Samples: 1600/5760, Loss: 0.5763869285583496\n",
      "Epoch: 37, Samples: 1632/5760, Loss: 0.5497205257415771\n",
      "Epoch: 37, Samples: 1664/5760, Loss: 0.4802899658679962\n",
      "Epoch: 37, Samples: 1696/5760, Loss: 0.42642396688461304\n",
      "Epoch: 37, Samples: 1728/5760, Loss: 0.3339514136314392\n",
      "Epoch: 37, Samples: 1760/5760, Loss: 0.7110896706581116\n",
      "Epoch: 37, Samples: 1792/5760, Loss: 0.5712649822235107\n",
      "Epoch: 37, Samples: 1824/5760, Loss: 0.8363845944404602\n",
      "Epoch: 37, Samples: 1856/5760, Loss: 0.6343839764595032\n",
      "Epoch: 37, Samples: 1888/5760, Loss: 0.4785296618938446\n",
      "Epoch: 37, Samples: 1920/5760, Loss: 0.3929111361503601\n",
      "Epoch: 37, Samples: 1952/5760, Loss: 0.26755252480506897\n",
      "Epoch: 37, Samples: 1984/5760, Loss: 0.8777437806129456\n",
      "Epoch: 37, Samples: 2016/5760, Loss: 0.6182176470756531\n",
      "Epoch: 37, Samples: 2048/5760, Loss: 0.5232792496681213\n",
      "Epoch: 37, Samples: 2080/5760, Loss: 0.5651527643203735\n",
      "Epoch: 37, Samples: 2112/5760, Loss: 0.6289852857589722\n",
      "Epoch: 37, Samples: 2144/5760, Loss: 0.5825077891349792\n",
      "Epoch: 37, Samples: 2176/5760, Loss: 0.7077370882034302\n",
      "Epoch: 37, Samples: 2208/5760, Loss: 0.46541619300842285\n",
      "Epoch: 37, Samples: 2240/5760, Loss: 0.7398587465286255\n",
      "Epoch: 37, Samples: 2272/5760, Loss: 0.5014575719833374\n",
      "Epoch: 37, Samples: 2304/5760, Loss: 0.9736313819885254\n",
      "Epoch: 37, Samples: 2336/5760, Loss: 0.5928027629852295\n",
      "Epoch: 37, Samples: 2368/5760, Loss: 0.6809126138687134\n",
      "Epoch: 37, Samples: 2400/5760, Loss: 0.7423747181892395\n",
      "Epoch: 37, Samples: 2432/5760, Loss: 0.5075568556785583\n",
      "Epoch: 37, Samples: 2464/5760, Loss: 0.6011407971382141\n",
      "Epoch: 37, Samples: 2496/5760, Loss: 0.48411813378334045\n",
      "Epoch: 37, Samples: 2528/5760, Loss: 0.5453239679336548\n",
      "Epoch: 37, Samples: 2560/5760, Loss: 0.44389593601226807\n",
      "Epoch: 37, Samples: 2592/5760, Loss: 0.5616621971130371\n",
      "Epoch: 37, Samples: 2624/5760, Loss: 0.8828124403953552\n",
      "Epoch: 37, Samples: 2656/5760, Loss: 0.39083966612815857\n",
      "Epoch: 37, Samples: 2688/5760, Loss: 0.6688642501831055\n",
      "Epoch: 37, Samples: 2720/5760, Loss: 0.4038262367248535\n",
      "Epoch: 37, Samples: 2752/5760, Loss: 0.6330434083938599\n",
      "Epoch: 37, Samples: 2784/5760, Loss: 0.40902474522590637\n",
      "Epoch: 37, Samples: 2816/5760, Loss: 0.769856333732605\n",
      "Epoch: 37, Samples: 2848/5760, Loss: 0.6906965970993042\n",
      "Epoch: 37, Samples: 2880/5760, Loss: 0.6647117137908936\n",
      "Epoch: 37, Samples: 2912/5760, Loss: 1.0143646001815796\n",
      "Epoch: 37, Samples: 2944/5760, Loss: 0.7396610975265503\n",
      "Epoch: 37, Samples: 2976/5760, Loss: 0.5658988952636719\n",
      "Epoch: 37, Samples: 3008/5760, Loss: 0.5048291683197021\n",
      "Epoch: 37, Samples: 3040/5760, Loss: 0.5216789245605469\n",
      "Epoch: 37, Samples: 3072/5760, Loss: 0.7493380308151245\n",
      "Epoch: 37, Samples: 3104/5760, Loss: 0.5805054903030396\n",
      "Epoch: 37, Samples: 3136/5760, Loss: 0.5090938806533813\n",
      "Epoch: 37, Samples: 3168/5760, Loss: 0.5607208013534546\n",
      "Epoch: 37, Samples: 3200/5760, Loss: 0.47363516688346863\n",
      "Epoch: 37, Samples: 3232/5760, Loss: 0.7868382334709167\n",
      "Epoch: 37, Samples: 3264/5760, Loss: 0.507623016834259\n",
      "Epoch: 37, Samples: 3296/5760, Loss: 0.8789371252059937\n",
      "Epoch: 37, Samples: 3328/5760, Loss: 0.6046233177185059\n",
      "Epoch: 37, Samples: 3360/5760, Loss: 0.6475964784622192\n",
      "Epoch: 37, Samples: 3392/5760, Loss: 0.322226881980896\n",
      "Epoch: 37, Samples: 3424/5760, Loss: 0.6374272704124451\n",
      "Epoch: 37, Samples: 3456/5760, Loss: 0.587989330291748\n",
      "Epoch: 37, Samples: 3488/5760, Loss: 0.591267466545105\n",
      "Epoch: 37, Samples: 3520/5760, Loss: 0.9549691677093506\n",
      "Epoch: 37, Samples: 3552/5760, Loss: 0.6411370038986206\n",
      "Epoch: 37, Samples: 3584/5760, Loss: 0.5058497190475464\n",
      "Epoch: 37, Samples: 3616/5760, Loss: 0.5857306718826294\n",
      "Epoch: 37, Samples: 3648/5760, Loss: 0.46378520131111145\n",
      "Epoch: 37, Samples: 3680/5760, Loss: 0.5695034265518188\n",
      "Epoch: 37, Samples: 3712/5760, Loss: 0.3594946563243866\n",
      "Epoch: 37, Samples: 3744/5760, Loss: 0.5224447846412659\n",
      "Epoch: 37, Samples: 3776/5760, Loss: 0.5299178957939148\n",
      "Epoch: 37, Samples: 3808/5760, Loss: 0.5854717493057251\n",
      "Epoch: 37, Samples: 3840/5760, Loss: 0.7349237203598022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37, Samples: 3872/5760, Loss: 0.7225445508956909\n",
      "Epoch: 37, Samples: 3904/5760, Loss: 0.43797168135643005\n",
      "Epoch: 37, Samples: 3936/5760, Loss: 0.606999397277832\n",
      "Epoch: 37, Samples: 3968/5760, Loss: 0.3265075087547302\n",
      "Epoch: 37, Samples: 4000/5760, Loss: 0.32877522706985474\n",
      "Epoch: 37, Samples: 4032/5760, Loss: 0.7466599345207214\n",
      "Epoch: 37, Samples: 4064/5760, Loss: 0.5902058482170105\n",
      "Epoch: 37, Samples: 4096/5760, Loss: 0.4923138916492462\n",
      "Epoch: 37, Samples: 4128/5760, Loss: 0.5446294546127319\n",
      "Epoch: 37, Samples: 4160/5760, Loss: 0.5377960205078125\n",
      "Epoch: 37, Samples: 4192/5760, Loss: 0.6919403076171875\n",
      "Epoch: 37, Samples: 4224/5760, Loss: 0.5333149433135986\n",
      "Epoch: 37, Samples: 4256/5760, Loss: 0.44442319869995117\n",
      "Epoch: 37, Samples: 4288/5760, Loss: 0.6903764009475708\n",
      "Epoch: 37, Samples: 4320/5760, Loss: 0.47718268632888794\n",
      "Epoch: 37, Samples: 4352/5760, Loss: 0.8686964511871338\n",
      "Epoch: 37, Samples: 4384/5760, Loss: 0.6733021140098572\n",
      "Epoch: 37, Samples: 4416/5760, Loss: 0.476281076669693\n",
      "Epoch: 37, Samples: 4448/5760, Loss: 0.6180276274681091\n",
      "Epoch: 37, Samples: 4480/5760, Loss: 0.5402429103851318\n",
      "Epoch: 37, Samples: 4512/5760, Loss: 0.8013070821762085\n",
      "Epoch: 37, Samples: 4544/5760, Loss: 0.6590962409973145\n",
      "Epoch: 37, Samples: 4576/5760, Loss: 0.5734471082687378\n",
      "Epoch: 37, Samples: 4608/5760, Loss: 0.5312394499778748\n",
      "Epoch: 37, Samples: 4640/5760, Loss: 0.40545332431793213\n",
      "Epoch: 37, Samples: 4672/5760, Loss: 0.5637228488922119\n",
      "Epoch: 37, Samples: 4704/5760, Loss: 0.765030026435852\n",
      "Epoch: 37, Samples: 4736/5760, Loss: 0.9296637773513794\n",
      "Epoch: 37, Samples: 4768/5760, Loss: 0.4268423318862915\n",
      "Epoch: 37, Samples: 4800/5760, Loss: 0.38433676958084106\n",
      "Epoch: 37, Samples: 4832/5760, Loss: 0.5597957968711853\n",
      "Epoch: 37, Samples: 4864/5760, Loss: 0.830004096031189\n",
      "Epoch: 37, Samples: 4896/5760, Loss: 0.6872354745864868\n",
      "Epoch: 37, Samples: 4928/5760, Loss: 0.4369645416736603\n",
      "Epoch: 37, Samples: 4960/5760, Loss: 0.41623201966285706\n",
      "Epoch: 37, Samples: 4992/5760, Loss: 0.6457980871200562\n",
      "Epoch: 37, Samples: 5024/5760, Loss: 0.530119001865387\n",
      "Epoch: 37, Samples: 5056/5760, Loss: 0.4391131103038788\n",
      "Epoch: 37, Samples: 5088/5760, Loss: 0.4340897798538208\n",
      "Epoch: 37, Samples: 5120/5760, Loss: 0.8127624988555908\n",
      "Epoch: 37, Samples: 5152/5760, Loss: 0.4169973134994507\n",
      "Epoch: 37, Samples: 5184/5760, Loss: 0.49936002492904663\n",
      "Epoch: 37, Samples: 5216/5760, Loss: 0.6586170196533203\n",
      "Epoch: 37, Samples: 5248/5760, Loss: 0.49604055285453796\n",
      "Epoch: 37, Samples: 5280/5760, Loss: 0.5072596073150635\n",
      "Epoch: 37, Samples: 5312/5760, Loss: 0.5679140090942383\n",
      "Epoch: 37, Samples: 5344/5760, Loss: 0.3845658600330353\n",
      "Epoch: 37, Samples: 5376/5760, Loss: 0.6792099475860596\n",
      "Epoch: 37, Samples: 5408/5760, Loss: 0.4773932695388794\n",
      "Epoch: 37, Samples: 5440/5760, Loss: 0.6256467700004578\n",
      "Epoch: 37, Samples: 5472/5760, Loss: 0.6764715909957886\n",
      "Epoch: 37, Samples: 5504/5760, Loss: 0.6176819205284119\n",
      "Epoch: 37, Samples: 5536/5760, Loss: 0.6419996619224548\n",
      "Epoch: 37, Samples: 5568/5760, Loss: 0.42289817333221436\n",
      "Epoch: 37, Samples: 5600/5760, Loss: 0.8249008655548096\n",
      "Epoch: 37, Samples: 5632/5760, Loss: 0.4798298180103302\n",
      "Epoch: 37, Samples: 5664/5760, Loss: 0.38705870509147644\n",
      "Epoch: 37, Samples: 5696/5760, Loss: 0.6227678060531616\n",
      "Epoch: 37, Samples: 5728/5760, Loss: 1.1738145351409912\n",
      "\n",
      "Epoch: 37\n",
      "Training set: Average loss: 0.5857\n",
      "Validation set: Average loss: 1.5525, Accuracy: 499/818 (61%)\n",
      "Epoch: 38, Samples: 0/5760, Loss: 0.5834238529205322\n",
      "Epoch: 38, Samples: 32/5760, Loss: 0.41067296266555786\n",
      "Epoch: 38, Samples: 64/5760, Loss: 0.37146759033203125\n",
      "Epoch: 38, Samples: 96/5760, Loss: 0.3726916015148163\n",
      "Epoch: 38, Samples: 128/5760, Loss: 0.1923755705356598\n",
      "Epoch: 38, Samples: 160/5760, Loss: 0.43211424350738525\n",
      "Epoch: 38, Samples: 192/5760, Loss: 0.7753472924232483\n",
      "Epoch: 38, Samples: 224/5760, Loss: 0.7807192802429199\n",
      "Epoch: 38, Samples: 256/5760, Loss: 0.5260941982269287\n",
      "Epoch: 38, Samples: 288/5760, Loss: 0.6277979016304016\n",
      "Epoch: 38, Samples: 320/5760, Loss: 0.6122555136680603\n",
      "Epoch: 38, Samples: 352/5760, Loss: 0.6981419920921326\n",
      "Epoch: 38, Samples: 384/5760, Loss: 0.6271007061004639\n",
      "Epoch: 38, Samples: 416/5760, Loss: 0.5296913385391235\n",
      "Epoch: 38, Samples: 448/5760, Loss: 0.3982361853122711\n",
      "Epoch: 38, Samples: 480/5760, Loss: 0.41851288080215454\n",
      "Epoch: 38, Samples: 512/5760, Loss: 0.7884628176689148\n",
      "Epoch: 38, Samples: 544/5760, Loss: 0.9299034476280212\n",
      "Epoch: 38, Samples: 576/5760, Loss: 0.811664342880249\n",
      "Epoch: 38, Samples: 608/5760, Loss: 0.5249202251434326\n",
      "Epoch: 38, Samples: 640/5760, Loss: 0.4174182116985321\n",
      "Epoch: 38, Samples: 672/5760, Loss: 0.7160769701004028\n",
      "Epoch: 38, Samples: 704/5760, Loss: 0.5698219537734985\n",
      "Epoch: 38, Samples: 736/5760, Loss: 0.5283529758453369\n",
      "Epoch: 38, Samples: 768/5760, Loss: 0.4547189176082611\n",
      "Epoch: 38, Samples: 800/5760, Loss: 0.48692724108695984\n",
      "Epoch: 38, Samples: 832/5760, Loss: 0.4748026132583618\n",
      "Epoch: 38, Samples: 864/5760, Loss: 0.5377998948097229\n",
      "Epoch: 38, Samples: 896/5760, Loss: 0.7920309901237488\n",
      "Epoch: 38, Samples: 928/5760, Loss: 0.709961473941803\n",
      "Epoch: 38, Samples: 960/5760, Loss: 1.028623104095459\n",
      "Epoch: 38, Samples: 992/5760, Loss: 0.4828107953071594\n",
      "Epoch: 38, Samples: 1024/5760, Loss: 0.4216088354587555\n",
      "Epoch: 38, Samples: 1056/5760, Loss: 0.4333351254463196\n",
      "Epoch: 38, Samples: 1088/5760, Loss: 0.8348109722137451\n",
      "Epoch: 38, Samples: 1120/5760, Loss: 0.47585973143577576\n",
      "Epoch: 38, Samples: 1152/5760, Loss: 0.5932278633117676\n",
      "Epoch: 38, Samples: 1184/5760, Loss: 0.4362187385559082\n",
      "Epoch: 38, Samples: 1216/5760, Loss: 0.4448261559009552\n",
      "Epoch: 38, Samples: 1248/5760, Loss: 0.40949389338493347\n",
      "Epoch: 38, Samples: 1280/5760, Loss: 0.6792519688606262\n",
      "Epoch: 38, Samples: 1312/5760, Loss: 0.7239292860031128\n",
      "Epoch: 38, Samples: 1344/5760, Loss: 0.29402267932891846\n",
      "Epoch: 38, Samples: 1376/5760, Loss: 0.8103547096252441\n",
      "Epoch: 38, Samples: 1408/5760, Loss: 0.4664013087749481\n",
      "Epoch: 38, Samples: 1440/5760, Loss: 0.6558990478515625\n",
      "Epoch: 38, Samples: 1472/5760, Loss: 0.6776562929153442\n",
      "Epoch: 38, Samples: 1504/5760, Loss: 0.39405113458633423\n",
      "Epoch: 38, Samples: 1536/5760, Loss: 0.46726781129837036\n",
      "Epoch: 38, Samples: 1568/5760, Loss: 0.6393871307373047\n",
      "Epoch: 38, Samples: 1600/5760, Loss: 0.42136621475219727\n",
      "Epoch: 38, Samples: 1632/5760, Loss: 0.5399352312088013\n",
      "Epoch: 38, Samples: 1664/5760, Loss: 0.39369070529937744\n",
      "Epoch: 38, Samples: 1696/5760, Loss: 0.4960116744041443\n",
      "Epoch: 38, Samples: 1728/5760, Loss: 0.8740575313568115\n",
      "Epoch: 38, Samples: 1760/5760, Loss: 0.6005204916000366\n",
      "Epoch: 38, Samples: 1792/5760, Loss: 0.43362003564834595\n",
      "Epoch: 38, Samples: 1824/5760, Loss: 0.34178146719932556\n",
      "Epoch: 38, Samples: 1856/5760, Loss: 0.5235987305641174\n",
      "Epoch: 38, Samples: 1888/5760, Loss: 0.4763753414154053\n",
      "Epoch: 38, Samples: 1920/5760, Loss: 0.4905800223350525\n",
      "Epoch: 38, Samples: 1952/5760, Loss: 0.41618114709854126\n",
      "Epoch: 38, Samples: 1984/5760, Loss: 0.5924957990646362\n",
      "Epoch: 38, Samples: 2016/5760, Loss: 0.49318623542785645\n",
      "Epoch: 38, Samples: 2048/5760, Loss: 0.4287310838699341\n",
      "Epoch: 38, Samples: 2080/5760, Loss: 0.6596516966819763\n",
      "Epoch: 38, Samples: 2112/5760, Loss: 0.5072239637374878\n",
      "Epoch: 38, Samples: 2144/5760, Loss: 0.6128408908843994\n",
      "Epoch: 38, Samples: 2176/5760, Loss: 0.44648322463035583\n",
      "Epoch: 38, Samples: 2208/5760, Loss: 0.4024673104286194\n",
      "Epoch: 38, Samples: 2240/5760, Loss: 0.4246167540550232\n",
      "Epoch: 38, Samples: 2272/5760, Loss: 0.3371639847755432\n",
      "Epoch: 38, Samples: 2304/5760, Loss: 0.46760690212249756\n",
      "Epoch: 38, Samples: 2336/5760, Loss: 0.3661041259765625\n",
      "Epoch: 38, Samples: 2368/5760, Loss: 0.37496083974838257\n",
      "Epoch: 38, Samples: 2400/5760, Loss: 0.6028964519500732\n",
      "Epoch: 38, Samples: 2432/5760, Loss: 0.5471030473709106\n",
      "Epoch: 38, Samples: 2464/5760, Loss: 0.6957057118415833\n",
      "Epoch: 38, Samples: 2496/5760, Loss: 0.5658551454544067\n",
      "Epoch: 38, Samples: 2528/5760, Loss: 0.4590385854244232\n",
      "Epoch: 38, Samples: 2560/5760, Loss: 0.45103347301483154\n",
      "Epoch: 38, Samples: 2592/5760, Loss: 1.0605794191360474\n",
      "Epoch: 38, Samples: 2624/5760, Loss: 0.35246798396110535\n",
      "Epoch: 38, Samples: 2656/5760, Loss: 0.4630659520626068\n",
      "Epoch: 38, Samples: 2688/5760, Loss: 0.48423680663108826\n",
      "Epoch: 38, Samples: 2720/5760, Loss: 0.4844994843006134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, Samples: 2752/5760, Loss: 0.7965719699859619\n",
      "Epoch: 38, Samples: 2784/5760, Loss: 0.48425453901290894\n",
      "Epoch: 38, Samples: 2816/5760, Loss: 0.6015921831130981\n",
      "Epoch: 38, Samples: 2848/5760, Loss: 0.5628151893615723\n",
      "Epoch: 38, Samples: 2880/5760, Loss: 0.4107801020145416\n",
      "Epoch: 38, Samples: 2912/5760, Loss: 0.5130759477615356\n",
      "Epoch: 38, Samples: 2944/5760, Loss: 0.42888566851615906\n",
      "Epoch: 38, Samples: 2976/5760, Loss: 0.6613551378250122\n",
      "Epoch: 38, Samples: 3008/5760, Loss: 0.5931469202041626\n",
      "Epoch: 38, Samples: 3040/5760, Loss: 0.3745008707046509\n",
      "Epoch: 38, Samples: 3072/5760, Loss: 0.714544415473938\n",
      "Epoch: 38, Samples: 3104/5760, Loss: 1.024397611618042\n",
      "Epoch: 38, Samples: 3136/5760, Loss: 0.46524253487586975\n",
      "Epoch: 38, Samples: 3168/5760, Loss: 0.4834210276603699\n",
      "Epoch: 38, Samples: 3200/5760, Loss: 0.41801828145980835\n",
      "Epoch: 38, Samples: 3232/5760, Loss: 0.5079813003540039\n",
      "Epoch: 38, Samples: 3264/5760, Loss: 0.6308537721633911\n",
      "Epoch: 38, Samples: 3296/5760, Loss: 0.6233885288238525\n",
      "Epoch: 38, Samples: 3328/5760, Loss: 0.5867875814437866\n",
      "Epoch: 38, Samples: 3360/5760, Loss: 0.8178901672363281\n",
      "Epoch: 38, Samples: 3392/5760, Loss: 0.6348638534545898\n",
      "Epoch: 38, Samples: 3424/5760, Loss: 0.39000025391578674\n",
      "Epoch: 38, Samples: 3456/5760, Loss: 0.6977134943008423\n",
      "Epoch: 38, Samples: 3488/5760, Loss: 0.44089454412460327\n",
      "Epoch: 38, Samples: 3520/5760, Loss: 0.28875094652175903\n",
      "Epoch: 38, Samples: 3552/5760, Loss: 0.48054036498069763\n",
      "Epoch: 38, Samples: 3584/5760, Loss: 0.5778858661651611\n",
      "Epoch: 38, Samples: 3616/5760, Loss: 0.7776354551315308\n",
      "Epoch: 38, Samples: 3648/5760, Loss: 0.3273933529853821\n",
      "Epoch: 38, Samples: 3680/5760, Loss: 0.2699129283428192\n",
      "Epoch: 38, Samples: 3712/5760, Loss: 0.5726314783096313\n",
      "Epoch: 38, Samples: 3744/5760, Loss: 0.8162678480148315\n",
      "Epoch: 38, Samples: 3776/5760, Loss: 0.2802884876728058\n",
      "Epoch: 38, Samples: 3808/5760, Loss: 0.4689212739467621\n",
      "Epoch: 38, Samples: 3840/5760, Loss: 0.7122799754142761\n",
      "Epoch: 38, Samples: 3872/5760, Loss: 0.748206615447998\n",
      "Epoch: 38, Samples: 3904/5760, Loss: 0.6086302399635315\n",
      "Epoch: 38, Samples: 3936/5760, Loss: 0.6155451536178589\n",
      "Epoch: 38, Samples: 3968/5760, Loss: 0.30927327275276184\n",
      "Epoch: 38, Samples: 4000/5760, Loss: 0.6166473031044006\n",
      "Epoch: 38, Samples: 4032/5760, Loss: 0.8589118719100952\n",
      "Epoch: 38, Samples: 4064/5760, Loss: 0.6053323745727539\n",
      "Epoch: 38, Samples: 4096/5760, Loss: 0.5892454385757446\n",
      "Epoch: 38, Samples: 4128/5760, Loss: 0.6873055100440979\n",
      "Epoch: 38, Samples: 4160/5760, Loss: 0.6668013334274292\n",
      "Epoch: 38, Samples: 4192/5760, Loss: 0.6938440799713135\n",
      "Epoch: 38, Samples: 4224/5760, Loss: 0.8646318912506104\n",
      "Epoch: 38, Samples: 4256/5760, Loss: 0.4673076570034027\n",
      "Epoch: 38, Samples: 4288/5760, Loss: 0.8292829990386963\n",
      "Epoch: 38, Samples: 4320/5760, Loss: 0.429365873336792\n",
      "Epoch: 38, Samples: 4352/5760, Loss: 0.6020888090133667\n",
      "Epoch: 38, Samples: 4384/5760, Loss: 0.5478314161300659\n",
      "Epoch: 38, Samples: 4416/5760, Loss: 0.6558561325073242\n",
      "Epoch: 38, Samples: 4448/5760, Loss: 0.49948567152023315\n",
      "Epoch: 38, Samples: 4480/5760, Loss: 0.6331126689910889\n",
      "Epoch: 38, Samples: 4512/5760, Loss: 0.6169507503509521\n",
      "Epoch: 38, Samples: 4544/5760, Loss: 0.41691452264785767\n",
      "Epoch: 38, Samples: 4576/5760, Loss: 0.3283742666244507\n",
      "Epoch: 38, Samples: 4608/5760, Loss: 0.6148852109909058\n",
      "Epoch: 38, Samples: 4640/5760, Loss: 0.5630340576171875\n",
      "Epoch: 38, Samples: 4672/5760, Loss: 0.5758745074272156\n",
      "Epoch: 38, Samples: 4704/5760, Loss: 0.4682961702346802\n",
      "Epoch: 38, Samples: 4736/5760, Loss: 0.5449353456497192\n",
      "Epoch: 38, Samples: 4768/5760, Loss: 0.41313818097114563\n",
      "Epoch: 38, Samples: 4800/5760, Loss: 0.5273419618606567\n",
      "Epoch: 38, Samples: 4832/5760, Loss: 0.4253982901573181\n",
      "Epoch: 38, Samples: 4864/5760, Loss: 0.3794296681880951\n",
      "Epoch: 38, Samples: 4896/5760, Loss: 0.4596279263496399\n",
      "Epoch: 38, Samples: 4928/5760, Loss: 0.3928862512111664\n",
      "Epoch: 38, Samples: 4960/5760, Loss: 0.6658474206924438\n",
      "Epoch: 38, Samples: 4992/5760, Loss: 0.918986439704895\n",
      "Epoch: 38, Samples: 5024/5760, Loss: 0.5422340631484985\n",
      "Epoch: 38, Samples: 5056/5760, Loss: 0.4173726737499237\n",
      "Epoch: 38, Samples: 5088/5760, Loss: 0.6910154223442078\n",
      "Epoch: 38, Samples: 5120/5760, Loss: 0.6443590521812439\n",
      "Epoch: 38, Samples: 5152/5760, Loss: 0.6469063758850098\n",
      "Epoch: 38, Samples: 5184/5760, Loss: 0.38322144746780396\n",
      "Epoch: 38, Samples: 5216/5760, Loss: 0.4445648491382599\n",
      "Epoch: 38, Samples: 5248/5760, Loss: 0.4479867219924927\n",
      "Epoch: 38, Samples: 5280/5760, Loss: 0.2756299078464508\n",
      "Epoch: 38, Samples: 5312/5760, Loss: 0.6377652883529663\n",
      "Epoch: 38, Samples: 5344/5760, Loss: 0.7670332193374634\n",
      "Epoch: 38, Samples: 5376/5760, Loss: 0.5162096619606018\n",
      "Epoch: 38, Samples: 5408/5760, Loss: 0.42628398537635803\n",
      "Epoch: 38, Samples: 5440/5760, Loss: 0.4574751555919647\n",
      "Epoch: 38, Samples: 5472/5760, Loss: 0.5453505516052246\n",
      "Epoch: 38, Samples: 5504/5760, Loss: 0.8347026109695435\n",
      "Epoch: 38, Samples: 5536/5760, Loss: 0.514022707939148\n",
      "Epoch: 38, Samples: 5568/5760, Loss: 0.35846495628356934\n",
      "Epoch: 38, Samples: 5600/5760, Loss: 0.31469282507896423\n",
      "Epoch: 38, Samples: 5632/5760, Loss: 0.4308469593524933\n",
      "Epoch: 38, Samples: 5664/5760, Loss: 0.7281279563903809\n",
      "Epoch: 38, Samples: 5696/5760, Loss: 0.8410484790802002\n",
      "Epoch: 38, Samples: 5728/5760, Loss: 4.4046220779418945\n",
      "\n",
      "Epoch: 38\n",
      "Training set: Average loss: 0.5739\n",
      "Validation set: Average loss: 1.8443, Accuracy: 456/818 (56%)\n",
      "Epoch: 39, Samples: 0/5760, Loss: 0.46899449825286865\n",
      "Epoch: 39, Samples: 32/5760, Loss: 0.6646226048469543\n",
      "Epoch: 39, Samples: 64/5760, Loss: 0.6911593675613403\n",
      "Epoch: 39, Samples: 96/5760, Loss: 0.4552777409553528\n",
      "Epoch: 39, Samples: 128/5760, Loss: 0.5729496479034424\n",
      "Epoch: 39, Samples: 160/5760, Loss: 0.6480212211608887\n",
      "Epoch: 39, Samples: 192/5760, Loss: 0.5941710472106934\n",
      "Epoch: 39, Samples: 224/5760, Loss: 0.5714314579963684\n",
      "Epoch: 39, Samples: 256/5760, Loss: 0.6086111664772034\n",
      "Epoch: 39, Samples: 288/5760, Loss: 0.5505334138870239\n",
      "Epoch: 39, Samples: 320/5760, Loss: 0.5715372562408447\n",
      "Epoch: 39, Samples: 352/5760, Loss: 0.422214537858963\n",
      "Epoch: 39, Samples: 384/5760, Loss: 0.6863048076629639\n",
      "Epoch: 39, Samples: 416/5760, Loss: 0.6639914512634277\n",
      "Epoch: 39, Samples: 448/5760, Loss: 0.8014631271362305\n",
      "Epoch: 39, Samples: 480/5760, Loss: 0.6369212865829468\n",
      "Epoch: 39, Samples: 512/5760, Loss: 0.5166785717010498\n",
      "Epoch: 39, Samples: 544/5760, Loss: 0.5336564779281616\n",
      "Epoch: 39, Samples: 576/5760, Loss: 0.477853387594223\n",
      "Epoch: 39, Samples: 608/5760, Loss: 0.5089555978775024\n",
      "Epoch: 39, Samples: 640/5760, Loss: 0.4859170913696289\n",
      "Epoch: 39, Samples: 672/5760, Loss: 0.4613901972770691\n",
      "Epoch: 39, Samples: 704/5760, Loss: 0.6131448745727539\n",
      "Epoch: 39, Samples: 736/5760, Loss: 0.48353496193885803\n",
      "Epoch: 39, Samples: 768/5760, Loss: 0.7289720773696899\n",
      "Epoch: 39, Samples: 800/5760, Loss: 0.6133798360824585\n",
      "Epoch: 39, Samples: 832/5760, Loss: 0.2888665795326233\n",
      "Epoch: 39, Samples: 864/5760, Loss: 0.8249908685684204\n",
      "Epoch: 39, Samples: 896/5760, Loss: 0.4969392716884613\n",
      "Epoch: 39, Samples: 928/5760, Loss: 0.5126963257789612\n",
      "Epoch: 39, Samples: 960/5760, Loss: 0.32992422580718994\n",
      "Epoch: 39, Samples: 992/5760, Loss: 0.3652956783771515\n",
      "Epoch: 39, Samples: 1024/5760, Loss: 0.4254264831542969\n",
      "Epoch: 39, Samples: 1056/5760, Loss: 0.43407630920410156\n",
      "Epoch: 39, Samples: 1088/5760, Loss: 0.28803950548171997\n",
      "Epoch: 39, Samples: 1120/5760, Loss: 0.3245221972465515\n",
      "Epoch: 39, Samples: 1152/5760, Loss: 0.5345448851585388\n",
      "Epoch: 39, Samples: 1184/5760, Loss: 0.5068978071212769\n",
      "Epoch: 39, Samples: 1216/5760, Loss: 0.34233182668685913\n",
      "Epoch: 39, Samples: 1248/5760, Loss: 0.4434967041015625\n",
      "Epoch: 39, Samples: 1280/5760, Loss: 0.5680583119392395\n",
      "Epoch: 39, Samples: 1312/5760, Loss: 0.3090484142303467\n",
      "Epoch: 39, Samples: 1344/5760, Loss: 0.7784068584442139\n",
      "Epoch: 39, Samples: 1376/5760, Loss: 0.6525939702987671\n",
      "Epoch: 39, Samples: 1408/5760, Loss: 0.43459808826446533\n",
      "Epoch: 39, Samples: 1440/5760, Loss: 0.5023882389068604\n",
      "Epoch: 39, Samples: 1472/5760, Loss: 0.8409658074378967\n",
      "Epoch: 39, Samples: 1504/5760, Loss: 0.3067370355129242\n",
      "Epoch: 39, Samples: 1536/5760, Loss: 0.5676270723342896\n",
      "Epoch: 39, Samples: 1568/5760, Loss: 0.443311870098114\n",
      "Epoch: 39, Samples: 1600/5760, Loss: 0.6026428937911987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, Samples: 1632/5760, Loss: 0.5743141174316406\n",
      "Epoch: 39, Samples: 1664/5760, Loss: 0.3676784336566925\n",
      "Epoch: 39, Samples: 1696/5760, Loss: 0.4360072612762451\n",
      "Epoch: 39, Samples: 1728/5760, Loss: 0.49291911721229553\n",
      "Epoch: 39, Samples: 1760/5760, Loss: 0.3219906687736511\n",
      "Epoch: 39, Samples: 1792/5760, Loss: 0.7439600229263306\n",
      "Epoch: 39, Samples: 1824/5760, Loss: 0.608122706413269\n",
      "Epoch: 39, Samples: 1856/5760, Loss: 0.34704455733299255\n",
      "Epoch: 39, Samples: 1888/5760, Loss: 0.2842521667480469\n",
      "Epoch: 39, Samples: 1920/5760, Loss: 0.4773530066013336\n",
      "Epoch: 39, Samples: 1952/5760, Loss: 0.4806850552558899\n",
      "Epoch: 39, Samples: 1984/5760, Loss: 0.7013057470321655\n",
      "Epoch: 39, Samples: 2016/5760, Loss: 0.478826105594635\n",
      "Epoch: 39, Samples: 2048/5760, Loss: 0.5728356838226318\n",
      "Epoch: 39, Samples: 2080/5760, Loss: 0.3560104966163635\n",
      "Epoch: 39, Samples: 2112/5760, Loss: 0.4944174587726593\n",
      "Epoch: 39, Samples: 2144/5760, Loss: 0.5276246666908264\n",
      "Epoch: 39, Samples: 2176/5760, Loss: 0.4780900478363037\n",
      "Epoch: 39, Samples: 2208/5760, Loss: 0.340851753950119\n",
      "Epoch: 39, Samples: 2240/5760, Loss: 0.446432501077652\n",
      "Epoch: 39, Samples: 2272/5760, Loss: 0.6556198596954346\n",
      "Epoch: 39, Samples: 2304/5760, Loss: 0.28300923109054565\n",
      "Epoch: 39, Samples: 2336/5760, Loss: 0.4311082065105438\n",
      "Epoch: 39, Samples: 2368/5760, Loss: 0.330705463886261\n",
      "Epoch: 39, Samples: 2400/5760, Loss: 0.5535491704940796\n",
      "Epoch: 39, Samples: 2432/5760, Loss: 0.5002984404563904\n",
      "Epoch: 39, Samples: 2464/5760, Loss: 0.2935791611671448\n",
      "Epoch: 39, Samples: 2496/5760, Loss: 0.42320355772972107\n",
      "Epoch: 39, Samples: 2528/5760, Loss: 0.47347524762153625\n",
      "Epoch: 39, Samples: 2560/5760, Loss: 0.6087549328804016\n",
      "Epoch: 39, Samples: 2592/5760, Loss: 0.4448099732398987\n",
      "Epoch: 39, Samples: 2624/5760, Loss: 0.5026941895484924\n",
      "Epoch: 39, Samples: 2656/5760, Loss: 0.40754082798957825\n",
      "Epoch: 39, Samples: 2688/5760, Loss: 0.3762950301170349\n",
      "Epoch: 39, Samples: 2720/5760, Loss: 0.2778490483760834\n",
      "Epoch: 39, Samples: 2752/5760, Loss: 0.5851680636405945\n",
      "Epoch: 39, Samples: 2784/5760, Loss: 0.3128763735294342\n",
      "Epoch: 39, Samples: 2816/5760, Loss: 0.42322203516960144\n",
      "Epoch: 39, Samples: 2848/5760, Loss: 0.429893434047699\n",
      "Epoch: 39, Samples: 2880/5760, Loss: 0.32592713832855225\n",
      "Epoch: 39, Samples: 2912/5760, Loss: 0.6769174933433533\n",
      "Epoch: 39, Samples: 2944/5760, Loss: 0.33294761180877686\n",
      "Epoch: 39, Samples: 2976/5760, Loss: 0.6906002759933472\n",
      "Epoch: 39, Samples: 3008/5760, Loss: 0.4237622618675232\n",
      "Epoch: 39, Samples: 3040/5760, Loss: 0.6739263534545898\n",
      "Epoch: 39, Samples: 3072/5760, Loss: 0.5458519458770752\n",
      "Epoch: 39, Samples: 3104/5760, Loss: 0.3785228431224823\n",
      "Epoch: 39, Samples: 3136/5760, Loss: 0.4356071949005127\n",
      "Epoch: 39, Samples: 3168/5760, Loss: 0.4384697675704956\n",
      "Epoch: 39, Samples: 3200/5760, Loss: 0.3695421814918518\n",
      "Epoch: 39, Samples: 3232/5760, Loss: 0.549882173538208\n",
      "Epoch: 39, Samples: 3264/5760, Loss: 0.5774258375167847\n",
      "Epoch: 39, Samples: 3296/5760, Loss: 0.40343743562698364\n",
      "Epoch: 39, Samples: 3328/5760, Loss: 0.6124844551086426\n",
      "Epoch: 39, Samples: 3360/5760, Loss: 0.27430447936058044\n",
      "Epoch: 39, Samples: 3392/5760, Loss: 0.6672121286392212\n",
      "Epoch: 39, Samples: 3424/5760, Loss: 0.507891058921814\n",
      "Epoch: 39, Samples: 3456/5760, Loss: 0.6562782526016235\n",
      "Epoch: 39, Samples: 3488/5760, Loss: 0.4823116660118103\n",
      "Epoch: 39, Samples: 3520/5760, Loss: 0.5467967987060547\n",
      "Epoch: 39, Samples: 3552/5760, Loss: 0.4847378134727478\n",
      "Epoch: 39, Samples: 3584/5760, Loss: 0.3335655629634857\n",
      "Epoch: 39, Samples: 3616/5760, Loss: 0.5772715210914612\n",
      "Epoch: 39, Samples: 3648/5760, Loss: 0.6940805912017822\n",
      "Epoch: 39, Samples: 3680/5760, Loss: 0.34947827458381653\n",
      "Epoch: 39, Samples: 3712/5760, Loss: 0.6362351179122925\n",
      "Epoch: 39, Samples: 3744/5760, Loss: 0.4922858476638794\n",
      "Epoch: 39, Samples: 3776/5760, Loss: 0.4251049757003784\n",
      "Epoch: 39, Samples: 3808/5760, Loss: 0.38617563247680664\n",
      "Epoch: 39, Samples: 3840/5760, Loss: 0.5595066547393799\n",
      "Epoch: 39, Samples: 3872/5760, Loss: 0.5158954858779907\n",
      "Epoch: 39, Samples: 3904/5760, Loss: 0.4280168116092682\n",
      "Epoch: 39, Samples: 3936/5760, Loss: 0.3810359835624695\n",
      "Epoch: 39, Samples: 3968/5760, Loss: 0.4877176880836487\n",
      "Epoch: 39, Samples: 4000/5760, Loss: 0.848071813583374\n",
      "Epoch: 39, Samples: 4032/5760, Loss: 0.5721642971038818\n",
      "Epoch: 39, Samples: 4064/5760, Loss: 0.3253949284553528\n",
      "Epoch: 39, Samples: 4096/5760, Loss: 0.352031409740448\n",
      "Epoch: 39, Samples: 4128/5760, Loss: 0.29741185903549194\n",
      "Epoch: 39, Samples: 4160/5760, Loss: 0.5207010507583618\n",
      "Epoch: 39, Samples: 4192/5760, Loss: 0.4189845323562622\n",
      "Epoch: 39, Samples: 4224/5760, Loss: 0.4004855155944824\n",
      "Epoch: 39, Samples: 4256/5760, Loss: 0.3409953713417053\n",
      "Epoch: 39, Samples: 4288/5760, Loss: 0.3666853904724121\n",
      "Epoch: 39, Samples: 4320/5760, Loss: 0.5222165584564209\n",
      "Epoch: 39, Samples: 4352/5760, Loss: 0.36042872071266174\n",
      "Epoch: 39, Samples: 4384/5760, Loss: 0.7745911478996277\n",
      "Epoch: 39, Samples: 4416/5760, Loss: 0.5126100778579712\n",
      "Epoch: 39, Samples: 4448/5760, Loss: 0.4907238185405731\n",
      "Epoch: 39, Samples: 4480/5760, Loss: 0.44497495889663696\n",
      "Epoch: 39, Samples: 4512/5760, Loss: 0.5729990005493164\n",
      "Epoch: 39, Samples: 4544/5760, Loss: 0.4133448004722595\n",
      "Epoch: 39, Samples: 4576/5760, Loss: 0.5830996036529541\n",
      "Epoch: 39, Samples: 4608/5760, Loss: 0.8939824104309082\n",
      "Epoch: 39, Samples: 4640/5760, Loss: 0.7324225902557373\n",
      "Epoch: 39, Samples: 4672/5760, Loss: 0.3511408865451813\n",
      "Epoch: 39, Samples: 4704/5760, Loss: 0.4133574366569519\n",
      "Epoch: 39, Samples: 4736/5760, Loss: 0.8528104424476624\n",
      "Epoch: 39, Samples: 4768/5760, Loss: 0.5291299819946289\n",
      "Epoch: 39, Samples: 4800/5760, Loss: 0.4611646234989166\n",
      "Epoch: 39, Samples: 4832/5760, Loss: 0.3391115367412567\n",
      "Epoch: 39, Samples: 4864/5760, Loss: 0.4833736717700958\n",
      "Epoch: 39, Samples: 4896/5760, Loss: 0.8331015110015869\n",
      "Epoch: 39, Samples: 4928/5760, Loss: 0.3123186230659485\n",
      "Epoch: 39, Samples: 4960/5760, Loss: 0.6205441951751709\n",
      "Epoch: 39, Samples: 4992/5760, Loss: 0.4010835886001587\n",
      "Epoch: 39, Samples: 5024/5760, Loss: 0.4515645503997803\n",
      "Epoch: 39, Samples: 5056/5760, Loss: 0.7749322652816772\n",
      "Epoch: 39, Samples: 5088/5760, Loss: 0.4645002782344818\n",
      "Epoch: 39, Samples: 5120/5760, Loss: 0.5314468145370483\n",
      "Epoch: 39, Samples: 5152/5760, Loss: 0.5557554960250854\n",
      "Epoch: 39, Samples: 5184/5760, Loss: 0.6283760666847229\n",
      "Epoch: 39, Samples: 5216/5760, Loss: 0.8478648662567139\n",
      "Epoch: 39, Samples: 5248/5760, Loss: 0.2943818271160126\n",
      "Epoch: 39, Samples: 5280/5760, Loss: 0.3055949807167053\n",
      "Epoch: 39, Samples: 5312/5760, Loss: 0.6754211783409119\n",
      "Epoch: 39, Samples: 5344/5760, Loss: 0.7364746332168579\n",
      "Epoch: 39, Samples: 5376/5760, Loss: 0.24309229850769043\n",
      "Epoch: 39, Samples: 5408/5760, Loss: 0.698280930519104\n",
      "Epoch: 39, Samples: 5440/5760, Loss: 0.37851864099502563\n",
      "Epoch: 39, Samples: 5472/5760, Loss: 0.6440357565879822\n",
      "Epoch: 39, Samples: 5504/5760, Loss: 0.3278624415397644\n",
      "Epoch: 39, Samples: 5536/5760, Loss: 0.7392712831497192\n",
      "Epoch: 39, Samples: 5568/5760, Loss: 0.597420871257782\n",
      "Epoch: 39, Samples: 5600/5760, Loss: 0.36208468675613403\n",
      "Epoch: 39, Samples: 5632/5760, Loss: 0.7497250437736511\n",
      "Epoch: 39, Samples: 5664/5760, Loss: 0.5337153673171997\n",
      "Epoch: 39, Samples: 5696/5760, Loss: 0.6309768557548523\n",
      "Epoch: 39, Samples: 5728/5760, Loss: 2.5525753498077393\n",
      "\n",
      "Epoch: 39\n",
      "Training set: Average loss: 0.5188\n",
      "Validation set: Average loss: 1.4949, Accuracy: 522/818 (64%)\n",
      "Epoch: 40, Samples: 0/5760, Loss: 0.30677464604377747\n",
      "Epoch: 40, Samples: 32/5760, Loss: 0.5590677857398987\n",
      "Epoch: 40, Samples: 64/5760, Loss: 0.6189182996749878\n",
      "Epoch: 40, Samples: 96/5760, Loss: 0.2634066939353943\n",
      "Epoch: 40, Samples: 128/5760, Loss: 0.5983490943908691\n",
      "Epoch: 40, Samples: 160/5760, Loss: 0.7657175064086914\n",
      "Epoch: 40, Samples: 192/5760, Loss: 0.7378365993499756\n",
      "Epoch: 40, Samples: 224/5760, Loss: 0.34984102845191956\n",
      "Epoch: 40, Samples: 256/5760, Loss: 1.011947512626648\n",
      "Epoch: 40, Samples: 288/5760, Loss: 0.6123002767562866\n",
      "Epoch: 40, Samples: 320/5760, Loss: 0.25506532192230225\n",
      "Epoch: 40, Samples: 352/5760, Loss: 0.5720490217208862\n",
      "Epoch: 40, Samples: 384/5760, Loss: 0.9917953014373779\n",
      "Epoch: 40, Samples: 416/5760, Loss: 0.49149298667907715\n",
      "Epoch: 40, Samples: 448/5760, Loss: 0.6750718355178833\n",
      "Epoch: 40, Samples: 480/5760, Loss: 0.4802932143211365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Samples: 512/5760, Loss: 0.4843313992023468\n",
      "Epoch: 40, Samples: 544/5760, Loss: 0.4662695527076721\n",
      "Epoch: 40, Samples: 576/5760, Loss: 0.30963996052742004\n",
      "Epoch: 40, Samples: 608/5760, Loss: 0.5597615242004395\n",
      "Epoch: 40, Samples: 640/5760, Loss: 0.4742249548435211\n",
      "Epoch: 40, Samples: 672/5760, Loss: 0.5093808174133301\n",
      "Epoch: 40, Samples: 704/5760, Loss: 0.5766375064849854\n",
      "Epoch: 40, Samples: 736/5760, Loss: 0.4751858413219452\n",
      "Epoch: 40, Samples: 768/5760, Loss: 0.5586988925933838\n",
      "Epoch: 40, Samples: 800/5760, Loss: 0.2742208242416382\n",
      "Epoch: 40, Samples: 832/5760, Loss: 0.5744768381118774\n",
      "Epoch: 40, Samples: 864/5760, Loss: 0.3907940685749054\n",
      "Epoch: 40, Samples: 896/5760, Loss: 0.4213421046733856\n",
      "Epoch: 40, Samples: 928/5760, Loss: 0.8178956508636475\n",
      "Epoch: 40, Samples: 960/5760, Loss: 0.656961977481842\n",
      "Epoch: 40, Samples: 992/5760, Loss: 0.20731109380722046\n",
      "Epoch: 40, Samples: 1024/5760, Loss: 0.30365434288978577\n",
      "Epoch: 40, Samples: 1056/5760, Loss: 0.41163140535354614\n",
      "Epoch: 40, Samples: 1088/5760, Loss: 0.39898884296417236\n",
      "Epoch: 40, Samples: 1120/5760, Loss: 0.3960557281970978\n",
      "Epoch: 40, Samples: 1152/5760, Loss: 0.3734268546104431\n",
      "Epoch: 40, Samples: 1184/5760, Loss: 0.6634899973869324\n",
      "Epoch: 40, Samples: 1216/5760, Loss: 0.39562922716140747\n",
      "Epoch: 40, Samples: 1248/5760, Loss: 0.7075128555297852\n",
      "Epoch: 40, Samples: 1280/5760, Loss: 0.5760070085525513\n",
      "Epoch: 40, Samples: 1312/5760, Loss: 0.30915331840515137\n",
      "Epoch: 40, Samples: 1344/5760, Loss: 0.6188983917236328\n",
      "Epoch: 40, Samples: 1376/5760, Loss: 0.5861491560935974\n",
      "Epoch: 40, Samples: 1408/5760, Loss: 0.8414191007614136\n",
      "Epoch: 40, Samples: 1440/5760, Loss: 0.30677545070648193\n",
      "Epoch: 40, Samples: 1472/5760, Loss: 0.549473762512207\n",
      "Epoch: 40, Samples: 1504/5760, Loss: 0.5795135498046875\n",
      "Epoch: 40, Samples: 1536/5760, Loss: 0.5558980703353882\n",
      "Epoch: 40, Samples: 1568/5760, Loss: 0.5500761270523071\n",
      "Epoch: 40, Samples: 1600/5760, Loss: 0.24867382645606995\n",
      "Epoch: 40, Samples: 1632/5760, Loss: 0.5233603715896606\n",
      "Epoch: 40, Samples: 1664/5760, Loss: 0.4890892803668976\n",
      "Epoch: 40, Samples: 1696/5760, Loss: 0.460989385843277\n",
      "Epoch: 40, Samples: 1728/5760, Loss: 0.29255807399749756\n",
      "Epoch: 40, Samples: 1760/5760, Loss: 0.3732745051383972\n",
      "Epoch: 40, Samples: 1792/5760, Loss: 0.3021707236766815\n",
      "Epoch: 40, Samples: 1824/5760, Loss: 0.5518214106559753\n",
      "Epoch: 40, Samples: 1856/5760, Loss: 0.6731438636779785\n",
      "Epoch: 40, Samples: 1888/5760, Loss: 0.4595673978328705\n",
      "Epoch: 40, Samples: 1920/5760, Loss: 0.3733266592025757\n",
      "Epoch: 40, Samples: 1952/5760, Loss: 0.6453748345375061\n",
      "Epoch: 40, Samples: 1984/5760, Loss: 0.5483411550521851\n",
      "Epoch: 40, Samples: 2016/5760, Loss: 0.5083194375038147\n",
      "Epoch: 40, Samples: 2048/5760, Loss: 0.335414320230484\n",
      "Epoch: 40, Samples: 2080/5760, Loss: 0.47724610567092896\n",
      "Epoch: 40, Samples: 2112/5760, Loss: 0.656477689743042\n",
      "Epoch: 40, Samples: 2144/5760, Loss: 0.31258732080459595\n",
      "Epoch: 40, Samples: 2176/5760, Loss: 0.30631813406944275\n",
      "Epoch: 40, Samples: 2208/5760, Loss: 0.6499946117401123\n",
      "Epoch: 40, Samples: 2240/5760, Loss: 0.3984392285346985\n",
      "Epoch: 40, Samples: 2272/5760, Loss: 0.7998510599136353\n",
      "Epoch: 40, Samples: 2304/5760, Loss: 0.4290921688079834\n",
      "Epoch: 40, Samples: 2336/5760, Loss: 0.38644906878471375\n",
      "Epoch: 40, Samples: 2368/5760, Loss: 0.5775055289268494\n",
      "Epoch: 40, Samples: 2400/5760, Loss: 0.4447544813156128\n",
      "Epoch: 40, Samples: 2432/5760, Loss: 0.570899248123169\n",
      "Epoch: 40, Samples: 2464/5760, Loss: 0.40399429202079773\n",
      "Epoch: 40, Samples: 2496/5760, Loss: 0.2993241250514984\n",
      "Epoch: 40, Samples: 2528/5760, Loss: 0.4316365718841553\n",
      "Epoch: 40, Samples: 2560/5760, Loss: 0.5078837275505066\n",
      "Epoch: 40, Samples: 2592/5760, Loss: 0.6433122158050537\n",
      "Epoch: 40, Samples: 2624/5760, Loss: 0.5792604088783264\n",
      "Epoch: 40, Samples: 2656/5760, Loss: 0.5934433937072754\n",
      "Epoch: 40, Samples: 2688/5760, Loss: 0.33327677845954895\n",
      "Epoch: 40, Samples: 2720/5760, Loss: 0.602205753326416\n",
      "Epoch: 40, Samples: 2752/5760, Loss: 0.37012553215026855\n",
      "Epoch: 40, Samples: 2784/5760, Loss: 0.3468996584415436\n",
      "Epoch: 40, Samples: 2816/5760, Loss: 0.6134944558143616\n",
      "Epoch: 40, Samples: 2848/5760, Loss: 0.912704586982727\n",
      "Epoch: 40, Samples: 2880/5760, Loss: 0.18976286053657532\n",
      "Epoch: 40, Samples: 2912/5760, Loss: 0.6045085191726685\n",
      "Epoch: 40, Samples: 2944/5760, Loss: 0.3945966064929962\n",
      "Epoch: 40, Samples: 2976/5760, Loss: 0.42074650526046753\n",
      "Epoch: 40, Samples: 3008/5760, Loss: 0.6799558401107788\n",
      "Epoch: 40, Samples: 3040/5760, Loss: 0.7615253925323486\n",
      "Epoch: 40, Samples: 3072/5760, Loss: 0.42806896567344666\n",
      "Epoch: 40, Samples: 3104/5760, Loss: 0.464176744222641\n",
      "Epoch: 40, Samples: 3136/5760, Loss: 0.6058863401412964\n",
      "Epoch: 40, Samples: 3168/5760, Loss: 0.358181357383728\n",
      "Epoch: 40, Samples: 3200/5760, Loss: 0.5631692409515381\n",
      "Epoch: 40, Samples: 3232/5760, Loss: 0.39545369148254395\n",
      "Epoch: 40, Samples: 3264/5760, Loss: 0.38991525769233704\n",
      "Epoch: 40, Samples: 3296/5760, Loss: 0.5381324887275696\n",
      "Epoch: 40, Samples: 3328/5760, Loss: 0.4494389295578003\n",
      "Epoch: 40, Samples: 3360/5760, Loss: 0.4681602716445923\n",
      "Epoch: 40, Samples: 3392/5760, Loss: 0.30417218804359436\n",
      "Epoch: 40, Samples: 3424/5760, Loss: 0.2950781285762787\n",
      "Epoch: 40, Samples: 3456/5760, Loss: 0.6583659648895264\n",
      "Epoch: 40, Samples: 3488/5760, Loss: 0.5455416440963745\n",
      "Epoch: 40, Samples: 3520/5760, Loss: 0.3835376501083374\n",
      "Epoch: 40, Samples: 3552/5760, Loss: 0.426265150308609\n",
      "Epoch: 40, Samples: 3584/5760, Loss: 0.6634236574172974\n",
      "Epoch: 40, Samples: 3616/5760, Loss: 0.4757062792778015\n",
      "Epoch: 40, Samples: 3648/5760, Loss: 0.30313247442245483\n",
      "Epoch: 40, Samples: 3680/5760, Loss: 0.4555838406085968\n",
      "Epoch: 40, Samples: 3712/5760, Loss: 0.28021562099456787\n",
      "Epoch: 40, Samples: 3744/5760, Loss: 0.3743097484111786\n",
      "Epoch: 40, Samples: 3776/5760, Loss: 0.38614770770072937\n",
      "Epoch: 40, Samples: 3808/5760, Loss: 0.4023086130619049\n",
      "Epoch: 40, Samples: 3840/5760, Loss: 0.2692818343639374\n",
      "Epoch: 40, Samples: 3872/5760, Loss: 0.5594406127929688\n",
      "Epoch: 40, Samples: 3904/5760, Loss: 0.6717236042022705\n",
      "Epoch: 40, Samples: 3936/5760, Loss: 0.8384425640106201\n",
      "Epoch: 40, Samples: 3968/5760, Loss: 0.2937217354774475\n",
      "Epoch: 40, Samples: 4000/5760, Loss: 0.4039110243320465\n",
      "Epoch: 40, Samples: 4032/5760, Loss: 0.5079512596130371\n",
      "Epoch: 40, Samples: 4064/5760, Loss: 0.5956737399101257\n",
      "Epoch: 40, Samples: 4096/5760, Loss: 0.5552390813827515\n",
      "Epoch: 40, Samples: 4128/5760, Loss: 0.4256530702114105\n",
      "Epoch: 40, Samples: 4160/5760, Loss: 0.32605665922164917\n",
      "Epoch: 40, Samples: 4192/5760, Loss: 0.3958514332771301\n",
      "Epoch: 40, Samples: 4224/5760, Loss: 0.3734591007232666\n",
      "Epoch: 40, Samples: 4256/5760, Loss: 0.4740692377090454\n",
      "Epoch: 40, Samples: 4288/5760, Loss: 0.5871046781539917\n",
      "Epoch: 40, Samples: 4320/5760, Loss: 0.37543824315071106\n",
      "Epoch: 40, Samples: 4352/5760, Loss: 0.4234924912452698\n",
      "Epoch: 40, Samples: 4384/5760, Loss: 0.3934664726257324\n",
      "Epoch: 40, Samples: 4416/5760, Loss: 0.451271116733551\n",
      "Epoch: 40, Samples: 4448/5760, Loss: 0.3933417499065399\n",
      "Epoch: 40, Samples: 4480/5760, Loss: 0.741393506526947\n",
      "Epoch: 40, Samples: 4512/5760, Loss: 0.43644028902053833\n",
      "Epoch: 40, Samples: 4544/5760, Loss: 0.3279896080493927\n",
      "Epoch: 40, Samples: 4576/5760, Loss: 0.2526623606681824\n",
      "Epoch: 40, Samples: 4608/5760, Loss: 0.5152498483657837\n",
      "Epoch: 40, Samples: 4640/5760, Loss: 0.3858828842639923\n",
      "Epoch: 40, Samples: 4672/5760, Loss: 0.3934652507305145\n",
      "Epoch: 40, Samples: 4704/5760, Loss: 0.3837567865848541\n",
      "Epoch: 40, Samples: 4736/5760, Loss: 0.2611687183380127\n",
      "Epoch: 40, Samples: 4768/5760, Loss: 0.3324011564254761\n",
      "Epoch: 40, Samples: 4800/5760, Loss: 0.3706169128417969\n",
      "Epoch: 40, Samples: 4832/5760, Loss: 0.5886008143424988\n",
      "Epoch: 40, Samples: 4864/5760, Loss: 0.5669223666191101\n",
      "Epoch: 40, Samples: 4896/5760, Loss: 0.2913011312484741\n",
      "Epoch: 40, Samples: 4928/5760, Loss: 0.5015220046043396\n",
      "Epoch: 40, Samples: 4960/5760, Loss: 0.28516799211502075\n",
      "Epoch: 40, Samples: 4992/5760, Loss: 0.5245522856712341\n",
      "Epoch: 40, Samples: 5024/5760, Loss: 0.3597727417945862\n",
      "Epoch: 40, Samples: 5056/5760, Loss: 0.5175068378448486\n",
      "Epoch: 40, Samples: 5088/5760, Loss: 0.4913417100906372\n",
      "Epoch: 40, Samples: 5120/5760, Loss: 0.4530898332595825\n",
      "Epoch: 40, Samples: 5152/5760, Loss: 0.8319445848464966\n",
      "Epoch: 40, Samples: 5184/5760, Loss: 0.4273604452610016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Samples: 5216/5760, Loss: 0.6706651449203491\n",
      "Epoch: 40, Samples: 5248/5760, Loss: 0.6450627446174622\n",
      "Epoch: 40, Samples: 5280/5760, Loss: 0.23059944808483124\n",
      "Epoch: 40, Samples: 5312/5760, Loss: 0.40370506048202515\n",
      "Epoch: 40, Samples: 5344/5760, Loss: 0.41530129313468933\n",
      "Epoch: 40, Samples: 5376/5760, Loss: 0.2979448437690735\n",
      "Epoch: 40, Samples: 5408/5760, Loss: 0.4676436185836792\n",
      "Epoch: 40, Samples: 5440/5760, Loss: 0.5044273138046265\n",
      "Epoch: 40, Samples: 5472/5760, Loss: 0.3813048303127289\n",
      "Epoch: 40, Samples: 5504/5760, Loss: 0.6053740978240967\n",
      "Epoch: 40, Samples: 5536/5760, Loss: 0.42135459184646606\n",
      "Epoch: 40, Samples: 5568/5760, Loss: 0.5802491307258606\n",
      "Epoch: 40, Samples: 5600/5760, Loss: 0.5683282017707825\n",
      "Epoch: 40, Samples: 5632/5760, Loss: 0.42188388109207153\n",
      "Epoch: 40, Samples: 5664/5760, Loss: 0.5961964726448059\n",
      "Epoch: 40, Samples: 5696/5760, Loss: 0.5009227991104126\n",
      "Epoch: 40, Samples: 5728/5760, Loss: 2.6611385345458984\n",
      "\n",
      "Epoch: 40\n",
      "Training set: Average loss: 0.4955\n",
      "Validation set: Average loss: 1.4200, Accuracy: 534/818 (65%)\n",
      "Saving model (epoch 40) with lowest validation loss: 1.4200129233873808\n",
      "Epoch: 41, Samples: 0/5760, Loss: 0.26776883006095886\n",
      "Epoch: 41, Samples: 32/5760, Loss: 0.3747571110725403\n",
      "Epoch: 41, Samples: 64/5760, Loss: 0.2764374315738678\n",
      "Epoch: 41, Samples: 96/5760, Loss: 0.3064008951187134\n",
      "Epoch: 41, Samples: 128/5760, Loss: 0.3936672806739807\n",
      "Epoch: 41, Samples: 160/5760, Loss: 0.6196902990341187\n",
      "Epoch: 41, Samples: 192/5760, Loss: 0.4915095269680023\n",
      "Epoch: 41, Samples: 224/5760, Loss: 0.7897970080375671\n",
      "Epoch: 41, Samples: 256/5760, Loss: 0.5790008306503296\n",
      "Epoch: 41, Samples: 288/5760, Loss: 0.35725241899490356\n",
      "Epoch: 41, Samples: 320/5760, Loss: 0.4291357696056366\n",
      "Epoch: 41, Samples: 352/5760, Loss: 0.5315169095993042\n",
      "Epoch: 41, Samples: 384/5760, Loss: 0.4181649386882782\n",
      "Epoch: 41, Samples: 416/5760, Loss: 0.38711321353912354\n",
      "Epoch: 41, Samples: 448/5760, Loss: 0.5958127975463867\n",
      "Epoch: 41, Samples: 480/5760, Loss: 0.45695796608924866\n",
      "Epoch: 41, Samples: 512/5760, Loss: 0.5401052236557007\n",
      "Epoch: 41, Samples: 544/5760, Loss: 0.28740420937538147\n",
      "Epoch: 41, Samples: 576/5760, Loss: 0.37587907910346985\n",
      "Epoch: 41, Samples: 608/5760, Loss: 0.2762082517147064\n",
      "Epoch: 41, Samples: 640/5760, Loss: 0.4242270588874817\n",
      "Epoch: 41, Samples: 672/5760, Loss: 0.3856576681137085\n",
      "Epoch: 41, Samples: 704/5760, Loss: 0.24822700023651123\n",
      "Epoch: 41, Samples: 736/5760, Loss: 0.37499043345451355\n",
      "Epoch: 41, Samples: 768/5760, Loss: 0.333060085773468\n",
      "Epoch: 41, Samples: 800/5760, Loss: 0.443791925907135\n",
      "Epoch: 41, Samples: 832/5760, Loss: 0.5693346261978149\n",
      "Epoch: 41, Samples: 864/5760, Loss: 0.4373501241207123\n",
      "Epoch: 41, Samples: 896/5760, Loss: 0.6821048855781555\n",
      "Epoch: 41, Samples: 928/5760, Loss: 0.6146413683891296\n",
      "Epoch: 41, Samples: 960/5760, Loss: 0.6772191524505615\n",
      "Epoch: 41, Samples: 992/5760, Loss: 0.5400358438491821\n",
      "Epoch: 41, Samples: 1024/5760, Loss: 0.9153363704681396\n",
      "Epoch: 41, Samples: 1056/5760, Loss: 0.518497109413147\n",
      "Epoch: 41, Samples: 1088/5760, Loss: 0.7447404861450195\n",
      "Epoch: 41, Samples: 1120/5760, Loss: 0.25990259647369385\n",
      "Epoch: 41, Samples: 1152/5760, Loss: 0.4336317777633667\n",
      "Epoch: 41, Samples: 1184/5760, Loss: 0.3176944851875305\n",
      "Epoch: 41, Samples: 1216/5760, Loss: 0.4585629999637604\n",
      "Epoch: 41, Samples: 1248/5760, Loss: 0.46140021085739136\n",
      "Epoch: 41, Samples: 1280/5760, Loss: 0.25329917669296265\n",
      "Epoch: 41, Samples: 1312/5760, Loss: 0.5040788054466248\n",
      "Epoch: 41, Samples: 1344/5760, Loss: 0.3837178349494934\n",
      "Epoch: 41, Samples: 1376/5760, Loss: 0.5796107053756714\n",
      "Epoch: 41, Samples: 1408/5760, Loss: 0.38120365142822266\n",
      "Epoch: 41, Samples: 1440/5760, Loss: 0.3521878123283386\n",
      "Epoch: 41, Samples: 1472/5760, Loss: 0.99464350938797\n",
      "Epoch: 41, Samples: 1504/5760, Loss: 0.29273471236228943\n",
      "Epoch: 41, Samples: 1536/5760, Loss: 0.6766837239265442\n",
      "Epoch: 41, Samples: 1568/5760, Loss: 0.5106861591339111\n",
      "Epoch: 41, Samples: 1600/5760, Loss: 0.3751511573791504\n",
      "Epoch: 41, Samples: 1632/5760, Loss: 0.448040634393692\n",
      "Epoch: 41, Samples: 1664/5760, Loss: 0.50580894947052\n",
      "Epoch: 41, Samples: 1696/5760, Loss: 0.6137403249740601\n",
      "Epoch: 41, Samples: 1728/5760, Loss: 0.3433224558830261\n",
      "Epoch: 41, Samples: 1760/5760, Loss: 0.5811467170715332\n",
      "Epoch: 41, Samples: 1792/5760, Loss: 0.4237985908985138\n",
      "Epoch: 41, Samples: 1824/5760, Loss: 0.3679350018501282\n",
      "Epoch: 41, Samples: 1856/5760, Loss: 0.3648017644882202\n",
      "Epoch: 41, Samples: 1888/5760, Loss: 0.7178913354873657\n",
      "Epoch: 41, Samples: 1920/5760, Loss: 0.49756067991256714\n",
      "Epoch: 41, Samples: 1952/5760, Loss: 0.4746280312538147\n",
      "Epoch: 41, Samples: 1984/5760, Loss: 0.5571569204330444\n",
      "Epoch: 41, Samples: 2016/5760, Loss: 0.4368675947189331\n",
      "Epoch: 41, Samples: 2048/5760, Loss: 0.35331153869628906\n",
      "Epoch: 41, Samples: 2080/5760, Loss: 0.3437909781932831\n",
      "Epoch: 41, Samples: 2112/5760, Loss: 0.31750181317329407\n",
      "Epoch: 41, Samples: 2144/5760, Loss: 0.3587549328804016\n",
      "Epoch: 41, Samples: 2176/5760, Loss: 0.33478328585624695\n",
      "Epoch: 41, Samples: 2208/5760, Loss: 0.4597211480140686\n",
      "Epoch: 41, Samples: 2240/5760, Loss: 0.3207232356071472\n",
      "Epoch: 41, Samples: 2272/5760, Loss: 0.44127559661865234\n",
      "Epoch: 41, Samples: 2304/5760, Loss: 0.45233669877052307\n",
      "Epoch: 41, Samples: 2336/5760, Loss: 0.34215790033340454\n",
      "Epoch: 41, Samples: 2368/5760, Loss: 0.42424336075782776\n",
      "Epoch: 41, Samples: 2400/5760, Loss: 0.6491994261741638\n",
      "Epoch: 41, Samples: 2432/5760, Loss: 0.49870041012763977\n",
      "Epoch: 41, Samples: 2464/5760, Loss: 0.43940967321395874\n",
      "Epoch: 41, Samples: 2496/5760, Loss: 0.3313090205192566\n",
      "Epoch: 41, Samples: 2528/5760, Loss: 0.32321128249168396\n",
      "Epoch: 41, Samples: 2560/5760, Loss: 0.42507317662239075\n",
      "Epoch: 41, Samples: 2592/5760, Loss: 0.23298001289367676\n",
      "Epoch: 41, Samples: 2624/5760, Loss: 0.4076995253562927\n",
      "Epoch: 41, Samples: 2656/5760, Loss: 0.45365840196609497\n",
      "Epoch: 41, Samples: 2688/5760, Loss: 0.3095274865627289\n",
      "Epoch: 41, Samples: 2720/5760, Loss: 0.29871252179145813\n",
      "Epoch: 41, Samples: 2752/5760, Loss: 0.43497827649116516\n",
      "Epoch: 41, Samples: 2784/5760, Loss: 0.4031851589679718\n",
      "Epoch: 41, Samples: 2816/5760, Loss: 0.500435471534729\n",
      "Epoch: 41, Samples: 2848/5760, Loss: 0.4271841049194336\n",
      "Epoch: 41, Samples: 2880/5760, Loss: 0.5094029903411865\n",
      "Epoch: 41, Samples: 2912/5760, Loss: 0.6112804412841797\n",
      "Epoch: 41, Samples: 2944/5760, Loss: 0.4252803921699524\n",
      "Epoch: 41, Samples: 2976/5760, Loss: 0.30198705196380615\n",
      "Epoch: 41, Samples: 3008/5760, Loss: 0.2734241187572479\n",
      "Epoch: 41, Samples: 3040/5760, Loss: 0.16195735335350037\n",
      "Epoch: 41, Samples: 3072/5760, Loss: 0.3449089229106903\n",
      "Epoch: 41, Samples: 3104/5760, Loss: 0.4794987142086029\n",
      "Epoch: 41, Samples: 3136/5760, Loss: 0.6362931728363037\n",
      "Epoch: 41, Samples: 3168/5760, Loss: 0.3246947228908539\n",
      "Epoch: 41, Samples: 3200/5760, Loss: 0.3807288408279419\n",
      "Epoch: 41, Samples: 3232/5760, Loss: 0.3481903076171875\n",
      "Epoch: 41, Samples: 3264/5760, Loss: 0.6117216348648071\n",
      "Epoch: 41, Samples: 3296/5760, Loss: 0.7887110710144043\n",
      "Epoch: 41, Samples: 3328/5760, Loss: 0.3327403962612152\n",
      "Epoch: 41, Samples: 3360/5760, Loss: 0.22699862718582153\n",
      "Epoch: 41, Samples: 3392/5760, Loss: 0.7525120973587036\n",
      "Epoch: 41, Samples: 3424/5760, Loss: 0.39447280764579773\n",
      "Epoch: 41, Samples: 3456/5760, Loss: 0.2398086041212082\n",
      "Epoch: 41, Samples: 3488/5760, Loss: 0.35933443903923035\n",
      "Epoch: 41, Samples: 3520/5760, Loss: 0.2846048176288605\n",
      "Epoch: 41, Samples: 3552/5760, Loss: 0.48449161648750305\n",
      "Epoch: 41, Samples: 3584/5760, Loss: 0.4759505093097687\n",
      "Epoch: 41, Samples: 3616/5760, Loss: 0.34506112337112427\n",
      "Epoch: 41, Samples: 3648/5760, Loss: 0.5355862975120544\n",
      "Epoch: 41, Samples: 3680/5760, Loss: 0.3123370409011841\n",
      "Epoch: 41, Samples: 3712/5760, Loss: 0.32897356152534485\n",
      "Epoch: 41, Samples: 3744/5760, Loss: 0.4102090895175934\n",
      "Epoch: 41, Samples: 3776/5760, Loss: 0.44384241104125977\n",
      "Epoch: 41, Samples: 3808/5760, Loss: 0.4686674475669861\n",
      "Epoch: 41, Samples: 3840/5760, Loss: 0.3670794367790222\n",
      "Epoch: 41, Samples: 3872/5760, Loss: 0.20355771481990814\n",
      "Epoch: 41, Samples: 3904/5760, Loss: 0.4991318881511688\n",
      "Epoch: 41, Samples: 3936/5760, Loss: 0.5504213571548462\n",
      "Epoch: 41, Samples: 3968/5760, Loss: 0.41004034876823425\n",
      "Epoch: 41, Samples: 4000/5760, Loss: 0.37742897868156433\n",
      "Epoch: 41, Samples: 4032/5760, Loss: 0.42312389612197876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Samples: 4064/5760, Loss: 0.2563253939151764\n",
      "Epoch: 41, Samples: 4096/5760, Loss: 0.3753115236759186\n",
      "Epoch: 41, Samples: 4128/5760, Loss: 0.5330936908721924\n",
      "Epoch: 41, Samples: 4160/5760, Loss: 0.680745005607605\n",
      "Epoch: 41, Samples: 4192/5760, Loss: 0.29536330699920654\n",
      "Epoch: 41, Samples: 4224/5760, Loss: 0.4714464545249939\n",
      "Epoch: 41, Samples: 4256/5760, Loss: 0.42217838764190674\n",
      "Epoch: 41, Samples: 4288/5760, Loss: 0.34071221947669983\n",
      "Epoch: 41, Samples: 4320/5760, Loss: 0.35038793087005615\n",
      "Epoch: 41, Samples: 4352/5760, Loss: 0.4198963940143585\n",
      "Epoch: 41, Samples: 4384/5760, Loss: 0.45918893814086914\n",
      "Epoch: 41, Samples: 4416/5760, Loss: 0.3195314407348633\n",
      "Epoch: 41, Samples: 4448/5760, Loss: 0.3018606901168823\n",
      "Epoch: 41, Samples: 4480/5760, Loss: 0.376628577709198\n",
      "Epoch: 41, Samples: 4512/5760, Loss: 0.3369532823562622\n",
      "Epoch: 41, Samples: 4544/5760, Loss: 0.5516819953918457\n",
      "Epoch: 41, Samples: 4576/5760, Loss: 0.5252021551132202\n",
      "Epoch: 41, Samples: 4608/5760, Loss: 0.5053658485412598\n",
      "Epoch: 41, Samples: 4640/5760, Loss: 0.5484822988510132\n",
      "Epoch: 41, Samples: 4672/5760, Loss: 0.2704845070838928\n",
      "Epoch: 41, Samples: 4704/5760, Loss: 0.45091089606285095\n",
      "Epoch: 41, Samples: 4736/5760, Loss: 0.368826687335968\n",
      "Epoch: 41, Samples: 4768/5760, Loss: 0.48724251985549927\n",
      "Epoch: 41, Samples: 4800/5760, Loss: 0.5649705529212952\n",
      "Epoch: 41, Samples: 4832/5760, Loss: 0.302773654460907\n",
      "Epoch: 41, Samples: 4864/5760, Loss: 0.6162136793136597\n",
      "Epoch: 41, Samples: 4896/5760, Loss: 0.4594326913356781\n",
      "Epoch: 41, Samples: 4928/5760, Loss: 0.4077499806880951\n",
      "Epoch: 41, Samples: 4960/5760, Loss: 0.28383225202560425\n",
      "Epoch: 41, Samples: 4992/5760, Loss: 0.3870263397693634\n",
      "Epoch: 41, Samples: 5024/5760, Loss: 0.5072920322418213\n",
      "Epoch: 41, Samples: 5056/5760, Loss: 0.4283286929130554\n",
      "Epoch: 41, Samples: 5088/5760, Loss: 0.5006921887397766\n",
      "Epoch: 41, Samples: 5120/5760, Loss: 0.7660294771194458\n",
      "Epoch: 41, Samples: 5152/5760, Loss: 0.32082146406173706\n",
      "Epoch: 41, Samples: 5184/5760, Loss: 0.4080067276954651\n",
      "Epoch: 41, Samples: 5216/5760, Loss: 0.41750484704971313\n",
      "Epoch: 41, Samples: 5248/5760, Loss: 0.31152862310409546\n",
      "Epoch: 41, Samples: 5280/5760, Loss: 0.4893706738948822\n",
      "Epoch: 41, Samples: 5312/5760, Loss: 0.5182554721832275\n",
      "Epoch: 41, Samples: 5344/5760, Loss: 0.33860349655151367\n",
      "Epoch: 41, Samples: 5376/5760, Loss: 0.5801102519035339\n",
      "Epoch: 41, Samples: 5408/5760, Loss: 0.47836804389953613\n",
      "Epoch: 41, Samples: 5440/5760, Loss: 0.2742668390274048\n",
      "Epoch: 41, Samples: 5472/5760, Loss: 0.44854527711868286\n",
      "Epoch: 41, Samples: 5504/5760, Loss: 0.45442208647727966\n",
      "Epoch: 41, Samples: 5536/5760, Loss: 0.5145733952522278\n",
      "Epoch: 41, Samples: 5568/5760, Loss: 0.4186195731163025\n",
      "Epoch: 41, Samples: 5600/5760, Loss: 0.2816982865333557\n",
      "Epoch: 41, Samples: 5632/5760, Loss: 0.39953872561454773\n",
      "Epoch: 41, Samples: 5664/5760, Loss: 0.8968799114227295\n",
      "Epoch: 41, Samples: 5696/5760, Loss: 0.3665427565574646\n",
      "Epoch: 41, Samples: 5728/5760, Loss: 2.181281566619873\n",
      "\n",
      "Epoch: 41\n",
      "Training set: Average loss: 0.4495\n",
      "Validation set: Average loss: 1.5709, Accuracy: 505/818 (62%)\n",
      "Epoch: 42, Samples: 0/5760, Loss: 0.4462833106517792\n",
      "Epoch: 42, Samples: 32/5760, Loss: 0.5956699848175049\n",
      "Epoch: 42, Samples: 64/5760, Loss: 0.3768426179885864\n",
      "Epoch: 42, Samples: 96/5760, Loss: 0.40175771713256836\n",
      "Epoch: 42, Samples: 128/5760, Loss: 0.3164033591747284\n",
      "Epoch: 42, Samples: 160/5760, Loss: 0.2939477860927582\n",
      "Epoch: 42, Samples: 192/5760, Loss: 0.4423760175704956\n",
      "Epoch: 42, Samples: 224/5760, Loss: 0.40619006752967834\n",
      "Epoch: 42, Samples: 256/5760, Loss: 0.5878987312316895\n",
      "Epoch: 42, Samples: 288/5760, Loss: 0.26581448316574097\n",
      "Epoch: 42, Samples: 320/5760, Loss: 0.47029268741607666\n",
      "Epoch: 42, Samples: 352/5760, Loss: 0.8590927720069885\n",
      "Epoch: 42, Samples: 384/5760, Loss: 0.5646654367446899\n",
      "Epoch: 42, Samples: 416/5760, Loss: 0.43606406450271606\n",
      "Epoch: 42, Samples: 448/5760, Loss: 0.309658944606781\n",
      "Epoch: 42, Samples: 480/5760, Loss: 0.4396587014198303\n",
      "Epoch: 42, Samples: 512/5760, Loss: 0.2924065887928009\n",
      "Epoch: 42, Samples: 544/5760, Loss: 0.4743978977203369\n",
      "Epoch: 42, Samples: 576/5760, Loss: 0.32080066204071045\n",
      "Epoch: 42, Samples: 608/5760, Loss: 0.6013659834861755\n",
      "Epoch: 42, Samples: 640/5760, Loss: 0.4376090168952942\n",
      "Epoch: 42, Samples: 672/5760, Loss: 0.35715243220329285\n",
      "Epoch: 42, Samples: 704/5760, Loss: 0.5559914112091064\n",
      "Epoch: 42, Samples: 736/5760, Loss: 0.5853238105773926\n",
      "Epoch: 42, Samples: 768/5760, Loss: 0.5457902550697327\n",
      "Epoch: 42, Samples: 800/5760, Loss: 0.19019946455955505\n",
      "Epoch: 42, Samples: 832/5760, Loss: 0.2594407796859741\n",
      "Epoch: 42, Samples: 864/5760, Loss: 0.43163084983825684\n",
      "Epoch: 42, Samples: 896/5760, Loss: 0.41036146879196167\n",
      "Epoch: 42, Samples: 928/5760, Loss: 1.0048911571502686\n",
      "Epoch: 42, Samples: 960/5760, Loss: 0.3129344582557678\n",
      "Epoch: 42, Samples: 992/5760, Loss: 0.36420103907585144\n",
      "Epoch: 42, Samples: 1024/5760, Loss: 0.48693862557411194\n",
      "Epoch: 42, Samples: 1056/5760, Loss: 0.5829470753669739\n",
      "Epoch: 42, Samples: 1088/5760, Loss: 0.5157730579376221\n",
      "Epoch: 42, Samples: 1120/5760, Loss: 0.49408698081970215\n",
      "Epoch: 42, Samples: 1152/5760, Loss: 0.6110290884971619\n",
      "Epoch: 42, Samples: 1184/5760, Loss: 0.37303081154823303\n",
      "Epoch: 42, Samples: 1216/5760, Loss: 0.5298259854316711\n",
      "Epoch: 42, Samples: 1248/5760, Loss: 0.29665306210517883\n",
      "Epoch: 42, Samples: 1280/5760, Loss: 0.4470166265964508\n",
      "Epoch: 42, Samples: 1312/5760, Loss: 0.39576786756515503\n",
      "Epoch: 42, Samples: 1344/5760, Loss: 0.355344295501709\n",
      "Epoch: 42, Samples: 1376/5760, Loss: 0.5129419565200806\n",
      "Epoch: 42, Samples: 1408/5760, Loss: 0.49222370982170105\n",
      "Epoch: 42, Samples: 1440/5760, Loss: 0.5224368572235107\n",
      "Epoch: 42, Samples: 1472/5760, Loss: 0.20445609092712402\n",
      "Epoch: 42, Samples: 1504/5760, Loss: 0.28071415424346924\n",
      "Epoch: 42, Samples: 1536/5760, Loss: 0.3161843717098236\n",
      "Epoch: 42, Samples: 1568/5760, Loss: 0.302460640668869\n",
      "Epoch: 42, Samples: 1600/5760, Loss: 0.5458142757415771\n",
      "Epoch: 42, Samples: 1632/5760, Loss: 0.4696815609931946\n",
      "Epoch: 42, Samples: 1664/5760, Loss: 0.4473899006843567\n",
      "Epoch: 42, Samples: 1696/5760, Loss: 0.38941770792007446\n",
      "Epoch: 42, Samples: 1728/5760, Loss: 0.61266028881073\n",
      "Epoch: 42, Samples: 1760/5760, Loss: 0.6329004764556885\n",
      "Epoch: 42, Samples: 1792/5760, Loss: 0.7209264039993286\n",
      "Epoch: 42, Samples: 1824/5760, Loss: 0.4223950505256653\n",
      "Epoch: 42, Samples: 1856/5760, Loss: 0.3522842228412628\n",
      "Epoch: 42, Samples: 1888/5760, Loss: 0.2451687455177307\n",
      "Epoch: 42, Samples: 1920/5760, Loss: 0.4261009395122528\n",
      "Epoch: 42, Samples: 1952/5760, Loss: 0.3944723606109619\n",
      "Epoch: 42, Samples: 1984/5760, Loss: 0.56496262550354\n",
      "Epoch: 42, Samples: 2016/5760, Loss: 0.44392308592796326\n",
      "Epoch: 42, Samples: 2048/5760, Loss: 0.47480034828186035\n",
      "Epoch: 42, Samples: 2080/5760, Loss: 0.3993247449398041\n",
      "Epoch: 42, Samples: 2112/5760, Loss: 0.5100021362304688\n",
      "Epoch: 42, Samples: 2144/5760, Loss: 0.29681071639060974\n",
      "Epoch: 42, Samples: 2176/5760, Loss: 0.6085107326507568\n",
      "Epoch: 42, Samples: 2208/5760, Loss: 0.3633326292037964\n",
      "Epoch: 42, Samples: 2240/5760, Loss: 0.5002590417861938\n",
      "Epoch: 42, Samples: 2272/5760, Loss: 0.3745286464691162\n",
      "Epoch: 42, Samples: 2304/5760, Loss: 0.3331470191478729\n",
      "Epoch: 42, Samples: 2336/5760, Loss: 0.3327780067920685\n",
      "Epoch: 42, Samples: 2368/5760, Loss: 0.7219972014427185\n",
      "Epoch: 42, Samples: 2400/5760, Loss: 0.42156338691711426\n",
      "Epoch: 42, Samples: 2432/5760, Loss: 0.3725549280643463\n",
      "Epoch: 42, Samples: 2464/5760, Loss: 0.325357049703598\n",
      "Epoch: 42, Samples: 2496/5760, Loss: 0.5304076671600342\n",
      "Epoch: 42, Samples: 2528/5760, Loss: 0.3312229812145233\n",
      "Epoch: 42, Samples: 2560/5760, Loss: 0.5237576961517334\n",
      "Epoch: 42, Samples: 2592/5760, Loss: 0.4112476110458374\n",
      "Epoch: 42, Samples: 2624/5760, Loss: 0.4654642939567566\n",
      "Epoch: 42, Samples: 2656/5760, Loss: 0.6032941341400146\n",
      "Epoch: 42, Samples: 2688/5760, Loss: 0.2690846025943756\n",
      "Epoch: 42, Samples: 2720/5760, Loss: 0.19012801349163055\n",
      "Epoch: 42, Samples: 2752/5760, Loss: 0.6256155967712402\n",
      "Epoch: 42, Samples: 2784/5760, Loss: 0.1955350935459137\n",
      "Epoch: 42, Samples: 2816/5760, Loss: 0.3799808621406555\n",
      "Epoch: 42, Samples: 2848/5760, Loss: 0.40398386120796204\n",
      "Epoch: 42, Samples: 2880/5760, Loss: 0.208143949508667\n",
      "Epoch: 42, Samples: 2912/5760, Loss: 0.23586593568325043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, Samples: 2944/5760, Loss: 0.4902486205101013\n",
      "Epoch: 42, Samples: 2976/5760, Loss: 0.25682532787323\n",
      "Epoch: 42, Samples: 3008/5760, Loss: 0.3873196840286255\n",
      "Epoch: 42, Samples: 3040/5760, Loss: 0.3580896556377411\n",
      "Epoch: 42, Samples: 3072/5760, Loss: 0.6847301721572876\n",
      "Epoch: 42, Samples: 3104/5760, Loss: 0.33491164445877075\n",
      "Epoch: 42, Samples: 3136/5760, Loss: 0.46742311120033264\n",
      "Epoch: 42, Samples: 3168/5760, Loss: 0.15138521790504456\n",
      "Epoch: 42, Samples: 3200/5760, Loss: 0.3906944990158081\n",
      "Epoch: 42, Samples: 3232/5760, Loss: 0.4136142432689667\n",
      "Epoch: 42, Samples: 3264/5760, Loss: 0.48655974864959717\n",
      "Epoch: 42, Samples: 3296/5760, Loss: 0.47058337926864624\n",
      "Epoch: 42, Samples: 3328/5760, Loss: 0.5413188934326172\n",
      "Epoch: 42, Samples: 3360/5760, Loss: 0.3889375925064087\n",
      "Epoch: 42, Samples: 3392/5760, Loss: 0.3319673538208008\n",
      "Epoch: 42, Samples: 3424/5760, Loss: 0.25433582067489624\n",
      "Epoch: 42, Samples: 3456/5760, Loss: 0.3331768810749054\n",
      "Epoch: 42, Samples: 3488/5760, Loss: 0.20845337212085724\n",
      "Epoch: 42, Samples: 3520/5760, Loss: 0.43918099999427795\n",
      "Epoch: 42, Samples: 3552/5760, Loss: 0.5357449054718018\n",
      "Epoch: 42, Samples: 3584/5760, Loss: 0.2415527105331421\n",
      "Epoch: 42, Samples: 3616/5760, Loss: 0.43447497487068176\n",
      "Epoch: 42, Samples: 3648/5760, Loss: 0.7657803297042847\n",
      "Epoch: 42, Samples: 3680/5760, Loss: 0.4131782352924347\n",
      "Epoch: 42, Samples: 3712/5760, Loss: 0.40486305952072144\n",
      "Epoch: 42, Samples: 3744/5760, Loss: 0.2945537865161896\n",
      "Epoch: 42, Samples: 3776/5760, Loss: 0.4216112196445465\n",
      "Epoch: 42, Samples: 3808/5760, Loss: 0.38092952966690063\n",
      "Epoch: 42, Samples: 3840/5760, Loss: 0.5210720300674438\n",
      "Epoch: 42, Samples: 3872/5760, Loss: 0.5221850872039795\n",
      "Epoch: 42, Samples: 3904/5760, Loss: 0.3237783908843994\n",
      "Epoch: 42, Samples: 3936/5760, Loss: 0.4347609877586365\n",
      "Epoch: 42, Samples: 3968/5760, Loss: 0.5215606689453125\n",
      "Epoch: 42, Samples: 4000/5760, Loss: 0.24914613366127014\n",
      "Epoch: 42, Samples: 4032/5760, Loss: 0.326446533203125\n",
      "Epoch: 42, Samples: 4064/5760, Loss: 0.30560165643692017\n",
      "Epoch: 42, Samples: 4096/5760, Loss: 0.6271902322769165\n",
      "Epoch: 42, Samples: 4128/5760, Loss: 0.43232813477516174\n",
      "Epoch: 42, Samples: 4160/5760, Loss: 0.4405388832092285\n",
      "Epoch: 42, Samples: 4192/5760, Loss: 0.5833052396774292\n",
      "Epoch: 42, Samples: 4224/5760, Loss: 0.5660173892974854\n",
      "Epoch: 42, Samples: 4256/5760, Loss: 0.30425918102264404\n",
      "Epoch: 42, Samples: 4288/5760, Loss: 0.2381201833486557\n",
      "Epoch: 42, Samples: 4320/5760, Loss: 0.6258383393287659\n",
      "Epoch: 42, Samples: 4352/5760, Loss: 0.6806955337524414\n",
      "Epoch: 42, Samples: 4384/5760, Loss: 0.8232757449150085\n",
      "Epoch: 42, Samples: 4416/5760, Loss: 0.2888279855251312\n",
      "Epoch: 42, Samples: 4448/5760, Loss: 0.4750933349132538\n",
      "Epoch: 42, Samples: 4480/5760, Loss: 0.5038880109786987\n",
      "Epoch: 42, Samples: 4512/5760, Loss: 0.5022138357162476\n",
      "Epoch: 42, Samples: 4544/5760, Loss: 0.4151107370853424\n",
      "Epoch: 42, Samples: 4576/5760, Loss: 0.48618584871292114\n",
      "Epoch: 42, Samples: 4608/5760, Loss: 0.8561863899230957\n",
      "Epoch: 42, Samples: 4640/5760, Loss: 0.3842203915119171\n",
      "Epoch: 42, Samples: 4672/5760, Loss: 0.2250654548406601\n",
      "Epoch: 42, Samples: 4704/5760, Loss: 0.47540831565856934\n",
      "Epoch: 42, Samples: 4736/5760, Loss: 0.5725967884063721\n",
      "Epoch: 42, Samples: 4768/5760, Loss: 0.4244753122329712\n",
      "Epoch: 42, Samples: 4800/5760, Loss: 0.6413773894309998\n",
      "Epoch: 42, Samples: 4832/5760, Loss: 0.47854241728782654\n",
      "Epoch: 42, Samples: 4864/5760, Loss: 0.5077798962593079\n",
      "Epoch: 42, Samples: 4896/5760, Loss: 0.5202984809875488\n",
      "Epoch: 42, Samples: 4928/5760, Loss: 0.6811212301254272\n",
      "Epoch: 42, Samples: 4960/5760, Loss: 0.5237030386924744\n",
      "Epoch: 42, Samples: 4992/5760, Loss: 0.4310630261898041\n",
      "Epoch: 42, Samples: 5024/5760, Loss: 0.2535611391067505\n",
      "Epoch: 42, Samples: 5056/5760, Loss: 0.3703729212284088\n",
      "Epoch: 42, Samples: 5088/5760, Loss: 0.3009779155254364\n",
      "Epoch: 42, Samples: 5120/5760, Loss: 0.3526725471019745\n",
      "Epoch: 42, Samples: 5152/5760, Loss: 0.4608389735221863\n",
      "Epoch: 42, Samples: 5184/5760, Loss: 0.3792126178741455\n",
      "Epoch: 42, Samples: 5216/5760, Loss: 0.5884912014007568\n",
      "Epoch: 42, Samples: 5248/5760, Loss: 0.5003303289413452\n",
      "Epoch: 42, Samples: 5280/5760, Loss: 0.5952388048171997\n",
      "Epoch: 42, Samples: 5312/5760, Loss: 0.2012074589729309\n",
      "Epoch: 42, Samples: 5344/5760, Loss: 0.6347362399101257\n",
      "Epoch: 42, Samples: 5376/5760, Loss: 0.60076904296875\n",
      "Epoch: 42, Samples: 5408/5760, Loss: 0.4879608750343323\n",
      "Epoch: 42, Samples: 5440/5760, Loss: 0.408763587474823\n",
      "Epoch: 42, Samples: 5472/5760, Loss: 0.29679182171821594\n",
      "Epoch: 42, Samples: 5504/5760, Loss: 0.25681519508361816\n",
      "Epoch: 42, Samples: 5536/5760, Loss: 0.39052069187164307\n",
      "Epoch: 42, Samples: 5568/5760, Loss: 0.2053154855966568\n",
      "Epoch: 42, Samples: 5600/5760, Loss: 0.3724091649055481\n",
      "Epoch: 42, Samples: 5632/5760, Loss: 0.19260433316230774\n",
      "Epoch: 42, Samples: 5664/5760, Loss: 0.41649961471557617\n",
      "Epoch: 42, Samples: 5696/5760, Loss: 0.4758709669113159\n",
      "Epoch: 42, Samples: 5728/5760, Loss: 2.214244842529297\n",
      "\n",
      "Epoch: 42\n",
      "Training set: Average loss: 0.4462\n",
      "Validation set: Average loss: 1.4989, Accuracy: 525/818 (64%)\n",
      "Epoch: 43, Samples: 0/5760, Loss: 0.2739931046962738\n",
      "Epoch: 43, Samples: 32/5760, Loss: 0.2704816162586212\n",
      "Epoch: 43, Samples: 64/5760, Loss: 0.2984912395477295\n",
      "Epoch: 43, Samples: 96/5760, Loss: 0.3840080499649048\n",
      "Epoch: 43, Samples: 128/5760, Loss: 0.32172247767448425\n",
      "Epoch: 43, Samples: 160/5760, Loss: 0.41521984338760376\n",
      "Epoch: 43, Samples: 192/5760, Loss: 0.5416626930236816\n",
      "Epoch: 43, Samples: 224/5760, Loss: 0.6065006256103516\n",
      "Epoch: 43, Samples: 256/5760, Loss: 0.3311340808868408\n",
      "Epoch: 43, Samples: 288/5760, Loss: 0.4916285276412964\n",
      "Epoch: 43, Samples: 320/5760, Loss: 0.7844944000244141\n",
      "Epoch: 43, Samples: 352/5760, Loss: 0.5053945779800415\n",
      "Epoch: 43, Samples: 384/5760, Loss: 0.6162375807762146\n",
      "Epoch: 43, Samples: 416/5760, Loss: 0.35932204127311707\n",
      "Epoch: 43, Samples: 448/5760, Loss: 0.34071314334869385\n",
      "Epoch: 43, Samples: 480/5760, Loss: 0.4287712574005127\n",
      "Epoch: 43, Samples: 512/5760, Loss: 0.5098171830177307\n",
      "Epoch: 43, Samples: 544/5760, Loss: 0.4223201274871826\n",
      "Epoch: 43, Samples: 576/5760, Loss: 0.4470461905002594\n",
      "Epoch: 43, Samples: 608/5760, Loss: 0.45655274391174316\n",
      "Epoch: 43, Samples: 640/5760, Loss: 0.5286545157432556\n",
      "Epoch: 43, Samples: 672/5760, Loss: 0.39042067527770996\n",
      "Epoch: 43, Samples: 704/5760, Loss: 0.41391026973724365\n",
      "Epoch: 43, Samples: 736/5760, Loss: 0.37167102098464966\n",
      "Epoch: 43, Samples: 768/5760, Loss: 0.569003701210022\n",
      "Epoch: 43, Samples: 800/5760, Loss: 0.39075541496276855\n",
      "Epoch: 43, Samples: 832/5760, Loss: 0.4467189908027649\n",
      "Epoch: 43, Samples: 864/5760, Loss: 0.26530179381370544\n",
      "Epoch: 43, Samples: 896/5760, Loss: 0.1930236965417862\n",
      "Epoch: 43, Samples: 928/5760, Loss: 0.30200904607772827\n",
      "Epoch: 43, Samples: 960/5760, Loss: 0.31981709599494934\n",
      "Epoch: 43, Samples: 992/5760, Loss: 0.3640368580818176\n",
      "Epoch: 43, Samples: 1024/5760, Loss: 0.21257410943508148\n",
      "Epoch: 43, Samples: 1056/5760, Loss: 0.39260321855545044\n",
      "Epoch: 43, Samples: 1088/5760, Loss: 0.34476059675216675\n",
      "Epoch: 43, Samples: 1120/5760, Loss: 0.42052555084228516\n",
      "Epoch: 43, Samples: 1152/5760, Loss: 0.32793501019477844\n",
      "Epoch: 43, Samples: 1184/5760, Loss: 0.4246425926685333\n",
      "Epoch: 43, Samples: 1216/5760, Loss: 0.39684778451919556\n",
      "Epoch: 43, Samples: 1248/5760, Loss: 0.5357904434204102\n",
      "Epoch: 43, Samples: 1280/5760, Loss: 0.41567596793174744\n",
      "Epoch: 43, Samples: 1312/5760, Loss: 0.21421106159687042\n",
      "Epoch: 43, Samples: 1344/5760, Loss: 0.3742949962615967\n",
      "Epoch: 43, Samples: 1376/5760, Loss: 0.34577131271362305\n",
      "Epoch: 43, Samples: 1408/5760, Loss: 0.43326622247695923\n",
      "Epoch: 43, Samples: 1440/5760, Loss: 0.31797245144844055\n",
      "Epoch: 43, Samples: 1472/5760, Loss: 0.3131330907344818\n",
      "Epoch: 43, Samples: 1504/5760, Loss: 0.4134092330932617\n",
      "Epoch: 43, Samples: 1536/5760, Loss: 0.2401176244020462\n",
      "Epoch: 43, Samples: 1568/5760, Loss: 0.3020053505897522\n",
      "Epoch: 43, Samples: 1600/5760, Loss: 0.2694176137447357\n",
      "Epoch: 43, Samples: 1632/5760, Loss: 0.1938348263502121\n",
      "Epoch: 43, Samples: 1664/5760, Loss: 0.35748979449272156\n",
      "Epoch: 43, Samples: 1696/5760, Loss: 0.3438285291194916\n",
      "Epoch: 43, Samples: 1728/5760, Loss: 0.374174028635025\n",
      "Epoch: 43, Samples: 1760/5760, Loss: 0.4467015564441681\n",
      "Epoch: 43, Samples: 1792/5760, Loss: 0.33243006467819214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43, Samples: 1824/5760, Loss: 0.26063576340675354\n",
      "Epoch: 43, Samples: 1856/5760, Loss: 0.4385204017162323\n",
      "Epoch: 43, Samples: 1888/5760, Loss: 0.3081800043582916\n",
      "Epoch: 43, Samples: 1920/5760, Loss: 0.2440340667963028\n",
      "Epoch: 43, Samples: 1952/5760, Loss: 0.6297043561935425\n",
      "Epoch: 43, Samples: 1984/5760, Loss: 0.35884517431259155\n",
      "Epoch: 43, Samples: 2016/5760, Loss: 0.28935590386390686\n",
      "Epoch: 43, Samples: 2048/5760, Loss: 0.25893187522888184\n",
      "Epoch: 43, Samples: 2080/5760, Loss: 0.3164346516132355\n",
      "Epoch: 43, Samples: 2112/5760, Loss: 0.3711191415786743\n",
      "Epoch: 43, Samples: 2144/5760, Loss: 0.39729732275009155\n",
      "Epoch: 43, Samples: 2176/5760, Loss: 0.36059507727622986\n",
      "Epoch: 43, Samples: 2208/5760, Loss: 0.45022663474082947\n",
      "Epoch: 43, Samples: 2240/5760, Loss: 0.46315285563468933\n",
      "Epoch: 43, Samples: 2272/5760, Loss: 0.23427262902259827\n",
      "Epoch: 43, Samples: 2304/5760, Loss: 0.3556261360645294\n",
      "Epoch: 43, Samples: 2336/5760, Loss: 0.37903013825416565\n",
      "Epoch: 43, Samples: 2368/5760, Loss: 0.2725062370300293\n",
      "Epoch: 43, Samples: 2400/5760, Loss: 0.49731510877609253\n",
      "Epoch: 43, Samples: 2432/5760, Loss: 0.3388114273548126\n",
      "Epoch: 43, Samples: 2464/5760, Loss: 0.33525460958480835\n",
      "Epoch: 43, Samples: 2496/5760, Loss: 0.4176618456840515\n",
      "Epoch: 43, Samples: 2528/5760, Loss: 0.4160250425338745\n",
      "Epoch: 43, Samples: 2560/5760, Loss: 0.4732264578342438\n",
      "Epoch: 43, Samples: 2592/5760, Loss: 0.4379880428314209\n",
      "Epoch: 43, Samples: 2624/5760, Loss: 0.36275309324264526\n",
      "Epoch: 43, Samples: 2656/5760, Loss: 0.35136374831199646\n",
      "Epoch: 43, Samples: 2688/5760, Loss: 0.36381444334983826\n",
      "Epoch: 43, Samples: 2720/5760, Loss: 0.6469939947128296\n",
      "Epoch: 43, Samples: 2752/5760, Loss: 0.23705659806728363\n",
      "Epoch: 43, Samples: 2784/5760, Loss: 0.34864702820777893\n",
      "Epoch: 43, Samples: 2816/5760, Loss: 0.4646978974342346\n",
      "Epoch: 43, Samples: 2848/5760, Loss: 0.2923705279827118\n",
      "Epoch: 43, Samples: 2880/5760, Loss: 0.2254047840833664\n",
      "Epoch: 43, Samples: 2912/5760, Loss: 0.39554136991500854\n",
      "Epoch: 43, Samples: 2944/5760, Loss: 0.24564313888549805\n",
      "Epoch: 43, Samples: 2976/5760, Loss: 0.43323105573654175\n",
      "Epoch: 43, Samples: 3008/5760, Loss: 0.2475648671388626\n",
      "Epoch: 43, Samples: 3040/5760, Loss: 0.4938902258872986\n",
      "Epoch: 43, Samples: 3072/5760, Loss: 0.3441575765609741\n",
      "Epoch: 43, Samples: 3104/5760, Loss: 0.2101721167564392\n",
      "Epoch: 43, Samples: 3136/5760, Loss: 0.4370917081832886\n",
      "Epoch: 43, Samples: 3168/5760, Loss: 0.5934332609176636\n",
      "Epoch: 43, Samples: 3200/5760, Loss: 0.2772621214389801\n",
      "Epoch: 43, Samples: 3232/5760, Loss: 0.22780932486057281\n",
      "Epoch: 43, Samples: 3264/5760, Loss: 0.3019068241119385\n",
      "Epoch: 43, Samples: 3296/5760, Loss: 0.4155896306037903\n",
      "Epoch: 43, Samples: 3328/5760, Loss: 0.4817584156990051\n",
      "Epoch: 43, Samples: 3360/5760, Loss: 0.36394187808036804\n",
      "Epoch: 43, Samples: 3392/5760, Loss: 0.24048686027526855\n",
      "Epoch: 43, Samples: 3424/5760, Loss: 0.2602226734161377\n",
      "Epoch: 43, Samples: 3456/5760, Loss: 0.545164942741394\n",
      "Epoch: 43, Samples: 3488/5760, Loss: 0.3133625090122223\n",
      "Epoch: 43, Samples: 3520/5760, Loss: 0.23657016456127167\n",
      "Epoch: 43, Samples: 3552/5760, Loss: 0.15812097489833832\n",
      "Epoch: 43, Samples: 3584/5760, Loss: 0.26824861764907837\n",
      "Epoch: 43, Samples: 3616/5760, Loss: 0.48804986476898193\n",
      "Epoch: 43, Samples: 3648/5760, Loss: 0.36628544330596924\n",
      "Epoch: 43, Samples: 3680/5760, Loss: 0.45879918336868286\n",
      "Epoch: 43, Samples: 3712/5760, Loss: 0.3993541896343231\n",
      "Epoch: 43, Samples: 3744/5760, Loss: 0.4063849449157715\n",
      "Epoch: 43, Samples: 3776/5760, Loss: 0.15094085037708282\n",
      "Epoch: 43, Samples: 3808/5760, Loss: 0.6397297382354736\n",
      "Epoch: 43, Samples: 3840/5760, Loss: 0.29145920276641846\n",
      "Epoch: 43, Samples: 3872/5760, Loss: 0.5692188739776611\n",
      "Epoch: 43, Samples: 3904/5760, Loss: 0.49650177359580994\n",
      "Epoch: 43, Samples: 3936/5760, Loss: 0.28601670265197754\n",
      "Epoch: 43, Samples: 3968/5760, Loss: 0.2801527678966522\n",
      "Epoch: 43, Samples: 4000/5760, Loss: 0.17325423657894135\n",
      "Epoch: 43, Samples: 4032/5760, Loss: 0.26962995529174805\n",
      "Epoch: 43, Samples: 4064/5760, Loss: 0.3981032371520996\n",
      "Epoch: 43, Samples: 4096/5760, Loss: 0.30331262946128845\n",
      "Epoch: 43, Samples: 4128/5760, Loss: 0.4958747327327728\n",
      "Epoch: 43, Samples: 4160/5760, Loss: 0.3633013963699341\n",
      "Epoch: 43, Samples: 4192/5760, Loss: 0.4700717031955719\n",
      "Epoch: 43, Samples: 4224/5760, Loss: 0.27935904264450073\n",
      "Epoch: 43, Samples: 4256/5760, Loss: 0.2331908941268921\n",
      "Epoch: 43, Samples: 4288/5760, Loss: 0.38124892115592957\n",
      "Epoch: 43, Samples: 4320/5760, Loss: 0.2866526246070862\n",
      "Epoch: 43, Samples: 4352/5760, Loss: 0.31539011001586914\n",
      "Epoch: 43, Samples: 4384/5760, Loss: 0.3326602280139923\n",
      "Epoch: 43, Samples: 4416/5760, Loss: 0.32081225514411926\n",
      "Epoch: 43, Samples: 4448/5760, Loss: 0.3690408170223236\n",
      "Epoch: 43, Samples: 4480/5760, Loss: 0.35104620456695557\n",
      "Epoch: 43, Samples: 4512/5760, Loss: 0.4793965220451355\n",
      "Epoch: 43, Samples: 4544/5760, Loss: 0.3367442190647125\n",
      "Epoch: 43, Samples: 4576/5760, Loss: 0.2797485589981079\n",
      "Epoch: 43, Samples: 4608/5760, Loss: 0.32125744223594666\n",
      "Epoch: 43, Samples: 4640/5760, Loss: 0.18502748012542725\n",
      "Epoch: 43, Samples: 4672/5760, Loss: 0.5328267812728882\n",
      "Epoch: 43, Samples: 4704/5760, Loss: 0.3295857608318329\n",
      "Epoch: 43, Samples: 4736/5760, Loss: 0.4443027973175049\n",
      "Epoch: 43, Samples: 4768/5760, Loss: 0.15961939096450806\n",
      "Epoch: 43, Samples: 4800/5760, Loss: 0.29734671115875244\n",
      "Epoch: 43, Samples: 4832/5760, Loss: 0.41407468914985657\n",
      "Epoch: 43, Samples: 4864/5760, Loss: 0.356832355260849\n",
      "Epoch: 43, Samples: 4896/5760, Loss: 0.34273770451545715\n",
      "Epoch: 43, Samples: 4928/5760, Loss: 0.507579505443573\n",
      "Epoch: 43, Samples: 4960/5760, Loss: 0.3618030548095703\n",
      "Epoch: 43, Samples: 4992/5760, Loss: 0.2817862927913666\n",
      "Epoch: 43, Samples: 5024/5760, Loss: 0.3896656036376953\n",
      "Epoch: 43, Samples: 5056/5760, Loss: 0.2319847047328949\n",
      "Epoch: 43, Samples: 5088/5760, Loss: 0.3723616600036621\n",
      "Epoch: 43, Samples: 5120/5760, Loss: 0.5430769324302673\n",
      "Epoch: 43, Samples: 5152/5760, Loss: 0.2848326563835144\n",
      "Epoch: 43, Samples: 5184/5760, Loss: 0.4357636272907257\n",
      "Epoch: 43, Samples: 5216/5760, Loss: 0.27662646770477295\n",
      "Epoch: 43, Samples: 5248/5760, Loss: 0.4777602553367615\n",
      "Epoch: 43, Samples: 5280/5760, Loss: 0.25039464235305786\n",
      "Epoch: 43, Samples: 5312/5760, Loss: 0.7347348928451538\n",
      "Epoch: 43, Samples: 5344/5760, Loss: 0.44359278678894043\n",
      "Epoch: 43, Samples: 5376/5760, Loss: 0.6317501068115234\n",
      "Epoch: 43, Samples: 5408/5760, Loss: 0.4221804738044739\n",
      "Epoch: 43, Samples: 5440/5760, Loss: 0.3639800250530243\n",
      "Epoch: 43, Samples: 5472/5760, Loss: 0.22819627821445465\n",
      "Epoch: 43, Samples: 5504/5760, Loss: 0.30841368436813354\n",
      "Epoch: 43, Samples: 5536/5760, Loss: 0.35257187485694885\n",
      "Epoch: 43, Samples: 5568/5760, Loss: 0.24555259943008423\n",
      "Epoch: 43, Samples: 5600/5760, Loss: 0.418254554271698\n",
      "Epoch: 43, Samples: 5632/5760, Loss: 0.2687886953353882\n",
      "Epoch: 43, Samples: 5664/5760, Loss: 0.17715556919574738\n",
      "Epoch: 43, Samples: 5696/5760, Loss: 0.44522157311439514\n",
      "Epoch: 43, Samples: 5728/5760, Loss: 1.0706137418746948\n",
      "\n",
      "Epoch: 43\n",
      "Training set: Average loss: 0.3743\n",
      "Validation set: Average loss: 1.4815, Accuracy: 526/818 (64%)\n",
      "Epoch: 44, Samples: 0/5760, Loss: 0.22943641245365143\n",
      "Epoch: 44, Samples: 32/5760, Loss: 0.2859911322593689\n",
      "Epoch: 44, Samples: 64/5760, Loss: 0.34810563921928406\n",
      "Epoch: 44, Samples: 96/5760, Loss: 0.44225412607192993\n",
      "Epoch: 44, Samples: 128/5760, Loss: 0.17269925773143768\n",
      "Epoch: 44, Samples: 160/5760, Loss: 0.17700538039207458\n",
      "Epoch: 44, Samples: 192/5760, Loss: 0.2777267098426819\n",
      "Epoch: 44, Samples: 224/5760, Loss: 0.6112836003303528\n",
      "Epoch: 44, Samples: 256/5760, Loss: 0.23663286864757538\n",
      "Epoch: 44, Samples: 288/5760, Loss: 0.3874533176422119\n",
      "Epoch: 44, Samples: 320/5760, Loss: 0.31579047441482544\n",
      "Epoch: 44, Samples: 352/5760, Loss: 0.34461408853530884\n",
      "Epoch: 44, Samples: 384/5760, Loss: 0.3534442186355591\n",
      "Epoch: 44, Samples: 416/5760, Loss: 0.4826573133468628\n",
      "Epoch: 44, Samples: 448/5760, Loss: 0.2011149376630783\n",
      "Epoch: 44, Samples: 480/5760, Loss: 0.21453776955604553\n",
      "Epoch: 44, Samples: 512/5760, Loss: 0.35299617052078247\n",
      "Epoch: 44, Samples: 544/5760, Loss: 0.3646283745765686\n",
      "Epoch: 44, Samples: 576/5760, Loss: 0.492667555809021\n",
      "Epoch: 44, Samples: 608/5760, Loss: 0.6326040625572205\n",
      "Epoch: 44, Samples: 640/5760, Loss: 0.1891966164112091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Samples: 672/5760, Loss: 0.23576286435127258\n",
      "Epoch: 44, Samples: 704/5760, Loss: 0.5700305700302124\n",
      "Epoch: 44, Samples: 736/5760, Loss: 0.32956188917160034\n",
      "Epoch: 44, Samples: 768/5760, Loss: 0.26927050948143005\n",
      "Epoch: 44, Samples: 800/5760, Loss: 0.24586205184459686\n",
      "Epoch: 44, Samples: 832/5760, Loss: 0.41981983184814453\n",
      "Epoch: 44, Samples: 864/5760, Loss: 0.3149016797542572\n",
      "Epoch: 44, Samples: 896/5760, Loss: 0.44091567397117615\n",
      "Epoch: 44, Samples: 928/5760, Loss: 0.320144385099411\n",
      "Epoch: 44, Samples: 960/5760, Loss: 0.6035423874855042\n",
      "Epoch: 44, Samples: 992/5760, Loss: 0.3605923354625702\n",
      "Epoch: 44, Samples: 1024/5760, Loss: 0.3817027807235718\n",
      "Epoch: 44, Samples: 1056/5760, Loss: 0.4015420377254486\n",
      "Epoch: 44, Samples: 1088/5760, Loss: 0.31487801671028137\n",
      "Epoch: 44, Samples: 1120/5760, Loss: 0.2187689244747162\n",
      "Epoch: 44, Samples: 1152/5760, Loss: 0.26002007722854614\n",
      "Epoch: 44, Samples: 1184/5760, Loss: 0.4124632775783539\n",
      "Epoch: 44, Samples: 1216/5760, Loss: 0.16285908222198486\n",
      "Epoch: 44, Samples: 1248/5760, Loss: 0.6126380562782288\n",
      "Epoch: 44, Samples: 1280/5760, Loss: 0.39789995551109314\n",
      "Epoch: 44, Samples: 1312/5760, Loss: 0.5130308866500854\n",
      "Epoch: 44, Samples: 1344/5760, Loss: 0.411507785320282\n",
      "Epoch: 44, Samples: 1376/5760, Loss: 0.19262681901454926\n",
      "Epoch: 44, Samples: 1408/5760, Loss: 0.38506585359573364\n",
      "Epoch: 44, Samples: 1440/5760, Loss: 0.2534789741039276\n",
      "Epoch: 44, Samples: 1472/5760, Loss: 0.33567339181900024\n",
      "Epoch: 44, Samples: 1504/5760, Loss: 0.422728955745697\n",
      "Epoch: 44, Samples: 1536/5760, Loss: 0.3020328879356384\n",
      "Epoch: 44, Samples: 1568/5760, Loss: 0.460052490234375\n",
      "Epoch: 44, Samples: 1600/5760, Loss: 0.4030318856239319\n",
      "Epoch: 44, Samples: 1632/5760, Loss: 0.44051697850227356\n",
      "Epoch: 44, Samples: 1664/5760, Loss: 0.14154404401779175\n",
      "Epoch: 44, Samples: 1696/5760, Loss: 0.2889665961265564\n",
      "Epoch: 44, Samples: 1728/5760, Loss: 0.387137234210968\n",
      "Epoch: 44, Samples: 1760/5760, Loss: 0.35278692841529846\n",
      "Epoch: 44, Samples: 1792/5760, Loss: 0.1719571202993393\n",
      "Epoch: 44, Samples: 1824/5760, Loss: 0.3608956038951874\n",
      "Epoch: 44, Samples: 1856/5760, Loss: 0.4340776205062866\n",
      "Epoch: 44, Samples: 1888/5760, Loss: 0.16755624115467072\n",
      "Epoch: 44, Samples: 1920/5760, Loss: 0.45958638191223145\n",
      "Epoch: 44, Samples: 1952/5760, Loss: 0.28312012553215027\n",
      "Epoch: 44, Samples: 1984/5760, Loss: 0.5010559558868408\n",
      "Epoch: 44, Samples: 2016/5760, Loss: 0.2707062363624573\n",
      "Epoch: 44, Samples: 2048/5760, Loss: 0.38692429661750793\n",
      "Epoch: 44, Samples: 2080/5760, Loss: 0.4791780114173889\n",
      "Epoch: 44, Samples: 2112/5760, Loss: 0.3249775171279907\n",
      "Epoch: 44, Samples: 2144/5760, Loss: 0.2840442657470703\n",
      "Epoch: 44, Samples: 2176/5760, Loss: 0.3614308536052704\n",
      "Epoch: 44, Samples: 2208/5760, Loss: 0.5702809691429138\n",
      "Epoch: 44, Samples: 2240/5760, Loss: 0.3048722445964813\n",
      "Epoch: 44, Samples: 2272/5760, Loss: 0.3468862771987915\n",
      "Epoch: 44, Samples: 2304/5760, Loss: 0.23976898193359375\n",
      "Epoch: 44, Samples: 2336/5760, Loss: 0.41312670707702637\n",
      "Epoch: 44, Samples: 2368/5760, Loss: 0.2690865993499756\n",
      "Epoch: 44, Samples: 2400/5760, Loss: 0.5782699584960938\n",
      "Epoch: 44, Samples: 2432/5760, Loss: 0.3894164264202118\n",
      "Epoch: 44, Samples: 2464/5760, Loss: 0.38039281964302063\n",
      "Epoch: 44, Samples: 2496/5760, Loss: 0.5754992961883545\n",
      "Epoch: 44, Samples: 2528/5760, Loss: 0.26533403992652893\n",
      "Epoch: 44, Samples: 2560/5760, Loss: 0.23032966256141663\n",
      "Epoch: 44, Samples: 2592/5760, Loss: 0.40621140599250793\n",
      "Epoch: 44, Samples: 2624/5760, Loss: 0.2995006740093231\n",
      "Epoch: 44, Samples: 2656/5760, Loss: 0.2197902947664261\n",
      "Epoch: 44, Samples: 2688/5760, Loss: 0.35267722606658936\n",
      "Epoch: 44, Samples: 2720/5760, Loss: 0.4407406151294708\n",
      "Epoch: 44, Samples: 2752/5760, Loss: 0.20422987639904022\n",
      "Epoch: 44, Samples: 2784/5760, Loss: 0.5057963132858276\n",
      "Epoch: 44, Samples: 2816/5760, Loss: 0.18761050701141357\n",
      "Epoch: 44, Samples: 2848/5760, Loss: 0.29476863145828247\n",
      "Epoch: 44, Samples: 2880/5760, Loss: 0.27560168504714966\n",
      "Epoch: 44, Samples: 2912/5760, Loss: 0.25574928522109985\n",
      "Epoch: 44, Samples: 2944/5760, Loss: 0.31999579071998596\n",
      "Epoch: 44, Samples: 2976/5760, Loss: 0.6443325281143188\n",
      "Epoch: 44, Samples: 3008/5760, Loss: 0.3133034408092499\n",
      "Epoch: 44, Samples: 3040/5760, Loss: 0.23711219429969788\n",
      "Epoch: 44, Samples: 3072/5760, Loss: 0.3720995783805847\n",
      "Epoch: 44, Samples: 3104/5760, Loss: 0.4070442020893097\n",
      "Epoch: 44, Samples: 3136/5760, Loss: 0.4560093581676483\n",
      "Epoch: 44, Samples: 3168/5760, Loss: 0.45722275972366333\n",
      "Epoch: 44, Samples: 3200/5760, Loss: 0.26742732524871826\n",
      "Epoch: 44, Samples: 3232/5760, Loss: 0.24126170575618744\n",
      "Epoch: 44, Samples: 3264/5760, Loss: 0.26563143730163574\n",
      "Epoch: 44, Samples: 3296/5760, Loss: 0.37087124586105347\n",
      "Epoch: 44, Samples: 3328/5760, Loss: 0.3721899688243866\n",
      "Epoch: 44, Samples: 3360/5760, Loss: 0.34633201360702515\n",
      "Epoch: 44, Samples: 3392/5760, Loss: 0.35001900792121887\n",
      "Epoch: 44, Samples: 3424/5760, Loss: 0.1492946892976761\n",
      "Epoch: 44, Samples: 3456/5760, Loss: 0.2964654564857483\n",
      "Epoch: 44, Samples: 3488/5760, Loss: 0.3416784405708313\n",
      "Epoch: 44, Samples: 3520/5760, Loss: 0.31252139806747437\n",
      "Epoch: 44, Samples: 3552/5760, Loss: 0.26982009410858154\n",
      "Epoch: 44, Samples: 3584/5760, Loss: 0.2862018346786499\n",
      "Epoch: 44, Samples: 3616/5760, Loss: 0.5449510216712952\n",
      "Epoch: 44, Samples: 3648/5760, Loss: 0.2985544204711914\n",
      "Epoch: 44, Samples: 3680/5760, Loss: 0.2465265840291977\n",
      "Epoch: 44, Samples: 3712/5760, Loss: 0.38819804787635803\n",
      "Epoch: 44, Samples: 3744/5760, Loss: 0.32482269406318665\n",
      "Epoch: 44, Samples: 3776/5760, Loss: 0.2857031226158142\n",
      "Epoch: 44, Samples: 3808/5760, Loss: 0.335834801197052\n",
      "Epoch: 44, Samples: 3840/5760, Loss: 0.32783079147338867\n",
      "Epoch: 44, Samples: 3872/5760, Loss: 0.3129483461380005\n",
      "Epoch: 44, Samples: 3904/5760, Loss: 0.3601889908313751\n",
      "Epoch: 44, Samples: 3936/5760, Loss: 0.3723694086074829\n",
      "Epoch: 44, Samples: 3968/5760, Loss: 0.44639670848846436\n",
      "Epoch: 44, Samples: 4000/5760, Loss: 0.38273900747299194\n",
      "Epoch: 44, Samples: 4032/5760, Loss: 0.24948862195014954\n",
      "Epoch: 44, Samples: 4064/5760, Loss: 0.741320013999939\n",
      "Epoch: 44, Samples: 4096/5760, Loss: 0.1652766615152359\n",
      "Epoch: 44, Samples: 4128/5760, Loss: 0.3011825680732727\n",
      "Epoch: 44, Samples: 4160/5760, Loss: 0.37735995650291443\n",
      "Epoch: 44, Samples: 4192/5760, Loss: 0.19967277348041534\n",
      "Epoch: 44, Samples: 4224/5760, Loss: 0.3597196638584137\n",
      "Epoch: 44, Samples: 4256/5760, Loss: 0.3077618181705475\n",
      "Epoch: 44, Samples: 4288/5760, Loss: 0.26394328474998474\n",
      "Epoch: 44, Samples: 4320/5760, Loss: 0.41780078411102295\n",
      "Epoch: 44, Samples: 4352/5760, Loss: 0.16901397705078125\n",
      "Epoch: 44, Samples: 4384/5760, Loss: 0.3098409175872803\n",
      "Epoch: 44, Samples: 4416/5760, Loss: 0.3823154866695404\n",
      "Epoch: 44, Samples: 4448/5760, Loss: 0.6947081089019775\n",
      "Epoch: 44, Samples: 4480/5760, Loss: 0.48100027441978455\n",
      "Epoch: 44, Samples: 4512/5760, Loss: 0.33230069279670715\n",
      "Epoch: 44, Samples: 4544/5760, Loss: 0.3290543854236603\n",
      "Epoch: 44, Samples: 4576/5760, Loss: 0.4683092534542084\n",
      "Epoch: 44, Samples: 4608/5760, Loss: 0.417849600315094\n",
      "Epoch: 44, Samples: 4640/5760, Loss: 0.2156432867050171\n",
      "Epoch: 44, Samples: 4672/5760, Loss: 0.7033566236495972\n",
      "Epoch: 44, Samples: 4704/5760, Loss: 0.17389178276062012\n",
      "Epoch: 44, Samples: 4736/5760, Loss: 0.2693396508693695\n",
      "Epoch: 44, Samples: 4768/5760, Loss: 0.4166457951068878\n",
      "Epoch: 44, Samples: 4800/5760, Loss: 0.2780725955963135\n",
      "Epoch: 44, Samples: 4832/5760, Loss: 0.3174676299095154\n",
      "Epoch: 44, Samples: 4864/5760, Loss: 0.47563350200653076\n",
      "Epoch: 44, Samples: 4896/5760, Loss: 0.26501256227493286\n",
      "Epoch: 44, Samples: 4928/5760, Loss: 0.39968743920326233\n",
      "Epoch: 44, Samples: 4960/5760, Loss: 0.2982722222805023\n",
      "Epoch: 44, Samples: 4992/5760, Loss: 0.46898770332336426\n",
      "Epoch: 44, Samples: 5024/5760, Loss: 0.40114396810531616\n",
      "Epoch: 44, Samples: 5056/5760, Loss: 0.3508011996746063\n",
      "Epoch: 44, Samples: 5088/5760, Loss: 0.5093904733657837\n",
      "Epoch: 44, Samples: 5120/5760, Loss: 0.40138429403305054\n",
      "Epoch: 44, Samples: 5152/5760, Loss: 0.308810293674469\n",
      "Epoch: 44, Samples: 5184/5760, Loss: 0.2558256983757019\n",
      "Epoch: 44, Samples: 5216/5760, Loss: 0.21282948553562164\n",
      "Epoch: 44, Samples: 5248/5760, Loss: 0.41904106736183167\n",
      "Epoch: 44, Samples: 5280/5760, Loss: 0.3522151708602905\n",
      "Epoch: 44, Samples: 5312/5760, Loss: 0.8696852326393127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Samples: 5344/5760, Loss: 0.19644390046596527\n",
      "Epoch: 44, Samples: 5376/5760, Loss: 0.19449833035469055\n",
      "Epoch: 44, Samples: 5408/5760, Loss: 0.4602167308330536\n",
      "Epoch: 44, Samples: 5440/5760, Loss: 0.26374390721321106\n",
      "Epoch: 44, Samples: 5472/5760, Loss: 0.46203339099884033\n",
      "Epoch: 44, Samples: 5504/5760, Loss: 0.24424736201763153\n",
      "Epoch: 44, Samples: 5536/5760, Loss: 0.3075125515460968\n",
      "Epoch: 44, Samples: 5568/5760, Loss: 0.44568634033203125\n",
      "Epoch: 44, Samples: 5600/5760, Loss: 0.1909167319536209\n",
      "Epoch: 44, Samples: 5632/5760, Loss: 0.24882502853870392\n",
      "Epoch: 44, Samples: 5664/5760, Loss: 0.29606831073760986\n",
      "Epoch: 44, Samples: 5696/5760, Loss: 0.4114128351211548\n",
      "Epoch: 44, Samples: 5728/5760, Loss: 2.3692784309387207\n",
      "\n",
      "Epoch: 44\n",
      "Training set: Average loss: 0.3627\n",
      "Validation set: Average loss: 1.5330, Accuracy: 521/818 (64%)\n",
      "Epoch: 45, Samples: 0/5760, Loss: 0.5753682851791382\n",
      "Epoch: 45, Samples: 32/5760, Loss: 0.28685104846954346\n",
      "Epoch: 45, Samples: 64/5760, Loss: 0.3964850902557373\n",
      "Epoch: 45, Samples: 96/5760, Loss: 0.25244802236557007\n",
      "Epoch: 45, Samples: 128/5760, Loss: 0.7331252098083496\n",
      "Epoch: 45, Samples: 160/5760, Loss: 0.5594797134399414\n",
      "Epoch: 45, Samples: 192/5760, Loss: 0.5955151915550232\n",
      "Epoch: 45, Samples: 224/5760, Loss: 0.5161349177360535\n",
      "Epoch: 45, Samples: 256/5760, Loss: 0.4043703079223633\n",
      "Epoch: 45, Samples: 288/5760, Loss: 0.6443580389022827\n",
      "Epoch: 45, Samples: 320/5760, Loss: 0.4530918300151825\n",
      "Epoch: 45, Samples: 352/5760, Loss: 0.42405965924263\n",
      "Epoch: 45, Samples: 384/5760, Loss: 0.7071325778961182\n",
      "Epoch: 45, Samples: 416/5760, Loss: 0.5678061246871948\n",
      "Epoch: 45, Samples: 448/5760, Loss: 0.32600876688957214\n",
      "Epoch: 45, Samples: 480/5760, Loss: 0.6401425004005432\n",
      "Epoch: 45, Samples: 512/5760, Loss: 0.35877394676208496\n",
      "Epoch: 45, Samples: 544/5760, Loss: 0.46540021896362305\n",
      "Epoch: 45, Samples: 576/5760, Loss: 0.7157850861549377\n",
      "Epoch: 45, Samples: 608/5760, Loss: 0.2840151786804199\n",
      "Epoch: 45, Samples: 640/5760, Loss: 0.4351002275943756\n",
      "Epoch: 45, Samples: 672/5760, Loss: 0.4709940552711487\n",
      "Epoch: 45, Samples: 704/5760, Loss: 0.3245817720890045\n",
      "Epoch: 45, Samples: 736/5760, Loss: 0.317361444234848\n",
      "Epoch: 45, Samples: 768/5760, Loss: 0.34979310631752014\n",
      "Epoch: 45, Samples: 800/5760, Loss: 0.29825150966644287\n",
      "Epoch: 45, Samples: 832/5760, Loss: 0.3412308394908905\n",
      "Epoch: 45, Samples: 864/5760, Loss: 0.2526680827140808\n",
      "Epoch: 45, Samples: 896/5760, Loss: 0.23209555447101593\n",
      "Epoch: 45, Samples: 928/5760, Loss: 0.3553442656993866\n",
      "Epoch: 45, Samples: 960/5760, Loss: 0.29173269867897034\n",
      "Epoch: 45, Samples: 992/5760, Loss: 0.34618744254112244\n",
      "Epoch: 45, Samples: 1024/5760, Loss: 0.2038448601961136\n",
      "Epoch: 45, Samples: 1056/5760, Loss: 0.5270053148269653\n",
      "Epoch: 45, Samples: 1088/5760, Loss: 0.43405941128730774\n",
      "Epoch: 45, Samples: 1120/5760, Loss: 0.26134875416755676\n",
      "Epoch: 45, Samples: 1152/5760, Loss: 0.3878791630268097\n",
      "Epoch: 45, Samples: 1184/5760, Loss: 0.15562032163143158\n",
      "Epoch: 45, Samples: 1216/5760, Loss: 0.22455112636089325\n",
      "Epoch: 45, Samples: 1248/5760, Loss: 0.397856205701828\n",
      "Epoch: 45, Samples: 1280/5760, Loss: 0.37562042474746704\n",
      "Epoch: 45, Samples: 1312/5760, Loss: 0.21426963806152344\n",
      "Epoch: 45, Samples: 1344/5760, Loss: 0.1765414923429489\n",
      "Epoch: 45, Samples: 1376/5760, Loss: 0.2710074782371521\n",
      "Epoch: 45, Samples: 1408/5760, Loss: 0.47900819778442383\n",
      "Epoch: 45, Samples: 1440/5760, Loss: 0.49456313252449036\n",
      "Epoch: 45, Samples: 1472/5760, Loss: 0.3512622117996216\n",
      "Epoch: 45, Samples: 1504/5760, Loss: 0.21663011610507965\n",
      "Epoch: 45, Samples: 1536/5760, Loss: 0.4811103940010071\n",
      "Epoch: 45, Samples: 1568/5760, Loss: 0.26098915934562683\n",
      "Epoch: 45, Samples: 1600/5760, Loss: 0.3269016742706299\n",
      "Epoch: 45, Samples: 1632/5760, Loss: 0.34353888034820557\n",
      "Epoch: 45, Samples: 1664/5760, Loss: 0.23245586454868317\n",
      "Epoch: 45, Samples: 1696/5760, Loss: 0.2594735026359558\n",
      "Epoch: 45, Samples: 1728/5760, Loss: 0.37585893273353577\n",
      "Epoch: 45, Samples: 1760/5760, Loss: 0.26672840118408203\n",
      "Epoch: 45, Samples: 1792/5760, Loss: 0.3101575970649719\n",
      "Epoch: 45, Samples: 1824/5760, Loss: 0.2455626279115677\n",
      "Epoch: 45, Samples: 1856/5760, Loss: 0.2429407238960266\n",
      "Epoch: 45, Samples: 1888/5760, Loss: 0.43601682782173157\n",
      "Epoch: 45, Samples: 1920/5760, Loss: 0.22280994057655334\n",
      "Epoch: 45, Samples: 1952/5760, Loss: 0.2700309157371521\n",
      "Epoch: 45, Samples: 1984/5760, Loss: 0.25452232360839844\n",
      "Epoch: 45, Samples: 2016/5760, Loss: 0.25008243322372437\n",
      "Epoch: 45, Samples: 2048/5760, Loss: 0.4065864086151123\n",
      "Epoch: 45, Samples: 2080/5760, Loss: 0.4007904827594757\n",
      "Epoch: 45, Samples: 2112/5760, Loss: 0.3314650058746338\n",
      "Epoch: 45, Samples: 2144/5760, Loss: 0.3478824496269226\n",
      "Epoch: 45, Samples: 2176/5760, Loss: 0.3654930293560028\n",
      "Epoch: 45, Samples: 2208/5760, Loss: 0.41700074076652527\n",
      "Epoch: 45, Samples: 2240/5760, Loss: 0.6750929951667786\n",
      "Epoch: 45, Samples: 2272/5760, Loss: 0.584936261177063\n",
      "Epoch: 45, Samples: 2304/5760, Loss: 0.13072539865970612\n",
      "Epoch: 45, Samples: 2336/5760, Loss: 0.43604663014411926\n",
      "Epoch: 45, Samples: 2368/5760, Loss: 0.2825464606285095\n",
      "Epoch: 45, Samples: 2400/5760, Loss: 0.4149594008922577\n",
      "Epoch: 45, Samples: 2432/5760, Loss: 0.21127477288246155\n",
      "Epoch: 45, Samples: 2464/5760, Loss: 0.33164161443710327\n",
      "Epoch: 45, Samples: 2496/5760, Loss: 0.22950786352157593\n",
      "Epoch: 45, Samples: 2528/5760, Loss: 0.2950209975242615\n",
      "Epoch: 45, Samples: 2560/5760, Loss: 0.3735828697681427\n",
      "Epoch: 45, Samples: 2592/5760, Loss: 0.12237650156021118\n",
      "Epoch: 45, Samples: 2624/5760, Loss: 0.17602407932281494\n",
      "Epoch: 45, Samples: 2656/5760, Loss: 0.2369791567325592\n",
      "Epoch: 45, Samples: 2688/5760, Loss: 0.45352864265441895\n",
      "Epoch: 45, Samples: 2720/5760, Loss: 0.22636252641677856\n",
      "Epoch: 45, Samples: 2752/5760, Loss: 0.20991986989974976\n",
      "Epoch: 45, Samples: 2784/5760, Loss: 0.24144887924194336\n",
      "Epoch: 45, Samples: 2816/5760, Loss: 0.37419161200523376\n",
      "Epoch: 45, Samples: 2848/5760, Loss: 0.2815527319908142\n",
      "Epoch: 45, Samples: 2880/5760, Loss: 0.338116317987442\n",
      "Epoch: 45, Samples: 2912/5760, Loss: 0.2562111020088196\n",
      "Epoch: 45, Samples: 2944/5760, Loss: 0.24750497937202454\n",
      "Epoch: 45, Samples: 2976/5760, Loss: 0.3287048637866974\n",
      "Epoch: 45, Samples: 3008/5760, Loss: 0.2990839183330536\n",
      "Epoch: 45, Samples: 3040/5760, Loss: 0.4851231575012207\n",
      "Epoch: 45, Samples: 3072/5760, Loss: 0.2710391879081726\n",
      "Epoch: 45, Samples: 3104/5760, Loss: 0.4271596372127533\n",
      "Epoch: 45, Samples: 3136/5760, Loss: 0.18212169408798218\n",
      "Epoch: 45, Samples: 3168/5760, Loss: 0.308575302362442\n",
      "Epoch: 45, Samples: 3200/5760, Loss: 0.31398722529411316\n",
      "Epoch: 45, Samples: 3232/5760, Loss: 0.44846850633621216\n",
      "Epoch: 45, Samples: 3264/5760, Loss: 0.42380279302597046\n",
      "Epoch: 45, Samples: 3296/5760, Loss: 0.3180769383907318\n",
      "Epoch: 45, Samples: 3328/5760, Loss: 0.36416783928871155\n",
      "Epoch: 45, Samples: 3360/5760, Loss: 0.3952943980693817\n",
      "Epoch: 45, Samples: 3392/5760, Loss: 0.1925651729106903\n",
      "Epoch: 45, Samples: 3424/5760, Loss: 0.6774332523345947\n",
      "Epoch: 45, Samples: 3456/5760, Loss: 0.2772865295410156\n",
      "Epoch: 45, Samples: 3488/5760, Loss: 0.29569315910339355\n",
      "Epoch: 45, Samples: 3520/5760, Loss: 0.20440265536308289\n",
      "Epoch: 45, Samples: 3552/5760, Loss: 0.47197967767715454\n",
      "Epoch: 45, Samples: 3584/5760, Loss: 0.2803710103034973\n",
      "Epoch: 45, Samples: 3616/5760, Loss: 0.15203656256198883\n",
      "Epoch: 45, Samples: 3648/5760, Loss: 0.24376744031906128\n",
      "Epoch: 45, Samples: 3680/5760, Loss: 0.5482602119445801\n",
      "Epoch: 45, Samples: 3712/5760, Loss: 0.20795129239559174\n",
      "Epoch: 45, Samples: 3744/5760, Loss: 0.24613675475120544\n",
      "Epoch: 45, Samples: 3776/5760, Loss: 0.26984745264053345\n",
      "Epoch: 45, Samples: 3808/5760, Loss: 0.31500765681266785\n",
      "Epoch: 45, Samples: 3840/5760, Loss: 0.5994560122489929\n",
      "Epoch: 45, Samples: 3872/5760, Loss: 0.25718769431114197\n",
      "Epoch: 45, Samples: 3904/5760, Loss: 0.22644378244876862\n",
      "Epoch: 45, Samples: 3936/5760, Loss: 0.3513157069683075\n",
      "Epoch: 45, Samples: 3968/5760, Loss: 0.27469050884246826\n",
      "Epoch: 45, Samples: 4000/5760, Loss: 0.4119219481945038\n",
      "Epoch: 45, Samples: 4032/5760, Loss: 0.257724404335022\n",
      "Epoch: 45, Samples: 4064/5760, Loss: 0.2195081114768982\n",
      "Epoch: 45, Samples: 4096/5760, Loss: 0.3038950264453888\n",
      "Epoch: 45, Samples: 4128/5760, Loss: 0.4239298105239868\n",
      "Epoch: 45, Samples: 4160/5760, Loss: 0.39352214336395264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45, Samples: 4192/5760, Loss: 0.19541409611701965\n",
      "Epoch: 45, Samples: 4224/5760, Loss: 0.35312578082084656\n",
      "Epoch: 45, Samples: 4256/5760, Loss: 0.28446894884109497\n",
      "Epoch: 45, Samples: 4288/5760, Loss: 0.26991090178489685\n",
      "Epoch: 45, Samples: 4320/5760, Loss: 0.28802210092544556\n",
      "Epoch: 45, Samples: 4352/5760, Loss: 0.17788299918174744\n",
      "Epoch: 45, Samples: 4384/5760, Loss: 0.36964693665504456\n",
      "Epoch: 45, Samples: 4416/5760, Loss: 0.14654365181922913\n",
      "Epoch: 45, Samples: 4448/5760, Loss: 0.15865802764892578\n",
      "Epoch: 45, Samples: 4480/5760, Loss: 0.3145998418331146\n",
      "Epoch: 45, Samples: 4512/5760, Loss: 0.3405226469039917\n",
      "Epoch: 45, Samples: 4544/5760, Loss: 0.317283570766449\n",
      "Epoch: 45, Samples: 4576/5760, Loss: 0.19226346909999847\n",
      "Epoch: 45, Samples: 4608/5760, Loss: 0.15217046439647675\n",
      "Epoch: 45, Samples: 4640/5760, Loss: 0.2814258933067322\n",
      "Epoch: 45, Samples: 4672/5760, Loss: 0.25626763701438904\n",
      "Epoch: 45, Samples: 4704/5760, Loss: 0.5323807001113892\n",
      "Epoch: 45, Samples: 4736/5760, Loss: 0.3110794126987457\n",
      "Epoch: 45, Samples: 4768/5760, Loss: 0.4120219349861145\n",
      "Epoch: 45, Samples: 4800/5760, Loss: 0.259162575006485\n",
      "Epoch: 45, Samples: 4832/5760, Loss: 0.18124797940254211\n",
      "Epoch: 45, Samples: 4864/5760, Loss: 0.2060912549495697\n",
      "Epoch: 45, Samples: 4896/5760, Loss: 0.3136168122291565\n",
      "Epoch: 45, Samples: 4928/5760, Loss: 0.2707820534706116\n",
      "Epoch: 45, Samples: 4960/5760, Loss: 0.2544706165790558\n",
      "Epoch: 45, Samples: 4992/5760, Loss: 0.2518959939479828\n",
      "Epoch: 45, Samples: 5024/5760, Loss: 0.17552895843982697\n",
      "Epoch: 45, Samples: 5056/5760, Loss: 0.23243726789951324\n",
      "Epoch: 45, Samples: 5088/5760, Loss: 0.20823420584201813\n",
      "Epoch: 45, Samples: 5120/5760, Loss: 0.39527627825737\n",
      "Epoch: 45, Samples: 5152/5760, Loss: 0.1843685805797577\n",
      "Epoch: 45, Samples: 5184/5760, Loss: 0.24739080667495728\n",
      "Epoch: 45, Samples: 5216/5760, Loss: 0.22080962359905243\n",
      "Epoch: 45, Samples: 5248/5760, Loss: 0.1544133722782135\n",
      "Epoch: 45, Samples: 5280/5760, Loss: 0.33372095227241516\n",
      "Epoch: 45, Samples: 5312/5760, Loss: 0.2936861217021942\n",
      "Epoch: 45, Samples: 5344/5760, Loss: 0.17891250550746918\n",
      "Epoch: 45, Samples: 5376/5760, Loss: 0.2036774605512619\n",
      "Epoch: 45, Samples: 5408/5760, Loss: 0.2066929042339325\n",
      "Epoch: 45, Samples: 5440/5760, Loss: 0.3201214671134949\n",
      "Epoch: 45, Samples: 5472/5760, Loss: 0.3572350740432739\n",
      "Epoch: 45, Samples: 5504/5760, Loss: 0.29879671335220337\n",
      "Epoch: 45, Samples: 5536/5760, Loss: 0.24787993729114532\n",
      "Epoch: 45, Samples: 5568/5760, Loss: 0.1345190703868866\n",
      "Epoch: 45, Samples: 5600/5760, Loss: 0.1969621330499649\n",
      "Epoch: 45, Samples: 5632/5760, Loss: 0.32947295904159546\n",
      "Epoch: 45, Samples: 5664/5760, Loss: 0.3436189591884613\n",
      "Epoch: 45, Samples: 5696/5760, Loss: 0.5549436211585999\n",
      "Epoch: 45, Samples: 5728/5760, Loss: 1.232865333557129\n",
      "\n",
      "Epoch: 45\n",
      "Training set: Average loss: 0.3338\n",
      "Validation set: Average loss: 1.3952, Accuracy: 540/818 (66%)\n",
      "Saving model (epoch 45) with lowest validation loss: 1.395247207238124\n",
      "Epoch: 46, Samples: 0/5760, Loss: 0.1550590842962265\n",
      "Epoch: 46, Samples: 32/5760, Loss: 0.32318297028541565\n",
      "Epoch: 46, Samples: 64/5760, Loss: 0.31016117334365845\n",
      "Epoch: 46, Samples: 96/5760, Loss: 0.3369311988353729\n",
      "Epoch: 46, Samples: 128/5760, Loss: 0.29558265209198\n",
      "Epoch: 46, Samples: 160/5760, Loss: 0.21900953352451324\n",
      "Epoch: 46, Samples: 192/5760, Loss: 0.22328568994998932\n",
      "Epoch: 46, Samples: 224/5760, Loss: 0.3008608818054199\n",
      "Epoch: 46, Samples: 256/5760, Loss: 0.28379830718040466\n",
      "Epoch: 46, Samples: 288/5760, Loss: 0.20157749950885773\n",
      "Epoch: 46, Samples: 320/5760, Loss: 0.329976886510849\n",
      "Epoch: 46, Samples: 352/5760, Loss: 0.2005837857723236\n",
      "Epoch: 46, Samples: 384/5760, Loss: 0.13077905774116516\n",
      "Epoch: 46, Samples: 416/5760, Loss: 0.3798036277294159\n",
      "Epoch: 46, Samples: 448/5760, Loss: 0.3864494860172272\n",
      "Epoch: 46, Samples: 480/5760, Loss: 0.2874859571456909\n",
      "Epoch: 46, Samples: 512/5760, Loss: 0.22939349710941315\n",
      "Epoch: 46, Samples: 544/5760, Loss: 0.42440149188041687\n",
      "Epoch: 46, Samples: 576/5760, Loss: 0.20314040780067444\n",
      "Epoch: 46, Samples: 608/5760, Loss: 0.22736993432044983\n",
      "Epoch: 46, Samples: 640/5760, Loss: 0.24122731387615204\n",
      "Epoch: 46, Samples: 672/5760, Loss: 0.24341918528079987\n",
      "Epoch: 46, Samples: 704/5760, Loss: 0.23343653976917267\n",
      "Epoch: 46, Samples: 736/5760, Loss: 0.3752029836177826\n",
      "Epoch: 46, Samples: 768/5760, Loss: 0.19527947902679443\n",
      "Epoch: 46, Samples: 800/5760, Loss: 0.24526694416999817\n",
      "Epoch: 46, Samples: 832/5760, Loss: 0.2600923776626587\n",
      "Epoch: 46, Samples: 864/5760, Loss: 0.45434632897377014\n",
      "Epoch: 46, Samples: 896/5760, Loss: 0.305664598941803\n",
      "Epoch: 46, Samples: 928/5760, Loss: 0.20006099343299866\n",
      "Epoch: 46, Samples: 960/5760, Loss: 0.13827472925186157\n",
      "Epoch: 46, Samples: 992/5760, Loss: 0.3195168375968933\n",
      "Epoch: 46, Samples: 1024/5760, Loss: 0.3796371817588806\n",
      "Epoch: 46, Samples: 1056/5760, Loss: 0.29172104597091675\n",
      "Epoch: 46, Samples: 1088/5760, Loss: 0.21481835842132568\n",
      "Epoch: 46, Samples: 1120/5760, Loss: 0.2619587481021881\n",
      "Epoch: 46, Samples: 1152/5760, Loss: 0.1562773585319519\n",
      "Epoch: 46, Samples: 1184/5760, Loss: 0.23346678912639618\n",
      "Epoch: 46, Samples: 1216/5760, Loss: 0.3813774585723877\n",
      "Epoch: 46, Samples: 1248/5760, Loss: 0.3814326524734497\n",
      "Epoch: 46, Samples: 1280/5760, Loss: 0.35366594791412354\n",
      "Epoch: 46, Samples: 1312/5760, Loss: 0.10896044969558716\n",
      "Epoch: 46, Samples: 1344/5760, Loss: 0.4133134186267853\n",
      "Epoch: 46, Samples: 1376/5760, Loss: 0.3682912588119507\n",
      "Epoch: 46, Samples: 1408/5760, Loss: 0.2829279601573944\n",
      "Epoch: 46, Samples: 1440/5760, Loss: 0.14121116697788239\n",
      "Epoch: 46, Samples: 1472/5760, Loss: 0.2566749155521393\n",
      "Epoch: 46, Samples: 1504/5760, Loss: 0.3001900911331177\n",
      "Epoch: 46, Samples: 1536/5760, Loss: 0.2485247403383255\n",
      "Epoch: 46, Samples: 1568/5760, Loss: 0.25509214401245117\n",
      "Epoch: 46, Samples: 1600/5760, Loss: 0.3065270483493805\n",
      "Epoch: 46, Samples: 1632/5760, Loss: 0.24310092628002167\n",
      "Epoch: 46, Samples: 1664/5760, Loss: 0.3237963616847992\n",
      "Epoch: 46, Samples: 1696/5760, Loss: 0.2701374590396881\n",
      "Epoch: 46, Samples: 1728/5760, Loss: 0.24263067543506622\n",
      "Epoch: 46, Samples: 1760/5760, Loss: 0.17438721656799316\n",
      "Epoch: 46, Samples: 1792/5760, Loss: 0.231817364692688\n",
      "Epoch: 46, Samples: 1824/5760, Loss: 0.30716946721076965\n",
      "Epoch: 46, Samples: 1856/5760, Loss: 0.232239231467247\n",
      "Epoch: 46, Samples: 1888/5760, Loss: 0.15575315058231354\n",
      "Epoch: 46, Samples: 1920/5760, Loss: 0.26329663395881653\n",
      "Epoch: 46, Samples: 1952/5760, Loss: 0.2153417319059372\n",
      "Epoch: 46, Samples: 1984/5760, Loss: 0.2812983989715576\n",
      "Epoch: 46, Samples: 2016/5760, Loss: 0.2916795313358307\n",
      "Epoch: 46, Samples: 2048/5760, Loss: 0.2795661985874176\n",
      "Epoch: 46, Samples: 2080/5760, Loss: 0.20712636411190033\n",
      "Epoch: 46, Samples: 2112/5760, Loss: 0.22831983864307404\n",
      "Epoch: 46, Samples: 2144/5760, Loss: 0.2152225375175476\n",
      "Epoch: 46, Samples: 2176/5760, Loss: 0.45277565717697144\n",
      "Epoch: 46, Samples: 2208/5760, Loss: 0.3159879148006439\n",
      "Epoch: 46, Samples: 2240/5760, Loss: 0.21300938725471497\n",
      "Epoch: 46, Samples: 2272/5760, Loss: 0.2517600655555725\n",
      "Epoch: 46, Samples: 2304/5760, Loss: 0.3913338780403137\n",
      "Epoch: 46, Samples: 2336/5760, Loss: 0.23399081826210022\n",
      "Epoch: 46, Samples: 2368/5760, Loss: 0.25053727626800537\n",
      "Epoch: 46, Samples: 2400/5760, Loss: 0.23016971349716187\n",
      "Epoch: 46, Samples: 2432/5760, Loss: 0.3237970173358917\n",
      "Epoch: 46, Samples: 2464/5760, Loss: 0.2937493622303009\n",
      "Epoch: 46, Samples: 2496/5760, Loss: 0.23962044715881348\n",
      "Epoch: 46, Samples: 2528/5760, Loss: 0.22741878032684326\n",
      "Epoch: 46, Samples: 2560/5760, Loss: 0.3680298626422882\n",
      "Epoch: 46, Samples: 2592/5760, Loss: 0.2431863397359848\n",
      "Epoch: 46, Samples: 2624/5760, Loss: 0.13155493140220642\n",
      "Epoch: 46, Samples: 2656/5760, Loss: 0.19096702337265015\n",
      "Epoch: 46, Samples: 2688/5760, Loss: 0.5523143410682678\n",
      "Epoch: 46, Samples: 2720/5760, Loss: 0.28333917260169983\n",
      "Epoch: 46, Samples: 2752/5760, Loss: 0.151199609041214\n",
      "Epoch: 46, Samples: 2784/5760, Loss: 0.23890402913093567\n",
      "Epoch: 46, Samples: 2816/5760, Loss: 0.3253640830516815\n",
      "Epoch: 46, Samples: 2848/5760, Loss: 0.24471329152584076\n",
      "Epoch: 46, Samples: 2880/5760, Loss: 0.20610593259334564\n",
      "Epoch: 46, Samples: 2912/5760, Loss: 0.2929067611694336\n",
      "Epoch: 46, Samples: 2944/5760, Loss: 0.3191182613372803\n",
      "Epoch: 46, Samples: 2976/5760, Loss: 0.1457432210445404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Samples: 3008/5760, Loss: 0.28039270639419556\n",
      "Epoch: 46, Samples: 3040/5760, Loss: 0.4305284917354584\n",
      "Epoch: 46, Samples: 3072/5760, Loss: 0.20914730429649353\n",
      "Epoch: 46, Samples: 3104/5760, Loss: 0.4416561722755432\n",
      "Epoch: 46, Samples: 3136/5760, Loss: 0.27737492322921753\n",
      "Epoch: 46, Samples: 3168/5760, Loss: 0.5244935154914856\n",
      "Epoch: 46, Samples: 3200/5760, Loss: 0.1591925173997879\n",
      "Epoch: 46, Samples: 3232/5760, Loss: 0.3596566319465637\n",
      "Epoch: 46, Samples: 3264/5760, Loss: 0.3757876753807068\n",
      "Epoch: 46, Samples: 3296/5760, Loss: 0.29393261671066284\n",
      "Epoch: 46, Samples: 3328/5760, Loss: 0.36086225509643555\n",
      "Epoch: 46, Samples: 3360/5760, Loss: 0.2551218569278717\n",
      "Epoch: 46, Samples: 3392/5760, Loss: 0.3155761957168579\n",
      "Epoch: 46, Samples: 3424/5760, Loss: 0.30625492334365845\n",
      "Epoch: 46, Samples: 3456/5760, Loss: 0.2010246217250824\n",
      "Epoch: 46, Samples: 3488/5760, Loss: 0.24729114770889282\n",
      "Epoch: 46, Samples: 3520/5760, Loss: 0.2509710192680359\n",
      "Epoch: 46, Samples: 3552/5760, Loss: 0.16718383133411407\n",
      "Epoch: 46, Samples: 3584/5760, Loss: 0.35357698798179626\n",
      "Epoch: 46, Samples: 3616/5760, Loss: 0.33083391189575195\n",
      "Epoch: 46, Samples: 3648/5760, Loss: 0.10996188223361969\n",
      "Epoch: 46, Samples: 3680/5760, Loss: 0.1986239105463028\n",
      "Epoch: 46, Samples: 3712/5760, Loss: 0.19398103654384613\n",
      "Epoch: 46, Samples: 3744/5760, Loss: 0.40559640526771545\n",
      "Epoch: 46, Samples: 3776/5760, Loss: 0.19345255196094513\n",
      "Epoch: 46, Samples: 3808/5760, Loss: 0.2282496988773346\n",
      "Epoch: 46, Samples: 3840/5760, Loss: 0.19566380977630615\n",
      "Epoch: 46, Samples: 3872/5760, Loss: 0.13629190623760223\n",
      "Epoch: 46, Samples: 3904/5760, Loss: 0.2950769066810608\n",
      "Epoch: 46, Samples: 3936/5760, Loss: 0.2530271112918854\n",
      "Epoch: 46, Samples: 3968/5760, Loss: 0.22839237749576569\n",
      "Epoch: 46, Samples: 4000/5760, Loss: 0.32572388648986816\n",
      "Epoch: 46, Samples: 4032/5760, Loss: 0.28765296936035156\n",
      "Epoch: 46, Samples: 4064/5760, Loss: 0.38636085391044617\n",
      "Epoch: 46, Samples: 4096/5760, Loss: 0.18093879520893097\n",
      "Epoch: 46, Samples: 4128/5760, Loss: 0.1903313845396042\n",
      "Epoch: 46, Samples: 4160/5760, Loss: 0.29380157589912415\n",
      "Epoch: 46, Samples: 4192/5760, Loss: 0.31583932042121887\n",
      "Epoch: 46, Samples: 4224/5760, Loss: 0.25029700994491577\n",
      "Epoch: 46, Samples: 4256/5760, Loss: 0.19031497836112976\n",
      "Epoch: 46, Samples: 4288/5760, Loss: 0.3607850670814514\n",
      "Epoch: 46, Samples: 4320/5760, Loss: 0.256470263004303\n",
      "Epoch: 46, Samples: 4352/5760, Loss: 0.14738167822360992\n",
      "Epoch: 46, Samples: 4384/5760, Loss: 0.26022985577583313\n",
      "Epoch: 46, Samples: 4416/5760, Loss: 0.48604631423950195\n",
      "Epoch: 46, Samples: 4448/5760, Loss: 0.25490301847457886\n",
      "Epoch: 46, Samples: 4480/5760, Loss: 0.24842357635498047\n",
      "Epoch: 46, Samples: 4512/5760, Loss: 0.3503759503364563\n",
      "Epoch: 46, Samples: 4544/5760, Loss: 0.21927165985107422\n",
      "Epoch: 46, Samples: 4576/5760, Loss: 0.22430995106697083\n",
      "Epoch: 46, Samples: 4608/5760, Loss: 0.2243434637784958\n",
      "Epoch: 46, Samples: 4640/5760, Loss: 0.3480275869369507\n",
      "Epoch: 46, Samples: 4672/5760, Loss: 0.425133615732193\n",
      "Epoch: 46, Samples: 4704/5760, Loss: 0.2533184289932251\n",
      "Epoch: 46, Samples: 4736/5760, Loss: 0.1495486944913864\n",
      "Epoch: 46, Samples: 4768/5760, Loss: 0.36586838960647583\n",
      "Epoch: 46, Samples: 4800/5760, Loss: 0.3832772970199585\n",
      "Epoch: 46, Samples: 4832/5760, Loss: 0.23020632565021515\n",
      "Epoch: 46, Samples: 4864/5760, Loss: 0.24902474880218506\n",
      "Epoch: 46, Samples: 4896/5760, Loss: 0.22668956220149994\n",
      "Epoch: 46, Samples: 4928/5760, Loss: 0.18729174137115479\n",
      "Epoch: 46, Samples: 4960/5760, Loss: 0.20290151238441467\n",
      "Epoch: 46, Samples: 4992/5760, Loss: 0.17153051495552063\n",
      "Epoch: 46, Samples: 5024/5760, Loss: 0.23033107817173004\n",
      "Epoch: 46, Samples: 5056/5760, Loss: 0.13854975998401642\n",
      "Epoch: 46, Samples: 5088/5760, Loss: 0.24440079927444458\n",
      "Epoch: 46, Samples: 5120/5760, Loss: 0.1820041686296463\n",
      "Epoch: 46, Samples: 5152/5760, Loss: 0.1379348784685135\n",
      "Epoch: 46, Samples: 5184/5760, Loss: 0.31363457441329956\n",
      "Epoch: 46, Samples: 5216/5760, Loss: 0.2603798806667328\n",
      "Epoch: 46, Samples: 5248/5760, Loss: 0.2819623649120331\n",
      "Epoch: 46, Samples: 5280/5760, Loss: 0.35106804966926575\n",
      "Epoch: 46, Samples: 5312/5760, Loss: 0.36486271023750305\n",
      "Epoch: 46, Samples: 5344/5760, Loss: 0.2720254063606262\n",
      "Epoch: 46, Samples: 5376/5760, Loss: 0.27708280086517334\n",
      "Epoch: 46, Samples: 5408/5760, Loss: 0.24735026061534882\n",
      "Epoch: 46, Samples: 5440/5760, Loss: 0.22674551606178284\n",
      "Epoch: 46, Samples: 5472/5760, Loss: 0.2407052218914032\n",
      "Epoch: 46, Samples: 5504/5760, Loss: 0.4050939977169037\n",
      "Epoch: 46, Samples: 5536/5760, Loss: 0.43972375988960266\n",
      "Epoch: 46, Samples: 5568/5760, Loss: 0.14975015819072723\n",
      "Epoch: 46, Samples: 5600/5760, Loss: 0.4081723392009735\n",
      "Epoch: 46, Samples: 5632/5760, Loss: 0.26344960927963257\n",
      "Epoch: 46, Samples: 5664/5760, Loss: 0.24978488683700562\n",
      "Epoch: 46, Samples: 5696/5760, Loss: 0.1732504963874817\n",
      "Epoch: 46, Samples: 5728/5760, Loss: 2.3327176570892334\n",
      "\n",
      "Epoch: 46\n",
      "Training set: Average loss: 0.2816\n",
      "Validation set: Average loss: 1.4783, Accuracy: 514/818 (63%)\n",
      "Epoch: 47, Samples: 0/5760, Loss: 0.25694596767425537\n",
      "Epoch: 47, Samples: 32/5760, Loss: 0.2613993287086487\n",
      "Epoch: 47, Samples: 64/5760, Loss: 0.2959480881690979\n",
      "Epoch: 47, Samples: 96/5760, Loss: 0.23014096915721893\n",
      "Epoch: 47, Samples: 128/5760, Loss: 0.34546640515327454\n",
      "Epoch: 47, Samples: 160/5760, Loss: 0.24435776472091675\n",
      "Epoch: 47, Samples: 192/5760, Loss: 0.4262012839317322\n",
      "Epoch: 47, Samples: 224/5760, Loss: 0.19581882655620575\n",
      "Epoch: 47, Samples: 256/5760, Loss: 0.17922018468379974\n",
      "Epoch: 47, Samples: 288/5760, Loss: 0.3128032982349396\n",
      "Epoch: 47, Samples: 320/5760, Loss: 0.15600605309009552\n",
      "Epoch: 47, Samples: 352/5760, Loss: 0.36810070276260376\n",
      "Epoch: 47, Samples: 384/5760, Loss: 0.45620015263557434\n",
      "Epoch: 47, Samples: 416/5760, Loss: 0.23574715852737427\n",
      "Epoch: 47, Samples: 448/5760, Loss: 0.46970224380493164\n",
      "Epoch: 47, Samples: 480/5760, Loss: 0.3349592983722687\n",
      "Epoch: 47, Samples: 512/5760, Loss: 0.3080071210861206\n",
      "Epoch: 47, Samples: 544/5760, Loss: 0.18240316212177277\n",
      "Epoch: 47, Samples: 576/5760, Loss: 0.2922672927379608\n",
      "Epoch: 47, Samples: 608/5760, Loss: 0.20745663344860077\n",
      "Epoch: 47, Samples: 640/5760, Loss: 0.0941767692565918\n",
      "Epoch: 47, Samples: 672/5760, Loss: 0.31966185569763184\n",
      "Epoch: 47, Samples: 704/5760, Loss: 0.19388046860694885\n",
      "Epoch: 47, Samples: 736/5760, Loss: 0.3544616997241974\n",
      "Epoch: 47, Samples: 768/5760, Loss: 0.23432110249996185\n",
      "Epoch: 47, Samples: 800/5760, Loss: 0.2685769200325012\n",
      "Epoch: 47, Samples: 832/5760, Loss: 0.22441618144512177\n",
      "Epoch: 47, Samples: 864/5760, Loss: 0.21203462779521942\n",
      "Epoch: 47, Samples: 896/5760, Loss: 0.29155710339546204\n",
      "Epoch: 47, Samples: 928/5760, Loss: 0.26985159516334534\n",
      "Epoch: 47, Samples: 960/5760, Loss: 0.41131681203842163\n",
      "Epoch: 47, Samples: 992/5760, Loss: 0.4913494884967804\n",
      "Epoch: 47, Samples: 1024/5760, Loss: 0.1789456605911255\n",
      "Epoch: 47, Samples: 1056/5760, Loss: 0.1621103137731552\n",
      "Epoch: 47, Samples: 1088/5760, Loss: 0.24577008187770844\n",
      "Epoch: 47, Samples: 1120/5760, Loss: 0.27557191252708435\n",
      "Epoch: 47, Samples: 1152/5760, Loss: 0.3382738530635834\n",
      "Epoch: 47, Samples: 1184/5760, Loss: 0.18531912565231323\n",
      "Epoch: 47, Samples: 1216/5760, Loss: 0.2349025011062622\n",
      "Epoch: 47, Samples: 1248/5760, Loss: 0.32081666588783264\n",
      "Epoch: 47, Samples: 1280/5760, Loss: 0.30510783195495605\n",
      "Epoch: 47, Samples: 1312/5760, Loss: 0.16036076843738556\n",
      "Epoch: 47, Samples: 1344/5760, Loss: 0.1426382064819336\n",
      "Epoch: 47, Samples: 1376/5760, Loss: 0.29773053526878357\n",
      "Epoch: 47, Samples: 1408/5760, Loss: 0.1779593974351883\n",
      "Epoch: 47, Samples: 1440/5760, Loss: 0.2001236379146576\n",
      "Epoch: 47, Samples: 1472/5760, Loss: 0.2439143806695938\n",
      "Epoch: 47, Samples: 1504/5760, Loss: 0.17982804775238037\n",
      "Epoch: 47, Samples: 1536/5760, Loss: 0.8505560159683228\n",
      "Epoch: 47, Samples: 1568/5760, Loss: 0.22895322740077972\n",
      "Epoch: 47, Samples: 1600/5760, Loss: 0.19920168817043304\n",
      "Epoch: 47, Samples: 1632/5760, Loss: 0.20777948200702667\n",
      "Epoch: 47, Samples: 1664/5760, Loss: 0.1633191704750061\n",
      "Epoch: 47, Samples: 1696/5760, Loss: 0.3019959330558777\n",
      "Epoch: 47, Samples: 1728/5760, Loss: 0.26686185598373413\n",
      "Epoch: 47, Samples: 1760/5760, Loss: 0.2039189487695694\n",
      "Epoch: 47, Samples: 1792/5760, Loss: 0.34433335065841675\n",
      "Epoch: 47, Samples: 1824/5760, Loss: 0.21196699142456055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Samples: 1856/5760, Loss: 0.25570738315582275\n",
      "Epoch: 47, Samples: 1888/5760, Loss: 0.42129766941070557\n",
      "Epoch: 47, Samples: 1920/5760, Loss: 0.43560126423835754\n",
      "Epoch: 47, Samples: 1952/5760, Loss: 0.21734409034252167\n",
      "Epoch: 47, Samples: 1984/5760, Loss: 0.20011329650878906\n",
      "Epoch: 47, Samples: 2016/5760, Loss: 0.390951544046402\n",
      "Epoch: 47, Samples: 2048/5760, Loss: 0.31381532549858093\n",
      "Epoch: 47, Samples: 2080/5760, Loss: 0.10797347128391266\n",
      "Epoch: 47, Samples: 2112/5760, Loss: 0.33861926198005676\n",
      "Epoch: 47, Samples: 2144/5760, Loss: 0.22718629240989685\n",
      "Epoch: 47, Samples: 2176/5760, Loss: 0.22307269275188446\n",
      "Epoch: 47, Samples: 2208/5760, Loss: 0.36432814598083496\n",
      "Epoch: 47, Samples: 2240/5760, Loss: 0.24053728580474854\n",
      "Epoch: 47, Samples: 2272/5760, Loss: 0.24110029637813568\n",
      "Epoch: 47, Samples: 2304/5760, Loss: 0.2843756377696991\n",
      "Epoch: 47, Samples: 2336/5760, Loss: 0.31653088331222534\n",
      "Epoch: 47, Samples: 2368/5760, Loss: 0.26350465416908264\n",
      "Epoch: 47, Samples: 2400/5760, Loss: 0.5740214586257935\n",
      "Epoch: 47, Samples: 2432/5760, Loss: 0.24639026820659637\n",
      "Epoch: 47, Samples: 2464/5760, Loss: 0.17228350043296814\n",
      "Epoch: 47, Samples: 2496/5760, Loss: 0.28022482991218567\n",
      "Epoch: 47, Samples: 2528/5760, Loss: 0.3011320233345032\n",
      "Epoch: 47, Samples: 2560/5760, Loss: 0.28246065974235535\n",
      "Epoch: 47, Samples: 2592/5760, Loss: 0.19012075662612915\n",
      "Epoch: 47, Samples: 2624/5760, Loss: 0.1600002944469452\n",
      "Epoch: 47, Samples: 2656/5760, Loss: 0.23274189233779907\n",
      "Epoch: 47, Samples: 2688/5760, Loss: 0.36798810958862305\n",
      "Epoch: 47, Samples: 2720/5760, Loss: 0.2164274901151657\n",
      "Epoch: 47, Samples: 2752/5760, Loss: 0.20355066657066345\n",
      "Epoch: 47, Samples: 2784/5760, Loss: 0.23509879410266876\n",
      "Epoch: 47, Samples: 2816/5760, Loss: 0.587753415107727\n",
      "Epoch: 47, Samples: 2848/5760, Loss: 0.24199481308460236\n",
      "Epoch: 47, Samples: 2880/5760, Loss: 0.3615122139453888\n",
      "Epoch: 47, Samples: 2912/5760, Loss: 0.13761459290981293\n",
      "Epoch: 47, Samples: 2944/5760, Loss: 0.41701897978782654\n",
      "Epoch: 47, Samples: 2976/5760, Loss: 0.3549834191799164\n",
      "Epoch: 47, Samples: 3008/5760, Loss: 0.17980101704597473\n",
      "Epoch: 47, Samples: 3040/5760, Loss: 0.18751341104507446\n",
      "Epoch: 47, Samples: 3072/5760, Loss: 0.2083769589662552\n",
      "Epoch: 47, Samples: 3104/5760, Loss: 0.17827747762203217\n",
      "Epoch: 47, Samples: 3136/5760, Loss: 0.23428285121917725\n",
      "Epoch: 47, Samples: 3168/5760, Loss: 0.37101486325263977\n",
      "Epoch: 47, Samples: 3200/5760, Loss: 0.29614365100860596\n",
      "Epoch: 47, Samples: 3232/5760, Loss: 0.2868499755859375\n",
      "Epoch: 47, Samples: 3264/5760, Loss: 0.1153683066368103\n",
      "Epoch: 47, Samples: 3296/5760, Loss: 0.24153202772140503\n",
      "Epoch: 47, Samples: 3328/5760, Loss: 0.3073456287384033\n",
      "Epoch: 47, Samples: 3360/5760, Loss: 0.1152719110250473\n",
      "Epoch: 47, Samples: 3392/5760, Loss: 0.24943366646766663\n",
      "Epoch: 47, Samples: 3424/5760, Loss: 0.20672576129436493\n",
      "Epoch: 47, Samples: 3456/5760, Loss: 0.1545310616493225\n",
      "Epoch: 47, Samples: 3488/5760, Loss: 0.2889516353607178\n",
      "Epoch: 47, Samples: 3520/5760, Loss: 0.2900148034095764\n",
      "Epoch: 47, Samples: 3552/5760, Loss: 0.23143917322158813\n",
      "Epoch: 47, Samples: 3584/5760, Loss: 0.2592071294784546\n",
      "Epoch: 47, Samples: 3616/5760, Loss: 0.21073798835277557\n",
      "Epoch: 47, Samples: 3648/5760, Loss: 0.3793736398220062\n",
      "Epoch: 47, Samples: 3680/5760, Loss: 0.23641520738601685\n",
      "Epoch: 47, Samples: 3712/5760, Loss: 0.14251913130283356\n",
      "Epoch: 47, Samples: 3744/5760, Loss: 0.2517397105693817\n",
      "Epoch: 47, Samples: 3776/5760, Loss: 0.2610955238342285\n",
      "Epoch: 47, Samples: 3808/5760, Loss: 0.26732805371284485\n",
      "Epoch: 47, Samples: 3840/5760, Loss: 0.18529263138771057\n",
      "Epoch: 47, Samples: 3872/5760, Loss: 0.3863218128681183\n",
      "Epoch: 47, Samples: 3904/5760, Loss: 0.1645563840866089\n",
      "Epoch: 47, Samples: 3936/5760, Loss: 0.1911979466676712\n",
      "Epoch: 47, Samples: 3968/5760, Loss: 0.22137778997421265\n",
      "Epoch: 47, Samples: 4000/5760, Loss: 0.2429027110338211\n",
      "Epoch: 47, Samples: 4032/5760, Loss: 0.31374040246009827\n",
      "Epoch: 47, Samples: 4064/5760, Loss: 0.4214233458042145\n",
      "Epoch: 47, Samples: 4096/5760, Loss: 0.3026914894580841\n",
      "Epoch: 47, Samples: 4128/5760, Loss: 0.1785970777273178\n",
      "Epoch: 47, Samples: 4160/5760, Loss: 0.12454454600811005\n",
      "Epoch: 47, Samples: 4192/5760, Loss: 0.29257139563560486\n",
      "Epoch: 47, Samples: 4224/5760, Loss: 0.16985535621643066\n",
      "Epoch: 47, Samples: 4256/5760, Loss: 0.2018813192844391\n",
      "Epoch: 47, Samples: 4288/5760, Loss: 0.5068424940109253\n",
      "Epoch: 47, Samples: 4320/5760, Loss: 0.31732404232025146\n",
      "Epoch: 47, Samples: 4352/5760, Loss: 0.25717175006866455\n",
      "Epoch: 47, Samples: 4384/5760, Loss: 0.18687471747398376\n",
      "Epoch: 47, Samples: 4416/5760, Loss: 0.24228665232658386\n",
      "Epoch: 47, Samples: 4448/5760, Loss: 0.169601172208786\n",
      "Epoch: 47, Samples: 4480/5760, Loss: 0.23369652032852173\n",
      "Epoch: 47, Samples: 4512/5760, Loss: 0.13540524244308472\n",
      "Epoch: 47, Samples: 4544/5760, Loss: 0.28365710377693176\n",
      "Epoch: 47, Samples: 4576/5760, Loss: 0.20150455832481384\n",
      "Epoch: 47, Samples: 4608/5760, Loss: 0.17210541665554047\n",
      "Epoch: 47, Samples: 4640/5760, Loss: 0.25451886653900146\n",
      "Epoch: 47, Samples: 4672/5760, Loss: 0.263841450214386\n",
      "Epoch: 47, Samples: 4704/5760, Loss: 0.4063097834587097\n",
      "Epoch: 47, Samples: 4736/5760, Loss: 0.2940569221973419\n",
      "Epoch: 47, Samples: 4768/5760, Loss: 0.16779787838459015\n",
      "Epoch: 47, Samples: 4800/5760, Loss: 0.18761321902275085\n",
      "Epoch: 47, Samples: 4832/5760, Loss: 0.23921805620193481\n",
      "Epoch: 47, Samples: 4864/5760, Loss: 0.18840371072292328\n",
      "Epoch: 47, Samples: 4896/5760, Loss: 0.10485610365867615\n",
      "Epoch: 47, Samples: 4928/5760, Loss: 0.19479843974113464\n",
      "Epoch: 47, Samples: 4960/5760, Loss: 0.28244081139564514\n",
      "Epoch: 47, Samples: 4992/5760, Loss: 0.23304811120033264\n",
      "Epoch: 47, Samples: 5024/5760, Loss: 0.191898375749588\n",
      "Epoch: 47, Samples: 5056/5760, Loss: 0.2554674744606018\n",
      "Epoch: 47, Samples: 5088/5760, Loss: 0.2131684124469757\n",
      "Epoch: 47, Samples: 5120/5760, Loss: 0.1813603937625885\n",
      "Epoch: 47, Samples: 5152/5760, Loss: 0.3426211476325989\n",
      "Epoch: 47, Samples: 5184/5760, Loss: 0.24963000416755676\n",
      "Epoch: 47, Samples: 5216/5760, Loss: 0.18925641477108002\n",
      "Epoch: 47, Samples: 5248/5760, Loss: 0.21779866516590118\n",
      "Epoch: 47, Samples: 5280/5760, Loss: 0.33733704686164856\n",
      "Epoch: 47, Samples: 5312/5760, Loss: 0.16726163029670715\n",
      "Epoch: 47, Samples: 5344/5760, Loss: 0.2148091346025467\n",
      "Epoch: 47, Samples: 5376/5760, Loss: 0.2868611216545105\n",
      "Epoch: 47, Samples: 5408/5760, Loss: 0.13863486051559448\n",
      "Epoch: 47, Samples: 5440/5760, Loss: 0.35360926389694214\n",
      "Epoch: 47, Samples: 5472/5760, Loss: 0.17522044479846954\n",
      "Epoch: 47, Samples: 5504/5760, Loss: 0.3676924407482147\n",
      "Epoch: 47, Samples: 5536/5760, Loss: 0.336896151304245\n",
      "Epoch: 47, Samples: 5568/5760, Loss: 0.16072501242160797\n",
      "Epoch: 47, Samples: 5600/5760, Loss: 0.37600386142730713\n",
      "Epoch: 47, Samples: 5632/5760, Loss: 0.2653856575489044\n",
      "Epoch: 47, Samples: 5664/5760, Loss: 0.19979126751422882\n",
      "Epoch: 47, Samples: 5696/5760, Loss: 0.2589748501777649\n",
      "Epoch: 47, Samples: 5728/5760, Loss: 1.2771315574645996\n",
      "\n",
      "Epoch: 47\n",
      "Training set: Average loss: 0.2666\n",
      "Validation set: Average loss: 1.5178, Accuracy: 517/818 (63%)\n",
      "Epoch: 48, Samples: 0/5760, Loss: 0.17658978700637817\n",
      "Epoch: 48, Samples: 32/5760, Loss: 0.43888670206069946\n",
      "Epoch: 48, Samples: 64/5760, Loss: 0.21097524464130402\n",
      "Epoch: 48, Samples: 96/5760, Loss: 0.2432149350643158\n",
      "Epoch: 48, Samples: 128/5760, Loss: 0.3655026853084564\n",
      "Epoch: 48, Samples: 160/5760, Loss: 0.6368788480758667\n",
      "Epoch: 48, Samples: 192/5760, Loss: 0.23927238583564758\n",
      "Epoch: 48, Samples: 224/5760, Loss: 0.1965741068124771\n",
      "Epoch: 48, Samples: 256/5760, Loss: 0.25102436542510986\n",
      "Epoch: 48, Samples: 288/5760, Loss: 0.32695862650871277\n",
      "Epoch: 48, Samples: 320/5760, Loss: 0.4913784861564636\n",
      "Epoch: 48, Samples: 352/5760, Loss: 0.34436941146850586\n",
      "Epoch: 48, Samples: 384/5760, Loss: 0.5152276158332825\n",
      "Epoch: 48, Samples: 416/5760, Loss: 0.2575300335884094\n",
      "Epoch: 48, Samples: 448/5760, Loss: 0.3527663052082062\n",
      "Epoch: 48, Samples: 480/5760, Loss: 0.2598733603954315\n",
      "Epoch: 48, Samples: 512/5760, Loss: 0.1709517240524292\n",
      "Epoch: 48, Samples: 544/5760, Loss: 0.3798385262489319\n",
      "Epoch: 48, Samples: 576/5760, Loss: 0.29441240429878235\n",
      "Epoch: 48, Samples: 608/5760, Loss: 0.3059980571269989\n",
      "Epoch: 48, Samples: 640/5760, Loss: 0.3331400752067566\n",
      "Epoch: 48, Samples: 672/5760, Loss: 0.24148043990135193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48, Samples: 704/5760, Loss: 0.17463041841983795\n",
      "Epoch: 48, Samples: 736/5760, Loss: 0.3333069384098053\n",
      "Epoch: 48, Samples: 768/5760, Loss: 0.467286616563797\n",
      "Epoch: 48, Samples: 800/5760, Loss: 0.23597906529903412\n",
      "Epoch: 48, Samples: 832/5760, Loss: 0.12060166895389557\n",
      "Epoch: 48, Samples: 864/5760, Loss: 0.42466115951538086\n",
      "Epoch: 48, Samples: 896/5760, Loss: 0.1383727788925171\n",
      "Epoch: 48, Samples: 928/5760, Loss: 0.393614798784256\n",
      "Epoch: 48, Samples: 960/5760, Loss: 0.26193609833717346\n",
      "Epoch: 48, Samples: 992/5760, Loss: 0.34047141671180725\n",
      "Epoch: 48, Samples: 1024/5760, Loss: 0.20960737764835358\n",
      "Epoch: 48, Samples: 1056/5760, Loss: 0.20219583809375763\n",
      "Epoch: 48, Samples: 1088/5760, Loss: 0.22815439105033875\n",
      "Epoch: 48, Samples: 1120/5760, Loss: 0.13767370581626892\n",
      "Epoch: 48, Samples: 1152/5760, Loss: 0.2250722199678421\n",
      "Epoch: 48, Samples: 1184/5760, Loss: 0.3685725927352905\n",
      "Epoch: 48, Samples: 1216/5760, Loss: 0.5183727741241455\n",
      "Epoch: 48, Samples: 1248/5760, Loss: 0.2763655483722687\n",
      "Epoch: 48, Samples: 1280/5760, Loss: 0.21500559151172638\n",
      "Epoch: 48, Samples: 1312/5760, Loss: 0.2967485785484314\n",
      "Epoch: 48, Samples: 1344/5760, Loss: 0.33916693925857544\n",
      "Epoch: 48, Samples: 1376/5760, Loss: 0.47764915227890015\n",
      "Epoch: 48, Samples: 1408/5760, Loss: 0.39760667085647583\n",
      "Epoch: 48, Samples: 1440/5760, Loss: 0.26720690727233887\n",
      "Epoch: 48, Samples: 1472/5760, Loss: 0.2667722702026367\n",
      "Epoch: 48, Samples: 1504/5760, Loss: 0.17737725377082825\n",
      "Epoch: 48, Samples: 1536/5760, Loss: 0.3255710005760193\n",
      "Epoch: 48, Samples: 1568/5760, Loss: 0.1577211618423462\n",
      "Epoch: 48, Samples: 1600/5760, Loss: 0.3224487602710724\n",
      "Epoch: 48, Samples: 1632/5760, Loss: 0.10905751585960388\n",
      "Epoch: 48, Samples: 1664/5760, Loss: 0.1870417296886444\n",
      "Epoch: 48, Samples: 1696/5760, Loss: 0.28718823194503784\n",
      "Epoch: 48, Samples: 1728/5760, Loss: 0.291446328163147\n",
      "Epoch: 48, Samples: 1760/5760, Loss: 0.19639576971530914\n",
      "Epoch: 48, Samples: 1792/5760, Loss: 0.23613855242729187\n",
      "Epoch: 48, Samples: 1824/5760, Loss: 0.476394385099411\n",
      "Epoch: 48, Samples: 1856/5760, Loss: 0.24573321640491486\n",
      "Epoch: 48, Samples: 1888/5760, Loss: 0.1858392357826233\n",
      "Epoch: 48, Samples: 1920/5760, Loss: 0.4259391129016876\n",
      "Epoch: 48, Samples: 1952/5760, Loss: 0.22210612893104553\n",
      "Epoch: 48, Samples: 1984/5760, Loss: 0.33498862385749817\n",
      "Epoch: 48, Samples: 2016/5760, Loss: 0.2952202558517456\n",
      "Epoch: 48, Samples: 2048/5760, Loss: 0.6855071187019348\n",
      "Epoch: 48, Samples: 2080/5760, Loss: 0.14125260710716248\n",
      "Epoch: 48, Samples: 2112/5760, Loss: 0.28631702065467834\n",
      "Epoch: 48, Samples: 2144/5760, Loss: 0.11610448360443115\n",
      "Epoch: 48, Samples: 2176/5760, Loss: 0.10414832830429077\n",
      "Epoch: 48, Samples: 2208/5760, Loss: 0.11052823066711426\n",
      "Epoch: 48, Samples: 2240/5760, Loss: 0.12819872796535492\n",
      "Epoch: 48, Samples: 2272/5760, Loss: 0.2216518670320511\n",
      "Epoch: 48, Samples: 2304/5760, Loss: 0.43483710289001465\n",
      "Epoch: 48, Samples: 2336/5760, Loss: 0.223099946975708\n",
      "Epoch: 48, Samples: 2368/5760, Loss: 0.2732478976249695\n",
      "Epoch: 48, Samples: 2400/5760, Loss: 0.27097421884536743\n",
      "Epoch: 48, Samples: 2432/5760, Loss: 0.4250713884830475\n",
      "Epoch: 48, Samples: 2464/5760, Loss: 0.2509706914424896\n",
      "Epoch: 48, Samples: 2496/5760, Loss: 0.17746824026107788\n",
      "Epoch: 48, Samples: 2528/5760, Loss: 0.22690986096858978\n",
      "Epoch: 48, Samples: 2560/5760, Loss: 0.16786693036556244\n",
      "Epoch: 48, Samples: 2592/5760, Loss: 0.25838881731033325\n",
      "Epoch: 48, Samples: 2624/5760, Loss: 0.2749442756175995\n",
      "Epoch: 48, Samples: 2656/5760, Loss: 0.17941303551197052\n",
      "Epoch: 48, Samples: 2688/5760, Loss: 0.30556854605674744\n",
      "Epoch: 48, Samples: 2720/5760, Loss: 0.22235554456710815\n",
      "Epoch: 48, Samples: 2752/5760, Loss: 0.21835806965827942\n",
      "Epoch: 48, Samples: 2784/5760, Loss: 0.24604766070842743\n",
      "Epoch: 48, Samples: 2816/5760, Loss: 0.2242807000875473\n",
      "Epoch: 48, Samples: 2848/5760, Loss: 0.13661924004554749\n",
      "Epoch: 48, Samples: 2880/5760, Loss: 0.17971059679985046\n",
      "Epoch: 48, Samples: 2912/5760, Loss: 0.26851844787597656\n",
      "Epoch: 48, Samples: 2944/5760, Loss: 0.2119808942079544\n",
      "Epoch: 48, Samples: 2976/5760, Loss: 0.20337484776973724\n",
      "Epoch: 48, Samples: 3008/5760, Loss: 0.09165215492248535\n",
      "Epoch: 48, Samples: 3040/5760, Loss: 0.10008598864078522\n",
      "Epoch: 48, Samples: 3072/5760, Loss: 0.19730058312416077\n",
      "Epoch: 48, Samples: 3104/5760, Loss: 0.30407795310020447\n",
      "Epoch: 48, Samples: 3136/5760, Loss: 0.23278756439685822\n",
      "Epoch: 48, Samples: 3168/5760, Loss: 0.19046743214130402\n",
      "Epoch: 48, Samples: 3200/5760, Loss: 0.2119632512331009\n",
      "Epoch: 48, Samples: 3232/5760, Loss: 0.21666105091571808\n",
      "Epoch: 48, Samples: 3264/5760, Loss: 0.602239727973938\n",
      "Epoch: 48, Samples: 3296/5760, Loss: 0.28174707293510437\n",
      "Epoch: 48, Samples: 3328/5760, Loss: 0.29056987166404724\n",
      "Epoch: 48, Samples: 3360/5760, Loss: 0.22883035242557526\n",
      "Epoch: 48, Samples: 3392/5760, Loss: 0.4304036498069763\n",
      "Epoch: 48, Samples: 3424/5760, Loss: 0.41750553250312805\n",
      "Epoch: 48, Samples: 3456/5760, Loss: 0.2609153389930725\n",
      "Epoch: 48, Samples: 3488/5760, Loss: 0.3187079429626465\n",
      "Epoch: 48, Samples: 3520/5760, Loss: 0.23856091499328613\n",
      "Epoch: 48, Samples: 3552/5760, Loss: 0.161810964345932\n",
      "Epoch: 48, Samples: 3584/5760, Loss: 0.25136059522628784\n",
      "Epoch: 48, Samples: 3616/5760, Loss: 0.36726734042167664\n",
      "Epoch: 48, Samples: 3648/5760, Loss: 0.2104579210281372\n",
      "Epoch: 48, Samples: 3680/5760, Loss: 0.11246591806411743\n",
      "Epoch: 48, Samples: 3712/5760, Loss: 0.20095232129096985\n",
      "Epoch: 48, Samples: 3744/5760, Loss: 0.3302609324455261\n",
      "Epoch: 48, Samples: 3776/5760, Loss: 0.23378504812717438\n",
      "Epoch: 48, Samples: 3808/5760, Loss: 0.27231383323669434\n",
      "Epoch: 48, Samples: 3840/5760, Loss: 0.24721001088619232\n",
      "Epoch: 48, Samples: 3872/5760, Loss: 0.15170064568519592\n",
      "Epoch: 48, Samples: 3904/5760, Loss: 0.23396430909633636\n",
      "Epoch: 48, Samples: 3936/5760, Loss: 0.21959911286830902\n",
      "Epoch: 48, Samples: 3968/5760, Loss: 0.33381983637809753\n",
      "Epoch: 48, Samples: 4000/5760, Loss: 0.25394636392593384\n",
      "Epoch: 48, Samples: 4032/5760, Loss: 0.2148265391588211\n",
      "Epoch: 48, Samples: 4064/5760, Loss: 0.27702051401138306\n",
      "Epoch: 48, Samples: 4096/5760, Loss: 0.49762362241744995\n",
      "Epoch: 48, Samples: 4128/5760, Loss: 0.4248805642127991\n",
      "Epoch: 48, Samples: 4160/5760, Loss: 0.22128593921661377\n",
      "Epoch: 48, Samples: 4192/5760, Loss: 0.224228173494339\n",
      "Epoch: 48, Samples: 4224/5760, Loss: 0.16071709990501404\n",
      "Epoch: 48, Samples: 4256/5760, Loss: 0.2759070098400116\n",
      "Epoch: 48, Samples: 4288/5760, Loss: 0.17406323552131653\n",
      "Epoch: 48, Samples: 4320/5760, Loss: 0.355262815952301\n",
      "Epoch: 48, Samples: 4352/5760, Loss: 0.3527778089046478\n",
      "Epoch: 48, Samples: 4384/5760, Loss: 0.15599264204502106\n",
      "Epoch: 48, Samples: 4416/5760, Loss: 0.2681500017642975\n",
      "Epoch: 48, Samples: 4448/5760, Loss: 0.27593132853507996\n",
      "Epoch: 48, Samples: 4480/5760, Loss: 0.24376824498176575\n",
      "Epoch: 48, Samples: 4512/5760, Loss: 0.24112792313098907\n",
      "Epoch: 48, Samples: 4544/5760, Loss: 0.1243407130241394\n",
      "Epoch: 48, Samples: 4576/5760, Loss: 0.23995698988437653\n",
      "Epoch: 48, Samples: 4608/5760, Loss: 0.1421986222267151\n",
      "Epoch: 48, Samples: 4640/5760, Loss: 0.286765992641449\n",
      "Epoch: 48, Samples: 4672/5760, Loss: 0.2800433933734894\n",
      "Epoch: 48, Samples: 4704/5760, Loss: 0.34124478697776794\n",
      "Epoch: 48, Samples: 4736/5760, Loss: 0.18904618918895721\n",
      "Epoch: 48, Samples: 4768/5760, Loss: 0.2137790322303772\n",
      "Epoch: 48, Samples: 4800/5760, Loss: 0.3813719153404236\n",
      "Epoch: 48, Samples: 4832/5760, Loss: 0.18103855848312378\n",
      "Epoch: 48, Samples: 4864/5760, Loss: 0.3974522054195404\n",
      "Epoch: 48, Samples: 4896/5760, Loss: 0.2552076280117035\n",
      "Epoch: 48, Samples: 4928/5760, Loss: 0.2667608857154846\n",
      "Epoch: 48, Samples: 4960/5760, Loss: 0.11245492100715637\n",
      "Epoch: 48, Samples: 4992/5760, Loss: 0.3397584557533264\n",
      "Epoch: 48, Samples: 5024/5760, Loss: 0.27097633481025696\n",
      "Epoch: 48, Samples: 5056/5760, Loss: 0.2906816899776459\n",
      "Epoch: 48, Samples: 5088/5760, Loss: 0.14062465727329254\n",
      "Epoch: 48, Samples: 5120/5760, Loss: 0.32912468910217285\n",
      "Epoch: 48, Samples: 5152/5760, Loss: 0.30601873993873596\n",
      "Epoch: 48, Samples: 5184/5760, Loss: 0.09278303384780884\n",
      "Epoch: 48, Samples: 5216/5760, Loss: 0.2850581109523773\n",
      "Epoch: 48, Samples: 5248/5760, Loss: 0.18744167685508728\n",
      "Epoch: 48, Samples: 5280/5760, Loss: 0.19144819676876068\n",
      "Epoch: 48, Samples: 5312/5760, Loss: 0.24321070313453674\n",
      "Epoch: 48, Samples: 5344/5760, Loss: 0.309874027967453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48, Samples: 5376/5760, Loss: 0.22595348954200745\n",
      "Epoch: 48, Samples: 5408/5760, Loss: 0.4875786602497101\n",
      "Epoch: 48, Samples: 5440/5760, Loss: 0.21189576387405396\n",
      "Epoch: 48, Samples: 5472/5760, Loss: 0.3188973665237427\n",
      "Epoch: 48, Samples: 5504/5760, Loss: 0.10876192152500153\n",
      "Epoch: 48, Samples: 5536/5760, Loss: 0.21638186275959015\n",
      "Epoch: 48, Samples: 5568/5760, Loss: 0.15695038437843323\n",
      "Epoch: 48, Samples: 5600/5760, Loss: 0.203305184841156\n",
      "Epoch: 48, Samples: 5632/5760, Loss: 0.17432910203933716\n",
      "Epoch: 48, Samples: 5664/5760, Loss: 0.37331923842430115\n",
      "Epoch: 48, Samples: 5696/5760, Loss: 0.19543913006782532\n",
      "Epoch: 48, Samples: 5728/5760, Loss: 1.7167890071868896\n",
      "\n",
      "Epoch: 48\n",
      "Training set: Average loss: 0.2742\n",
      "Validation set: Average loss: 1.4422, Accuracy: 531/818 (65%)\n",
      "Epoch: 49, Samples: 0/5760, Loss: 0.19667817652225494\n",
      "Epoch: 49, Samples: 32/5760, Loss: 0.12046366930007935\n",
      "Epoch: 49, Samples: 64/5760, Loss: 0.21008342504501343\n",
      "Epoch: 49, Samples: 96/5760, Loss: 0.5910513401031494\n",
      "Epoch: 49, Samples: 128/5760, Loss: 0.3700539171695709\n",
      "Epoch: 49, Samples: 160/5760, Loss: 0.40247419476509094\n",
      "Epoch: 49, Samples: 192/5760, Loss: 0.2544156014919281\n",
      "Epoch: 49, Samples: 224/5760, Loss: 0.8838450908660889\n",
      "Epoch: 49, Samples: 256/5760, Loss: 0.6505980491638184\n",
      "Epoch: 49, Samples: 288/5760, Loss: 0.3407963812351227\n",
      "Epoch: 49, Samples: 320/5760, Loss: 0.15650016069412231\n",
      "Epoch: 49, Samples: 352/5760, Loss: 0.9192619323730469\n",
      "Epoch: 49, Samples: 384/5760, Loss: 0.5926170349121094\n",
      "Epoch: 49, Samples: 416/5760, Loss: 0.48693376779556274\n",
      "Epoch: 49, Samples: 448/5760, Loss: 0.2745046317577362\n",
      "Epoch: 49, Samples: 480/5760, Loss: 0.4440394341945648\n",
      "Epoch: 49, Samples: 512/5760, Loss: 0.5356404781341553\n",
      "Epoch: 49, Samples: 544/5760, Loss: 0.30537381768226624\n",
      "Epoch: 49, Samples: 576/5760, Loss: 0.2961854636669159\n",
      "Epoch: 49, Samples: 608/5760, Loss: 0.2244020402431488\n",
      "Epoch: 49, Samples: 640/5760, Loss: 0.6408551931381226\n",
      "Epoch: 49, Samples: 672/5760, Loss: 0.1888388693332672\n",
      "Epoch: 49, Samples: 704/5760, Loss: 0.232638880610466\n",
      "Epoch: 49, Samples: 736/5760, Loss: 0.22715315222740173\n",
      "Epoch: 49, Samples: 768/5760, Loss: 0.3884022831916809\n",
      "Epoch: 49, Samples: 800/5760, Loss: 0.5084937810897827\n",
      "Epoch: 49, Samples: 832/5760, Loss: 0.2381306290626526\n",
      "Epoch: 49, Samples: 864/5760, Loss: 0.4598272144794464\n",
      "Epoch: 49, Samples: 896/5760, Loss: 0.2918018102645874\n",
      "Epoch: 49, Samples: 928/5760, Loss: 0.32174596190452576\n",
      "Epoch: 49, Samples: 960/5760, Loss: 0.1957787722349167\n",
      "Epoch: 49, Samples: 992/5760, Loss: 0.18868346512317657\n",
      "Epoch: 49, Samples: 1024/5760, Loss: 0.3404543697834015\n",
      "Epoch: 49, Samples: 1056/5760, Loss: 0.31011325120925903\n",
      "Epoch: 49, Samples: 1088/5760, Loss: 0.22027844190597534\n",
      "Epoch: 49, Samples: 1120/5760, Loss: 0.40336430072784424\n",
      "Epoch: 49, Samples: 1152/5760, Loss: 0.3264199197292328\n",
      "Epoch: 49, Samples: 1184/5760, Loss: 0.26810789108276367\n",
      "Epoch: 49, Samples: 1216/5760, Loss: 0.18637174367904663\n",
      "Epoch: 49, Samples: 1248/5760, Loss: 0.29223641753196716\n",
      "Epoch: 49, Samples: 1280/5760, Loss: 0.3538523018360138\n",
      "Epoch: 49, Samples: 1312/5760, Loss: 0.3967963457107544\n",
      "Epoch: 49, Samples: 1344/5760, Loss: 0.18700262904167175\n",
      "Epoch: 49, Samples: 1376/5760, Loss: 0.33904874324798584\n",
      "Epoch: 49, Samples: 1408/5760, Loss: 0.4256168603897095\n",
      "Epoch: 49, Samples: 1440/5760, Loss: 0.20443007349967957\n",
      "Epoch: 49, Samples: 1472/5760, Loss: 0.5062393546104431\n",
      "Epoch: 49, Samples: 1504/5760, Loss: 0.23440013825893402\n",
      "Epoch: 49, Samples: 1536/5760, Loss: 0.09473434090614319\n",
      "Epoch: 49, Samples: 1568/5760, Loss: 0.1702493280172348\n",
      "Epoch: 49, Samples: 1600/5760, Loss: 0.2742525041103363\n",
      "Epoch: 49, Samples: 1632/5760, Loss: 0.3417009115219116\n",
      "Epoch: 49, Samples: 1664/5760, Loss: 0.16393469274044037\n",
      "Epoch: 49, Samples: 1696/5760, Loss: 0.3252195417881012\n",
      "Epoch: 49, Samples: 1728/5760, Loss: 0.16120359301567078\n",
      "Epoch: 49, Samples: 1760/5760, Loss: 0.13725273311138153\n",
      "Epoch: 49, Samples: 1792/5760, Loss: 0.16766606271266937\n",
      "Epoch: 49, Samples: 1824/5760, Loss: 0.24623334407806396\n",
      "Epoch: 49, Samples: 1856/5760, Loss: 0.41205695271492004\n",
      "Epoch: 49, Samples: 1888/5760, Loss: 0.3166699707508087\n",
      "Epoch: 49, Samples: 1920/5760, Loss: 0.3848106861114502\n",
      "Epoch: 49, Samples: 1952/5760, Loss: 0.2835061848163605\n",
      "Epoch: 49, Samples: 1984/5760, Loss: 0.2239382266998291\n",
      "Epoch: 49, Samples: 2016/5760, Loss: 0.2723599970340729\n",
      "Epoch: 49, Samples: 2048/5760, Loss: 0.8319761753082275\n",
      "Epoch: 49, Samples: 2080/5760, Loss: 0.18326961994171143\n",
      "Epoch: 49, Samples: 2112/5760, Loss: 0.1976022720336914\n",
      "Epoch: 49, Samples: 2144/5760, Loss: 0.5462914705276489\n",
      "Epoch: 49, Samples: 2176/5760, Loss: 0.18543674051761627\n",
      "Epoch: 49, Samples: 2208/5760, Loss: 0.3384968638420105\n",
      "Epoch: 49, Samples: 2240/5760, Loss: 0.2805996835231781\n",
      "Epoch: 49, Samples: 2272/5760, Loss: 0.2730056047439575\n",
      "Epoch: 49, Samples: 2304/5760, Loss: 0.25510331988334656\n",
      "Epoch: 49, Samples: 2336/5760, Loss: 0.37553247809410095\n",
      "Epoch: 49, Samples: 2368/5760, Loss: 0.15263889729976654\n",
      "Epoch: 49, Samples: 2400/5760, Loss: 0.423716276884079\n",
      "Epoch: 49, Samples: 2432/5760, Loss: 0.2273947298526764\n",
      "Epoch: 49, Samples: 2464/5760, Loss: 0.2216322124004364\n",
      "Epoch: 49, Samples: 2496/5760, Loss: 0.16262665390968323\n",
      "Epoch: 49, Samples: 2528/5760, Loss: 0.25588250160217285\n",
      "Epoch: 49, Samples: 2560/5760, Loss: 0.15410129725933075\n",
      "Epoch: 49, Samples: 2592/5760, Loss: 0.15437398850917816\n",
      "Epoch: 49, Samples: 2624/5760, Loss: 0.1643621325492859\n",
      "Epoch: 49, Samples: 2656/5760, Loss: 0.42162126302719116\n",
      "Epoch: 49, Samples: 2688/5760, Loss: 0.25955384969711304\n",
      "Epoch: 49, Samples: 2720/5760, Loss: 0.2600572407245636\n",
      "Epoch: 49, Samples: 2752/5760, Loss: 0.20123234391212463\n",
      "Epoch: 49, Samples: 2784/5760, Loss: 0.23253394663333893\n",
      "Epoch: 49, Samples: 2816/5760, Loss: 0.1861976534128189\n",
      "Epoch: 49, Samples: 2848/5760, Loss: 0.4773070812225342\n",
      "Epoch: 49, Samples: 2880/5760, Loss: 0.18120457231998444\n",
      "Epoch: 49, Samples: 2912/5760, Loss: 0.13259665668010712\n",
      "Epoch: 49, Samples: 2944/5760, Loss: 0.3492462635040283\n",
      "Epoch: 49, Samples: 2976/5760, Loss: 0.21342086791992188\n",
      "Epoch: 49, Samples: 3008/5760, Loss: 0.5351120233535767\n",
      "Epoch: 49, Samples: 3040/5760, Loss: 0.19627287983894348\n",
      "Epoch: 49, Samples: 3072/5760, Loss: 0.1299508512020111\n",
      "Epoch: 49, Samples: 3104/5760, Loss: 0.17260903120040894\n",
      "Epoch: 49, Samples: 3136/5760, Loss: 0.2436903417110443\n",
      "Epoch: 49, Samples: 3168/5760, Loss: 0.24743370711803436\n",
      "Epoch: 49, Samples: 3200/5760, Loss: 0.2934691309928894\n",
      "Epoch: 49, Samples: 3232/5760, Loss: 0.3937664031982422\n",
      "Epoch: 49, Samples: 3264/5760, Loss: 0.2956371605396271\n",
      "Epoch: 49, Samples: 3296/5760, Loss: 0.16582928597927094\n",
      "Epoch: 49, Samples: 3328/5760, Loss: 0.20698115229606628\n",
      "Epoch: 49, Samples: 3360/5760, Loss: 0.43009689450263977\n",
      "Epoch: 49, Samples: 3392/5760, Loss: 0.18285958468914032\n",
      "Epoch: 49, Samples: 3424/5760, Loss: 0.2581864297389984\n",
      "Epoch: 49, Samples: 3456/5760, Loss: 0.2554456293582916\n",
      "Epoch: 49, Samples: 3488/5760, Loss: 0.14193198084831238\n",
      "Epoch: 49, Samples: 3520/5760, Loss: 0.278104692697525\n",
      "Epoch: 49, Samples: 3552/5760, Loss: 0.28372323513031006\n",
      "Epoch: 49, Samples: 3584/5760, Loss: 0.30163025856018066\n",
      "Epoch: 49, Samples: 3616/5760, Loss: 0.17978641390800476\n",
      "Epoch: 49, Samples: 3648/5760, Loss: 0.278574675321579\n",
      "Epoch: 49, Samples: 3680/5760, Loss: 0.37659454345703125\n",
      "Epoch: 49, Samples: 3712/5760, Loss: 0.3135906755924225\n",
      "Epoch: 49, Samples: 3744/5760, Loss: 0.3032948970794678\n",
      "Epoch: 49, Samples: 3776/5760, Loss: 0.12551650404930115\n",
      "Epoch: 49, Samples: 3808/5760, Loss: 0.12812189757823944\n",
      "Epoch: 49, Samples: 3840/5760, Loss: 0.27152976393699646\n",
      "Epoch: 49, Samples: 3872/5760, Loss: 0.23888994753360748\n",
      "Epoch: 49, Samples: 3904/5760, Loss: 0.16725364327430725\n",
      "Epoch: 49, Samples: 3936/5760, Loss: 0.2636095881462097\n",
      "Epoch: 49, Samples: 3968/5760, Loss: 0.3793894052505493\n",
      "Epoch: 49, Samples: 4000/5760, Loss: 0.24466346204280853\n",
      "Epoch: 49, Samples: 4032/5760, Loss: 0.14504916965961456\n",
      "Epoch: 49, Samples: 4064/5760, Loss: 0.23863694071769714\n",
      "Epoch: 49, Samples: 4096/5760, Loss: 0.18611960113048553\n",
      "Epoch: 49, Samples: 4128/5760, Loss: 0.13326941430568695\n",
      "Epoch: 49, Samples: 4160/5760, Loss: 0.22102180123329163\n",
      "Epoch: 49, Samples: 4192/5760, Loss: 0.20820215344429016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49, Samples: 4224/5760, Loss: 0.22109147906303406\n",
      "Epoch: 49, Samples: 4256/5760, Loss: 0.3148830831050873\n",
      "Epoch: 49, Samples: 4288/5760, Loss: 0.11366114020347595\n",
      "Epoch: 49, Samples: 4320/5760, Loss: 0.2799126207828522\n",
      "Epoch: 49, Samples: 4352/5760, Loss: 0.354889452457428\n",
      "Epoch: 49, Samples: 4384/5760, Loss: 0.40929386019706726\n",
      "Epoch: 49, Samples: 4416/5760, Loss: 0.3798156678676605\n",
      "Epoch: 49, Samples: 4448/5760, Loss: 0.21926644444465637\n",
      "Epoch: 49, Samples: 4480/5760, Loss: 0.1579405963420868\n",
      "Epoch: 49, Samples: 4512/5760, Loss: 0.2312866896390915\n",
      "Epoch: 49, Samples: 4544/5760, Loss: 0.42478615045547485\n",
      "Epoch: 49, Samples: 4576/5760, Loss: 0.17808061838150024\n",
      "Epoch: 49, Samples: 4608/5760, Loss: 0.21095788478851318\n",
      "Epoch: 49, Samples: 4640/5760, Loss: 0.34019455313682556\n",
      "Epoch: 49, Samples: 4672/5760, Loss: 0.15258939564228058\n",
      "Epoch: 49, Samples: 4704/5760, Loss: 0.2312227189540863\n",
      "Epoch: 49, Samples: 4736/5760, Loss: 0.456663578748703\n",
      "Epoch: 49, Samples: 4768/5760, Loss: 0.3832816481590271\n",
      "Epoch: 49, Samples: 4800/5760, Loss: 0.2199067324399948\n",
      "Epoch: 49, Samples: 4832/5760, Loss: 0.27603501081466675\n",
      "Epoch: 49, Samples: 4864/5760, Loss: 0.1865810751914978\n",
      "Epoch: 49, Samples: 4896/5760, Loss: 0.15006592869758606\n",
      "Epoch: 49, Samples: 4928/5760, Loss: 0.14478281140327454\n",
      "Epoch: 49, Samples: 4960/5760, Loss: 0.26494139432907104\n",
      "Epoch: 49, Samples: 4992/5760, Loss: 0.15706214308738708\n",
      "Epoch: 49, Samples: 5024/5760, Loss: 0.315305233001709\n",
      "Epoch: 49, Samples: 5056/5760, Loss: 0.22716280817985535\n",
      "Epoch: 49, Samples: 5088/5760, Loss: 0.29570212960243225\n",
      "Epoch: 49, Samples: 5120/5760, Loss: 0.13619787991046906\n",
      "Epoch: 49, Samples: 5152/5760, Loss: 0.2637670040130615\n",
      "Epoch: 49, Samples: 5184/5760, Loss: 0.08670634031295776\n",
      "Epoch: 49, Samples: 5216/5760, Loss: 0.13650107383728027\n",
      "Epoch: 49, Samples: 5248/5760, Loss: 0.29850292205810547\n",
      "Epoch: 49, Samples: 5280/5760, Loss: 0.22713246941566467\n",
      "Epoch: 49, Samples: 5312/5760, Loss: 0.3343677222728729\n",
      "Epoch: 49, Samples: 5344/5760, Loss: 0.11747224628925323\n",
      "Epoch: 49, Samples: 5376/5760, Loss: 0.30380892753601074\n",
      "Epoch: 49, Samples: 5408/5760, Loss: 0.18650297820568085\n",
      "Epoch: 49, Samples: 5440/5760, Loss: 0.17414602637290955\n",
      "Epoch: 49, Samples: 5472/5760, Loss: 0.13206471502780914\n",
      "Epoch: 49, Samples: 5504/5760, Loss: 0.1888151615858078\n",
      "Epoch: 49, Samples: 5536/5760, Loss: 0.15883705019950867\n",
      "Epoch: 49, Samples: 5568/5760, Loss: 0.28643691539764404\n",
      "Epoch: 49, Samples: 5600/5760, Loss: 0.22051690518856049\n",
      "Epoch: 49, Samples: 5632/5760, Loss: 0.25968408584594727\n",
      "Epoch: 49, Samples: 5664/5760, Loss: 0.21639296412467957\n",
      "Epoch: 49, Samples: 5696/5760, Loss: 0.1774665117263794\n",
      "Epoch: 49, Samples: 5728/5760, Loss: 0.7097312211990356\n",
      "\n",
      "Epoch: 49\n",
      "Training set: Average loss: 0.2819\n",
      "Validation set: Average loss: 1.3981, Accuracy: 533/818 (65%)\n",
      "Epoch: 50, Samples: 0/5760, Loss: 0.15757954120635986\n",
      "Epoch: 50, Samples: 32/5760, Loss: 0.3525426387786865\n",
      "Epoch: 50, Samples: 64/5760, Loss: 0.12582992017269135\n",
      "Epoch: 50, Samples: 96/5760, Loss: 0.2004365473985672\n",
      "Epoch: 50, Samples: 128/5760, Loss: 0.398051917552948\n",
      "Epoch: 50, Samples: 160/5760, Loss: 0.1557699590921402\n",
      "Epoch: 50, Samples: 192/5760, Loss: 0.1464107483625412\n",
      "Epoch: 50, Samples: 224/5760, Loss: 0.09720918536186218\n",
      "Epoch: 50, Samples: 256/5760, Loss: 0.5173130035400391\n",
      "Epoch: 50, Samples: 288/5760, Loss: 0.2675167918205261\n",
      "Epoch: 50, Samples: 320/5760, Loss: 0.22773277759552002\n",
      "Epoch: 50, Samples: 352/5760, Loss: 0.18064197897911072\n",
      "Epoch: 50, Samples: 384/5760, Loss: 0.21305985748767853\n",
      "Epoch: 50, Samples: 416/5760, Loss: 0.22767646610736847\n",
      "Epoch: 50, Samples: 448/5760, Loss: 0.18208138644695282\n",
      "Epoch: 50, Samples: 480/5760, Loss: 0.0740080326795578\n",
      "Epoch: 50, Samples: 512/5760, Loss: 0.24241739511489868\n",
      "Epoch: 50, Samples: 544/5760, Loss: 0.2544175982475281\n",
      "Epoch: 50, Samples: 576/5760, Loss: 0.22590836882591248\n",
      "Epoch: 50, Samples: 608/5760, Loss: 0.11424031853675842\n",
      "Epoch: 50, Samples: 640/5760, Loss: 0.1981053203344345\n",
      "Epoch: 50, Samples: 672/5760, Loss: 0.3941613435745239\n",
      "Epoch: 50, Samples: 704/5760, Loss: 0.20764735341072083\n",
      "Epoch: 50, Samples: 736/5760, Loss: 0.12853267788887024\n",
      "Epoch: 50, Samples: 768/5760, Loss: 0.08737710118293762\n",
      "Epoch: 50, Samples: 800/5760, Loss: 0.3131897747516632\n",
      "Epoch: 50, Samples: 832/5760, Loss: 0.1388997882604599\n",
      "Epoch: 50, Samples: 864/5760, Loss: 0.1050458699464798\n",
      "Epoch: 50, Samples: 896/5760, Loss: 0.4256304204463959\n",
      "Epoch: 50, Samples: 928/5760, Loss: 0.2913750112056732\n",
      "Epoch: 50, Samples: 960/5760, Loss: 0.18951711058616638\n",
      "Epoch: 50, Samples: 992/5760, Loss: 0.10750797390937805\n",
      "Epoch: 50, Samples: 1024/5760, Loss: 0.16758248209953308\n",
      "Epoch: 50, Samples: 1056/5760, Loss: 0.24993404746055603\n",
      "Epoch: 50, Samples: 1088/5760, Loss: 0.18569712340831757\n",
      "Epoch: 50, Samples: 1120/5760, Loss: 0.13605189323425293\n",
      "Epoch: 50, Samples: 1152/5760, Loss: 0.12826256453990936\n",
      "Epoch: 50, Samples: 1184/5760, Loss: 0.232060506939888\n",
      "Epoch: 50, Samples: 1216/5760, Loss: 0.18132781982421875\n",
      "Epoch: 50, Samples: 1248/5760, Loss: 0.15762214362621307\n",
      "Epoch: 50, Samples: 1280/5760, Loss: 0.14484672248363495\n",
      "Epoch: 50, Samples: 1312/5760, Loss: 0.1739884912967682\n",
      "Epoch: 50, Samples: 1344/5760, Loss: 0.13543036580085754\n",
      "Epoch: 50, Samples: 1376/5760, Loss: 0.30557510256767273\n",
      "Epoch: 50, Samples: 1408/5760, Loss: 0.0932500958442688\n",
      "Epoch: 50, Samples: 1440/5760, Loss: 0.16716738045215607\n",
      "Epoch: 50, Samples: 1472/5760, Loss: 0.10390940308570862\n",
      "Epoch: 50, Samples: 1504/5760, Loss: 0.3097670376300812\n",
      "Epoch: 50, Samples: 1536/5760, Loss: 0.15821395814418793\n",
      "Epoch: 50, Samples: 1568/5760, Loss: 0.209817573428154\n",
      "Epoch: 50, Samples: 1600/5760, Loss: 0.4026188850402832\n",
      "Epoch: 50, Samples: 1632/5760, Loss: 0.16116036474704742\n",
      "Epoch: 50, Samples: 1664/5760, Loss: 0.2713225483894348\n",
      "Epoch: 50, Samples: 1696/5760, Loss: 0.22623923420906067\n",
      "Epoch: 50, Samples: 1728/5760, Loss: 0.14361894130706787\n",
      "Epoch: 50, Samples: 1760/5760, Loss: 0.11059707403182983\n",
      "Epoch: 50, Samples: 1792/5760, Loss: 0.13140803575515747\n",
      "Epoch: 50, Samples: 1824/5760, Loss: 0.1546580046415329\n",
      "Epoch: 50, Samples: 1856/5760, Loss: 0.18534111976623535\n",
      "Epoch: 50, Samples: 1888/5760, Loss: 0.20279888808727264\n",
      "Epoch: 50, Samples: 1920/5760, Loss: 0.32881343364715576\n",
      "Epoch: 50, Samples: 1952/5760, Loss: 0.19355349242687225\n",
      "Epoch: 50, Samples: 1984/5760, Loss: 0.09725040197372437\n",
      "Epoch: 50, Samples: 2016/5760, Loss: 0.21209068596363068\n",
      "Epoch: 50, Samples: 2048/5760, Loss: 0.13704457879066467\n",
      "Epoch: 50, Samples: 2080/5760, Loss: 0.13613155484199524\n",
      "Epoch: 50, Samples: 2112/5760, Loss: 0.22931495308876038\n",
      "Epoch: 50, Samples: 2144/5760, Loss: 0.4492631256580353\n",
      "Epoch: 50, Samples: 2176/5760, Loss: 0.13004331290721893\n",
      "Epoch: 50, Samples: 2208/5760, Loss: 0.1996568888425827\n",
      "Epoch: 50, Samples: 2240/5760, Loss: 0.17441272735595703\n",
      "Epoch: 50, Samples: 2272/5760, Loss: 0.19054125249385834\n",
      "Epoch: 50, Samples: 2304/5760, Loss: 0.1889708936214447\n",
      "Epoch: 50, Samples: 2336/5760, Loss: 0.17343802750110626\n",
      "Epoch: 50, Samples: 2368/5760, Loss: 0.3621246814727783\n",
      "Epoch: 50, Samples: 2400/5760, Loss: 0.116532102227211\n",
      "Epoch: 50, Samples: 2432/5760, Loss: 0.28868094086647034\n",
      "Epoch: 50, Samples: 2464/5760, Loss: 0.2233012616634369\n",
      "Epoch: 50, Samples: 2496/5760, Loss: 0.221046581864357\n",
      "Epoch: 50, Samples: 2528/5760, Loss: 0.10474412143230438\n",
      "Epoch: 50, Samples: 2560/5760, Loss: 0.1674613505601883\n",
      "Epoch: 50, Samples: 2592/5760, Loss: 0.2807392477989197\n",
      "Epoch: 50, Samples: 2624/5760, Loss: 0.27498647570610046\n",
      "Epoch: 50, Samples: 2656/5760, Loss: 0.15234355628490448\n",
      "Epoch: 50, Samples: 2688/5760, Loss: 0.24191880226135254\n",
      "Epoch: 50, Samples: 2720/5760, Loss: 0.08023449778556824\n",
      "Epoch: 50, Samples: 2752/5760, Loss: 0.1613316386938095\n",
      "Epoch: 50, Samples: 2784/5760, Loss: 0.15383590757846832\n",
      "Epoch: 50, Samples: 2816/5760, Loss: 0.24353374540805817\n",
      "Epoch: 50, Samples: 2848/5760, Loss: 0.1353413611650467\n",
      "Epoch: 50, Samples: 2880/5760, Loss: 0.07891938090324402\n",
      "Epoch: 50, Samples: 2912/5760, Loss: 0.21144500374794006\n",
      "Epoch: 50, Samples: 2944/5760, Loss: 0.35283011198043823\n",
      "Epoch: 50, Samples: 2976/5760, Loss: 0.08616641163825989\n",
      "Epoch: 50, Samples: 3008/5760, Loss: 0.18665121495723724\n",
      "Epoch: 50, Samples: 3040/5760, Loss: 0.24393385648727417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Samples: 3072/5760, Loss: 0.13488665223121643\n",
      "Epoch: 50, Samples: 3104/5760, Loss: 0.23476026952266693\n",
      "Epoch: 50, Samples: 3136/5760, Loss: 0.1814093291759491\n",
      "Epoch: 50, Samples: 3168/5760, Loss: 0.10744385421276093\n",
      "Epoch: 50, Samples: 3200/5760, Loss: 0.2837746739387512\n",
      "Epoch: 50, Samples: 3232/5760, Loss: 0.1880468875169754\n",
      "Epoch: 50, Samples: 3264/5760, Loss: 0.1077965795993805\n",
      "Epoch: 50, Samples: 3296/5760, Loss: 0.10500502586364746\n",
      "Epoch: 50, Samples: 3328/5760, Loss: 0.14536049962043762\n",
      "Epoch: 50, Samples: 3360/5760, Loss: 0.3320556581020355\n",
      "Epoch: 50, Samples: 3392/5760, Loss: 0.0721789002418518\n",
      "Epoch: 50, Samples: 3424/5760, Loss: 0.46955811977386475\n",
      "Epoch: 50, Samples: 3456/5760, Loss: 0.15167507529258728\n",
      "Epoch: 50, Samples: 3488/5760, Loss: 0.19267338514328003\n",
      "Epoch: 50, Samples: 3520/5760, Loss: 0.12767824530601501\n",
      "Epoch: 50, Samples: 3552/5760, Loss: 0.28626081347465515\n",
      "Epoch: 50, Samples: 3584/5760, Loss: 0.18579387664794922\n",
      "Epoch: 50, Samples: 3616/5760, Loss: 0.4526282250881195\n",
      "Epoch: 50, Samples: 3648/5760, Loss: 0.21527312695980072\n",
      "Epoch: 50, Samples: 3680/5760, Loss: 0.2026546597480774\n",
      "Epoch: 50, Samples: 3712/5760, Loss: 0.2592192590236664\n",
      "Epoch: 50, Samples: 3744/5760, Loss: 0.2131577730178833\n",
      "Epoch: 50, Samples: 3776/5760, Loss: 0.2330905944108963\n",
      "Epoch: 50, Samples: 3808/5760, Loss: 0.15163427591323853\n",
      "Epoch: 50, Samples: 3840/5760, Loss: 0.12753154337406158\n",
      "Epoch: 50, Samples: 3872/5760, Loss: 0.14231108129024506\n",
      "Epoch: 50, Samples: 3904/5760, Loss: 0.1958770751953125\n",
      "Epoch: 50, Samples: 3936/5760, Loss: 0.15681743621826172\n",
      "Epoch: 50, Samples: 3968/5760, Loss: 0.21532344818115234\n",
      "Epoch: 50, Samples: 4000/5760, Loss: 0.19148890674114227\n",
      "Epoch: 50, Samples: 4032/5760, Loss: 0.2591429054737091\n",
      "Epoch: 50, Samples: 4064/5760, Loss: 0.16091424226760864\n",
      "Epoch: 50, Samples: 4096/5760, Loss: 0.29431965947151184\n",
      "Epoch: 50, Samples: 4128/5760, Loss: 0.11994323134422302\n",
      "Epoch: 50, Samples: 4160/5760, Loss: 0.2180253565311432\n",
      "Epoch: 50, Samples: 4192/5760, Loss: 0.1918705552816391\n",
      "Epoch: 50, Samples: 4224/5760, Loss: 0.2556510269641876\n",
      "Epoch: 50, Samples: 4256/5760, Loss: 0.13902407884597778\n",
      "Epoch: 50, Samples: 4288/5760, Loss: 0.21928280591964722\n",
      "Epoch: 50, Samples: 4320/5760, Loss: 0.16659405827522278\n",
      "Epoch: 50, Samples: 4352/5760, Loss: 0.11264318227767944\n",
      "Epoch: 50, Samples: 4384/5760, Loss: 0.2528322637081146\n",
      "Epoch: 50, Samples: 4416/5760, Loss: 0.06038618087768555\n",
      "Epoch: 50, Samples: 4448/5760, Loss: 0.2721822261810303\n",
      "Epoch: 50, Samples: 4480/5760, Loss: 0.20334188640117645\n",
      "Epoch: 50, Samples: 4512/5760, Loss: 0.12596414983272552\n",
      "Epoch: 50, Samples: 4544/5760, Loss: 0.2207697182893753\n",
      "Epoch: 50, Samples: 4576/5760, Loss: 0.20647455751895905\n",
      "Epoch: 50, Samples: 4608/5760, Loss: 0.11162544786930084\n",
      "Epoch: 50, Samples: 4640/5760, Loss: 0.14682529866695404\n",
      "Epoch: 50, Samples: 4672/5760, Loss: 0.2763752043247223\n",
      "Epoch: 50, Samples: 4704/5760, Loss: 0.27444106340408325\n",
      "Epoch: 50, Samples: 4736/5760, Loss: 0.09608939290046692\n",
      "Epoch: 50, Samples: 4768/5760, Loss: 0.09709125757217407\n",
      "Epoch: 50, Samples: 4800/5760, Loss: 0.2366677075624466\n",
      "Epoch: 50, Samples: 4832/5760, Loss: 0.12100213766098022\n",
      "Epoch: 50, Samples: 4864/5760, Loss: 0.13694371283054352\n",
      "Epoch: 50, Samples: 4896/5760, Loss: 0.1460481435060501\n",
      "Epoch: 50, Samples: 4928/5760, Loss: 0.698915958404541\n",
      "Epoch: 50, Samples: 4960/5760, Loss: 0.2034195065498352\n",
      "Epoch: 50, Samples: 4992/5760, Loss: 0.15894778072834015\n",
      "Epoch: 50, Samples: 5024/5760, Loss: 0.0703345239162445\n",
      "Epoch: 50, Samples: 5056/5760, Loss: 0.20607376098632812\n",
      "Epoch: 50, Samples: 5088/5760, Loss: 0.1500886082649231\n",
      "Epoch: 50, Samples: 5120/5760, Loss: 0.1339910924434662\n",
      "Epoch: 50, Samples: 5152/5760, Loss: 0.12919291853904724\n",
      "Epoch: 50, Samples: 5184/5760, Loss: 0.2648811340332031\n",
      "Epoch: 50, Samples: 5216/5760, Loss: 0.15568622946739197\n",
      "Epoch: 50, Samples: 5248/5760, Loss: 0.3177269697189331\n",
      "Epoch: 50, Samples: 5280/5760, Loss: 0.14497992396354675\n",
      "Epoch: 50, Samples: 5312/5760, Loss: 0.16853861510753632\n",
      "Epoch: 50, Samples: 5344/5760, Loss: 0.24394789338111877\n",
      "Epoch: 50, Samples: 5376/5760, Loss: 0.0873444676399231\n",
      "Epoch: 50, Samples: 5408/5760, Loss: 0.09144189953804016\n",
      "Epoch: 50, Samples: 5440/5760, Loss: 0.12682154774665833\n",
      "Epoch: 50, Samples: 5472/5760, Loss: 0.23823516070842743\n",
      "Epoch: 50, Samples: 5504/5760, Loss: 0.2305174171924591\n",
      "Epoch: 50, Samples: 5536/5760, Loss: 0.24252642691135406\n",
      "Epoch: 50, Samples: 5568/5760, Loss: 0.07715079188346863\n",
      "Epoch: 50, Samples: 5600/5760, Loss: 0.19143113493919373\n",
      "Epoch: 50, Samples: 5632/5760, Loss: 0.20866332948207855\n",
      "Epoch: 50, Samples: 5664/5760, Loss: 0.1805495172739029\n",
      "Epoch: 50, Samples: 5696/5760, Loss: 0.16248825192451477\n",
      "Epoch: 50, Samples: 5728/5760, Loss: 1.2451642751693726\n",
      "\n",
      "Epoch: 50\n",
      "Training set: Average loss: 0.2039\n",
      "Validation set: Average loss: 1.4211, Accuracy: 542/818 (66%)\n",
      "Training and validation complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class FlowerDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_paths, label_file, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels = np.load(label_file)\n",
    "        self.image_label_pairs = self._load_paths(image_paths)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def train_val_test_split(self, train_ratio, val_ratio):\n",
    "        dataset_length = len(self.image_label_pairs)\n",
    "        train_length = int(train_ratio * dataset_length)\n",
    "        val_length = int(val_ratio * dataset_length)\n",
    "        test_length = len(self) - train_length - val_length\n",
    "        splits = [train_length, val_length, test_length]\n",
    "        return random_split(self, splits)\n",
    "        \n",
    "    def _load_paths(self, file_path):\n",
    "        \"\"\"\n",
    "        params:  file_path, a path pointing to where the image paths are stored.\n",
    "        returns: dictionary with keys 'full_image_path', and values 'label'\n",
    "        \"\"\"\n",
    "        split_set = {}\n",
    "        with open(file_path) as f:\n",
    "            lines = f.readlines()\n",
    "            num_lines = len(lines)\n",
    "            assert(num_lines == len(self.labels))\n",
    "            for line_num in range(num_lines):\n",
    "                full_image_path = os.path.join(self.image_dir, lines[line_num].strip('\\n'))\n",
    "                split_set[full_image_path] = self.labels[line_num]\n",
    "        return pd.DataFrame.from_dict(split_set, orient='index')\n",
    "        \n",
    "    def _load_image(self, image_path):\n",
    "        img = Image.open(image_path)\n",
    "        img.load()\n",
    "        img = np.array(img)\n",
    "        if len(img.shape) == 2:\n",
    "            img = np.expand_dims(img, 2)\n",
    "            img = np.repeat(img, 3, 2)\n",
    "        return Image.fromarray(img)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_label_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # apply transforms\n",
    "        image_path = self.image_label_pairs.index[idx]\n",
    "        image = self._load_image(image_path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return {'image': image,\n",
    "                'label': label}\n",
    "    \n",
    "transform = transforms.Compose([transforms.CenterCrop(200), transforms.ToTensor()])\n",
    "dataset = FlowerDataset('data', 'image_paths.txt', 'labels.npy', transform=transform)\n",
    "train_set, val_set, test_set = dataset.train_val_test_split(0.7, 0.1)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        data, target = batch['image'].to(device), batch['label'].long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = CE(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        print('Epoch: {}, Samples: {}/{}, Loss: {}'.format(epoch, idx*batch_size,\n",
    "                                                           len(train_loader)*batch_size,\n",
    "                                                           loss.item()))\n",
    "    train_loss = torch.mean(torch.tensor(train_losses))\n",
    "    print('\\nEpoch: {}'.format(epoch))\n",
    "    print('Training set: Average loss: {:.4f}'.format(train_loss))\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "def validate(model, device, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(val_loader):\n",
    "            data, target = batch['image'].to(device), batch['label'].long().to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            # compute the batch loss\n",
    "            batch_loss = CE(output, target).item()\n",
    "            val_loss += batch_loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    # divide by the number of batches of batch size 32\n",
    "    # get the average validation over all bins\n",
    "    val_loss /= len(val_loader)\n",
    "    print('Validation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        val_loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "DATA_DIRECTORY = 'data/'\n",
    "use_cuda = 1\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = models.resnet18(num_classes=102).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "CE = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "    val_loss = validate(model, device, val_loader)\n",
    "\n",
    "    if (len(val_losses) > 0) and (val_loss < min(val_losses)):\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"Saving model (epoch {}) with lowest validation loss: {}\"\n",
    "              .format(epoch, val_loss))\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "print(\"Training and validation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "epoch_list = np.arange(1, num_epochs+1)\n",
    "plt.xticks(epoch_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(epoch_list, train_losses, label=\"Training loss\")\n",
    "plt.plot(epoch_list, val_losses, label=\"Validation loss\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 1011/1639 (62%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(test_loader):\n",
    "        data = batch['image'].to(device)\n",
    "        labels = batch['label'].long().to(device)\n",
    "        result = model(data)\n",
    "        pred = result.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    \n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(correct, len(test_loader.dataset), \n",
    "                                                       100. * correct / len(test_loader.dataset)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
