{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Samples: 0/5760, Loss: 4.981675624847412\n",
      "Epoch: 1, Samples: 32/5760, Loss: 5.093235492706299\n",
      "Epoch: 1, Samples: 64/5760, Loss: 4.838579177856445\n",
      "Epoch: 1, Samples: 96/5760, Loss: 4.988086700439453\n",
      "Epoch: 1, Samples: 128/5760, Loss: 4.844787120819092\n",
      "Epoch: 1, Samples: 160/5760, Loss: 4.841336250305176\n",
      "Epoch: 1, Samples: 192/5760, Loss: 4.810749530792236\n",
      "Epoch: 1, Samples: 224/5760, Loss: 4.851246356964111\n",
      "Epoch: 1, Samples: 256/5760, Loss: 4.909475803375244\n",
      "Epoch: 1, Samples: 288/5760, Loss: 4.735227584838867\n",
      "Epoch: 1, Samples: 320/5760, Loss: 4.785477638244629\n",
      "Epoch: 1, Samples: 352/5760, Loss: 4.52473783493042\n",
      "Epoch: 1, Samples: 384/5760, Loss: 4.818118095397949\n",
      "Epoch: 1, Samples: 416/5760, Loss: 4.818408966064453\n",
      "Epoch: 1, Samples: 448/5760, Loss: 4.7100510597229\n",
      "Epoch: 1, Samples: 480/5760, Loss: 4.6769890785217285\n",
      "Epoch: 1, Samples: 512/5760, Loss: 4.601886749267578\n",
      "Epoch: 1, Samples: 544/5760, Loss: 4.516341209411621\n",
      "Epoch: 1, Samples: 576/5760, Loss: 4.565003395080566\n",
      "Epoch: 1, Samples: 608/5760, Loss: 4.628861904144287\n",
      "Epoch: 1, Samples: 640/5760, Loss: 4.253196716308594\n",
      "Epoch: 1, Samples: 672/5760, Loss: 4.49751615524292\n",
      "Epoch: 1, Samples: 704/5760, Loss: 4.525439739227295\n",
      "Epoch: 1, Samples: 736/5760, Loss: 4.383618354797363\n",
      "Epoch: 1, Samples: 768/5760, Loss: 4.381085395812988\n",
      "Epoch: 1, Samples: 800/5760, Loss: 4.226978778839111\n",
      "Epoch: 1, Samples: 832/5760, Loss: 4.460021018981934\n",
      "Epoch: 1, Samples: 864/5760, Loss: 4.3193864822387695\n",
      "Epoch: 1, Samples: 896/5760, Loss: 4.306347846984863\n",
      "Epoch: 1, Samples: 928/5760, Loss: 4.490410804748535\n",
      "Epoch: 1, Samples: 960/5760, Loss: 4.250631809234619\n",
      "Epoch: 1, Samples: 992/5760, Loss: 4.156490325927734\n",
      "Epoch: 1, Samples: 1024/5760, Loss: 4.32936429977417\n",
      "Epoch: 1, Samples: 1056/5760, Loss: 4.479154109954834\n",
      "Epoch: 1, Samples: 1088/5760, Loss: 4.271378040313721\n",
      "Epoch: 1, Samples: 1120/5760, Loss: 4.4137067794799805\n",
      "Epoch: 1, Samples: 1152/5760, Loss: 4.234439373016357\n",
      "Epoch: 1, Samples: 1184/5760, Loss: 4.245443344116211\n",
      "Epoch: 1, Samples: 1216/5760, Loss: 4.2165207862854\n",
      "Epoch: 1, Samples: 1248/5760, Loss: 4.194981575012207\n",
      "Epoch: 1, Samples: 1280/5760, Loss: 4.193394184112549\n",
      "Epoch: 1, Samples: 1312/5760, Loss: 4.219138145446777\n",
      "Epoch: 1, Samples: 1344/5760, Loss: 4.072146415710449\n",
      "Epoch: 1, Samples: 1376/5760, Loss: 3.976452350616455\n",
      "Epoch: 1, Samples: 1408/5760, Loss: 4.196877479553223\n",
      "Epoch: 1, Samples: 1440/5760, Loss: 4.249729156494141\n",
      "Epoch: 1, Samples: 1472/5760, Loss: 4.168496131896973\n",
      "Epoch: 1, Samples: 1504/5760, Loss: 4.121410369873047\n",
      "Epoch: 1, Samples: 1536/5760, Loss: 3.975299119949341\n",
      "Epoch: 1, Samples: 1568/5760, Loss: 4.100870132446289\n",
      "Epoch: 1, Samples: 1600/5760, Loss: 3.8304543495178223\n",
      "Epoch: 1, Samples: 1632/5760, Loss: 3.9852499961853027\n",
      "Epoch: 1, Samples: 1664/5760, Loss: 4.001320838928223\n",
      "Epoch: 1, Samples: 1696/5760, Loss: 3.6422295570373535\n",
      "Epoch: 1, Samples: 1728/5760, Loss: 3.869323253631592\n",
      "Epoch: 1, Samples: 1760/5760, Loss: 4.210515022277832\n",
      "Epoch: 1, Samples: 1792/5760, Loss: 4.2325263023376465\n",
      "Epoch: 1, Samples: 1824/5760, Loss: 3.940861701965332\n",
      "Epoch: 1, Samples: 1856/5760, Loss: 4.0739665031433105\n",
      "Epoch: 1, Samples: 1888/5760, Loss: 3.874654769897461\n",
      "Epoch: 1, Samples: 1920/5760, Loss: 3.693798065185547\n",
      "Epoch: 1, Samples: 1952/5760, Loss: 3.8896844387054443\n",
      "Epoch: 1, Samples: 1984/5760, Loss: 3.6782662868499756\n",
      "Epoch: 1, Samples: 2016/5760, Loss: 4.276858329772949\n",
      "Epoch: 1, Samples: 2048/5760, Loss: 4.081868648529053\n",
      "Epoch: 1, Samples: 2080/5760, Loss: 4.1196064949035645\n",
      "Epoch: 1, Samples: 2112/5760, Loss: 3.887018918991089\n",
      "Epoch: 1, Samples: 2144/5760, Loss: 3.393045425415039\n",
      "Epoch: 1, Samples: 2176/5760, Loss: 3.9663872718811035\n",
      "Epoch: 1, Samples: 2208/5760, Loss: 3.7794229984283447\n",
      "Epoch: 1, Samples: 2240/5760, Loss: 3.910245895385742\n",
      "Epoch: 1, Samples: 2272/5760, Loss: 3.7788798809051514\n",
      "Epoch: 1, Samples: 2304/5760, Loss: 3.6660258769989014\n",
      "Epoch: 1, Samples: 2336/5760, Loss: 3.8079049587249756\n",
      "Epoch: 1, Samples: 2368/5760, Loss: 4.0161919593811035\n",
      "Epoch: 1, Samples: 2400/5760, Loss: 3.74835205078125\n",
      "Epoch: 1, Samples: 2432/5760, Loss: 3.758326768875122\n",
      "Epoch: 1, Samples: 2464/5760, Loss: 3.7327213287353516\n",
      "Epoch: 1, Samples: 2496/5760, Loss: 3.422959804534912\n",
      "Epoch: 1, Samples: 2528/5760, Loss: 3.6402392387390137\n",
      "Epoch: 1, Samples: 2560/5760, Loss: 3.8793487548828125\n",
      "Epoch: 1, Samples: 2592/5760, Loss: 3.891021728515625\n",
      "Epoch: 1, Samples: 2624/5760, Loss: 3.6301920413970947\n",
      "Epoch: 1, Samples: 2656/5760, Loss: 3.44879150390625\n",
      "Epoch: 1, Samples: 2688/5760, Loss: 3.7832932472229004\n",
      "Epoch: 1, Samples: 2720/5760, Loss: 3.8045129776000977\n",
      "Epoch: 1, Samples: 2752/5760, Loss: 3.7895586490631104\n",
      "Epoch: 1, Samples: 2784/5760, Loss: 3.401829481124878\n",
      "Epoch: 1, Samples: 2816/5760, Loss: 3.591012954711914\n",
      "Epoch: 1, Samples: 2848/5760, Loss: 3.8847270011901855\n",
      "Epoch: 1, Samples: 2880/5760, Loss: 3.7686073780059814\n",
      "Epoch: 1, Samples: 2912/5760, Loss: 3.7156050205230713\n",
      "Epoch: 1, Samples: 2944/5760, Loss: 3.5211775302886963\n",
      "Epoch: 1, Samples: 2976/5760, Loss: 3.729599952697754\n",
      "Epoch: 1, Samples: 3008/5760, Loss: 3.425792932510376\n",
      "Epoch: 1, Samples: 3040/5760, Loss: 3.4907476902008057\n",
      "Epoch: 1, Samples: 3072/5760, Loss: 3.6751277446746826\n",
      "Epoch: 1, Samples: 3104/5760, Loss: 3.652531147003174\n",
      "Epoch: 1, Samples: 3136/5760, Loss: 3.656763792037964\n",
      "Epoch: 1, Samples: 3168/5760, Loss: 3.8008902072906494\n",
      "Epoch: 1, Samples: 3200/5760, Loss: 3.538037061691284\n",
      "Epoch: 1, Samples: 3232/5760, Loss: 3.4589223861694336\n",
      "Epoch: 1, Samples: 3264/5760, Loss: 3.0941975116729736\n",
      "Epoch: 1, Samples: 3296/5760, Loss: 3.3066437244415283\n",
      "Epoch: 1, Samples: 3328/5760, Loss: 3.3614790439605713\n",
      "Epoch: 1, Samples: 3360/5760, Loss: 3.4961965084075928\n",
      "Epoch: 1, Samples: 3392/5760, Loss: 3.397829532623291\n",
      "Epoch: 1, Samples: 3424/5760, Loss: 3.593621253967285\n",
      "Epoch: 1, Samples: 3456/5760, Loss: 3.2475526332855225\n",
      "Epoch: 1, Samples: 3488/5760, Loss: 3.125089406967163\n",
      "Epoch: 1, Samples: 3520/5760, Loss: 3.118396282196045\n",
      "Epoch: 1, Samples: 3552/5760, Loss: 3.5068042278289795\n",
      "Epoch: 1, Samples: 3584/5760, Loss: 3.4694697856903076\n",
      "Epoch: 1, Samples: 3616/5760, Loss: 3.2224578857421875\n",
      "Epoch: 1, Samples: 3648/5760, Loss: 3.2765119075775146\n",
      "Epoch: 1, Samples: 3680/5760, Loss: 3.0361626148223877\n",
      "Epoch: 1, Samples: 3712/5760, Loss: 3.198685646057129\n",
      "Epoch: 1, Samples: 3744/5760, Loss: 3.710911273956299\n",
      "Epoch: 1, Samples: 3776/5760, Loss: 3.507657766342163\n",
      "Epoch: 1, Samples: 3808/5760, Loss: 3.280839443206787\n",
      "Epoch: 1, Samples: 3840/5760, Loss: 3.6582655906677246\n",
      "Epoch: 1, Samples: 3872/5760, Loss: 3.411034107208252\n",
      "Epoch: 1, Samples: 3904/5760, Loss: 3.087338924407959\n",
      "Epoch: 1, Samples: 3936/5760, Loss: 3.218891143798828\n",
      "Epoch: 1, Samples: 3968/5760, Loss: 3.2979259490966797\n",
      "Epoch: 1, Samples: 4000/5760, Loss: 3.142406463623047\n",
      "Epoch: 1, Samples: 4032/5760, Loss: 3.197103500366211\n",
      "Epoch: 1, Samples: 4064/5760, Loss: 2.896353006362915\n",
      "Epoch: 1, Samples: 4096/5760, Loss: 3.388339042663574\n",
      "Epoch: 1, Samples: 4128/5760, Loss: 2.8649487495422363\n",
      "Epoch: 1, Samples: 4160/5760, Loss: 3.1390247344970703\n",
      "Epoch: 1, Samples: 4192/5760, Loss: 2.900705575942993\n",
      "Epoch: 1, Samples: 4224/5760, Loss: 3.2468714714050293\n",
      "Epoch: 1, Samples: 4256/5760, Loss: 2.9028303623199463\n",
      "Epoch: 1, Samples: 4288/5760, Loss: 3.0917320251464844\n",
      "Epoch: 1, Samples: 4320/5760, Loss: 2.769138813018799\n",
      "Epoch: 1, Samples: 4352/5760, Loss: 3.0358757972717285\n",
      "Epoch: 1, Samples: 4384/5760, Loss: 2.9298598766326904\n",
      "Epoch: 1, Samples: 4416/5760, Loss: 3.3472018241882324\n",
      "Epoch: 1, Samples: 4448/5760, Loss: 3.2246246337890625\n",
      "Epoch: 1, Samples: 4480/5760, Loss: 2.705134153366089\n",
      "Epoch: 1, Samples: 4512/5760, Loss: 2.765080690383911\n",
      "Epoch: 1, Samples: 4544/5760, Loss: 2.722269296646118\n",
      "Epoch: 1, Samples: 4576/5760, Loss: 2.8730924129486084\n",
      "Epoch: 1, Samples: 4608/5760, Loss: 2.988466262817383\n",
      "Epoch: 1, Samples: 4640/5760, Loss: 2.533940315246582\n",
      "Epoch: 1, Samples: 4672/5760, Loss: 2.512641668319702\n",
      "Epoch: 1, Samples: 4704/5760, Loss: 2.549724578857422\n",
      "Epoch: 1, Samples: 4736/5760, Loss: 2.9614148139953613\n",
      "Epoch: 1, Samples: 4768/5760, Loss: 2.986656665802002\n",
      "Epoch: 1, Samples: 4800/5760, Loss: 3.2018120288848877\n",
      "Epoch: 1, Samples: 4832/5760, Loss: 2.956165313720703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Samples: 4864/5760, Loss: 3.109308958053589\n",
      "Epoch: 1, Samples: 4896/5760, Loss: 2.5502212047576904\n",
      "Epoch: 1, Samples: 4928/5760, Loss: 2.6242527961730957\n",
      "Epoch: 1, Samples: 4960/5760, Loss: 2.5027661323547363\n",
      "Epoch: 1, Samples: 4992/5760, Loss: 2.4958298206329346\n",
      "Epoch: 1, Samples: 5024/5760, Loss: 2.8655614852905273\n",
      "Epoch: 1, Samples: 5056/5760, Loss: 2.970979690551758\n",
      "Epoch: 1, Samples: 5088/5760, Loss: 2.8893725872039795\n",
      "Epoch: 1, Samples: 5120/5760, Loss: 3.3477981090545654\n",
      "Epoch: 1, Samples: 5152/5760, Loss: 2.7707324028015137\n",
      "Epoch: 1, Samples: 5184/5760, Loss: 2.5357718467712402\n",
      "Epoch: 1, Samples: 5216/5760, Loss: 2.825629472732544\n",
      "Epoch: 1, Samples: 5248/5760, Loss: 2.8034169673919678\n",
      "Epoch: 1, Samples: 5280/5760, Loss: 2.5551035404205322\n",
      "Epoch: 1, Samples: 5312/5760, Loss: 2.763857364654541\n",
      "Epoch: 1, Samples: 5344/5760, Loss: 2.806051015853882\n",
      "Epoch: 1, Samples: 5376/5760, Loss: 3.1017913818359375\n",
      "Epoch: 1, Samples: 5408/5760, Loss: 2.909381151199341\n",
      "Epoch: 1, Samples: 5440/5760, Loss: 2.706570625305176\n",
      "Epoch: 1, Samples: 5472/5760, Loss: 2.9498112201690674\n",
      "Epoch: 1, Samples: 5504/5760, Loss: 2.619401216506958\n",
      "Epoch: 1, Samples: 5536/5760, Loss: 2.5998289585113525\n",
      "Epoch: 1, Samples: 5568/5760, Loss: 2.544095754623413\n",
      "Epoch: 1, Samples: 5600/5760, Loss: 2.7466044425964355\n",
      "Epoch: 1, Samples: 5632/5760, Loss: 2.477090835571289\n",
      "Epoch: 1, Samples: 5664/5760, Loss: 2.2944726943969727\n",
      "Epoch: 1, Samples: 5696/5760, Loss: 2.715921640396118\n",
      "Epoch: 1, Samples: 5728/5760, Loss: 2.349888801574707\n",
      "\n",
      "Epoch: 1\n",
      "Training set: Average loss: 3.6266\n",
      "Validation set: Average loss: 2.5023, Accuracy: 413/818 (50%)\n",
      "Epoch: 2, Samples: 0/5760, Loss: 2.5264601707458496\n",
      "Epoch: 2, Samples: 32/5760, Loss: 2.372725248336792\n",
      "Epoch: 2, Samples: 64/5760, Loss: 2.514549493789673\n",
      "Epoch: 2, Samples: 96/5760, Loss: 2.1625044345855713\n",
      "Epoch: 2, Samples: 128/5760, Loss: 2.6492817401885986\n",
      "Epoch: 2, Samples: 160/5760, Loss: 2.2195935249328613\n",
      "Epoch: 2, Samples: 192/5760, Loss: 2.380223274230957\n",
      "Epoch: 2, Samples: 224/5760, Loss: 2.060133934020996\n",
      "Epoch: 2, Samples: 256/5760, Loss: 2.923036813735962\n",
      "Epoch: 2, Samples: 288/5760, Loss: 2.5116240978240967\n",
      "Epoch: 2, Samples: 320/5760, Loss: 1.989597201347351\n",
      "Epoch: 2, Samples: 352/5760, Loss: 2.5960960388183594\n",
      "Epoch: 2, Samples: 384/5760, Loss: 2.6445846557617188\n",
      "Epoch: 2, Samples: 416/5760, Loss: 2.1657683849334717\n",
      "Epoch: 2, Samples: 448/5760, Loss: 2.304072618484497\n",
      "Epoch: 2, Samples: 480/5760, Loss: 2.345510482788086\n",
      "Epoch: 2, Samples: 512/5760, Loss: 1.9972676038742065\n",
      "Epoch: 2, Samples: 544/5760, Loss: 2.0778605937957764\n",
      "Epoch: 2, Samples: 576/5760, Loss: 2.406494617462158\n",
      "Epoch: 2, Samples: 608/5760, Loss: 2.6280901432037354\n",
      "Epoch: 2, Samples: 640/5760, Loss: 2.331526041030884\n",
      "Epoch: 2, Samples: 672/5760, Loss: 2.1655828952789307\n",
      "Epoch: 2, Samples: 704/5760, Loss: 2.3435215950012207\n",
      "Epoch: 2, Samples: 736/5760, Loss: 2.047898054122925\n",
      "Epoch: 2, Samples: 768/5760, Loss: 2.058130979537964\n",
      "Epoch: 2, Samples: 800/5760, Loss: 2.6443281173706055\n",
      "Epoch: 2, Samples: 832/5760, Loss: 2.170692205429077\n",
      "Epoch: 2, Samples: 864/5760, Loss: 2.5798871517181396\n",
      "Epoch: 2, Samples: 896/5760, Loss: 2.5611627101898193\n",
      "Epoch: 2, Samples: 928/5760, Loss: 2.5023233890533447\n",
      "Epoch: 2, Samples: 960/5760, Loss: 1.9884909391403198\n",
      "Epoch: 2, Samples: 992/5760, Loss: 2.199211359024048\n",
      "Epoch: 2, Samples: 1024/5760, Loss: 2.1931099891662598\n",
      "Epoch: 2, Samples: 1056/5760, Loss: 2.2061336040496826\n",
      "Epoch: 2, Samples: 1088/5760, Loss: 2.07633113861084\n",
      "Epoch: 2, Samples: 1120/5760, Loss: 2.288681745529175\n",
      "Epoch: 2, Samples: 1152/5760, Loss: 2.4646382331848145\n",
      "Epoch: 2, Samples: 1184/5760, Loss: 2.4438679218292236\n",
      "Epoch: 2, Samples: 1216/5760, Loss: 2.4740986824035645\n",
      "Epoch: 2, Samples: 1248/5760, Loss: 2.1774063110351562\n",
      "Epoch: 2, Samples: 1280/5760, Loss: 2.0031578540802\n",
      "Epoch: 2, Samples: 1312/5760, Loss: 2.1295158863067627\n",
      "Epoch: 2, Samples: 1344/5760, Loss: 1.8233722448349\n",
      "Epoch: 2, Samples: 1376/5760, Loss: 2.112626075744629\n",
      "Epoch: 2, Samples: 1408/5760, Loss: 1.7575708627700806\n",
      "Epoch: 2, Samples: 1440/5760, Loss: 2.1666014194488525\n",
      "Epoch: 2, Samples: 1472/5760, Loss: 1.8805551528930664\n",
      "Epoch: 2, Samples: 1504/5760, Loss: 2.03090238571167\n",
      "Epoch: 2, Samples: 1536/5760, Loss: 1.8358200788497925\n",
      "Epoch: 2, Samples: 1568/5760, Loss: 2.1729116439819336\n",
      "Epoch: 2, Samples: 1600/5760, Loss: 2.237901210784912\n",
      "Epoch: 2, Samples: 1632/5760, Loss: 1.999362826347351\n",
      "Epoch: 2, Samples: 1664/5760, Loss: 2.1716933250427246\n",
      "Epoch: 2, Samples: 1696/5760, Loss: 2.3438823223114014\n",
      "Epoch: 2, Samples: 1728/5760, Loss: 2.5043911933898926\n",
      "Epoch: 2, Samples: 1760/5760, Loss: 1.8191473484039307\n",
      "Epoch: 2, Samples: 1792/5760, Loss: 2.2743263244628906\n",
      "Epoch: 2, Samples: 1824/5760, Loss: 2.3585493564605713\n",
      "Epoch: 2, Samples: 1856/5760, Loss: 1.938473105430603\n",
      "Epoch: 2, Samples: 1888/5760, Loss: 2.1261754035949707\n",
      "Epoch: 2, Samples: 1920/5760, Loss: 1.8769817352294922\n",
      "Epoch: 2, Samples: 1952/5760, Loss: 1.8368430137634277\n",
      "Epoch: 2, Samples: 1984/5760, Loss: 2.115358591079712\n",
      "Epoch: 2, Samples: 2016/5760, Loss: 2.125476598739624\n",
      "Epoch: 2, Samples: 2048/5760, Loss: 1.671794056892395\n",
      "Epoch: 2, Samples: 2080/5760, Loss: 2.157461643218994\n",
      "Epoch: 2, Samples: 2112/5760, Loss: 2.01078200340271\n",
      "Epoch: 2, Samples: 2144/5760, Loss: 1.978869080543518\n",
      "Epoch: 2, Samples: 2176/5760, Loss: 1.9864706993103027\n",
      "Epoch: 2, Samples: 2208/5760, Loss: 1.8197911977767944\n",
      "Epoch: 2, Samples: 2240/5760, Loss: 1.6268179416656494\n",
      "Epoch: 2, Samples: 2272/5760, Loss: 1.9718492031097412\n",
      "Epoch: 2, Samples: 2304/5760, Loss: 2.0136075019836426\n",
      "Epoch: 2, Samples: 2336/5760, Loss: 2.1638853549957275\n",
      "Epoch: 2, Samples: 2368/5760, Loss: 2.0057668685913086\n",
      "Epoch: 2, Samples: 2400/5760, Loss: 1.8371349573135376\n",
      "Epoch: 2, Samples: 2432/5760, Loss: 1.8022106885910034\n",
      "Epoch: 2, Samples: 2464/5760, Loss: 2.0291264057159424\n",
      "Epoch: 2, Samples: 2496/5760, Loss: 1.994923710823059\n",
      "Epoch: 2, Samples: 2528/5760, Loss: 1.5525000095367432\n",
      "Epoch: 2, Samples: 2560/5760, Loss: 2.0130209922790527\n",
      "Epoch: 2, Samples: 2592/5760, Loss: 1.8194935321807861\n",
      "Epoch: 2, Samples: 2624/5760, Loss: 1.7888141870498657\n",
      "Epoch: 2, Samples: 2656/5760, Loss: 2.1153619289398193\n",
      "Epoch: 2, Samples: 2688/5760, Loss: 2.1465792655944824\n",
      "Epoch: 2, Samples: 2720/5760, Loss: 1.9362565279006958\n",
      "Epoch: 2, Samples: 2752/5760, Loss: 1.9887391328811646\n",
      "Epoch: 2, Samples: 2784/5760, Loss: 1.894386649131775\n",
      "Epoch: 2, Samples: 2816/5760, Loss: 1.9720425605773926\n",
      "Epoch: 2, Samples: 2848/5760, Loss: 2.0703628063201904\n",
      "Epoch: 2, Samples: 2880/5760, Loss: 2.0451042652130127\n",
      "Epoch: 2, Samples: 2912/5760, Loss: 2.062559127807617\n",
      "Epoch: 2, Samples: 2944/5760, Loss: 1.861534595489502\n",
      "Epoch: 2, Samples: 2976/5760, Loss: 1.877562165260315\n",
      "Epoch: 2, Samples: 3008/5760, Loss: 1.7212698459625244\n",
      "Epoch: 2, Samples: 3040/5760, Loss: 2.0146613121032715\n",
      "Epoch: 2, Samples: 3072/5760, Loss: 2.1379926204681396\n",
      "Epoch: 2, Samples: 3104/5760, Loss: 1.8881545066833496\n",
      "Epoch: 2, Samples: 3136/5760, Loss: 2.142190456390381\n",
      "Epoch: 2, Samples: 3168/5760, Loss: 1.7247003316879272\n",
      "Epoch: 2, Samples: 3200/5760, Loss: 1.8745102882385254\n",
      "Epoch: 2, Samples: 3232/5760, Loss: 2.07768177986145\n",
      "Epoch: 2, Samples: 3264/5760, Loss: 1.940860629081726\n",
      "Epoch: 2, Samples: 3296/5760, Loss: 1.991720199584961\n",
      "Epoch: 2, Samples: 3328/5760, Loss: 2.010066270828247\n",
      "Epoch: 2, Samples: 3360/5760, Loss: 1.7731025218963623\n",
      "Epoch: 2, Samples: 3392/5760, Loss: 2.095175266265869\n",
      "Epoch: 2, Samples: 3424/5760, Loss: 1.8300361633300781\n",
      "Epoch: 2, Samples: 3456/5760, Loss: 1.940002679824829\n",
      "Epoch: 2, Samples: 3488/5760, Loss: 1.8971079587936401\n",
      "Epoch: 2, Samples: 3520/5760, Loss: 2.0466136932373047\n",
      "Epoch: 2, Samples: 3552/5760, Loss: 1.823211669921875\n",
      "Epoch: 2, Samples: 3584/5760, Loss: 1.6907002925872803\n",
      "Epoch: 2, Samples: 3616/5760, Loss: 1.5318747758865356\n",
      "Epoch: 2, Samples: 3648/5760, Loss: 1.8848059177398682\n",
      "Epoch: 2, Samples: 3680/5760, Loss: 1.8332812786102295\n",
      "Epoch: 2, Samples: 3712/5760, Loss: 1.4894382953643799\n",
      "Epoch: 2, Samples: 3744/5760, Loss: 1.8394839763641357\n",
      "Epoch: 2, Samples: 3776/5760, Loss: 2.0314114093780518\n",
      "Epoch: 2, Samples: 3808/5760, Loss: 1.7955803871154785\n",
      "Epoch: 2, Samples: 3840/5760, Loss: 2.1606030464172363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Samples: 3872/5760, Loss: 1.6289026737213135\n",
      "Epoch: 2, Samples: 3904/5760, Loss: 1.7575404644012451\n",
      "Epoch: 2, Samples: 3936/5760, Loss: 1.4349433183670044\n",
      "Epoch: 2, Samples: 3968/5760, Loss: 2.1443421840667725\n",
      "Epoch: 2, Samples: 4000/5760, Loss: 1.6541441679000854\n",
      "Epoch: 2, Samples: 4032/5760, Loss: 1.6755552291870117\n",
      "Epoch: 2, Samples: 4064/5760, Loss: 1.542382001876831\n",
      "Epoch: 2, Samples: 4096/5760, Loss: 1.136785626411438\n",
      "Epoch: 2, Samples: 4128/5760, Loss: 1.722957730293274\n",
      "Epoch: 2, Samples: 4160/5760, Loss: 1.4699292182922363\n",
      "Epoch: 2, Samples: 4192/5760, Loss: 1.5440130233764648\n",
      "Epoch: 2, Samples: 4224/5760, Loss: 1.6578527688980103\n",
      "Epoch: 2, Samples: 4256/5760, Loss: 1.8082846403121948\n",
      "Epoch: 2, Samples: 4288/5760, Loss: 1.913496732711792\n",
      "Epoch: 2, Samples: 4320/5760, Loss: 1.6602660417556763\n",
      "Epoch: 2, Samples: 4352/5760, Loss: 1.4613606929779053\n",
      "Epoch: 2, Samples: 4384/5760, Loss: 1.7006946802139282\n",
      "Epoch: 2, Samples: 4416/5760, Loss: 1.4747889041900635\n",
      "Epoch: 2, Samples: 4448/5760, Loss: 1.5641324520111084\n",
      "Epoch: 2, Samples: 4480/5760, Loss: 1.6074930429458618\n",
      "Epoch: 2, Samples: 4512/5760, Loss: 1.37614107131958\n",
      "Epoch: 2, Samples: 4544/5760, Loss: 1.4634641408920288\n",
      "Epoch: 2, Samples: 4576/5760, Loss: 1.286104679107666\n",
      "Epoch: 2, Samples: 4608/5760, Loss: 1.8038759231567383\n",
      "Epoch: 2, Samples: 4640/5760, Loss: 1.4970523118972778\n",
      "Epoch: 2, Samples: 4672/5760, Loss: 1.5523332357406616\n",
      "Epoch: 2, Samples: 4704/5760, Loss: 1.6410844326019287\n",
      "Epoch: 2, Samples: 4736/5760, Loss: 1.473770260810852\n",
      "Epoch: 2, Samples: 4768/5760, Loss: 1.8515150547027588\n",
      "Epoch: 2, Samples: 4800/5760, Loss: 1.7336276769638062\n",
      "Epoch: 2, Samples: 4832/5760, Loss: 1.479992151260376\n",
      "Epoch: 2, Samples: 4864/5760, Loss: 1.7541524171829224\n",
      "Epoch: 2, Samples: 4896/5760, Loss: 1.4948701858520508\n",
      "Epoch: 2, Samples: 4928/5760, Loss: 1.735023856163025\n",
      "Epoch: 2, Samples: 4960/5760, Loss: 1.1215482950210571\n",
      "Epoch: 2, Samples: 4992/5760, Loss: 1.8463845252990723\n",
      "Epoch: 2, Samples: 5024/5760, Loss: 1.7084407806396484\n",
      "Epoch: 2, Samples: 5056/5760, Loss: 1.4723204374313354\n",
      "Epoch: 2, Samples: 5088/5760, Loss: 1.5895071029663086\n",
      "Epoch: 2, Samples: 5120/5760, Loss: 1.892342448234558\n",
      "Epoch: 2, Samples: 5152/5760, Loss: 1.5976694822311401\n",
      "Epoch: 2, Samples: 5184/5760, Loss: 1.4584518671035767\n",
      "Epoch: 2, Samples: 5216/5760, Loss: 1.5009291172027588\n",
      "Epoch: 2, Samples: 5248/5760, Loss: 1.6259042024612427\n",
      "Epoch: 2, Samples: 5280/5760, Loss: 2.2524006366729736\n",
      "Epoch: 2, Samples: 5312/5760, Loss: 1.9611057043075562\n",
      "Epoch: 2, Samples: 5344/5760, Loss: 1.4825708866119385\n",
      "Epoch: 2, Samples: 5376/5760, Loss: 1.6146457195281982\n",
      "Epoch: 2, Samples: 5408/5760, Loss: 1.3667657375335693\n",
      "Epoch: 2, Samples: 5440/5760, Loss: 1.6038562059402466\n",
      "Epoch: 2, Samples: 5472/5760, Loss: 1.5207538604736328\n",
      "Epoch: 2, Samples: 5504/5760, Loss: 1.2195440530776978\n",
      "Epoch: 2, Samples: 5536/5760, Loss: 1.3344509601593018\n",
      "Epoch: 2, Samples: 5568/5760, Loss: 1.3207290172576904\n",
      "Epoch: 2, Samples: 5600/5760, Loss: 1.950014352798462\n",
      "Epoch: 2, Samples: 5632/5760, Loss: 1.0968767404556274\n",
      "Epoch: 2, Samples: 5664/5760, Loss: 1.5041003227233887\n",
      "Epoch: 2, Samples: 5696/5760, Loss: 1.5716253519058228\n",
      "Epoch: 2, Samples: 5728/5760, Loss: 2.1860389709472656\n",
      "\n",
      "Epoch: 2\n",
      "Training set: Average loss: 1.9274\n",
      "Validation set: Average loss: 1.5599, Accuracy: 568/818 (69%)\n",
      "Saving model (epoch 2) with lowest validation loss: 1.5598675425236042\n",
      "Epoch: 3, Samples: 0/5760, Loss: 1.0775800943374634\n",
      "Epoch: 3, Samples: 32/5760, Loss: 1.649490237236023\n",
      "Epoch: 3, Samples: 64/5760, Loss: 1.3026306629180908\n",
      "Epoch: 3, Samples: 96/5760, Loss: 1.128420352935791\n",
      "Epoch: 3, Samples: 128/5760, Loss: 1.3500564098358154\n",
      "Epoch: 3, Samples: 160/5760, Loss: 1.55777108669281\n",
      "Epoch: 3, Samples: 192/5760, Loss: 1.4656697511672974\n",
      "Epoch: 3, Samples: 224/5760, Loss: 1.1949127912521362\n",
      "Epoch: 3, Samples: 256/5760, Loss: 1.3626450300216675\n",
      "Epoch: 3, Samples: 288/5760, Loss: 1.429696798324585\n",
      "Epoch: 3, Samples: 320/5760, Loss: 1.434333086013794\n",
      "Epoch: 3, Samples: 352/5760, Loss: 1.2988837957382202\n",
      "Epoch: 3, Samples: 384/5760, Loss: 1.1602306365966797\n",
      "Epoch: 3, Samples: 416/5760, Loss: 1.2395421266555786\n",
      "Epoch: 3, Samples: 448/5760, Loss: 1.1158429384231567\n",
      "Epoch: 3, Samples: 480/5760, Loss: 1.2008814811706543\n",
      "Epoch: 3, Samples: 512/5760, Loss: 1.6995128393173218\n",
      "Epoch: 3, Samples: 544/5760, Loss: 1.3583072423934937\n",
      "Epoch: 3, Samples: 576/5760, Loss: 1.0009002685546875\n",
      "Epoch: 3, Samples: 608/5760, Loss: 1.4503775835037231\n",
      "Epoch: 3, Samples: 640/5760, Loss: 1.8438050746917725\n",
      "Epoch: 3, Samples: 672/5760, Loss: 1.3976433277130127\n",
      "Epoch: 3, Samples: 704/5760, Loss: 1.8461458683013916\n",
      "Epoch: 3, Samples: 736/5760, Loss: 0.925784707069397\n",
      "Epoch: 3, Samples: 768/5760, Loss: 0.9607349038124084\n",
      "Epoch: 3, Samples: 800/5760, Loss: 1.4342186450958252\n",
      "Epoch: 3, Samples: 832/5760, Loss: 1.3660017251968384\n",
      "Epoch: 3, Samples: 864/5760, Loss: 1.3879148960113525\n",
      "Epoch: 3, Samples: 896/5760, Loss: 1.2166606187820435\n",
      "Epoch: 3, Samples: 928/5760, Loss: 1.8139104843139648\n",
      "Epoch: 3, Samples: 960/5760, Loss: 1.0419918298721313\n",
      "Epoch: 3, Samples: 992/5760, Loss: 1.2586835622787476\n",
      "Epoch: 3, Samples: 1024/5760, Loss: 1.3478115797042847\n",
      "Epoch: 3, Samples: 1056/5760, Loss: 1.3594191074371338\n",
      "Epoch: 3, Samples: 1088/5760, Loss: 1.3688957691192627\n",
      "Epoch: 3, Samples: 1120/5760, Loss: 1.1389260292053223\n",
      "Epoch: 3, Samples: 1152/5760, Loss: 1.223544955253601\n",
      "Epoch: 3, Samples: 1184/5760, Loss: 1.6099903583526611\n",
      "Epoch: 3, Samples: 1216/5760, Loss: 1.070383071899414\n",
      "Epoch: 3, Samples: 1248/5760, Loss: 1.5293784141540527\n",
      "Epoch: 3, Samples: 1280/5760, Loss: 1.2366551160812378\n",
      "Epoch: 3, Samples: 1312/5760, Loss: 1.4331741333007812\n",
      "Epoch: 3, Samples: 1344/5760, Loss: 1.4693313837051392\n",
      "Epoch: 3, Samples: 1376/5760, Loss: 1.0293593406677246\n",
      "Epoch: 3, Samples: 1408/5760, Loss: 1.2505707740783691\n",
      "Epoch: 3, Samples: 1440/5760, Loss: 1.307377815246582\n",
      "Epoch: 3, Samples: 1472/5760, Loss: 1.1493362188339233\n",
      "Epoch: 3, Samples: 1504/5760, Loss: 1.4352377653121948\n",
      "Epoch: 3, Samples: 1536/5760, Loss: 1.4020459651947021\n",
      "Epoch: 3, Samples: 1568/5760, Loss: 1.0173537731170654\n",
      "Epoch: 3, Samples: 1600/5760, Loss: 1.1758633852005005\n",
      "Epoch: 3, Samples: 1632/5760, Loss: 1.6329011917114258\n",
      "Epoch: 3, Samples: 1664/5760, Loss: 1.4692007303237915\n",
      "Epoch: 3, Samples: 1696/5760, Loss: 1.1728217601776123\n",
      "Epoch: 3, Samples: 1728/5760, Loss: 1.0576516389846802\n",
      "Epoch: 3, Samples: 1760/5760, Loss: 1.2541147470474243\n",
      "Epoch: 3, Samples: 1792/5760, Loss: 1.1593329906463623\n",
      "Epoch: 3, Samples: 1824/5760, Loss: 1.1232576370239258\n",
      "Epoch: 3, Samples: 1856/5760, Loss: 1.2912428379058838\n",
      "Epoch: 3, Samples: 1888/5760, Loss: 1.5124022960662842\n",
      "Epoch: 3, Samples: 1920/5760, Loss: 1.2009270191192627\n",
      "Epoch: 3, Samples: 1952/5760, Loss: 1.0108472108840942\n",
      "Epoch: 3, Samples: 1984/5760, Loss: 1.3382823467254639\n",
      "Epoch: 3, Samples: 2016/5760, Loss: 1.1312315464019775\n",
      "Epoch: 3, Samples: 2048/5760, Loss: 1.3913469314575195\n",
      "Epoch: 3, Samples: 2080/5760, Loss: 1.150543451309204\n",
      "Epoch: 3, Samples: 2112/5760, Loss: 1.6003352403640747\n",
      "Epoch: 3, Samples: 2144/5760, Loss: 0.9420625567436218\n",
      "Epoch: 3, Samples: 2176/5760, Loss: 0.8948681950569153\n",
      "Epoch: 3, Samples: 2208/5760, Loss: 0.9927650690078735\n",
      "Epoch: 3, Samples: 2240/5760, Loss: 1.3256405591964722\n",
      "Epoch: 3, Samples: 2272/5760, Loss: 1.0749051570892334\n",
      "Epoch: 3, Samples: 2304/5760, Loss: 1.1364753246307373\n",
      "Epoch: 3, Samples: 2336/5760, Loss: 1.489311933517456\n",
      "Epoch: 3, Samples: 2368/5760, Loss: 1.1813201904296875\n",
      "Epoch: 3, Samples: 2400/5760, Loss: 1.141223430633545\n",
      "Epoch: 3, Samples: 2432/5760, Loss: 1.1025141477584839\n",
      "Epoch: 3, Samples: 2464/5760, Loss: 0.935513973236084\n",
      "Epoch: 3, Samples: 2496/5760, Loss: 1.2447590827941895\n",
      "Epoch: 3, Samples: 2528/5760, Loss: 1.0103135108947754\n",
      "Epoch: 3, Samples: 2560/5760, Loss: 1.3542343378067017\n",
      "Epoch: 3, Samples: 2592/5760, Loss: 1.2606773376464844\n",
      "Epoch: 3, Samples: 2624/5760, Loss: 1.1242318153381348\n",
      "Epoch: 3, Samples: 2656/5760, Loss: 1.3150787353515625\n",
      "Epoch: 3, Samples: 2688/5760, Loss: 1.4492813348770142\n",
      "Epoch: 3, Samples: 2720/5760, Loss: 0.7752484083175659\n",
      "Epoch: 3, Samples: 2752/5760, Loss: 1.5124497413635254\n",
      "Epoch: 3, Samples: 2784/5760, Loss: 1.0845543146133423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Samples: 2816/5760, Loss: 0.7566046714782715\n",
      "Epoch: 3, Samples: 2848/5760, Loss: 0.9338341355323792\n",
      "Epoch: 3, Samples: 2880/5760, Loss: 1.028013825416565\n",
      "Epoch: 3, Samples: 2912/5760, Loss: 1.0444105863571167\n",
      "Epoch: 3, Samples: 2944/5760, Loss: 0.8622292876243591\n",
      "Epoch: 3, Samples: 2976/5760, Loss: 0.9319918751716614\n",
      "Epoch: 3, Samples: 3008/5760, Loss: 0.9869404435157776\n",
      "Epoch: 3, Samples: 3040/5760, Loss: 1.0346956253051758\n",
      "Epoch: 3, Samples: 3072/5760, Loss: 1.1696256399154663\n",
      "Epoch: 3, Samples: 3104/5760, Loss: 1.0973968505859375\n",
      "Epoch: 3, Samples: 3136/5760, Loss: 1.1926771402359009\n",
      "Epoch: 3, Samples: 3168/5760, Loss: 1.004628300666809\n",
      "Epoch: 3, Samples: 3200/5760, Loss: 1.1826084852218628\n",
      "Epoch: 3, Samples: 3232/5760, Loss: 1.0404101610183716\n",
      "Epoch: 3, Samples: 3264/5760, Loss: 0.9176484942436218\n",
      "Epoch: 3, Samples: 3296/5760, Loss: 1.2525078058242798\n",
      "Epoch: 3, Samples: 3328/5760, Loss: 0.9921475052833557\n",
      "Epoch: 3, Samples: 3360/5760, Loss: 0.9892899990081787\n",
      "Epoch: 3, Samples: 3392/5760, Loss: 1.1287351846694946\n",
      "Epoch: 3, Samples: 3424/5760, Loss: 1.1793262958526611\n",
      "Epoch: 3, Samples: 3456/5760, Loss: 1.2071256637573242\n",
      "Epoch: 3, Samples: 3488/5760, Loss: 0.9827107787132263\n",
      "Epoch: 3, Samples: 3520/5760, Loss: 1.0114034414291382\n",
      "Epoch: 3, Samples: 3552/5760, Loss: 0.9985936880111694\n",
      "Epoch: 3, Samples: 3584/5760, Loss: 0.8516186475753784\n",
      "Epoch: 3, Samples: 3616/5760, Loss: 1.2062872648239136\n",
      "Epoch: 3, Samples: 3648/5760, Loss: 0.8523203134536743\n",
      "Epoch: 3, Samples: 3680/5760, Loss: 0.9185788631439209\n",
      "Epoch: 3, Samples: 3712/5760, Loss: 0.9135342836380005\n",
      "Epoch: 3, Samples: 3744/5760, Loss: 0.8755924105644226\n",
      "Epoch: 3, Samples: 3776/5760, Loss: 1.014299988746643\n",
      "Epoch: 3, Samples: 3808/5760, Loss: 1.2055968046188354\n",
      "Epoch: 3, Samples: 3840/5760, Loss: 1.0359349250793457\n",
      "Epoch: 3, Samples: 3872/5760, Loss: 1.145777940750122\n",
      "Epoch: 3, Samples: 3904/5760, Loss: 0.7377728223800659\n",
      "Epoch: 3, Samples: 3936/5760, Loss: 0.9417067766189575\n",
      "Epoch: 3, Samples: 3968/5760, Loss: 0.8672356605529785\n",
      "Epoch: 3, Samples: 4000/5760, Loss: 1.0318740606307983\n",
      "Epoch: 3, Samples: 4032/5760, Loss: 0.9378362894058228\n",
      "Epoch: 3, Samples: 4064/5760, Loss: 1.0197348594665527\n",
      "Epoch: 3, Samples: 4096/5760, Loss: 1.314116358757019\n",
      "Epoch: 3, Samples: 4128/5760, Loss: 1.2033852338790894\n",
      "Epoch: 3, Samples: 4160/5760, Loss: 1.177323818206787\n",
      "Epoch: 3, Samples: 4192/5760, Loss: 1.3196196556091309\n",
      "Epoch: 3, Samples: 4224/5760, Loss: 1.1431903839111328\n",
      "Epoch: 3, Samples: 4256/5760, Loss: 0.8532412052154541\n",
      "Epoch: 3, Samples: 4288/5760, Loss: 1.0034655332565308\n",
      "Epoch: 3, Samples: 4320/5760, Loss: 1.2109240293502808\n",
      "Epoch: 3, Samples: 4352/5760, Loss: 1.204132080078125\n",
      "Epoch: 3, Samples: 4384/5760, Loss: 0.9685389995574951\n",
      "Epoch: 3, Samples: 4416/5760, Loss: 1.316218614578247\n",
      "Epoch: 3, Samples: 4448/5760, Loss: 0.9749995470046997\n",
      "Epoch: 3, Samples: 4480/5760, Loss: 0.9731202125549316\n",
      "Epoch: 3, Samples: 4512/5760, Loss: 1.195999264717102\n",
      "Epoch: 3, Samples: 4544/5760, Loss: 0.7196249961853027\n",
      "Epoch: 3, Samples: 4576/5760, Loss: 0.7556653022766113\n",
      "Epoch: 3, Samples: 4608/5760, Loss: 1.0044140815734863\n",
      "Epoch: 3, Samples: 4640/5760, Loss: 1.247741937637329\n",
      "Epoch: 3, Samples: 4672/5760, Loss: 1.435933232307434\n",
      "Epoch: 3, Samples: 4704/5760, Loss: 0.8612073659896851\n",
      "Epoch: 3, Samples: 4736/5760, Loss: 1.0012121200561523\n",
      "Epoch: 3, Samples: 4768/5760, Loss: 1.0987868309020996\n",
      "Epoch: 3, Samples: 4800/5760, Loss: 0.8434460163116455\n",
      "Epoch: 3, Samples: 4832/5760, Loss: 1.158566951751709\n",
      "Epoch: 3, Samples: 4864/5760, Loss: 0.9353148937225342\n",
      "Epoch: 3, Samples: 4896/5760, Loss: 0.8609159588813782\n",
      "Epoch: 3, Samples: 4928/5760, Loss: 0.9682605266571045\n",
      "Epoch: 3, Samples: 4960/5760, Loss: 1.1213809251785278\n",
      "Epoch: 3, Samples: 4992/5760, Loss: 0.7837785482406616\n",
      "Epoch: 3, Samples: 5024/5760, Loss: 1.0777305364608765\n",
      "Epoch: 3, Samples: 5056/5760, Loss: 1.2215826511383057\n",
      "Epoch: 3, Samples: 5088/5760, Loss: 1.054084062576294\n",
      "Epoch: 3, Samples: 5120/5760, Loss: 0.8026559352874756\n",
      "Epoch: 3, Samples: 5152/5760, Loss: 1.0941978693008423\n",
      "Epoch: 3, Samples: 5184/5760, Loss: 1.046112060546875\n",
      "Epoch: 3, Samples: 5216/5760, Loss: 1.1878228187561035\n",
      "Epoch: 3, Samples: 5248/5760, Loss: 1.0483009815216064\n",
      "Epoch: 3, Samples: 5280/5760, Loss: 0.8523521423339844\n",
      "Epoch: 3, Samples: 5312/5760, Loss: 1.3002278804779053\n",
      "Epoch: 3, Samples: 5344/5760, Loss: 1.189227819442749\n",
      "Epoch: 3, Samples: 5376/5760, Loss: 1.0490140914916992\n",
      "Epoch: 3, Samples: 5408/5760, Loss: 0.7408204674720764\n",
      "Epoch: 3, Samples: 5440/5760, Loss: 0.9093270301818848\n",
      "Epoch: 3, Samples: 5472/5760, Loss: 0.953385055065155\n",
      "Epoch: 3, Samples: 5504/5760, Loss: 0.990625262260437\n",
      "Epoch: 3, Samples: 5536/5760, Loss: 1.170658826828003\n",
      "Epoch: 3, Samples: 5568/5760, Loss: 0.8062899708747864\n",
      "Epoch: 3, Samples: 5600/5760, Loss: 1.1166468858718872\n",
      "Epoch: 3, Samples: 5632/5760, Loss: 1.6460626125335693\n",
      "Epoch: 3, Samples: 5664/5760, Loss: 0.843268632888794\n",
      "Epoch: 3, Samples: 5696/5760, Loss: 1.0668033361434937\n",
      "Epoch: 3, Samples: 5728/5760, Loss: 2.939906597137451\n",
      "\n",
      "Epoch: 3\n",
      "Training set: Average loss: 1.1645\n",
      "Validation set: Average loss: 0.9773, Accuracy: 667/818 (82%)\n",
      "Saving model (epoch 3) with lowest validation loss: 0.9772833127241868\n",
      "Epoch: 4, Samples: 0/5760, Loss: 1.2371220588684082\n",
      "Epoch: 4, Samples: 32/5760, Loss: 1.0488201379776\n",
      "Epoch: 4, Samples: 64/5760, Loss: 0.786014199256897\n",
      "Epoch: 4, Samples: 96/5760, Loss: 1.1419267654418945\n",
      "Epoch: 4, Samples: 128/5760, Loss: 0.8250700235366821\n",
      "Epoch: 4, Samples: 160/5760, Loss: 0.8120954036712646\n",
      "Epoch: 4, Samples: 192/5760, Loss: 1.0129036903381348\n",
      "Epoch: 4, Samples: 224/5760, Loss: 0.833280086517334\n",
      "Epoch: 4, Samples: 256/5760, Loss: 0.794329822063446\n",
      "Epoch: 4, Samples: 288/5760, Loss: 0.7473217844963074\n",
      "Epoch: 4, Samples: 320/5760, Loss: 0.8299330472946167\n",
      "Epoch: 4, Samples: 352/5760, Loss: 0.6296495795249939\n",
      "Epoch: 4, Samples: 384/5760, Loss: 1.0820136070251465\n",
      "Epoch: 4, Samples: 416/5760, Loss: 1.0950417518615723\n",
      "Epoch: 4, Samples: 448/5760, Loss: 0.864725649356842\n",
      "Epoch: 4, Samples: 480/5760, Loss: 0.8669605255126953\n",
      "Epoch: 4, Samples: 512/5760, Loss: 0.9199498891830444\n",
      "Epoch: 4, Samples: 544/5760, Loss: 0.6403788328170776\n",
      "Epoch: 4, Samples: 576/5760, Loss: 0.7821345925331116\n",
      "Epoch: 4, Samples: 608/5760, Loss: 0.8613574504852295\n",
      "Epoch: 4, Samples: 640/5760, Loss: 0.8445713520050049\n",
      "Epoch: 4, Samples: 672/5760, Loss: 0.8859111666679382\n",
      "Epoch: 4, Samples: 704/5760, Loss: 0.7018678188323975\n",
      "Epoch: 4, Samples: 736/5760, Loss: 0.8423895835876465\n",
      "Epoch: 4, Samples: 768/5760, Loss: 0.7552282810211182\n",
      "Epoch: 4, Samples: 800/5760, Loss: 0.6278680562973022\n",
      "Epoch: 4, Samples: 832/5760, Loss: 0.8256903290748596\n",
      "Epoch: 4, Samples: 864/5760, Loss: 0.8180479407310486\n",
      "Epoch: 4, Samples: 896/5760, Loss: 0.9698381423950195\n",
      "Epoch: 4, Samples: 928/5760, Loss: 0.6784728765487671\n",
      "Epoch: 4, Samples: 960/5760, Loss: 0.8780333995819092\n",
      "Epoch: 4, Samples: 992/5760, Loss: 0.9467980861663818\n",
      "Epoch: 4, Samples: 1024/5760, Loss: 0.8509670495986938\n",
      "Epoch: 4, Samples: 1056/5760, Loss: 0.8593938946723938\n",
      "Epoch: 4, Samples: 1088/5760, Loss: 0.753616452217102\n",
      "Epoch: 4, Samples: 1120/5760, Loss: 0.73546302318573\n",
      "Epoch: 4, Samples: 1152/5760, Loss: 0.6591837406158447\n",
      "Epoch: 4, Samples: 1184/5760, Loss: 0.9978741407394409\n",
      "Epoch: 4, Samples: 1216/5760, Loss: 0.6127054691314697\n",
      "Epoch: 4, Samples: 1248/5760, Loss: 0.6734174489974976\n",
      "Epoch: 4, Samples: 1280/5760, Loss: 0.4473462700843811\n",
      "Epoch: 4, Samples: 1312/5760, Loss: 0.728507399559021\n",
      "Epoch: 4, Samples: 1344/5760, Loss: 0.7672642469406128\n",
      "Epoch: 4, Samples: 1376/5760, Loss: 0.9190345406532288\n",
      "Epoch: 4, Samples: 1408/5760, Loss: 0.9069042205810547\n",
      "Epoch: 4, Samples: 1440/5760, Loss: 0.623847484588623\n",
      "Epoch: 4, Samples: 1472/5760, Loss: 0.7768031358718872\n",
      "Epoch: 4, Samples: 1504/5760, Loss: 0.8073768019676208\n",
      "Epoch: 4, Samples: 1536/5760, Loss: 0.8188320398330688\n",
      "Epoch: 4, Samples: 1568/5760, Loss: 0.8540066480636597\n",
      "Epoch: 4, Samples: 1600/5760, Loss: 0.9643948674201965\n",
      "Epoch: 4, Samples: 1632/5760, Loss: 0.8175365924835205\n",
      "Epoch: 4, Samples: 1664/5760, Loss: 0.6805694103240967\n",
      "Epoch: 4, Samples: 1696/5760, Loss: 0.6260799169540405\n",
      "Epoch: 4, Samples: 1728/5760, Loss: 0.7290453910827637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Samples: 1760/5760, Loss: 0.8127442598342896\n",
      "Epoch: 4, Samples: 1792/5760, Loss: 0.7711822390556335\n",
      "Epoch: 4, Samples: 1824/5760, Loss: 1.097281575202942\n",
      "Epoch: 4, Samples: 1856/5760, Loss: 0.8611927032470703\n",
      "Epoch: 4, Samples: 1888/5760, Loss: 0.5673954486846924\n",
      "Epoch: 4, Samples: 1920/5760, Loss: 0.7456719875335693\n",
      "Epoch: 4, Samples: 1952/5760, Loss: 0.6199333667755127\n",
      "Epoch: 4, Samples: 1984/5760, Loss: 0.7084217667579651\n",
      "Epoch: 4, Samples: 2016/5760, Loss: 0.9151898622512817\n",
      "Epoch: 4, Samples: 2048/5760, Loss: 1.3497066497802734\n",
      "Epoch: 4, Samples: 2080/5760, Loss: 0.6168617010116577\n",
      "Epoch: 4, Samples: 2112/5760, Loss: 0.8581717014312744\n",
      "Epoch: 4, Samples: 2144/5760, Loss: 0.7511909008026123\n",
      "Epoch: 4, Samples: 2176/5760, Loss: 0.7797274589538574\n",
      "Epoch: 4, Samples: 2208/5760, Loss: 0.517604410648346\n",
      "Epoch: 4, Samples: 2240/5760, Loss: 0.5592511296272278\n",
      "Epoch: 4, Samples: 2272/5760, Loss: 0.7449363470077515\n",
      "Epoch: 4, Samples: 2304/5760, Loss: 1.050944447517395\n",
      "Epoch: 4, Samples: 2336/5760, Loss: 1.143438696861267\n",
      "Epoch: 4, Samples: 2368/5760, Loss: 0.5744213461875916\n",
      "Epoch: 4, Samples: 2400/5760, Loss: 0.615324079990387\n",
      "Epoch: 4, Samples: 2432/5760, Loss: 0.7136306762695312\n",
      "Epoch: 4, Samples: 2464/5760, Loss: 0.7829381823539734\n",
      "Epoch: 4, Samples: 2496/5760, Loss: 0.619745671749115\n",
      "Epoch: 4, Samples: 2528/5760, Loss: 0.8225464820861816\n",
      "Epoch: 4, Samples: 2560/5760, Loss: 0.8562390208244324\n",
      "Epoch: 4, Samples: 2592/5760, Loss: 0.7729926109313965\n",
      "Epoch: 4, Samples: 2624/5760, Loss: 0.6363471150398254\n",
      "Epoch: 4, Samples: 2656/5760, Loss: 0.8167973160743713\n",
      "Epoch: 4, Samples: 2688/5760, Loss: 0.6528979539871216\n",
      "Epoch: 4, Samples: 2720/5760, Loss: 0.5525004863739014\n",
      "Epoch: 4, Samples: 2752/5760, Loss: 0.7192448377609253\n",
      "Epoch: 4, Samples: 2784/5760, Loss: 0.6634511947631836\n",
      "Epoch: 4, Samples: 2816/5760, Loss: 0.9536066651344299\n",
      "Epoch: 4, Samples: 2848/5760, Loss: 0.940418004989624\n",
      "Epoch: 4, Samples: 2880/5760, Loss: 0.685509443283081\n",
      "Epoch: 4, Samples: 2912/5760, Loss: 0.9171870350837708\n",
      "Epoch: 4, Samples: 2944/5760, Loss: 0.6796164512634277\n",
      "Epoch: 4, Samples: 2976/5760, Loss: 0.5949897170066833\n",
      "Epoch: 4, Samples: 3008/5760, Loss: 0.6399244070053101\n",
      "Epoch: 4, Samples: 3040/5760, Loss: 0.5746203660964966\n",
      "Epoch: 4, Samples: 3072/5760, Loss: 0.6606650948524475\n",
      "Epoch: 4, Samples: 3104/5760, Loss: 0.7907747030258179\n",
      "Epoch: 4, Samples: 3136/5760, Loss: 0.7042029500007629\n",
      "Epoch: 4, Samples: 3168/5760, Loss: 0.7394884824752808\n",
      "Epoch: 4, Samples: 3200/5760, Loss: 1.0148488283157349\n",
      "Epoch: 4, Samples: 3232/5760, Loss: 0.6038843393325806\n",
      "Epoch: 4, Samples: 3264/5760, Loss: 0.8121417760848999\n",
      "Epoch: 4, Samples: 3296/5760, Loss: 0.6385589838027954\n",
      "Epoch: 4, Samples: 3328/5760, Loss: 0.6310315132141113\n",
      "Epoch: 4, Samples: 3360/5760, Loss: 0.9164693355560303\n",
      "Epoch: 4, Samples: 3392/5760, Loss: 0.7602683901786804\n",
      "Epoch: 4, Samples: 3424/5760, Loss: 0.5498690605163574\n",
      "Epoch: 4, Samples: 3456/5760, Loss: 0.5638324618339539\n",
      "Epoch: 4, Samples: 3488/5760, Loss: 0.724387526512146\n",
      "Epoch: 4, Samples: 3520/5760, Loss: 0.719009518623352\n",
      "Epoch: 4, Samples: 3552/5760, Loss: 0.9523182511329651\n",
      "Epoch: 4, Samples: 3584/5760, Loss: 0.5996094346046448\n",
      "Epoch: 4, Samples: 3616/5760, Loss: 0.9707682132720947\n",
      "Epoch: 4, Samples: 3648/5760, Loss: 0.9817020893096924\n",
      "Epoch: 4, Samples: 3680/5760, Loss: 0.7389228343963623\n",
      "Epoch: 4, Samples: 3712/5760, Loss: 1.0169817209243774\n",
      "Epoch: 4, Samples: 3744/5760, Loss: 0.7593055963516235\n",
      "Epoch: 4, Samples: 3776/5760, Loss: 0.6269381642341614\n",
      "Epoch: 4, Samples: 3808/5760, Loss: 0.8610051870346069\n",
      "Epoch: 4, Samples: 3840/5760, Loss: 0.683600127696991\n",
      "Epoch: 4, Samples: 3872/5760, Loss: 0.3405840992927551\n",
      "Epoch: 4, Samples: 3904/5760, Loss: 0.8644151091575623\n",
      "Epoch: 4, Samples: 3936/5760, Loss: 0.8967761993408203\n",
      "Epoch: 4, Samples: 3968/5760, Loss: 0.6839824914932251\n",
      "Epoch: 4, Samples: 4000/5760, Loss: 0.6525304317474365\n",
      "Epoch: 4, Samples: 4032/5760, Loss: 0.8432642221450806\n",
      "Epoch: 4, Samples: 4064/5760, Loss: 0.9492624998092651\n",
      "Epoch: 4, Samples: 4096/5760, Loss: 0.6443333029747009\n",
      "Epoch: 4, Samples: 4128/5760, Loss: 0.8567324876785278\n",
      "Epoch: 4, Samples: 4160/5760, Loss: 0.7274571061134338\n",
      "Epoch: 4, Samples: 4192/5760, Loss: 0.6553024053573608\n",
      "Epoch: 4, Samples: 4224/5760, Loss: 0.5652925968170166\n",
      "Epoch: 4, Samples: 4256/5760, Loss: 0.7644160389900208\n",
      "Epoch: 4, Samples: 4288/5760, Loss: 0.6123480200767517\n",
      "Epoch: 4, Samples: 4320/5760, Loss: 0.7504913210868835\n",
      "Epoch: 4, Samples: 4352/5760, Loss: 0.5538511276245117\n",
      "Epoch: 4, Samples: 4384/5760, Loss: 0.8016858696937561\n",
      "Epoch: 4, Samples: 4416/5760, Loss: 0.5672874450683594\n",
      "Epoch: 4, Samples: 4448/5760, Loss: 0.5912439227104187\n",
      "Epoch: 4, Samples: 4480/5760, Loss: 0.6407984495162964\n",
      "Epoch: 4, Samples: 4512/5760, Loss: 0.6592390537261963\n",
      "Epoch: 4, Samples: 4544/5760, Loss: 0.5662014484405518\n",
      "Epoch: 4, Samples: 4576/5760, Loss: 0.6182762980461121\n",
      "Epoch: 4, Samples: 4608/5760, Loss: 0.6890273094177246\n",
      "Epoch: 4, Samples: 4640/5760, Loss: 0.6379413604736328\n",
      "Epoch: 4, Samples: 4672/5760, Loss: 0.7402358651161194\n",
      "Epoch: 4, Samples: 4704/5760, Loss: 0.7512447237968445\n",
      "Epoch: 4, Samples: 4736/5760, Loss: 0.7480141520500183\n",
      "Epoch: 4, Samples: 4768/5760, Loss: 0.5665867328643799\n",
      "Epoch: 4, Samples: 4800/5760, Loss: 0.6978834867477417\n",
      "Epoch: 4, Samples: 4832/5760, Loss: 0.8506401777267456\n",
      "Epoch: 4, Samples: 4864/5760, Loss: 0.8331606388092041\n",
      "Epoch: 4, Samples: 4896/5760, Loss: 0.5176396369934082\n",
      "Epoch: 4, Samples: 4928/5760, Loss: 0.3578264117240906\n",
      "Epoch: 4, Samples: 4960/5760, Loss: 0.7723156809806824\n",
      "Epoch: 4, Samples: 4992/5760, Loss: 0.5751031637191772\n",
      "Epoch: 4, Samples: 5024/5760, Loss: 0.74186110496521\n",
      "Epoch: 4, Samples: 5056/5760, Loss: 1.0127966403961182\n",
      "Epoch: 4, Samples: 5088/5760, Loss: 0.4065367579460144\n",
      "Epoch: 4, Samples: 5120/5760, Loss: 0.6185696125030518\n",
      "Epoch: 4, Samples: 5152/5760, Loss: 0.9877984523773193\n",
      "Epoch: 4, Samples: 5184/5760, Loss: 0.8834301233291626\n",
      "Epoch: 4, Samples: 5216/5760, Loss: 0.8016887903213501\n",
      "Epoch: 4, Samples: 5248/5760, Loss: 0.6137032508850098\n",
      "Epoch: 4, Samples: 5280/5760, Loss: 0.8229465484619141\n",
      "Epoch: 4, Samples: 5312/5760, Loss: 0.535179853439331\n",
      "Epoch: 4, Samples: 5344/5760, Loss: 0.6933306455612183\n",
      "Epoch: 4, Samples: 5376/5760, Loss: 0.7621824741363525\n",
      "Epoch: 4, Samples: 5408/5760, Loss: 0.3784697353839874\n",
      "Epoch: 4, Samples: 5440/5760, Loss: 0.7541926503181458\n",
      "Epoch: 4, Samples: 5472/5760, Loss: 0.7746341228485107\n",
      "Epoch: 4, Samples: 5504/5760, Loss: 0.5240174531936646\n",
      "Epoch: 4, Samples: 5536/5760, Loss: 0.4600960314273834\n",
      "Epoch: 4, Samples: 5568/5760, Loss: 0.7028156518936157\n",
      "Epoch: 4, Samples: 5600/5760, Loss: 0.7804244756698608\n",
      "Epoch: 4, Samples: 5632/5760, Loss: 0.896769642829895\n",
      "Epoch: 4, Samples: 5664/5760, Loss: 0.9541009664535522\n",
      "Epoch: 4, Samples: 5696/5760, Loss: 0.7641234397888184\n",
      "Epoch: 4, Samples: 5728/5760, Loss: 1.679253339767456\n",
      "\n",
      "Epoch: 4\n",
      "Training set: Average loss: 0.7642\n",
      "Validation set: Average loss: 0.7322, Accuracy: 717/818 (88%)\n",
      "Saving model (epoch 4) with lowest validation loss: 0.7322493493556976\n",
      "Epoch: 5, Samples: 0/5760, Loss: 0.5583773851394653\n",
      "Epoch: 5, Samples: 32/5760, Loss: 0.3945104479789734\n",
      "Epoch: 5, Samples: 64/5760, Loss: 0.5049607157707214\n",
      "Epoch: 5, Samples: 96/5760, Loss: 0.7622820734977722\n",
      "Epoch: 5, Samples: 128/5760, Loss: 0.7016479969024658\n",
      "Epoch: 5, Samples: 160/5760, Loss: 0.5690953135490417\n",
      "Epoch: 5, Samples: 192/5760, Loss: 0.7082684636116028\n",
      "Epoch: 5, Samples: 224/5760, Loss: 0.6737743616104126\n",
      "Epoch: 5, Samples: 256/5760, Loss: 0.5706697702407837\n",
      "Epoch: 5, Samples: 288/5760, Loss: 0.4354836046695709\n",
      "Epoch: 5, Samples: 320/5760, Loss: 0.6070955395698547\n",
      "Epoch: 5, Samples: 352/5760, Loss: 0.8928884863853455\n",
      "Epoch: 5, Samples: 384/5760, Loss: 0.4254375696182251\n",
      "Epoch: 5, Samples: 416/5760, Loss: 0.4540629982948303\n",
      "Epoch: 5, Samples: 448/5760, Loss: 0.4841749668121338\n",
      "Epoch: 5, Samples: 480/5760, Loss: 0.4304673373699188\n",
      "Epoch: 5, Samples: 512/5760, Loss: 0.5703001618385315\n",
      "Epoch: 5, Samples: 544/5760, Loss: 0.6589046716690063\n",
      "Epoch: 5, Samples: 576/5760, Loss: 0.6448441743850708\n",
      "Epoch: 5, Samples: 608/5760, Loss: 0.6183576583862305\n",
      "Epoch: 5, Samples: 640/5760, Loss: 0.48446178436279297\n",
      "Epoch: 5, Samples: 672/5760, Loss: 0.37460383772850037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Samples: 704/5760, Loss: 0.7530789375305176\n",
      "Epoch: 5, Samples: 736/5760, Loss: 0.34818369150161743\n",
      "Epoch: 5, Samples: 768/5760, Loss: 0.4311977028846741\n",
      "Epoch: 5, Samples: 800/5760, Loss: 0.5927693247795105\n",
      "Epoch: 5, Samples: 832/5760, Loss: 0.5341148972511292\n",
      "Epoch: 5, Samples: 864/5760, Loss: 0.5135416984558105\n",
      "Epoch: 5, Samples: 896/5760, Loss: 0.4947529435157776\n",
      "Epoch: 5, Samples: 928/5760, Loss: 0.6943727731704712\n",
      "Epoch: 5, Samples: 960/5760, Loss: 0.8102668523788452\n",
      "Epoch: 5, Samples: 992/5760, Loss: 0.5628613829612732\n",
      "Epoch: 5, Samples: 1024/5760, Loss: 0.7019474506378174\n",
      "Epoch: 5, Samples: 1056/5760, Loss: 0.5515800714492798\n",
      "Epoch: 5, Samples: 1088/5760, Loss: 0.5524475574493408\n",
      "Epoch: 5, Samples: 1120/5760, Loss: 0.49519604444503784\n",
      "Epoch: 5, Samples: 1152/5760, Loss: 0.5163905024528503\n",
      "Epoch: 5, Samples: 1184/5760, Loss: 0.6310302019119263\n",
      "Epoch: 5, Samples: 1216/5760, Loss: 0.6831123232841492\n",
      "Epoch: 5, Samples: 1248/5760, Loss: 0.5550491809844971\n",
      "Epoch: 5, Samples: 1280/5760, Loss: 0.4558122456073761\n",
      "Epoch: 5, Samples: 1312/5760, Loss: 0.43619999289512634\n",
      "Epoch: 5, Samples: 1344/5760, Loss: 0.6091040372848511\n",
      "Epoch: 5, Samples: 1376/5760, Loss: 0.6179171800613403\n",
      "Epoch: 5, Samples: 1408/5760, Loss: 0.43647870421409607\n",
      "Epoch: 5, Samples: 1440/5760, Loss: 0.7226741909980774\n",
      "Epoch: 5, Samples: 1472/5760, Loss: 0.6433494687080383\n",
      "Epoch: 5, Samples: 1504/5760, Loss: 0.5815964937210083\n",
      "Epoch: 5, Samples: 1536/5760, Loss: 0.4725678861141205\n",
      "Epoch: 5, Samples: 1568/5760, Loss: 0.5987329483032227\n",
      "Epoch: 5, Samples: 1600/5760, Loss: 0.5484535694122314\n",
      "Epoch: 5, Samples: 1632/5760, Loss: 0.6947941780090332\n",
      "Epoch: 5, Samples: 1664/5760, Loss: 0.5924026966094971\n",
      "Epoch: 5, Samples: 1696/5760, Loss: 0.49235960841178894\n",
      "Epoch: 5, Samples: 1728/5760, Loss: 0.28740817308425903\n",
      "Epoch: 5, Samples: 1760/5760, Loss: 0.4463338851928711\n",
      "Epoch: 5, Samples: 1792/5760, Loss: 0.6268296241760254\n",
      "Epoch: 5, Samples: 1824/5760, Loss: 0.32777315378189087\n",
      "Epoch: 5, Samples: 1856/5760, Loss: 0.5126516222953796\n",
      "Epoch: 5, Samples: 1888/5760, Loss: 0.671098530292511\n",
      "Epoch: 5, Samples: 1920/5760, Loss: 0.46992355585098267\n",
      "Epoch: 5, Samples: 1952/5760, Loss: 0.39160963892936707\n",
      "Epoch: 5, Samples: 1984/5760, Loss: 0.5079928636550903\n",
      "Epoch: 5, Samples: 2016/5760, Loss: 0.6991097927093506\n",
      "Epoch: 5, Samples: 2048/5760, Loss: 0.7357373237609863\n",
      "Epoch: 5, Samples: 2080/5760, Loss: 0.4059731960296631\n",
      "Epoch: 5, Samples: 2112/5760, Loss: 0.6047247648239136\n",
      "Epoch: 5, Samples: 2144/5760, Loss: 0.5922503471374512\n",
      "Epoch: 5, Samples: 2176/5760, Loss: 0.5769908428192139\n",
      "Epoch: 5, Samples: 2208/5760, Loss: 0.5032410025596619\n",
      "Epoch: 5, Samples: 2240/5760, Loss: 0.639022946357727\n",
      "Epoch: 5, Samples: 2272/5760, Loss: 0.3886559009552002\n",
      "Epoch: 5, Samples: 2304/5760, Loss: 0.6563819050788879\n",
      "Epoch: 5, Samples: 2336/5760, Loss: 0.6752959489822388\n",
      "Epoch: 5, Samples: 2368/5760, Loss: 0.5781639218330383\n",
      "Epoch: 5, Samples: 2400/5760, Loss: 0.36157214641571045\n",
      "Epoch: 5, Samples: 2432/5760, Loss: 0.46073007583618164\n",
      "Epoch: 5, Samples: 2464/5760, Loss: 0.4542791247367859\n",
      "Epoch: 5, Samples: 2496/5760, Loss: 0.42754319310188293\n",
      "Epoch: 5, Samples: 2528/5760, Loss: 0.6955918073654175\n",
      "Epoch: 5, Samples: 2560/5760, Loss: 0.514665424823761\n",
      "Epoch: 5, Samples: 2592/5760, Loss: 0.2878810167312622\n",
      "Epoch: 5, Samples: 2624/5760, Loss: 0.611378014087677\n",
      "Epoch: 5, Samples: 2656/5760, Loss: 0.5488433837890625\n",
      "Epoch: 5, Samples: 2688/5760, Loss: 0.44777384400367737\n",
      "Epoch: 5, Samples: 2720/5760, Loss: 0.4447222650051117\n",
      "Epoch: 5, Samples: 2752/5760, Loss: 0.44550055265426636\n",
      "Epoch: 5, Samples: 2784/5760, Loss: 0.41637763381004333\n",
      "Epoch: 5, Samples: 2816/5760, Loss: 0.8903878927230835\n",
      "Epoch: 5, Samples: 2848/5760, Loss: 0.5976014137268066\n",
      "Epoch: 5, Samples: 2880/5760, Loss: 0.6387544274330139\n",
      "Epoch: 5, Samples: 2912/5760, Loss: 0.4738485813140869\n",
      "Epoch: 5, Samples: 2944/5760, Loss: 0.4309004843235016\n",
      "Epoch: 5, Samples: 2976/5760, Loss: 0.4999673664569855\n",
      "Epoch: 5, Samples: 3008/5760, Loss: 0.6110236644744873\n",
      "Epoch: 5, Samples: 3040/5760, Loss: 0.48258817195892334\n",
      "Epoch: 5, Samples: 3072/5760, Loss: 0.592567503452301\n",
      "Epoch: 5, Samples: 3104/5760, Loss: 0.4232136011123657\n",
      "Epoch: 5, Samples: 3136/5760, Loss: 0.4797959625720978\n",
      "Epoch: 5, Samples: 3168/5760, Loss: 0.624556839466095\n",
      "Epoch: 5, Samples: 3200/5760, Loss: 0.3013755977153778\n",
      "Epoch: 5, Samples: 3232/5760, Loss: 0.6090580821037292\n",
      "Epoch: 5, Samples: 3264/5760, Loss: 0.5643126964569092\n",
      "Epoch: 5, Samples: 3296/5760, Loss: 0.6825282573699951\n",
      "Epoch: 5, Samples: 3328/5760, Loss: 0.498717337846756\n",
      "Epoch: 5, Samples: 3360/5760, Loss: 0.5634323358535767\n",
      "Epoch: 5, Samples: 3392/5760, Loss: 0.5063406229019165\n",
      "Epoch: 5, Samples: 3424/5760, Loss: 0.49001896381378174\n",
      "Epoch: 5, Samples: 3456/5760, Loss: 0.5669743418693542\n",
      "Epoch: 5, Samples: 3488/5760, Loss: 0.4815324544906616\n",
      "Epoch: 5, Samples: 3520/5760, Loss: 0.41890013217926025\n",
      "Epoch: 5, Samples: 3552/5760, Loss: 0.4792833924293518\n",
      "Epoch: 5, Samples: 3584/5760, Loss: 0.4201294183731079\n",
      "Epoch: 5, Samples: 3616/5760, Loss: 0.507739245891571\n",
      "Epoch: 5, Samples: 3648/5760, Loss: 0.4200117588043213\n",
      "Epoch: 5, Samples: 3680/5760, Loss: 0.3602542281150818\n",
      "Epoch: 5, Samples: 3712/5760, Loss: 0.36406418681144714\n",
      "Epoch: 5, Samples: 3744/5760, Loss: 0.49786823987960815\n",
      "Epoch: 5, Samples: 3776/5760, Loss: 0.5772997736930847\n",
      "Epoch: 5, Samples: 3808/5760, Loss: 0.5101538300514221\n",
      "Epoch: 5, Samples: 3840/5760, Loss: 0.575743556022644\n",
      "Epoch: 5, Samples: 3872/5760, Loss: 0.3678649067878723\n",
      "Epoch: 5, Samples: 3904/5760, Loss: 0.6024061441421509\n",
      "Epoch: 5, Samples: 3936/5760, Loss: 0.6861016750335693\n",
      "Epoch: 5, Samples: 3968/5760, Loss: 0.4544377326965332\n",
      "Epoch: 5, Samples: 4000/5760, Loss: 0.4757446050643921\n",
      "Epoch: 5, Samples: 4032/5760, Loss: 0.5428800582885742\n",
      "Epoch: 5, Samples: 4064/5760, Loss: 0.4682632088661194\n",
      "Epoch: 5, Samples: 4096/5760, Loss: 0.4856150150299072\n",
      "Epoch: 5, Samples: 4128/5760, Loss: 0.5584248304367065\n",
      "Epoch: 5, Samples: 4160/5760, Loss: 0.5510815382003784\n",
      "Epoch: 5, Samples: 4192/5760, Loss: 0.5022190809249878\n",
      "Epoch: 5, Samples: 4224/5760, Loss: 0.5263763666152954\n",
      "Epoch: 5, Samples: 4256/5760, Loss: 0.6029783487319946\n",
      "Epoch: 5, Samples: 4288/5760, Loss: 0.9043415784835815\n",
      "Epoch: 5, Samples: 4320/5760, Loss: 0.47750115394592285\n",
      "Epoch: 5, Samples: 4352/5760, Loss: 0.6687791347503662\n",
      "Epoch: 5, Samples: 4384/5760, Loss: 0.5359228849411011\n",
      "Epoch: 5, Samples: 4416/5760, Loss: 0.4760815501213074\n",
      "Epoch: 5, Samples: 4448/5760, Loss: 0.38371723890304565\n",
      "Epoch: 5, Samples: 4480/5760, Loss: 0.5102359056472778\n",
      "Epoch: 5, Samples: 4512/5760, Loss: 0.35365256667137146\n",
      "Epoch: 5, Samples: 4544/5760, Loss: 0.735525369644165\n",
      "Epoch: 5, Samples: 4576/5760, Loss: 0.8832953572273254\n",
      "Epoch: 5, Samples: 4608/5760, Loss: 0.4087991416454315\n",
      "Epoch: 5, Samples: 4640/5760, Loss: 0.532088041305542\n",
      "Epoch: 5, Samples: 4672/5760, Loss: 0.5690463781356812\n",
      "Epoch: 5, Samples: 4704/5760, Loss: 0.6732161641120911\n",
      "Epoch: 5, Samples: 4736/5760, Loss: 0.5082169771194458\n",
      "Epoch: 5, Samples: 4768/5760, Loss: 0.46037036180496216\n",
      "Epoch: 5, Samples: 4800/5760, Loss: 0.36846455931663513\n",
      "Epoch: 5, Samples: 4832/5760, Loss: 0.6066496968269348\n",
      "Epoch: 5, Samples: 4864/5760, Loss: 0.5392094850540161\n",
      "Epoch: 5, Samples: 4896/5760, Loss: 0.4517202079296112\n",
      "Epoch: 5, Samples: 4928/5760, Loss: 0.5014959573745728\n",
      "Epoch: 5, Samples: 4960/5760, Loss: 0.5369415283203125\n",
      "Epoch: 5, Samples: 4992/5760, Loss: 0.5248384475708008\n",
      "Epoch: 5, Samples: 5024/5760, Loss: 0.6381998062133789\n",
      "Epoch: 5, Samples: 5056/5760, Loss: 0.33785006403923035\n",
      "Epoch: 5, Samples: 5088/5760, Loss: 0.4786183834075928\n",
      "Epoch: 5, Samples: 5120/5760, Loss: 0.5901306867599487\n",
      "Epoch: 5, Samples: 5152/5760, Loss: 0.8041518926620483\n",
      "Epoch: 5, Samples: 5184/5760, Loss: 0.5038141012191772\n",
      "Epoch: 5, Samples: 5216/5760, Loss: 0.27042627334594727\n",
      "Epoch: 5, Samples: 5248/5760, Loss: 0.6120811700820923\n",
      "Epoch: 5, Samples: 5280/5760, Loss: 0.37645018100738525\n",
      "Epoch: 5, Samples: 5312/5760, Loss: 0.4165458679199219\n",
      "Epoch: 5, Samples: 5344/5760, Loss: 0.6085189580917358\n",
      "Epoch: 5, Samples: 5376/5760, Loss: 0.5489768981933594\n",
      "Epoch: 5, Samples: 5408/5760, Loss: 0.5203008651733398\n",
      "Epoch: 5, Samples: 5440/5760, Loss: 0.5213916897773743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Samples: 5472/5760, Loss: 0.46549928188323975\n",
      "Epoch: 5, Samples: 5504/5760, Loss: 0.5959598422050476\n",
      "Epoch: 5, Samples: 5536/5760, Loss: 0.6651023626327515\n",
      "Epoch: 5, Samples: 5568/5760, Loss: 0.39048337936401367\n",
      "Epoch: 5, Samples: 5600/5760, Loss: 0.4273005723953247\n",
      "Epoch: 5, Samples: 5632/5760, Loss: 0.571307361125946\n",
      "Epoch: 5, Samples: 5664/5760, Loss: 0.4915602207183838\n",
      "Epoch: 5, Samples: 5696/5760, Loss: 0.30192968249320984\n",
      "Epoch: 5, Samples: 5728/5760, Loss: 2.18290114402771\n",
      "\n",
      "Epoch: 5\n",
      "Training set: Average loss: 0.5453\n",
      "Validation set: Average loss: 0.6160, Accuracy: 730/818 (89%)\n",
      "Saving model (epoch 5) with lowest validation loss: 0.6160411078196305\n",
      "Epoch: 6, Samples: 0/5760, Loss: 0.27340954542160034\n",
      "Epoch: 6, Samples: 32/5760, Loss: 0.28851816058158875\n",
      "Epoch: 6, Samples: 64/5760, Loss: 0.3234747350215912\n",
      "Epoch: 6, Samples: 96/5760, Loss: 0.5552862286567688\n",
      "Epoch: 6, Samples: 128/5760, Loss: 0.2679573893547058\n",
      "Epoch: 6, Samples: 160/5760, Loss: 0.3460949659347534\n",
      "Epoch: 6, Samples: 192/5760, Loss: 0.29758965969085693\n",
      "Epoch: 6, Samples: 224/5760, Loss: 0.3676033020019531\n",
      "Epoch: 6, Samples: 256/5760, Loss: 0.4055812954902649\n",
      "Epoch: 6, Samples: 288/5760, Loss: 0.4170683026313782\n",
      "Epoch: 6, Samples: 320/5760, Loss: 0.3933566212654114\n",
      "Epoch: 6, Samples: 352/5760, Loss: 0.6537317037582397\n",
      "Epoch: 6, Samples: 384/5760, Loss: 0.464977890253067\n",
      "Epoch: 6, Samples: 416/5760, Loss: 0.586306095123291\n",
      "Epoch: 6, Samples: 448/5760, Loss: 0.4128006100654602\n",
      "Epoch: 6, Samples: 480/5760, Loss: 0.3696373701095581\n",
      "Epoch: 6, Samples: 512/5760, Loss: 0.3565601408481598\n",
      "Epoch: 6, Samples: 544/5760, Loss: 0.28640982508659363\n",
      "Epoch: 6, Samples: 576/5760, Loss: 0.45925188064575195\n",
      "Epoch: 6, Samples: 608/5760, Loss: 0.3530092239379883\n",
      "Epoch: 6, Samples: 640/5760, Loss: 0.5916909575462341\n",
      "Epoch: 6, Samples: 672/5760, Loss: 0.4335687756538391\n",
      "Epoch: 6, Samples: 704/5760, Loss: 0.2776974141597748\n",
      "Epoch: 6, Samples: 736/5760, Loss: 0.26077911257743835\n",
      "Epoch: 6, Samples: 768/5760, Loss: 0.28518134355545044\n",
      "Epoch: 6, Samples: 800/5760, Loss: 0.412459135055542\n",
      "Epoch: 6, Samples: 832/5760, Loss: 0.6233874559402466\n",
      "Epoch: 6, Samples: 864/5760, Loss: 0.49657830595970154\n",
      "Epoch: 6, Samples: 896/5760, Loss: 0.3548394441604614\n",
      "Epoch: 6, Samples: 928/5760, Loss: 0.5070281028747559\n",
      "Epoch: 6, Samples: 960/5760, Loss: 0.4645997881889343\n",
      "Epoch: 6, Samples: 992/5760, Loss: 0.32468047738075256\n",
      "Epoch: 6, Samples: 1024/5760, Loss: 0.5692370533943176\n",
      "Epoch: 6, Samples: 1056/5760, Loss: 0.6051392555236816\n",
      "Epoch: 6, Samples: 1088/5760, Loss: 0.20138800144195557\n",
      "Epoch: 6, Samples: 1120/5760, Loss: 0.6746090650558472\n",
      "Epoch: 6, Samples: 1152/5760, Loss: 0.3637276887893677\n",
      "Epoch: 6, Samples: 1184/5760, Loss: 0.48726749420166016\n",
      "Epoch: 6, Samples: 1216/5760, Loss: 0.41482508182525635\n",
      "Epoch: 6, Samples: 1248/5760, Loss: 0.5137688517570496\n",
      "Epoch: 6, Samples: 1280/5760, Loss: 0.4087377190589905\n",
      "Epoch: 6, Samples: 1312/5760, Loss: 0.3919462263584137\n",
      "Epoch: 6, Samples: 1344/5760, Loss: 0.42565256357192993\n",
      "Epoch: 6, Samples: 1376/5760, Loss: 0.37828654050827026\n",
      "Epoch: 6, Samples: 1408/5760, Loss: 0.2632165253162384\n",
      "Epoch: 6, Samples: 1440/5760, Loss: 0.47062379121780396\n",
      "Epoch: 6, Samples: 1472/5760, Loss: 0.33915576338768005\n",
      "Epoch: 6, Samples: 1504/5760, Loss: 0.4092991352081299\n",
      "Epoch: 6, Samples: 1536/5760, Loss: 0.2990834712982178\n",
      "Epoch: 6, Samples: 1568/5760, Loss: 0.3667899966239929\n",
      "Epoch: 6, Samples: 1600/5760, Loss: 0.2574598789215088\n",
      "Epoch: 6, Samples: 1632/5760, Loss: 0.7751652002334595\n",
      "Epoch: 6, Samples: 1664/5760, Loss: 0.4577828645706177\n",
      "Epoch: 6, Samples: 1696/5760, Loss: 0.33880022168159485\n",
      "Epoch: 6, Samples: 1728/5760, Loss: 0.39070263504981995\n",
      "Epoch: 6, Samples: 1760/5760, Loss: 0.4861609935760498\n",
      "Epoch: 6, Samples: 1792/5760, Loss: 0.4509018659591675\n",
      "Epoch: 6, Samples: 1824/5760, Loss: 0.4502297043800354\n",
      "Epoch: 6, Samples: 1856/5760, Loss: 0.2754204571247101\n",
      "Epoch: 6, Samples: 1888/5760, Loss: 0.452734112739563\n",
      "Epoch: 6, Samples: 1920/5760, Loss: 0.5488530993461609\n",
      "Epoch: 6, Samples: 1952/5760, Loss: 0.8170039057731628\n",
      "Epoch: 6, Samples: 1984/5760, Loss: 0.4240790009498596\n",
      "Epoch: 6, Samples: 2016/5760, Loss: 0.2516478896141052\n",
      "Epoch: 6, Samples: 2048/5760, Loss: 0.7053033113479614\n",
      "Epoch: 6, Samples: 2080/5760, Loss: 0.4702620208263397\n",
      "Epoch: 6, Samples: 2112/5760, Loss: 0.32526856660842896\n",
      "Epoch: 6, Samples: 2144/5760, Loss: 0.6069577932357788\n",
      "Epoch: 6, Samples: 2176/5760, Loss: 0.24075089395046234\n",
      "Epoch: 6, Samples: 2208/5760, Loss: 0.3858281373977661\n",
      "Epoch: 6, Samples: 2240/5760, Loss: 0.40042829513549805\n",
      "Epoch: 6, Samples: 2272/5760, Loss: 0.34181955456733704\n",
      "Epoch: 6, Samples: 2304/5760, Loss: 0.3106532394886017\n",
      "Epoch: 6, Samples: 2336/5760, Loss: 0.28803104162216187\n",
      "Epoch: 6, Samples: 2368/5760, Loss: 0.3129870593547821\n",
      "Epoch: 6, Samples: 2400/5760, Loss: 0.38741979002952576\n",
      "Epoch: 6, Samples: 2432/5760, Loss: 0.43470391631126404\n",
      "Epoch: 6, Samples: 2464/5760, Loss: 0.4144836366176605\n",
      "Epoch: 6, Samples: 2496/5760, Loss: 0.3942449390888214\n",
      "Epoch: 6, Samples: 2528/5760, Loss: 0.25241291522979736\n",
      "Epoch: 6, Samples: 2560/5760, Loss: 0.3318864703178406\n",
      "Epoch: 6, Samples: 2592/5760, Loss: 0.48903974890708923\n",
      "Epoch: 6, Samples: 2624/5760, Loss: 0.38904958963394165\n",
      "Epoch: 6, Samples: 2656/5760, Loss: 0.1572294980287552\n",
      "Epoch: 6, Samples: 2688/5760, Loss: 0.46439865231513977\n",
      "Epoch: 6, Samples: 2720/5760, Loss: 0.418362557888031\n",
      "Epoch: 6, Samples: 2752/5760, Loss: 0.3140563666820526\n",
      "Epoch: 6, Samples: 2784/5760, Loss: 0.3753238320350647\n",
      "Epoch: 6, Samples: 2816/5760, Loss: 0.5646456480026245\n",
      "Epoch: 6, Samples: 2848/5760, Loss: 0.3274635970592499\n",
      "Epoch: 6, Samples: 2880/5760, Loss: 0.28234630823135376\n",
      "Epoch: 6, Samples: 2912/5760, Loss: 0.3147425949573517\n",
      "Epoch: 6, Samples: 2944/5760, Loss: 0.5063285827636719\n",
      "Epoch: 6, Samples: 2976/5760, Loss: 0.2650477886199951\n",
      "Epoch: 6, Samples: 3008/5760, Loss: 0.3061138987541199\n",
      "Epoch: 6, Samples: 3040/5760, Loss: 0.3827836513519287\n",
      "Epoch: 6, Samples: 3072/5760, Loss: 0.3075222671031952\n",
      "Epoch: 6, Samples: 3104/5760, Loss: 0.3226977586746216\n",
      "Epoch: 6, Samples: 3136/5760, Loss: 0.3524632453918457\n",
      "Epoch: 6, Samples: 3168/5760, Loss: 0.554120659828186\n",
      "Epoch: 6, Samples: 3200/5760, Loss: 0.3339199423789978\n",
      "Epoch: 6, Samples: 3232/5760, Loss: 0.4288153350353241\n",
      "Epoch: 6, Samples: 3264/5760, Loss: 0.5750417709350586\n",
      "Epoch: 6, Samples: 3296/5760, Loss: 0.37191230058670044\n",
      "Epoch: 6, Samples: 3328/5760, Loss: 0.37375128269195557\n",
      "Epoch: 6, Samples: 3360/5760, Loss: 0.4486304521560669\n",
      "Epoch: 6, Samples: 3392/5760, Loss: 0.391504168510437\n",
      "Epoch: 6, Samples: 3424/5760, Loss: 0.3067445158958435\n",
      "Epoch: 6, Samples: 3456/5760, Loss: 0.44528380036354065\n",
      "Epoch: 6, Samples: 3488/5760, Loss: 0.2500166893005371\n",
      "Epoch: 6, Samples: 3520/5760, Loss: 0.3618619441986084\n",
      "Epoch: 6, Samples: 3552/5760, Loss: 0.3253403306007385\n",
      "Epoch: 6, Samples: 3584/5760, Loss: 0.28012290596961975\n",
      "Epoch: 6, Samples: 3616/5760, Loss: 0.3683149218559265\n",
      "Epoch: 6, Samples: 3648/5760, Loss: 0.2522781491279602\n",
      "Epoch: 6, Samples: 3680/5760, Loss: 0.6172213554382324\n",
      "Epoch: 6, Samples: 3712/5760, Loss: 0.337196946144104\n",
      "Epoch: 6, Samples: 3744/5760, Loss: 0.383588969707489\n",
      "Epoch: 6, Samples: 3776/5760, Loss: 0.24825190007686615\n",
      "Epoch: 6, Samples: 3808/5760, Loss: 0.3533313274383545\n",
      "Epoch: 6, Samples: 3840/5760, Loss: 0.3089541792869568\n",
      "Epoch: 6, Samples: 3872/5760, Loss: 0.34611985087394714\n",
      "Epoch: 6, Samples: 3904/5760, Loss: 0.23348906636238098\n",
      "Epoch: 6, Samples: 3936/5760, Loss: 0.37555453181266785\n",
      "Epoch: 6, Samples: 3968/5760, Loss: 0.3631919026374817\n",
      "Epoch: 6, Samples: 4000/5760, Loss: 0.24138693511486053\n",
      "Epoch: 6, Samples: 4032/5760, Loss: 0.3436781167984009\n",
      "Epoch: 6, Samples: 4064/5760, Loss: 0.4133833646774292\n",
      "Epoch: 6, Samples: 4096/5760, Loss: 0.33947205543518066\n",
      "Epoch: 6, Samples: 4128/5760, Loss: 0.476229190826416\n",
      "Epoch: 6, Samples: 4160/5760, Loss: 0.22036536037921906\n",
      "Epoch: 6, Samples: 4192/5760, Loss: 0.32004350423812866\n",
      "Epoch: 6, Samples: 4224/5760, Loss: 0.5249133110046387\n",
      "Epoch: 6, Samples: 4256/5760, Loss: 0.488368958234787\n",
      "Epoch: 6, Samples: 4288/5760, Loss: 0.513687014579773\n",
      "Epoch: 6, Samples: 4320/5760, Loss: 0.42827680706977844\n",
      "Epoch: 6, Samples: 4352/5760, Loss: 0.48903411626815796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Samples: 4384/5760, Loss: 0.3203886151313782\n",
      "Epoch: 6, Samples: 4416/5760, Loss: 0.34222128987312317\n",
      "Epoch: 6, Samples: 4448/5760, Loss: 0.23326678574085236\n",
      "Epoch: 6, Samples: 4480/5760, Loss: 0.6470291018486023\n",
      "Epoch: 6, Samples: 4512/5760, Loss: 0.3444872498512268\n",
      "Epoch: 6, Samples: 4544/5760, Loss: 0.46906983852386475\n",
      "Epoch: 6, Samples: 4576/5760, Loss: 0.47931692004203796\n",
      "Epoch: 6, Samples: 4608/5760, Loss: 0.15777254104614258\n",
      "Epoch: 6, Samples: 4640/5760, Loss: 0.34527674317359924\n",
      "Epoch: 6, Samples: 4672/5760, Loss: 0.2804027199745178\n",
      "Epoch: 6, Samples: 4704/5760, Loss: 0.2932080030441284\n",
      "Epoch: 6, Samples: 4736/5760, Loss: 0.47870349884033203\n",
      "Epoch: 6, Samples: 4768/5760, Loss: 0.35901546478271484\n",
      "Epoch: 6, Samples: 4800/5760, Loss: 0.40276670455932617\n",
      "Epoch: 6, Samples: 4832/5760, Loss: 0.24440395832061768\n",
      "Epoch: 6, Samples: 4864/5760, Loss: 0.4076423645019531\n",
      "Epoch: 6, Samples: 4896/5760, Loss: 0.596919059753418\n",
      "Epoch: 6, Samples: 4928/5760, Loss: 0.40428581833839417\n",
      "Epoch: 6, Samples: 4960/5760, Loss: 0.33928894996643066\n",
      "Epoch: 6, Samples: 4992/5760, Loss: 0.2515442371368408\n",
      "Epoch: 6, Samples: 5024/5760, Loss: 0.2625666856765747\n",
      "Epoch: 6, Samples: 5056/5760, Loss: 0.3450591266155243\n",
      "Epoch: 6, Samples: 5088/5760, Loss: 0.31518200039863586\n",
      "Epoch: 6, Samples: 5120/5760, Loss: 0.3133580684661865\n",
      "Epoch: 6, Samples: 5152/5760, Loss: 0.49279558658599854\n",
      "Epoch: 6, Samples: 5184/5760, Loss: 0.3115452826023102\n",
      "Epoch: 6, Samples: 5216/5760, Loss: 0.3918033838272095\n",
      "Epoch: 6, Samples: 5248/5760, Loss: 0.3425043821334839\n",
      "Epoch: 6, Samples: 5280/5760, Loss: 0.3723999857902527\n",
      "Epoch: 6, Samples: 5312/5760, Loss: 0.38528361916542053\n",
      "Epoch: 6, Samples: 5344/5760, Loss: 0.34053394198417664\n",
      "Epoch: 6, Samples: 5376/5760, Loss: 0.4891505539417267\n",
      "Epoch: 6, Samples: 5408/5760, Loss: 0.3409888446331024\n",
      "Epoch: 6, Samples: 5440/5760, Loss: 0.23309476673603058\n",
      "Epoch: 6, Samples: 5472/5760, Loss: 0.31825530529022217\n",
      "Epoch: 6, Samples: 5504/5760, Loss: 0.39979708194732666\n",
      "Epoch: 6, Samples: 5536/5760, Loss: 0.3045596182346344\n",
      "Epoch: 6, Samples: 5568/5760, Loss: 0.20614266395568848\n",
      "Epoch: 6, Samples: 5600/5760, Loss: 0.3352915048599243\n",
      "Epoch: 6, Samples: 5632/5760, Loss: 0.3015795350074768\n",
      "Epoch: 6, Samples: 5664/5760, Loss: 0.22929708659648895\n",
      "Epoch: 6, Samples: 5696/5760, Loss: 0.30790671706199646\n",
      "Epoch: 6, Samples: 5728/5760, Loss: 2.7221758365631104\n",
      "\n",
      "Epoch: 6\n",
      "Training set: Average loss: 0.3995\n",
      "Validation set: Average loss: 0.5226, Accuracy: 736/818 (90%)\n",
      "Saving model (epoch 6) with lowest validation loss: 0.5225756122515752\n",
      "Epoch: 7, Samples: 0/5760, Loss: 0.34215492010116577\n",
      "Epoch: 7, Samples: 32/5760, Loss: 0.18870364129543304\n",
      "Epoch: 7, Samples: 64/5760, Loss: 0.21212147176265717\n",
      "Epoch: 7, Samples: 96/5760, Loss: 0.4394369125366211\n",
      "Epoch: 7, Samples: 128/5760, Loss: 0.35519522428512573\n",
      "Epoch: 7, Samples: 160/5760, Loss: 0.2872803509235382\n",
      "Epoch: 7, Samples: 192/5760, Loss: 0.37169814109802246\n",
      "Epoch: 7, Samples: 224/5760, Loss: 0.2887404263019562\n",
      "Epoch: 7, Samples: 256/5760, Loss: 0.34404394030570984\n",
      "Epoch: 7, Samples: 288/5760, Loss: 0.33906427025794983\n",
      "Epoch: 7, Samples: 320/5760, Loss: 0.256259560585022\n",
      "Epoch: 7, Samples: 352/5760, Loss: 0.40455126762390137\n",
      "Epoch: 7, Samples: 384/5760, Loss: 0.3287997245788574\n",
      "Epoch: 7, Samples: 416/5760, Loss: 0.35406962037086487\n",
      "Epoch: 7, Samples: 448/5760, Loss: 0.304970920085907\n",
      "Epoch: 7, Samples: 480/5760, Loss: 0.21367155015468597\n",
      "Epoch: 7, Samples: 512/5760, Loss: 0.26067987084388733\n",
      "Epoch: 7, Samples: 544/5760, Loss: 0.45143067836761475\n",
      "Epoch: 7, Samples: 576/5760, Loss: 0.27960386872291565\n",
      "Epoch: 7, Samples: 608/5760, Loss: 0.4366859197616577\n",
      "Epoch: 7, Samples: 640/5760, Loss: 0.3187796175479889\n",
      "Epoch: 7, Samples: 672/5760, Loss: 0.1905069202184677\n",
      "Epoch: 7, Samples: 704/5760, Loss: 0.21434947848320007\n",
      "Epoch: 7, Samples: 736/5760, Loss: 0.19940400123596191\n",
      "Epoch: 7, Samples: 768/5760, Loss: 0.3319514989852905\n",
      "Epoch: 7, Samples: 800/5760, Loss: 0.16896294057369232\n",
      "Epoch: 7, Samples: 832/5760, Loss: 0.31062355637550354\n",
      "Epoch: 7, Samples: 864/5760, Loss: 0.2089591920375824\n",
      "Epoch: 7, Samples: 896/5760, Loss: 0.30563998222351074\n",
      "Epoch: 7, Samples: 928/5760, Loss: 0.20580820739269257\n",
      "Epoch: 7, Samples: 960/5760, Loss: 0.2148410975933075\n",
      "Epoch: 7, Samples: 992/5760, Loss: 0.2515300512313843\n",
      "Epoch: 7, Samples: 1024/5760, Loss: 0.18077287077903748\n",
      "Epoch: 7, Samples: 1056/5760, Loss: 0.519225001335144\n",
      "Epoch: 7, Samples: 1088/5760, Loss: 0.2698639929294586\n",
      "Epoch: 7, Samples: 1120/5760, Loss: 0.2046489715576172\n",
      "Epoch: 7, Samples: 1152/5760, Loss: 0.31613972783088684\n",
      "Epoch: 7, Samples: 1184/5760, Loss: 0.24572837352752686\n",
      "Epoch: 7, Samples: 1216/5760, Loss: 0.2718062400817871\n",
      "Epoch: 7, Samples: 1248/5760, Loss: 0.2911621630191803\n",
      "Epoch: 7, Samples: 1280/5760, Loss: 0.31683698296546936\n",
      "Epoch: 7, Samples: 1312/5760, Loss: 0.36573174595832825\n",
      "Epoch: 7, Samples: 1344/5760, Loss: 0.2304600328207016\n",
      "Epoch: 7, Samples: 1376/5760, Loss: 0.2619644105434418\n",
      "Epoch: 7, Samples: 1408/5760, Loss: 0.2410559356212616\n",
      "Epoch: 7, Samples: 1440/5760, Loss: 0.2628084123134613\n",
      "Epoch: 7, Samples: 1472/5760, Loss: 0.40338340401649475\n",
      "Epoch: 7, Samples: 1504/5760, Loss: 0.21305663883686066\n",
      "Epoch: 7, Samples: 1536/5760, Loss: 0.4365209639072418\n",
      "Epoch: 7, Samples: 1568/5760, Loss: 0.179828941822052\n",
      "Epoch: 7, Samples: 1600/5760, Loss: 0.45105236768722534\n",
      "Epoch: 7, Samples: 1632/5760, Loss: 0.26904603838920593\n",
      "Epoch: 7, Samples: 1664/5760, Loss: 0.5969251394271851\n",
      "Epoch: 7, Samples: 1696/5760, Loss: 0.2978345453739166\n",
      "Epoch: 7, Samples: 1728/5760, Loss: 0.4146101176738739\n",
      "Epoch: 7, Samples: 1760/5760, Loss: 0.37285560369491577\n",
      "Epoch: 7, Samples: 1792/5760, Loss: 0.2233344167470932\n",
      "Epoch: 7, Samples: 1824/5760, Loss: 0.32487261295318604\n",
      "Epoch: 7, Samples: 1856/5760, Loss: 0.34918877482414246\n",
      "Epoch: 7, Samples: 1888/5760, Loss: 0.2910785377025604\n",
      "Epoch: 7, Samples: 1920/5760, Loss: 0.19501978158950806\n",
      "Epoch: 7, Samples: 1952/5760, Loss: 0.39841532707214355\n",
      "Epoch: 7, Samples: 1984/5760, Loss: 0.2673024833202362\n",
      "Epoch: 7, Samples: 2016/5760, Loss: 0.2467098832130432\n",
      "Epoch: 7, Samples: 2048/5760, Loss: 0.30221882462501526\n",
      "Epoch: 7, Samples: 2080/5760, Loss: 0.269004225730896\n",
      "Epoch: 7, Samples: 2112/5760, Loss: 0.4481138288974762\n",
      "Epoch: 7, Samples: 2144/5760, Loss: 0.17453376948833466\n",
      "Epoch: 7, Samples: 2176/5760, Loss: 0.303629070520401\n",
      "Epoch: 7, Samples: 2208/5760, Loss: 0.3153955340385437\n",
      "Epoch: 7, Samples: 2240/5760, Loss: 0.17316605150699615\n",
      "Epoch: 7, Samples: 2272/5760, Loss: 0.15987913310527802\n",
      "Epoch: 7, Samples: 2304/5760, Loss: 0.27421289682388306\n",
      "Epoch: 7, Samples: 2336/5760, Loss: 0.5598991513252258\n",
      "Epoch: 7, Samples: 2368/5760, Loss: 0.199509397149086\n",
      "Epoch: 7, Samples: 2400/5760, Loss: 0.19959929585456848\n",
      "Epoch: 7, Samples: 2432/5760, Loss: 0.2590160369873047\n",
      "Epoch: 7, Samples: 2464/5760, Loss: 0.3600279688835144\n",
      "Epoch: 7, Samples: 2496/5760, Loss: 0.31204068660736084\n",
      "Epoch: 7, Samples: 2528/5760, Loss: 0.39004671573638916\n",
      "Epoch: 7, Samples: 2560/5760, Loss: 0.28539615869522095\n",
      "Epoch: 7, Samples: 2592/5760, Loss: 0.2694345712661743\n",
      "Epoch: 7, Samples: 2624/5760, Loss: 0.21018585562705994\n",
      "Epoch: 7, Samples: 2656/5760, Loss: 0.37554115056991577\n",
      "Epoch: 7, Samples: 2688/5760, Loss: 0.4384811818599701\n",
      "Epoch: 7, Samples: 2720/5760, Loss: 0.20455417037010193\n",
      "Epoch: 7, Samples: 2752/5760, Loss: 0.19568480551242828\n",
      "Epoch: 7, Samples: 2784/5760, Loss: 0.23541994392871857\n",
      "Epoch: 7, Samples: 2816/5760, Loss: 0.1403215378522873\n",
      "Epoch: 7, Samples: 2848/5760, Loss: 0.34136298298835754\n",
      "Epoch: 7, Samples: 2880/5760, Loss: 0.22590681910514832\n",
      "Epoch: 7, Samples: 2912/5760, Loss: 0.29323142766952515\n",
      "Epoch: 7, Samples: 2944/5760, Loss: 0.2933749556541443\n",
      "Epoch: 7, Samples: 2976/5760, Loss: 0.15646377205848694\n",
      "Epoch: 7, Samples: 3008/5760, Loss: 0.311002641916275\n",
      "Epoch: 7, Samples: 3040/5760, Loss: 0.2049645483493805\n",
      "Epoch: 7, Samples: 3072/5760, Loss: 0.3558691740036011\n",
      "Epoch: 7, Samples: 3104/5760, Loss: 0.3193763196468353\n",
      "Epoch: 7, Samples: 3136/5760, Loss: 0.2986930310726166\n",
      "Epoch: 7, Samples: 3168/5760, Loss: 0.2800075113773346\n",
      "Epoch: 7, Samples: 3200/5760, Loss: 0.23654159903526306\n",
      "Epoch: 7, Samples: 3232/5760, Loss: 0.15806451439857483\n",
      "Epoch: 7, Samples: 3264/5760, Loss: 0.1925717145204544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Samples: 3296/5760, Loss: 0.22356490790843964\n",
      "Epoch: 7, Samples: 3328/5760, Loss: 0.29653894901275635\n",
      "Epoch: 7, Samples: 3360/5760, Loss: 0.2633354067802429\n",
      "Epoch: 7, Samples: 3392/5760, Loss: 0.2492700219154358\n",
      "Epoch: 7, Samples: 3424/5760, Loss: 0.4279276728630066\n",
      "Epoch: 7, Samples: 3456/5760, Loss: 0.16600210964679718\n",
      "Epoch: 7, Samples: 3488/5760, Loss: 0.32370054721832275\n",
      "Epoch: 7, Samples: 3520/5760, Loss: 0.12914638221263885\n",
      "Epoch: 7, Samples: 3552/5760, Loss: 0.18746666610240936\n",
      "Epoch: 7, Samples: 3584/5760, Loss: 0.2929116487503052\n",
      "Epoch: 7, Samples: 3616/5760, Loss: 0.21340669691562653\n",
      "Epoch: 7, Samples: 3648/5760, Loss: 0.2633639872074127\n",
      "Epoch: 7, Samples: 3680/5760, Loss: 0.3188667893409729\n",
      "Epoch: 7, Samples: 3712/5760, Loss: 0.24157989025115967\n",
      "Epoch: 7, Samples: 3744/5760, Loss: 0.2625085413455963\n",
      "Epoch: 7, Samples: 3776/5760, Loss: 0.30865007638931274\n",
      "Epoch: 7, Samples: 3808/5760, Loss: 0.2830255329608917\n",
      "Epoch: 7, Samples: 3840/5760, Loss: 0.27433809638023376\n",
      "Epoch: 7, Samples: 3872/5760, Loss: 0.26774489879608154\n",
      "Epoch: 7, Samples: 3904/5760, Loss: 0.23983177542686462\n",
      "Epoch: 7, Samples: 3936/5760, Loss: 0.2046777755022049\n",
      "Epoch: 7, Samples: 3968/5760, Loss: 0.2167145311832428\n",
      "Epoch: 7, Samples: 4000/5760, Loss: 0.2823786735534668\n",
      "Epoch: 7, Samples: 4032/5760, Loss: 0.2259470820426941\n",
      "Epoch: 7, Samples: 4064/5760, Loss: 0.2930234670639038\n",
      "Epoch: 7, Samples: 4096/5760, Loss: 0.2781309485435486\n",
      "Epoch: 7, Samples: 4128/5760, Loss: 0.2377856820821762\n",
      "Epoch: 7, Samples: 4160/5760, Loss: 0.21241697669029236\n",
      "Epoch: 7, Samples: 4192/5760, Loss: 0.21161513030529022\n",
      "Epoch: 7, Samples: 4224/5760, Loss: 0.2227281928062439\n",
      "Epoch: 7, Samples: 4256/5760, Loss: 0.22702656686306\n",
      "Epoch: 7, Samples: 4288/5760, Loss: 0.4625313878059387\n",
      "Epoch: 7, Samples: 4320/5760, Loss: 0.28033381700515747\n",
      "Epoch: 7, Samples: 4352/5760, Loss: 0.3046182692050934\n",
      "Epoch: 7, Samples: 4384/5760, Loss: 0.31509774923324585\n",
      "Epoch: 7, Samples: 4416/5760, Loss: 0.302991658449173\n",
      "Epoch: 7, Samples: 4448/5760, Loss: 0.29069048166275024\n",
      "Epoch: 7, Samples: 4480/5760, Loss: 0.2753799855709076\n",
      "Epoch: 7, Samples: 4512/5760, Loss: 0.4644661247730255\n",
      "Epoch: 7, Samples: 4544/5760, Loss: 0.1993987262248993\n",
      "Epoch: 7, Samples: 4576/5760, Loss: 0.2350776046514511\n",
      "Epoch: 7, Samples: 4608/5760, Loss: 0.38415634632110596\n",
      "Epoch: 7, Samples: 4640/5760, Loss: 0.17288102209568024\n",
      "Epoch: 7, Samples: 4672/5760, Loss: 0.29530224204063416\n",
      "Epoch: 7, Samples: 4704/5760, Loss: 0.28311657905578613\n",
      "Epoch: 7, Samples: 4736/5760, Loss: 0.3040125370025635\n",
      "Epoch: 7, Samples: 4768/5760, Loss: 0.24833370745182037\n",
      "Epoch: 7, Samples: 4800/5760, Loss: 0.27628615498542786\n",
      "Epoch: 7, Samples: 4832/5760, Loss: 0.19385312497615814\n",
      "Epoch: 7, Samples: 4864/5760, Loss: 0.2513897716999054\n",
      "Epoch: 7, Samples: 4896/5760, Loss: 0.290375292301178\n",
      "Epoch: 7, Samples: 4928/5760, Loss: 0.3355553150177002\n",
      "Epoch: 7, Samples: 4960/5760, Loss: 0.18261313438415527\n",
      "Epoch: 7, Samples: 4992/5760, Loss: 0.18160825967788696\n",
      "Epoch: 7, Samples: 5024/5760, Loss: 0.263552725315094\n",
      "Epoch: 7, Samples: 5056/5760, Loss: 0.23415285348892212\n",
      "Epoch: 7, Samples: 5088/5760, Loss: 0.20314009487628937\n",
      "Epoch: 7, Samples: 5120/5760, Loss: 0.50315260887146\n",
      "Epoch: 7, Samples: 5152/5760, Loss: 0.36661630868911743\n",
      "Epoch: 7, Samples: 5184/5760, Loss: 0.22667817771434784\n",
      "Epoch: 7, Samples: 5216/5760, Loss: 0.24647852778434753\n",
      "Epoch: 7, Samples: 5248/5760, Loss: 0.23927424848079681\n",
      "Epoch: 7, Samples: 5280/5760, Loss: 0.22552917897701263\n",
      "Epoch: 7, Samples: 5312/5760, Loss: 0.1890197992324829\n",
      "Epoch: 7, Samples: 5344/5760, Loss: 0.22230713069438934\n",
      "Epoch: 7, Samples: 5376/5760, Loss: 0.47475817799568176\n",
      "Epoch: 7, Samples: 5408/5760, Loss: 0.21919555962085724\n",
      "Epoch: 7, Samples: 5440/5760, Loss: 0.4498535692691803\n",
      "Epoch: 7, Samples: 5472/5760, Loss: 0.2321709245443344\n",
      "Epoch: 7, Samples: 5504/5760, Loss: 0.35146379470825195\n",
      "Epoch: 7, Samples: 5536/5760, Loss: 0.20786486566066742\n",
      "Epoch: 7, Samples: 5568/5760, Loss: 0.26127326488494873\n",
      "Epoch: 7, Samples: 5600/5760, Loss: 0.37825047969818115\n",
      "Epoch: 7, Samples: 5632/5760, Loss: 0.317523717880249\n",
      "Epoch: 7, Samples: 5664/5760, Loss: 0.28967714309692383\n",
      "Epoch: 7, Samples: 5696/5760, Loss: 0.21978236734867096\n",
      "Epoch: 7, Samples: 5728/5760, Loss: 1.5199949741363525\n",
      "\n",
      "Epoch: 7\n",
      "Training set: Average loss: 0.2902\n",
      "Validation set: Average loss: 0.4620, Accuracy: 743/818 (91%)\n",
      "Saving model (epoch 7) with lowest validation loss: 0.4619763040771851\n",
      "Epoch: 8, Samples: 0/5760, Loss: 0.2105063498020172\n",
      "Epoch: 8, Samples: 32/5760, Loss: 0.20229797065258026\n",
      "Epoch: 8, Samples: 64/5760, Loss: 0.3916813135147095\n",
      "Epoch: 8, Samples: 96/5760, Loss: 0.18519341945648193\n",
      "Epoch: 8, Samples: 128/5760, Loss: 0.16972367465496063\n",
      "Epoch: 8, Samples: 160/5760, Loss: 0.12939117848873138\n",
      "Epoch: 8, Samples: 192/5760, Loss: 0.17020368576049805\n",
      "Epoch: 8, Samples: 224/5760, Loss: 0.3288346230983734\n",
      "Epoch: 8, Samples: 256/5760, Loss: 0.19305859506130219\n",
      "Epoch: 8, Samples: 288/5760, Loss: 0.194711834192276\n",
      "Epoch: 8, Samples: 320/5760, Loss: 0.18700960278511047\n",
      "Epoch: 8, Samples: 352/5760, Loss: 0.32541245222091675\n",
      "Epoch: 8, Samples: 384/5760, Loss: 0.27507543563842773\n",
      "Epoch: 8, Samples: 416/5760, Loss: 0.1525983065366745\n",
      "Epoch: 8, Samples: 448/5760, Loss: 0.2099805474281311\n",
      "Epoch: 8, Samples: 480/5760, Loss: 0.24451522529125214\n",
      "Epoch: 8, Samples: 512/5760, Loss: 0.16975361108779907\n",
      "Epoch: 8, Samples: 544/5760, Loss: 0.20167888700962067\n",
      "Epoch: 8, Samples: 576/5760, Loss: 0.18236355483531952\n",
      "Epoch: 8, Samples: 608/5760, Loss: 0.3120456337928772\n",
      "Epoch: 8, Samples: 640/5760, Loss: 0.21814559400081635\n",
      "Epoch: 8, Samples: 672/5760, Loss: 0.30167853832244873\n",
      "Epoch: 8, Samples: 704/5760, Loss: 0.12919242680072784\n",
      "Epoch: 8, Samples: 736/5760, Loss: 0.2136392742395401\n",
      "Epoch: 8, Samples: 768/5760, Loss: 0.2474130392074585\n",
      "Epoch: 8, Samples: 800/5760, Loss: 0.17149947583675385\n",
      "Epoch: 8, Samples: 832/5760, Loss: 0.1759393811225891\n",
      "Epoch: 8, Samples: 864/5760, Loss: 0.2127452790737152\n",
      "Epoch: 8, Samples: 896/5760, Loss: 0.2964213490486145\n",
      "Epoch: 8, Samples: 928/5760, Loss: 0.18769767880439758\n",
      "Epoch: 8, Samples: 960/5760, Loss: 0.1190354973077774\n",
      "Epoch: 8, Samples: 992/5760, Loss: 0.15526095032691956\n",
      "Epoch: 8, Samples: 1024/5760, Loss: 0.1451544612646103\n",
      "Epoch: 8, Samples: 1056/5760, Loss: 0.21028749644756317\n",
      "Epoch: 8, Samples: 1088/5760, Loss: 0.3140560984611511\n",
      "Epoch: 8, Samples: 1120/5760, Loss: 0.1497274488210678\n",
      "Epoch: 8, Samples: 1152/5760, Loss: 0.22659838199615479\n",
      "Epoch: 8, Samples: 1184/5760, Loss: 0.22306181490421295\n",
      "Epoch: 8, Samples: 1216/5760, Loss: 0.24742434918880463\n",
      "Epoch: 8, Samples: 1248/5760, Loss: 0.23332549631595612\n",
      "Epoch: 8, Samples: 1280/5760, Loss: 0.25165796279907227\n",
      "Epoch: 8, Samples: 1312/5760, Loss: 0.18069837987422943\n",
      "Epoch: 8, Samples: 1344/5760, Loss: 0.1713414490222931\n",
      "Epoch: 8, Samples: 1376/5760, Loss: 0.3751377761363983\n",
      "Epoch: 8, Samples: 1408/5760, Loss: 0.13634561002254486\n",
      "Epoch: 8, Samples: 1440/5760, Loss: 0.19941231608390808\n",
      "Epoch: 8, Samples: 1472/5760, Loss: 0.23508591949939728\n",
      "Epoch: 8, Samples: 1504/5760, Loss: 0.16624514758586884\n",
      "Epoch: 8, Samples: 1536/5760, Loss: 0.17441421747207642\n",
      "Epoch: 8, Samples: 1568/5760, Loss: 0.2128177285194397\n",
      "Epoch: 8, Samples: 1600/5760, Loss: 0.16015620529651642\n",
      "Epoch: 8, Samples: 1632/5760, Loss: 0.15704643726348877\n",
      "Epoch: 8, Samples: 1664/5760, Loss: 0.23361976444721222\n",
      "Epoch: 8, Samples: 1696/5760, Loss: 0.26390355825424194\n",
      "Epoch: 8, Samples: 1728/5760, Loss: 0.1858716458082199\n",
      "Epoch: 8, Samples: 1760/5760, Loss: 0.21060846745967865\n",
      "Epoch: 8, Samples: 1792/5760, Loss: 0.1510828286409378\n",
      "Epoch: 8, Samples: 1824/5760, Loss: 0.24083535373210907\n",
      "Epoch: 8, Samples: 1856/5760, Loss: 0.17762961983680725\n",
      "Epoch: 8, Samples: 1888/5760, Loss: 0.1955372393131256\n",
      "Epoch: 8, Samples: 1920/5760, Loss: 0.18784663081169128\n",
      "Epoch: 8, Samples: 1952/5760, Loss: 0.31091734766960144\n",
      "Epoch: 8, Samples: 1984/5760, Loss: 0.16669347882270813\n",
      "Epoch: 8, Samples: 2016/5760, Loss: 0.31037795543670654\n",
      "Epoch: 8, Samples: 2048/5760, Loss: 0.3597222566604614\n",
      "Epoch: 8, Samples: 2080/5760, Loss: 0.16119933128356934\n",
      "Epoch: 8, Samples: 2112/5760, Loss: 0.30944275856018066\n",
      "Epoch: 8, Samples: 2144/5760, Loss: 0.15054991841316223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Samples: 2176/5760, Loss: 0.21107220649719238\n",
      "Epoch: 8, Samples: 2208/5760, Loss: 0.23855745792388916\n",
      "Epoch: 8, Samples: 2240/5760, Loss: 0.2163565754890442\n",
      "Epoch: 8, Samples: 2272/5760, Loss: 0.17361459136009216\n",
      "Epoch: 8, Samples: 2304/5760, Loss: 0.355289101600647\n",
      "Epoch: 8, Samples: 2336/5760, Loss: 0.15230898559093475\n",
      "Epoch: 8, Samples: 2368/5760, Loss: 0.11606496572494507\n",
      "Epoch: 8, Samples: 2400/5760, Loss: 0.13726544380187988\n",
      "Epoch: 8, Samples: 2432/5760, Loss: 0.24764931201934814\n",
      "Epoch: 8, Samples: 2464/5760, Loss: 0.24620309472084045\n",
      "Epoch: 8, Samples: 2496/5760, Loss: 0.26034486293792725\n",
      "Epoch: 8, Samples: 2528/5760, Loss: 0.3048785924911499\n",
      "Epoch: 8, Samples: 2560/5760, Loss: 0.12190283834934235\n",
      "Epoch: 8, Samples: 2592/5760, Loss: 0.2063509225845337\n",
      "Epoch: 8, Samples: 2624/5760, Loss: 0.14655186235904694\n",
      "Epoch: 8, Samples: 2656/5760, Loss: 0.16030465066432953\n",
      "Epoch: 8, Samples: 2688/5760, Loss: 0.33563557267189026\n",
      "Epoch: 8, Samples: 2720/5760, Loss: 0.16635558009147644\n",
      "Epoch: 8, Samples: 2752/5760, Loss: 0.151033416390419\n",
      "Epoch: 8, Samples: 2784/5760, Loss: 0.2031717747449875\n",
      "Epoch: 8, Samples: 2816/5760, Loss: 0.23217414319515228\n",
      "Epoch: 8, Samples: 2848/5760, Loss: 0.14959141612052917\n",
      "Epoch: 8, Samples: 2880/5760, Loss: 0.14455848932266235\n",
      "Epoch: 8, Samples: 2912/5760, Loss: 0.2535218596458435\n",
      "Epoch: 8, Samples: 2944/5760, Loss: 0.3862343430519104\n",
      "Epoch: 8, Samples: 2976/5760, Loss: 0.17485103011131287\n",
      "Epoch: 8, Samples: 3008/5760, Loss: 0.12925700843334198\n",
      "Epoch: 8, Samples: 3040/5760, Loss: 0.3027413785457611\n",
      "Epoch: 8, Samples: 3072/5760, Loss: 0.32191982865333557\n",
      "Epoch: 8, Samples: 3104/5760, Loss: 0.19691136479377747\n",
      "Epoch: 8, Samples: 3136/5760, Loss: 0.14855104684829712\n",
      "Epoch: 8, Samples: 3168/5760, Loss: 0.14090916514396667\n",
      "Epoch: 8, Samples: 3200/5760, Loss: 0.14877517521381378\n",
      "Epoch: 8, Samples: 3232/5760, Loss: 0.25882574915885925\n",
      "Epoch: 8, Samples: 3264/5760, Loss: 0.2313324213027954\n",
      "Epoch: 8, Samples: 3296/5760, Loss: 0.222665473818779\n",
      "Epoch: 8, Samples: 3328/5760, Loss: 0.26896876096725464\n",
      "Epoch: 8, Samples: 3360/5760, Loss: 0.27598676085472107\n",
      "Epoch: 8, Samples: 3392/5760, Loss: 0.09848544001579285\n",
      "Epoch: 8, Samples: 3424/5760, Loss: 0.18259045481681824\n",
      "Epoch: 8, Samples: 3456/5760, Loss: 0.1715642809867859\n",
      "Epoch: 8, Samples: 3488/5760, Loss: 0.27312979102134705\n",
      "Epoch: 8, Samples: 3520/5760, Loss: 0.07386037707328796\n",
      "Epoch: 8, Samples: 3552/5760, Loss: 0.20198632776737213\n",
      "Epoch: 8, Samples: 3584/5760, Loss: 0.29979926347732544\n",
      "Epoch: 8, Samples: 3616/5760, Loss: 0.23181328177452087\n",
      "Epoch: 8, Samples: 3648/5760, Loss: 0.16853535175323486\n",
      "Epoch: 8, Samples: 3680/5760, Loss: 0.13401947915554047\n",
      "Epoch: 8, Samples: 3712/5760, Loss: 0.13303719460964203\n",
      "Epoch: 8, Samples: 3744/5760, Loss: 0.174734428524971\n",
      "Epoch: 8, Samples: 3776/5760, Loss: 0.15194953978061676\n",
      "Epoch: 8, Samples: 3808/5760, Loss: 0.3021950125694275\n",
      "Epoch: 8, Samples: 3840/5760, Loss: 0.23631611466407776\n",
      "Epoch: 8, Samples: 3872/5760, Loss: 0.20263560116291046\n",
      "Epoch: 8, Samples: 3904/5760, Loss: 0.12735290825366974\n",
      "Epoch: 8, Samples: 3936/5760, Loss: 0.32595503330230713\n",
      "Epoch: 8, Samples: 3968/5760, Loss: 0.19313016533851624\n",
      "Epoch: 8, Samples: 4000/5760, Loss: 0.20744511485099792\n",
      "Epoch: 8, Samples: 4032/5760, Loss: 0.1305616796016693\n",
      "Epoch: 8, Samples: 4064/5760, Loss: 0.26498717069625854\n",
      "Epoch: 8, Samples: 4096/5760, Loss: 0.2923068702220917\n",
      "Epoch: 8, Samples: 4128/5760, Loss: 0.1364981085062027\n",
      "Epoch: 8, Samples: 4160/5760, Loss: 0.1433614194393158\n",
      "Epoch: 8, Samples: 4192/5760, Loss: 0.13102218508720398\n",
      "Epoch: 8, Samples: 4224/5760, Loss: 0.21447254717350006\n",
      "Epoch: 8, Samples: 4256/5760, Loss: 0.16772261261940002\n",
      "Epoch: 8, Samples: 4288/5760, Loss: 0.1795053482055664\n",
      "Epoch: 8, Samples: 4320/5760, Loss: 0.25637781620025635\n",
      "Epoch: 8, Samples: 4352/5760, Loss: 0.3808267414569855\n",
      "Epoch: 8, Samples: 4384/5760, Loss: 0.12971381843090057\n",
      "Epoch: 8, Samples: 4416/5760, Loss: 0.10937540233135223\n",
      "Epoch: 8, Samples: 4448/5760, Loss: 0.2756263315677643\n",
      "Epoch: 8, Samples: 4480/5760, Loss: 0.2465217113494873\n",
      "Epoch: 8, Samples: 4512/5760, Loss: 0.26174354553222656\n",
      "Epoch: 8, Samples: 4544/5760, Loss: 0.23869040608406067\n",
      "Epoch: 8, Samples: 4576/5760, Loss: 0.14946569502353668\n",
      "Epoch: 8, Samples: 4608/5760, Loss: 0.24232299625873566\n",
      "Epoch: 8, Samples: 4640/5760, Loss: 0.24790051579475403\n",
      "Epoch: 8, Samples: 4672/5760, Loss: 0.17949016392230988\n",
      "Epoch: 8, Samples: 4704/5760, Loss: 0.3335457444190979\n",
      "Epoch: 8, Samples: 4736/5760, Loss: 0.18635094165802002\n",
      "Epoch: 8, Samples: 4768/5760, Loss: 0.22263306379318237\n",
      "Epoch: 8, Samples: 4800/5760, Loss: 0.268905907869339\n",
      "Epoch: 8, Samples: 4832/5760, Loss: 0.20630836486816406\n",
      "Epoch: 8, Samples: 4864/5760, Loss: 0.22533707320690155\n",
      "Epoch: 8, Samples: 4896/5760, Loss: 0.2107173651456833\n",
      "Epoch: 8, Samples: 4928/5760, Loss: 0.24458083510398865\n",
      "Epoch: 8, Samples: 4960/5760, Loss: 0.24359434843063354\n",
      "Epoch: 8, Samples: 4992/5760, Loss: 0.2871968448162079\n",
      "Epoch: 8, Samples: 5024/5760, Loss: 0.26373404264450073\n",
      "Epoch: 8, Samples: 5056/5760, Loss: 0.25744760036468506\n",
      "Epoch: 8, Samples: 5088/5760, Loss: 0.3479672074317932\n",
      "Epoch: 8, Samples: 5120/5760, Loss: 0.3185734748840332\n",
      "Epoch: 8, Samples: 5152/5760, Loss: 0.17410637438297272\n",
      "Epoch: 8, Samples: 5184/5760, Loss: 0.17752020061016083\n",
      "Epoch: 8, Samples: 5216/5760, Loss: 0.15034256875514984\n",
      "Epoch: 8, Samples: 5248/5760, Loss: 0.1462937593460083\n",
      "Epoch: 8, Samples: 5280/5760, Loss: 0.23742680251598358\n",
      "Epoch: 8, Samples: 5312/5760, Loss: 0.15747590363025665\n",
      "Epoch: 8, Samples: 5344/5760, Loss: 0.192362442612648\n",
      "Epoch: 8, Samples: 5376/5760, Loss: 0.37272000312805176\n",
      "Epoch: 8, Samples: 5408/5760, Loss: 0.13552692532539368\n",
      "Epoch: 8, Samples: 5440/5760, Loss: 0.1439228653907776\n",
      "Epoch: 8, Samples: 5472/5760, Loss: 0.19736768305301666\n",
      "Epoch: 8, Samples: 5504/5760, Loss: 0.3214930295944214\n",
      "Epoch: 8, Samples: 5536/5760, Loss: 0.13951639831066132\n",
      "Epoch: 8, Samples: 5568/5760, Loss: 0.1733238250017166\n",
      "Epoch: 8, Samples: 5600/5760, Loss: 0.10267683863639832\n",
      "Epoch: 8, Samples: 5632/5760, Loss: 0.23768852651119232\n",
      "Epoch: 8, Samples: 5664/5760, Loss: 0.13501021265983582\n",
      "Epoch: 8, Samples: 5696/5760, Loss: 0.3282996118068695\n",
      "Epoch: 8, Samples: 5728/5760, Loss: 1.1202571392059326\n",
      "\n",
      "Epoch: 8\n",
      "Training set: Average loss: 0.2184\n",
      "Validation set: Average loss: 0.4198, Accuracy: 756/818 (92%)\n",
      "Saving model (epoch 8) with lowest validation loss: 0.4197795150371698\n",
      "Epoch: 9, Samples: 0/5760, Loss: 0.17227059602737427\n",
      "Epoch: 9, Samples: 32/5760, Loss: 0.11504136025905609\n",
      "Epoch: 9, Samples: 64/5760, Loss: 0.17801238596439362\n",
      "Epoch: 9, Samples: 96/5760, Loss: 0.10828225314617157\n",
      "Epoch: 9, Samples: 128/5760, Loss: 0.22568556666374207\n",
      "Epoch: 9, Samples: 160/5760, Loss: 0.1588997095823288\n",
      "Epoch: 9, Samples: 192/5760, Loss: 0.25089046359062195\n",
      "Epoch: 9, Samples: 224/5760, Loss: 0.11396826803684235\n",
      "Epoch: 9, Samples: 256/5760, Loss: 0.17738200724124908\n",
      "Epoch: 9, Samples: 288/5760, Loss: 0.07494166493415833\n",
      "Epoch: 9, Samples: 320/5760, Loss: 0.2113705277442932\n",
      "Epoch: 9, Samples: 352/5760, Loss: 0.2666395306587219\n",
      "Epoch: 9, Samples: 384/5760, Loss: 0.1671883761882782\n",
      "Epoch: 9, Samples: 416/5760, Loss: 0.17598247528076172\n",
      "Epoch: 9, Samples: 448/5760, Loss: 0.16067415475845337\n",
      "Epoch: 9, Samples: 480/5760, Loss: 0.3915127217769623\n",
      "Epoch: 9, Samples: 512/5760, Loss: 0.25760942697525024\n",
      "Epoch: 9, Samples: 544/5760, Loss: 0.1750880926847458\n",
      "Epoch: 9, Samples: 576/5760, Loss: 0.15653683245182037\n",
      "Epoch: 9, Samples: 608/5760, Loss: 0.18011602759361267\n",
      "Epoch: 9, Samples: 640/5760, Loss: 0.12978877127170563\n",
      "Epoch: 9, Samples: 672/5760, Loss: 0.11884325742721558\n",
      "Epoch: 9, Samples: 704/5760, Loss: 0.2494843751192093\n",
      "Epoch: 9, Samples: 736/5760, Loss: 0.14897660911083221\n",
      "Epoch: 9, Samples: 768/5760, Loss: 0.07139672338962555\n",
      "Epoch: 9, Samples: 800/5760, Loss: 0.12430340051651001\n",
      "Epoch: 9, Samples: 832/5760, Loss: 0.13227351009845734\n",
      "Epoch: 9, Samples: 864/5760, Loss: 0.13084198534488678\n",
      "Epoch: 9, Samples: 896/5760, Loss: 0.16178980469703674\n",
      "Epoch: 9, Samples: 928/5760, Loss: 0.1442696452140808\n",
      "Epoch: 9, Samples: 960/5760, Loss: 0.2734193503856659\n",
      "Epoch: 9, Samples: 992/5760, Loss: 0.24172204732894897\n",
      "Epoch: 9, Samples: 1024/5760, Loss: 0.16169343888759613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Samples: 1056/5760, Loss: 0.13632002472877502\n",
      "Epoch: 9, Samples: 1088/5760, Loss: 0.14102719724178314\n",
      "Epoch: 9, Samples: 1120/5760, Loss: 0.20720252394676208\n",
      "Epoch: 9, Samples: 1152/5760, Loss: 0.11415794491767883\n",
      "Epoch: 9, Samples: 1184/5760, Loss: 0.2159465104341507\n",
      "Epoch: 9, Samples: 1216/5760, Loss: 0.11395163834095001\n",
      "Epoch: 9, Samples: 1248/5760, Loss: 0.11009016633033752\n",
      "Epoch: 9, Samples: 1280/5760, Loss: 0.16646844148635864\n",
      "Epoch: 9, Samples: 1312/5760, Loss: 0.13615332543849945\n",
      "Epoch: 9, Samples: 1344/5760, Loss: 0.09483759105205536\n",
      "Epoch: 9, Samples: 1376/5760, Loss: 0.12000858783721924\n",
      "Epoch: 9, Samples: 1408/5760, Loss: 0.23929433524608612\n",
      "Epoch: 9, Samples: 1440/5760, Loss: 0.11000195145606995\n",
      "Epoch: 9, Samples: 1472/5760, Loss: 0.15311303734779358\n",
      "Epoch: 9, Samples: 1504/5760, Loss: 0.1857897788286209\n",
      "Epoch: 9, Samples: 1536/5760, Loss: 0.1468057483434677\n",
      "Epoch: 9, Samples: 1568/5760, Loss: 0.15960437059402466\n",
      "Epoch: 9, Samples: 1600/5760, Loss: 0.08785636723041534\n",
      "Epoch: 9, Samples: 1632/5760, Loss: 0.11251023411750793\n",
      "Epoch: 9, Samples: 1664/5760, Loss: 0.13625645637512207\n",
      "Epoch: 9, Samples: 1696/5760, Loss: 0.10485319793224335\n",
      "Epoch: 9, Samples: 1728/5760, Loss: 0.1038278192281723\n",
      "Epoch: 9, Samples: 1760/5760, Loss: 0.1542741060256958\n",
      "Epoch: 9, Samples: 1792/5760, Loss: 0.21540679037570953\n",
      "Epoch: 9, Samples: 1824/5760, Loss: 0.14069326221942902\n",
      "Epoch: 9, Samples: 1856/5760, Loss: 0.2078806459903717\n",
      "Epoch: 9, Samples: 1888/5760, Loss: 0.12940815091133118\n",
      "Epoch: 9, Samples: 1920/5760, Loss: 0.22316379845142365\n",
      "Epoch: 9, Samples: 1952/5760, Loss: 0.11690671741962433\n",
      "Epoch: 9, Samples: 1984/5760, Loss: 0.13831186294555664\n",
      "Epoch: 9, Samples: 2016/5760, Loss: 0.06901134550571442\n",
      "Epoch: 9, Samples: 2048/5760, Loss: 0.1332014948129654\n",
      "Epoch: 9, Samples: 2080/5760, Loss: 0.1712966114282608\n",
      "Epoch: 9, Samples: 2112/5760, Loss: 0.17842219769954681\n",
      "Epoch: 9, Samples: 2144/5760, Loss: 0.1430165022611618\n",
      "Epoch: 9, Samples: 2176/5760, Loss: 0.27476581931114197\n",
      "Epoch: 9, Samples: 2208/5760, Loss: 0.1140761524438858\n",
      "Epoch: 9, Samples: 2240/5760, Loss: 0.32184121012687683\n",
      "Epoch: 9, Samples: 2272/5760, Loss: 0.10060074925422668\n",
      "Epoch: 9, Samples: 2304/5760, Loss: 0.3416658639907837\n",
      "Epoch: 9, Samples: 2336/5760, Loss: 0.10606682300567627\n",
      "Epoch: 9, Samples: 2368/5760, Loss: 0.28231969475746155\n",
      "Epoch: 9, Samples: 2400/5760, Loss: 0.18533118069171906\n",
      "Epoch: 9, Samples: 2432/5760, Loss: 0.13207314908504486\n",
      "Epoch: 9, Samples: 2464/5760, Loss: 0.10294847190380096\n",
      "Epoch: 9, Samples: 2496/5760, Loss: 0.16080635786056519\n",
      "Epoch: 9, Samples: 2528/5760, Loss: 0.15848834812641144\n",
      "Epoch: 9, Samples: 2560/5760, Loss: 0.17692069709300995\n",
      "Epoch: 9, Samples: 2592/5760, Loss: 0.2649151384830475\n",
      "Epoch: 9, Samples: 2624/5760, Loss: 0.12698869407176971\n",
      "Epoch: 9, Samples: 2656/5760, Loss: 0.15633663535118103\n",
      "Epoch: 9, Samples: 2688/5760, Loss: 0.2747843861579895\n",
      "Epoch: 9, Samples: 2720/5760, Loss: 0.13756856322288513\n",
      "Epoch: 9, Samples: 2752/5760, Loss: 0.07083927094936371\n",
      "Epoch: 9, Samples: 2784/5760, Loss: 0.10261979699134827\n",
      "Epoch: 9, Samples: 2816/5760, Loss: 0.13119524717330933\n",
      "Epoch: 9, Samples: 2848/5760, Loss: 0.17446190118789673\n",
      "Epoch: 9, Samples: 2880/5760, Loss: 0.12289923429489136\n",
      "Epoch: 9, Samples: 2912/5760, Loss: 0.1606830507516861\n",
      "Epoch: 9, Samples: 2944/5760, Loss: 0.07554404437541962\n",
      "Epoch: 9, Samples: 2976/5760, Loss: 0.16902464628219604\n",
      "Epoch: 9, Samples: 3008/5760, Loss: 0.16775618493556976\n",
      "Epoch: 9, Samples: 3040/5760, Loss: 0.09874165058135986\n",
      "Epoch: 9, Samples: 3072/5760, Loss: 0.17614544928073883\n",
      "Epoch: 9, Samples: 3104/5760, Loss: 0.1520349383354187\n",
      "Epoch: 9, Samples: 3136/5760, Loss: 0.1573425680398941\n",
      "Epoch: 9, Samples: 3168/5760, Loss: 0.17848613858222961\n",
      "Epoch: 9, Samples: 3200/5760, Loss: 0.17784056067466736\n",
      "Epoch: 9, Samples: 3232/5760, Loss: 0.10379950702190399\n",
      "Epoch: 9, Samples: 3264/5760, Loss: 0.1296788603067398\n",
      "Epoch: 9, Samples: 3296/5760, Loss: 0.15083260834217072\n",
      "Epoch: 9, Samples: 3328/5760, Loss: 0.08755470812320709\n",
      "Epoch: 9, Samples: 3360/5760, Loss: 0.07011298835277557\n",
      "Epoch: 9, Samples: 3392/5760, Loss: 0.10541599988937378\n",
      "Epoch: 9, Samples: 3424/5760, Loss: 0.18387943506240845\n",
      "Epoch: 9, Samples: 3456/5760, Loss: 0.15486641228199005\n",
      "Epoch: 9, Samples: 3488/5760, Loss: 0.17631138861179352\n",
      "Epoch: 9, Samples: 3520/5760, Loss: 0.14857175946235657\n",
      "Epoch: 9, Samples: 3552/5760, Loss: 0.23208075761795044\n",
      "Epoch: 9, Samples: 3584/5760, Loss: 0.1832774430513382\n",
      "Epoch: 9, Samples: 3616/5760, Loss: 0.19806936383247375\n",
      "Epoch: 9, Samples: 3648/5760, Loss: 0.18548132479190826\n",
      "Epoch: 9, Samples: 3680/5760, Loss: 0.15632474422454834\n",
      "Epoch: 9, Samples: 3712/5760, Loss: 0.2036120593547821\n",
      "Epoch: 9, Samples: 3744/5760, Loss: 0.14050595462322235\n",
      "Epoch: 9, Samples: 3776/5760, Loss: 0.07401537895202637\n",
      "Epoch: 9, Samples: 3808/5760, Loss: 0.09120333194732666\n",
      "Epoch: 9, Samples: 3840/5760, Loss: 0.14478322863578796\n",
      "Epoch: 9, Samples: 3872/5760, Loss: 0.10735324025154114\n",
      "Epoch: 9, Samples: 3904/5760, Loss: 0.20755936205387115\n",
      "Epoch: 9, Samples: 3936/5760, Loss: 0.1445653885602951\n",
      "Epoch: 9, Samples: 3968/5760, Loss: 0.17602723836898804\n",
      "Epoch: 9, Samples: 4000/5760, Loss: 0.1813650131225586\n",
      "Epoch: 9, Samples: 4032/5760, Loss: 0.4728422462940216\n",
      "Epoch: 9, Samples: 4064/5760, Loss: 0.16519425809383392\n",
      "Epoch: 9, Samples: 4096/5760, Loss: 0.10831715166568756\n",
      "Epoch: 9, Samples: 4128/5760, Loss: 0.15798483788967133\n",
      "Epoch: 9, Samples: 4160/5760, Loss: 0.2114623486995697\n",
      "Epoch: 9, Samples: 4192/5760, Loss: 0.2048315852880478\n",
      "Epoch: 9, Samples: 4224/5760, Loss: 0.2657688558101654\n",
      "Epoch: 9, Samples: 4256/5760, Loss: 0.27238065004348755\n",
      "Epoch: 9, Samples: 4288/5760, Loss: 0.18912598490715027\n",
      "Epoch: 9, Samples: 4320/5760, Loss: 0.15054835379123688\n",
      "Epoch: 9, Samples: 4352/5760, Loss: 0.12110395729541779\n",
      "Epoch: 9, Samples: 4384/5760, Loss: 0.12147445976734161\n",
      "Epoch: 9, Samples: 4416/5760, Loss: 0.19548900425434113\n",
      "Epoch: 9, Samples: 4448/5760, Loss: 0.25790658593177795\n",
      "Epoch: 9, Samples: 4480/5760, Loss: 0.27304935455322266\n",
      "Epoch: 9, Samples: 4512/5760, Loss: 0.14084023237228394\n",
      "Epoch: 9, Samples: 4544/5760, Loss: 0.16630353033542633\n",
      "Epoch: 9, Samples: 4576/5760, Loss: 0.14092068374156952\n",
      "Epoch: 9, Samples: 4608/5760, Loss: 0.11578235030174255\n",
      "Epoch: 9, Samples: 4640/5760, Loss: 0.1020270437002182\n",
      "Epoch: 9, Samples: 4672/5760, Loss: 0.12827320396900177\n",
      "Epoch: 9, Samples: 4704/5760, Loss: 0.2806350290775299\n",
      "Epoch: 9, Samples: 4736/5760, Loss: 0.10394184291362762\n",
      "Epoch: 9, Samples: 4768/5760, Loss: 0.0914333164691925\n",
      "Epoch: 9, Samples: 4800/5760, Loss: 0.33136555552482605\n",
      "Epoch: 9, Samples: 4832/5760, Loss: 0.11570611596107483\n",
      "Epoch: 9, Samples: 4864/5760, Loss: 0.19229020178318024\n",
      "Epoch: 9, Samples: 4896/5760, Loss: 0.1352871209383011\n",
      "Epoch: 9, Samples: 4928/5760, Loss: 0.20174629986286163\n",
      "Epoch: 9, Samples: 4960/5760, Loss: 0.09184803068637848\n",
      "Epoch: 9, Samples: 4992/5760, Loss: 0.12374575436115265\n",
      "Epoch: 9, Samples: 5024/5760, Loss: 0.2730332314968109\n",
      "Epoch: 9, Samples: 5056/5760, Loss: 0.184078186750412\n",
      "Epoch: 9, Samples: 5088/5760, Loss: 0.12333439290523529\n",
      "Epoch: 9, Samples: 5120/5760, Loss: 0.06469623744487762\n",
      "Epoch: 9, Samples: 5152/5760, Loss: 0.10214659571647644\n",
      "Epoch: 9, Samples: 5184/5760, Loss: 0.17265662550926208\n",
      "Epoch: 9, Samples: 5216/5760, Loss: 0.16481302678585052\n",
      "Epoch: 9, Samples: 5248/5760, Loss: 0.08812497556209564\n",
      "Epoch: 9, Samples: 5280/5760, Loss: 0.22603698074817657\n",
      "Epoch: 9, Samples: 5312/5760, Loss: 0.1919005811214447\n",
      "Epoch: 9, Samples: 5344/5760, Loss: 0.13134300708770752\n",
      "Epoch: 9, Samples: 5376/5760, Loss: 0.18768934905529022\n",
      "Epoch: 9, Samples: 5408/5760, Loss: 0.2958162724971771\n",
      "Epoch: 9, Samples: 5440/5760, Loss: 0.13719090819358826\n",
      "Epoch: 9, Samples: 5472/5760, Loss: 0.2810627818107605\n",
      "Epoch: 9, Samples: 5504/5760, Loss: 0.11254791915416718\n",
      "Epoch: 9, Samples: 5536/5760, Loss: 0.33569231629371643\n",
      "Epoch: 9, Samples: 5568/5760, Loss: 0.23297710716724396\n",
      "Epoch: 9, Samples: 5600/5760, Loss: 0.11419761180877686\n",
      "Epoch: 9, Samples: 5632/5760, Loss: 0.21435435116291046\n",
      "Epoch: 9, Samples: 5664/5760, Loss: 0.19803158938884735\n",
      "Epoch: 9, Samples: 5696/5760, Loss: 0.1552167385816574\n",
      "Epoch: 9, Samples: 5728/5760, Loss: 0.5595055818557739\n",
      "\n",
      "Epoch: 9\n",
      "Training set: Average loss: 0.1687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss: 0.4285, Accuracy: 741/818 (91%)\n",
      "Epoch: 10, Samples: 0/5760, Loss: 0.0632612407207489\n",
      "Epoch: 10, Samples: 32/5760, Loss: 0.11526094377040863\n",
      "Epoch: 10, Samples: 64/5760, Loss: 0.06662280857563019\n",
      "Epoch: 10, Samples: 96/5760, Loss: 0.07758261263370514\n",
      "Epoch: 10, Samples: 128/5760, Loss: 0.10655315220355988\n",
      "Epoch: 10, Samples: 160/5760, Loss: 0.1144159734249115\n",
      "Epoch: 10, Samples: 192/5760, Loss: 0.16287676990032196\n",
      "Epoch: 10, Samples: 224/5760, Loss: 0.13798481225967407\n",
      "Epoch: 10, Samples: 256/5760, Loss: 0.07823912799358368\n",
      "Epoch: 10, Samples: 288/5760, Loss: 0.0933220237493515\n",
      "Epoch: 10, Samples: 320/5760, Loss: 0.10903230309486389\n",
      "Epoch: 10, Samples: 352/5760, Loss: 0.130917489528656\n",
      "Epoch: 10, Samples: 384/5760, Loss: 0.17921048402786255\n",
      "Epoch: 10, Samples: 416/5760, Loss: 0.07767616212368011\n",
      "Epoch: 10, Samples: 448/5760, Loss: 0.07844151556491852\n",
      "Epoch: 10, Samples: 480/5760, Loss: 0.22872382402420044\n",
      "Epoch: 10, Samples: 512/5760, Loss: 0.18824678659439087\n",
      "Epoch: 10, Samples: 544/5760, Loss: 0.12194369733333588\n",
      "Epoch: 10, Samples: 576/5760, Loss: 0.16526195406913757\n",
      "Epoch: 10, Samples: 608/5760, Loss: 0.06881645321846008\n",
      "Epoch: 10, Samples: 640/5760, Loss: 0.07277655601501465\n",
      "Epoch: 10, Samples: 672/5760, Loss: 0.1200120598077774\n",
      "Epoch: 10, Samples: 704/5760, Loss: 0.10734052956104279\n",
      "Epoch: 10, Samples: 736/5760, Loss: 0.08024471998214722\n",
      "Epoch: 10, Samples: 768/5760, Loss: 0.1892787218093872\n",
      "Epoch: 10, Samples: 800/5760, Loss: 0.15267233550548553\n",
      "Epoch: 10, Samples: 832/5760, Loss: 0.17927196621894836\n",
      "Epoch: 10, Samples: 864/5760, Loss: 0.1406937539577484\n",
      "Epoch: 10, Samples: 896/5760, Loss: 0.1144871860742569\n",
      "Epoch: 10, Samples: 928/5760, Loss: 0.1290496587753296\n",
      "Epoch: 10, Samples: 960/5760, Loss: 0.13841865956783295\n",
      "Epoch: 10, Samples: 992/5760, Loss: 0.11696144938468933\n",
      "Epoch: 10, Samples: 1024/5760, Loss: 0.14677837491035461\n",
      "Epoch: 10, Samples: 1056/5760, Loss: 0.16368000209331512\n",
      "Epoch: 10, Samples: 1088/5760, Loss: 0.15609046816825867\n",
      "Epoch: 10, Samples: 1120/5760, Loss: 0.1684548258781433\n",
      "Epoch: 10, Samples: 1152/5760, Loss: 0.08485880494117737\n",
      "Epoch: 10, Samples: 1184/5760, Loss: 0.13520078361034393\n",
      "Epoch: 10, Samples: 1216/5760, Loss: 0.30960407853126526\n",
      "Epoch: 10, Samples: 1248/5760, Loss: 0.13639532029628754\n",
      "Epoch: 10, Samples: 1280/5760, Loss: 0.08988358080387115\n",
      "Epoch: 10, Samples: 1312/5760, Loss: 0.1242060661315918\n",
      "Epoch: 10, Samples: 1344/5760, Loss: 0.10098400712013245\n",
      "Epoch: 10, Samples: 1376/5760, Loss: 0.12504449486732483\n",
      "Epoch: 10, Samples: 1408/5760, Loss: 0.1688515543937683\n",
      "Epoch: 10, Samples: 1440/5760, Loss: 0.13606098294258118\n",
      "Epoch: 10, Samples: 1472/5760, Loss: 0.09590773284435272\n",
      "Epoch: 10, Samples: 1504/5760, Loss: 0.1337822526693344\n",
      "Epoch: 10, Samples: 1536/5760, Loss: 0.11274658143520355\n",
      "Epoch: 10, Samples: 1568/5760, Loss: 0.07846769690513611\n",
      "Epoch: 10, Samples: 1600/5760, Loss: 0.26132410764694214\n",
      "Epoch: 10, Samples: 1632/5760, Loss: 0.1453195959329605\n",
      "Epoch: 10, Samples: 1664/5760, Loss: 0.22609558701515198\n",
      "Epoch: 10, Samples: 1696/5760, Loss: 0.13817569613456726\n",
      "Epoch: 10, Samples: 1728/5760, Loss: 0.11522841453552246\n",
      "Epoch: 10, Samples: 1760/5760, Loss: 0.10157795250415802\n",
      "Epoch: 10, Samples: 1792/5760, Loss: 0.1268669217824936\n",
      "Epoch: 10, Samples: 1824/5760, Loss: 0.14515246450901031\n",
      "Epoch: 10, Samples: 1856/5760, Loss: 0.09324133396148682\n",
      "Epoch: 10, Samples: 1888/5760, Loss: 0.1288522183895111\n",
      "Epoch: 10, Samples: 1920/5760, Loss: 0.19957652688026428\n",
      "Epoch: 10, Samples: 1952/5760, Loss: 0.07397255301475525\n",
      "Epoch: 10, Samples: 1984/5760, Loss: 0.09771007299423218\n",
      "Epoch: 10, Samples: 2016/5760, Loss: 0.20872041583061218\n",
      "Epoch: 10, Samples: 2048/5760, Loss: 0.12604615092277527\n",
      "Epoch: 10, Samples: 2080/5760, Loss: 0.12932088971138\n",
      "Epoch: 10, Samples: 2112/5760, Loss: 0.1364424079656601\n",
      "Epoch: 10, Samples: 2144/5760, Loss: 0.08285003900527954\n",
      "Epoch: 10, Samples: 2176/5760, Loss: 0.0703018456697464\n",
      "Epoch: 10, Samples: 2208/5760, Loss: 0.13410314917564392\n",
      "Epoch: 10, Samples: 2240/5760, Loss: 0.15486083924770355\n",
      "Epoch: 10, Samples: 2272/5760, Loss: 0.14296749234199524\n",
      "Epoch: 10, Samples: 2304/5760, Loss: 0.07982999086380005\n",
      "Epoch: 10, Samples: 2336/5760, Loss: 0.19221936166286469\n",
      "Epoch: 10, Samples: 2368/5760, Loss: 0.1548559069633484\n",
      "Epoch: 10, Samples: 2400/5760, Loss: 0.10616026818752289\n",
      "Epoch: 10, Samples: 2432/5760, Loss: 0.11505870521068573\n",
      "Epoch: 10, Samples: 2464/5760, Loss: 0.104912668466568\n",
      "Epoch: 10, Samples: 2496/5760, Loss: 0.15110284090042114\n",
      "Epoch: 10, Samples: 2528/5760, Loss: 0.15096013247966766\n",
      "Epoch: 10, Samples: 2560/5760, Loss: 0.08584862947463989\n",
      "Epoch: 10, Samples: 2592/5760, Loss: 0.08898292481899261\n",
      "Epoch: 10, Samples: 2624/5760, Loss: 0.08838045597076416\n",
      "Epoch: 10, Samples: 2656/5760, Loss: 0.10927501320838928\n",
      "Epoch: 10, Samples: 2688/5760, Loss: 0.07438716292381287\n",
      "Epoch: 10, Samples: 2720/5760, Loss: 0.26640698313713074\n",
      "Epoch: 10, Samples: 2752/5760, Loss: 0.12679752707481384\n",
      "Epoch: 10, Samples: 2784/5760, Loss: 0.18619245290756226\n",
      "Epoch: 10, Samples: 2816/5760, Loss: 0.1537361890077591\n",
      "Epoch: 10, Samples: 2848/5760, Loss: 0.18511219322681427\n",
      "Epoch: 10, Samples: 2880/5760, Loss: 0.13403841853141785\n",
      "Epoch: 10, Samples: 2912/5760, Loss: 0.12842285633087158\n",
      "Epoch: 10, Samples: 2944/5760, Loss: 0.09522996842861176\n",
      "Epoch: 10, Samples: 2976/5760, Loss: 0.17538118362426758\n",
      "Epoch: 10, Samples: 3008/5760, Loss: 0.09800298511981964\n",
      "Epoch: 10, Samples: 3040/5760, Loss: 0.17513306438922882\n",
      "Epoch: 10, Samples: 3072/5760, Loss: 0.2871774733066559\n",
      "Epoch: 10, Samples: 3104/5760, Loss: 0.07001359760761261\n",
      "Epoch: 10, Samples: 3136/5760, Loss: 0.08352978527545929\n",
      "Epoch: 10, Samples: 3168/5760, Loss: 0.07746046781539917\n",
      "Epoch: 10, Samples: 3200/5760, Loss: 0.09864750504493713\n",
      "Epoch: 10, Samples: 3232/5760, Loss: 0.11276744306087494\n",
      "Epoch: 10, Samples: 3264/5760, Loss: 0.08684521913528442\n",
      "Epoch: 10, Samples: 3296/5760, Loss: 0.07893621921539307\n",
      "Epoch: 10, Samples: 3328/5760, Loss: 0.09308817982673645\n",
      "Epoch: 10, Samples: 3360/5760, Loss: 0.13562363386154175\n",
      "Epoch: 10, Samples: 3392/5760, Loss: 0.11716793477535248\n",
      "Epoch: 10, Samples: 3424/5760, Loss: 0.06790855526924133\n",
      "Epoch: 10, Samples: 3456/5760, Loss: 0.11943862587213516\n",
      "Epoch: 10, Samples: 3488/5760, Loss: 0.05161602795124054\n",
      "Epoch: 10, Samples: 3520/5760, Loss: 0.15398454666137695\n",
      "Epoch: 10, Samples: 3552/5760, Loss: 0.12869025766849518\n",
      "Epoch: 10, Samples: 3584/5760, Loss: 0.11035564541816711\n",
      "Epoch: 10, Samples: 3616/5760, Loss: 0.1966603696346283\n",
      "Epoch: 10, Samples: 3648/5760, Loss: 0.1647561937570572\n",
      "Epoch: 10, Samples: 3680/5760, Loss: 0.16208916902542114\n",
      "Epoch: 10, Samples: 3712/5760, Loss: 0.08627039194107056\n",
      "Epoch: 10, Samples: 3744/5760, Loss: 0.07524895668029785\n",
      "Epoch: 10, Samples: 3776/5760, Loss: 0.17386020720005035\n",
      "Epoch: 10, Samples: 3808/5760, Loss: 0.11661502718925476\n",
      "Epoch: 10, Samples: 3840/5760, Loss: 0.10029518604278564\n",
      "Epoch: 10, Samples: 3872/5760, Loss: 0.1071702241897583\n",
      "Epoch: 10, Samples: 3904/5760, Loss: 0.10782594978809357\n",
      "Epoch: 10, Samples: 3936/5760, Loss: 0.26855120062828064\n",
      "Epoch: 10, Samples: 3968/5760, Loss: 0.13344420492649078\n",
      "Epoch: 10, Samples: 4000/5760, Loss: 0.17696915566921234\n",
      "Epoch: 10, Samples: 4032/5760, Loss: 0.08639651536941528\n",
      "Epoch: 10, Samples: 4064/5760, Loss: 0.21488000452518463\n",
      "Epoch: 10, Samples: 4096/5760, Loss: 0.06571884453296661\n",
      "Epoch: 10, Samples: 4128/5760, Loss: 0.06238274276256561\n",
      "Epoch: 10, Samples: 4160/5760, Loss: 0.14297087490558624\n",
      "Epoch: 10, Samples: 4192/5760, Loss: 0.11219397187232971\n",
      "Epoch: 10, Samples: 4224/5760, Loss: 0.06305471062660217\n",
      "Epoch: 10, Samples: 4256/5760, Loss: 0.09752097725868225\n",
      "Epoch: 10, Samples: 4288/5760, Loss: 0.09289921820163727\n",
      "Epoch: 10, Samples: 4320/5760, Loss: 0.0880398154258728\n",
      "Epoch: 10, Samples: 4352/5760, Loss: 0.15865232050418854\n",
      "Epoch: 10, Samples: 4384/5760, Loss: 0.15762238204479218\n",
      "Epoch: 10, Samples: 4416/5760, Loss: 0.11719246208667755\n",
      "Epoch: 10, Samples: 4448/5760, Loss: 0.07098238170146942\n",
      "Epoch: 10, Samples: 4480/5760, Loss: 0.14302396774291992\n",
      "Epoch: 10, Samples: 4512/5760, Loss: 0.17168371379375458\n",
      "Epoch: 10, Samples: 4544/5760, Loss: 0.09906980395317078\n",
      "Epoch: 10, Samples: 4576/5760, Loss: 0.17367960512638092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Samples: 4608/5760, Loss: 0.08371695876121521\n",
      "Epoch: 10, Samples: 4640/5760, Loss: 0.0960046648979187\n",
      "Epoch: 10, Samples: 4672/5760, Loss: 0.12044525146484375\n",
      "Epoch: 10, Samples: 4704/5760, Loss: 0.1144823431968689\n",
      "Epoch: 10, Samples: 4736/5760, Loss: 0.14977143704891205\n",
      "Epoch: 10, Samples: 4768/5760, Loss: 0.12794330716133118\n",
      "Epoch: 10, Samples: 4800/5760, Loss: 0.1028425544500351\n",
      "Epoch: 10, Samples: 4832/5760, Loss: 0.1124931126832962\n",
      "Epoch: 10, Samples: 4864/5760, Loss: 0.10634084045886993\n",
      "Epoch: 10, Samples: 4896/5760, Loss: 0.1495128870010376\n",
      "Epoch: 10, Samples: 4928/5760, Loss: 0.20256467163562775\n",
      "Epoch: 10, Samples: 4960/5760, Loss: 0.3111112713813782\n",
      "Epoch: 10, Samples: 4992/5760, Loss: 0.09062990546226501\n",
      "Epoch: 10, Samples: 5024/5760, Loss: 0.1064104288816452\n",
      "Epoch: 10, Samples: 5056/5760, Loss: 0.18886174261569977\n",
      "Epoch: 10, Samples: 5088/5760, Loss: 0.1736472249031067\n",
      "Epoch: 10, Samples: 5120/5760, Loss: 0.07432104647159576\n",
      "Epoch: 10, Samples: 5152/5760, Loss: 0.12050776183605194\n",
      "Epoch: 10, Samples: 5184/5760, Loss: 0.11446639150381088\n",
      "Epoch: 10, Samples: 5216/5760, Loss: 0.1834890991449356\n",
      "Epoch: 10, Samples: 5248/5760, Loss: 0.08077986538410187\n",
      "Epoch: 10, Samples: 5280/5760, Loss: 0.14020389318466187\n",
      "Epoch: 10, Samples: 5312/5760, Loss: 0.2399047613143921\n",
      "Epoch: 10, Samples: 5344/5760, Loss: 0.19519104063510895\n",
      "Epoch: 10, Samples: 5376/5760, Loss: 0.07830573618412018\n",
      "Epoch: 10, Samples: 5408/5760, Loss: 0.12852001190185547\n",
      "Epoch: 10, Samples: 5440/5760, Loss: 0.09050916135311127\n",
      "Epoch: 10, Samples: 5472/5760, Loss: 0.11085483431816101\n",
      "Epoch: 10, Samples: 5504/5760, Loss: 0.10817205905914307\n",
      "Epoch: 10, Samples: 5536/5760, Loss: 0.156912162899971\n",
      "Epoch: 10, Samples: 5568/5760, Loss: 0.21745848655700684\n",
      "Epoch: 10, Samples: 5600/5760, Loss: 0.050372347235679626\n",
      "Epoch: 10, Samples: 5632/5760, Loss: 0.08021266758441925\n",
      "Epoch: 10, Samples: 5664/5760, Loss: 0.14570362865924835\n",
      "Epoch: 10, Samples: 5696/5760, Loss: 0.1482020765542984\n",
      "Epoch: 10, Samples: 5728/5760, Loss: 0.7739105224609375\n",
      "\n",
      "Epoch: 10\n",
      "Training set: Average loss: 0.1332\n",
      "Validation set: Average loss: 0.3739, Accuracy: 755/818 (92%)\n",
      "Saving model (epoch 10) with lowest validation loss: 0.37389469949098736\n",
      "Epoch: 11, Samples: 0/5760, Loss: 0.09104609489440918\n",
      "Epoch: 11, Samples: 32/5760, Loss: 0.12465006113052368\n",
      "Epoch: 11, Samples: 64/5760, Loss: 0.17351245880126953\n",
      "Epoch: 11, Samples: 96/5760, Loss: 0.12855207920074463\n",
      "Epoch: 11, Samples: 128/5760, Loss: 0.0649963915348053\n",
      "Epoch: 11, Samples: 160/5760, Loss: 0.07398666441440582\n",
      "Epoch: 11, Samples: 192/5760, Loss: 0.0844024270772934\n",
      "Epoch: 11, Samples: 224/5760, Loss: 0.07995247840881348\n",
      "Epoch: 11, Samples: 256/5760, Loss: 0.1283705234527588\n",
      "Epoch: 11, Samples: 288/5760, Loss: 0.08557282388210297\n",
      "Epoch: 11, Samples: 320/5760, Loss: 0.05253778398036957\n",
      "Epoch: 11, Samples: 352/5760, Loss: 0.06159588694572449\n",
      "Epoch: 11, Samples: 384/5760, Loss: 0.0844971090555191\n",
      "Epoch: 11, Samples: 416/5760, Loss: 0.1205415278673172\n",
      "Epoch: 11, Samples: 448/5760, Loss: 0.11782804131507874\n",
      "Epoch: 11, Samples: 480/5760, Loss: 0.11888879537582397\n",
      "Epoch: 11, Samples: 512/5760, Loss: 0.11304056644439697\n",
      "Epoch: 11, Samples: 544/5760, Loss: 0.08382140100002289\n",
      "Epoch: 11, Samples: 576/5760, Loss: 0.09234404563903809\n",
      "Epoch: 11, Samples: 608/5760, Loss: 0.08878171443939209\n",
      "Epoch: 11, Samples: 640/5760, Loss: 0.07461854815483093\n",
      "Epoch: 11, Samples: 672/5760, Loss: 0.12239710986614227\n",
      "Epoch: 11, Samples: 704/5760, Loss: 0.0574381947517395\n",
      "Epoch: 11, Samples: 736/5760, Loss: 0.11214278638362885\n",
      "Epoch: 11, Samples: 768/5760, Loss: 0.06590072810649872\n",
      "Epoch: 11, Samples: 800/5760, Loss: 0.14028845727443695\n",
      "Epoch: 11, Samples: 832/5760, Loss: 0.1782526969909668\n",
      "Epoch: 11, Samples: 864/5760, Loss: 0.09052442014217377\n",
      "Epoch: 11, Samples: 896/5760, Loss: 0.13603007793426514\n",
      "Epoch: 11, Samples: 928/5760, Loss: 0.12215098738670349\n",
      "Epoch: 11, Samples: 960/5760, Loss: 0.08125519752502441\n",
      "Epoch: 11, Samples: 992/5760, Loss: 0.07112300395965576\n",
      "Epoch: 11, Samples: 1024/5760, Loss: 0.06827382743358612\n",
      "Epoch: 11, Samples: 1056/5760, Loss: 0.07963068783283234\n",
      "Epoch: 11, Samples: 1088/5760, Loss: 0.10472217202186584\n",
      "Epoch: 11, Samples: 1120/5760, Loss: 0.05829267203807831\n",
      "Epoch: 11, Samples: 1152/5760, Loss: 0.07195413112640381\n",
      "Epoch: 11, Samples: 1184/5760, Loss: 0.13153713941574097\n",
      "Epoch: 11, Samples: 1216/5760, Loss: 0.0810767263174057\n",
      "Epoch: 11, Samples: 1248/5760, Loss: 0.09488742053508759\n",
      "Epoch: 11, Samples: 1280/5760, Loss: 0.09373290836811066\n",
      "Epoch: 11, Samples: 1312/5760, Loss: 0.09489946067333221\n",
      "Epoch: 11, Samples: 1344/5760, Loss: 0.061012640595436096\n",
      "Epoch: 11, Samples: 1376/5760, Loss: 0.11162106692790985\n",
      "Epoch: 11, Samples: 1408/5760, Loss: 0.12934313714504242\n",
      "Epoch: 11, Samples: 1440/5760, Loss: 0.05258205533027649\n",
      "Epoch: 11, Samples: 1472/5760, Loss: 0.038555145263671875\n",
      "Epoch: 11, Samples: 1504/5760, Loss: 0.20192603766918182\n",
      "Epoch: 11, Samples: 1536/5760, Loss: 0.11977395415306091\n",
      "Epoch: 11, Samples: 1568/5760, Loss: 0.11807537078857422\n",
      "Epoch: 11, Samples: 1600/5760, Loss: 0.13229282200336456\n",
      "Epoch: 11, Samples: 1632/5760, Loss: 0.07656320929527283\n",
      "Epoch: 11, Samples: 1664/5760, Loss: 0.09663765132427216\n",
      "Epoch: 11, Samples: 1696/5760, Loss: 0.06415587663650513\n",
      "Epoch: 11, Samples: 1728/5760, Loss: 0.12284255027770996\n",
      "Epoch: 11, Samples: 1760/5760, Loss: 0.08504047989845276\n",
      "Epoch: 11, Samples: 1792/5760, Loss: 0.10300830006599426\n",
      "Epoch: 11, Samples: 1824/5760, Loss: 0.11504384875297546\n",
      "Epoch: 11, Samples: 1856/5760, Loss: 0.06666560471057892\n",
      "Epoch: 11, Samples: 1888/5760, Loss: 0.10690371692180634\n",
      "Epoch: 11, Samples: 1920/5760, Loss: 0.06124711036682129\n",
      "Epoch: 11, Samples: 1952/5760, Loss: 0.09545481204986572\n",
      "Epoch: 11, Samples: 1984/5760, Loss: 0.07833212614059448\n",
      "Epoch: 11, Samples: 2016/5760, Loss: 0.09029228985309601\n",
      "Epoch: 11, Samples: 2048/5760, Loss: 0.06926694512367249\n",
      "Epoch: 11, Samples: 2080/5760, Loss: 0.06419013440608978\n",
      "Epoch: 11, Samples: 2112/5760, Loss: 0.17264962196350098\n",
      "Epoch: 11, Samples: 2144/5760, Loss: 0.12749037146568298\n",
      "Epoch: 11, Samples: 2176/5760, Loss: 0.09721484780311584\n",
      "Epoch: 11, Samples: 2208/5760, Loss: 0.10542459785938263\n",
      "Epoch: 11, Samples: 2240/5760, Loss: 0.1657317578792572\n",
      "Epoch: 11, Samples: 2272/5760, Loss: 0.07927650213241577\n",
      "Epoch: 11, Samples: 2304/5760, Loss: 0.08049032092094421\n",
      "Epoch: 11, Samples: 2336/5760, Loss: 0.12885239720344543\n",
      "Epoch: 11, Samples: 2368/5760, Loss: 0.06044238805770874\n",
      "Epoch: 11, Samples: 2400/5760, Loss: 0.08987459540367126\n",
      "Epoch: 11, Samples: 2432/5760, Loss: 0.08356910943984985\n",
      "Epoch: 11, Samples: 2464/5760, Loss: 0.08630558848381042\n",
      "Epoch: 11, Samples: 2496/5760, Loss: 0.09279774129390717\n",
      "Epoch: 11, Samples: 2528/5760, Loss: 0.08460521697998047\n",
      "Epoch: 11, Samples: 2560/5760, Loss: 0.11149236559867859\n",
      "Epoch: 11, Samples: 2592/5760, Loss: 0.07350772619247437\n",
      "Epoch: 11, Samples: 2624/5760, Loss: 0.112874835729599\n",
      "Epoch: 11, Samples: 2656/5760, Loss: 0.1211639940738678\n",
      "Epoch: 11, Samples: 2688/5760, Loss: 0.057818323373794556\n",
      "Epoch: 11, Samples: 2720/5760, Loss: 0.10912393033504486\n",
      "Epoch: 11, Samples: 2752/5760, Loss: 0.11102746427059174\n",
      "Epoch: 11, Samples: 2784/5760, Loss: 0.07138985395431519\n",
      "Epoch: 11, Samples: 2816/5760, Loss: 0.05453065037727356\n",
      "Epoch: 11, Samples: 2848/5760, Loss: 0.06613938510417938\n",
      "Epoch: 11, Samples: 2880/5760, Loss: 0.09037867188453674\n",
      "Epoch: 11, Samples: 2912/5760, Loss: 0.1107921153306961\n",
      "Epoch: 11, Samples: 2944/5760, Loss: 0.07227922976016998\n",
      "Epoch: 11, Samples: 2976/5760, Loss: 0.08718349039554596\n",
      "Epoch: 11, Samples: 3008/5760, Loss: 0.06442670524120331\n",
      "Epoch: 11, Samples: 3040/5760, Loss: 0.07697883248329163\n",
      "Epoch: 11, Samples: 3072/5760, Loss: 0.08890634775161743\n",
      "Epoch: 11, Samples: 3104/5760, Loss: 0.11473312973976135\n",
      "Epoch: 11, Samples: 3136/5760, Loss: 0.1763114184141159\n",
      "Epoch: 11, Samples: 3168/5760, Loss: 0.05409485101699829\n",
      "Epoch: 11, Samples: 3200/5760, Loss: 0.06420004367828369\n",
      "Epoch: 11, Samples: 3232/5760, Loss: 0.06549301743507385\n",
      "Epoch: 11, Samples: 3264/5760, Loss: 0.1056433916091919\n",
      "Epoch: 11, Samples: 3296/5760, Loss: 0.06993171572685242\n",
      "Epoch: 11, Samples: 3328/5760, Loss: 0.07041539251804352\n",
      "Epoch: 11, Samples: 3360/5760, Loss: 0.06531526148319244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Samples: 3392/5760, Loss: 0.08129246532917023\n",
      "Epoch: 11, Samples: 3424/5760, Loss: 0.11833707988262177\n",
      "Epoch: 11, Samples: 3456/5760, Loss: 0.04319038987159729\n",
      "Epoch: 11, Samples: 3488/5760, Loss: 0.06040504574775696\n",
      "Epoch: 11, Samples: 3520/5760, Loss: 0.09731727838516235\n",
      "Epoch: 11, Samples: 3552/5760, Loss: 0.15809796750545502\n",
      "Epoch: 11, Samples: 3584/5760, Loss: 0.0737757682800293\n",
      "Epoch: 11, Samples: 3616/5760, Loss: 0.14370107650756836\n",
      "Epoch: 11, Samples: 3648/5760, Loss: 0.12101805210113525\n",
      "Epoch: 11, Samples: 3680/5760, Loss: 0.045200422406196594\n",
      "Epoch: 11, Samples: 3712/5760, Loss: 0.09654557704925537\n",
      "Epoch: 11, Samples: 3744/5760, Loss: 0.05417138338088989\n",
      "Epoch: 11, Samples: 3776/5760, Loss: 0.20917831361293793\n",
      "Epoch: 11, Samples: 3808/5760, Loss: 0.0995863676071167\n",
      "Epoch: 11, Samples: 3840/5760, Loss: 0.07100257277488708\n",
      "Epoch: 11, Samples: 3872/5760, Loss: 0.08590532839298248\n",
      "Epoch: 11, Samples: 3904/5760, Loss: 0.08010494709014893\n",
      "Epoch: 11, Samples: 3936/5760, Loss: 0.11493995785713196\n",
      "Epoch: 11, Samples: 3968/5760, Loss: 0.04841366410255432\n",
      "Epoch: 11, Samples: 4000/5760, Loss: 0.24732856452465057\n",
      "Epoch: 11, Samples: 4032/5760, Loss: 0.2000577449798584\n",
      "Epoch: 11, Samples: 4064/5760, Loss: 0.0912829041481018\n",
      "Epoch: 11, Samples: 4096/5760, Loss: 0.11155025660991669\n",
      "Epoch: 11, Samples: 4128/5760, Loss: 0.26397407054901123\n",
      "Epoch: 11, Samples: 4160/5760, Loss: 0.09737393260002136\n",
      "Epoch: 11, Samples: 4192/5760, Loss: 0.08506560325622559\n",
      "Epoch: 11, Samples: 4224/5760, Loss: 0.13346534967422485\n",
      "Epoch: 11, Samples: 4256/5760, Loss: 0.0854450911283493\n",
      "Epoch: 11, Samples: 4288/5760, Loss: 0.0710669755935669\n",
      "Epoch: 11, Samples: 4320/5760, Loss: 0.17716947197914124\n",
      "Epoch: 11, Samples: 4352/5760, Loss: 0.1175825446844101\n",
      "Epoch: 11, Samples: 4384/5760, Loss: 0.09280122816562653\n",
      "Epoch: 11, Samples: 4416/5760, Loss: 0.07311071455478668\n",
      "Epoch: 11, Samples: 4448/5760, Loss: 0.057591527700424194\n",
      "Epoch: 11, Samples: 4480/5760, Loss: 0.08511240780353546\n",
      "Epoch: 11, Samples: 4512/5760, Loss: 0.16818363964557648\n",
      "Epoch: 11, Samples: 4544/5760, Loss: 0.09214891493320465\n",
      "Epoch: 11, Samples: 4576/5760, Loss: 0.14342468976974487\n",
      "Epoch: 11, Samples: 4608/5760, Loss: 0.05074898898601532\n",
      "Epoch: 11, Samples: 4640/5760, Loss: 0.04601980745792389\n",
      "Epoch: 11, Samples: 4672/5760, Loss: 0.0731559693813324\n",
      "Epoch: 11, Samples: 4704/5760, Loss: 0.12939824163913727\n",
      "Epoch: 11, Samples: 4736/5760, Loss: 0.06133297085762024\n",
      "Epoch: 11, Samples: 4768/5760, Loss: 0.046218156814575195\n",
      "Epoch: 11, Samples: 4800/5760, Loss: 0.08408614993095398\n",
      "Epoch: 11, Samples: 4832/5760, Loss: 0.10837498307228088\n",
      "Epoch: 11, Samples: 4864/5760, Loss: 0.0756165087223053\n",
      "Epoch: 11, Samples: 4896/5760, Loss: 0.10374855995178223\n",
      "Epoch: 11, Samples: 4928/5760, Loss: 0.1384563446044922\n",
      "Epoch: 11, Samples: 4960/5760, Loss: 0.08151544630527496\n",
      "Epoch: 11, Samples: 4992/5760, Loss: 0.16496481001377106\n",
      "Epoch: 11, Samples: 5024/5760, Loss: 0.07767848670482635\n",
      "Epoch: 11, Samples: 5056/5760, Loss: 0.14682768285274506\n",
      "Epoch: 11, Samples: 5088/5760, Loss: 0.14221996068954468\n",
      "Epoch: 11, Samples: 5120/5760, Loss: 0.11598168313503265\n",
      "Epoch: 11, Samples: 5152/5760, Loss: 0.18379288911819458\n",
      "Epoch: 11, Samples: 5184/5760, Loss: 0.14680112898349762\n",
      "Epoch: 11, Samples: 5216/5760, Loss: 0.12824498116970062\n",
      "Epoch: 11, Samples: 5248/5760, Loss: 0.09486939013004303\n",
      "Epoch: 11, Samples: 5280/5760, Loss: 0.07582102715969086\n",
      "Epoch: 11, Samples: 5312/5760, Loss: 0.1315879076719284\n",
      "Epoch: 11, Samples: 5344/5760, Loss: 0.10112433135509491\n",
      "Epoch: 11, Samples: 5376/5760, Loss: 0.12240555882453918\n",
      "Epoch: 11, Samples: 5408/5760, Loss: 0.06558766961097717\n",
      "Epoch: 11, Samples: 5440/5760, Loss: 0.12515564262866974\n",
      "Epoch: 11, Samples: 5472/5760, Loss: 0.16376568377017975\n",
      "Epoch: 11, Samples: 5504/5760, Loss: 0.11126016080379486\n",
      "Epoch: 11, Samples: 5536/5760, Loss: 0.08322654664516449\n",
      "Epoch: 11, Samples: 5568/5760, Loss: 0.0657755434513092\n",
      "Epoch: 11, Samples: 5600/5760, Loss: 0.06878367066383362\n",
      "Epoch: 11, Samples: 5632/5760, Loss: 0.08863699436187744\n",
      "Epoch: 11, Samples: 5664/5760, Loss: 0.12773443758487701\n",
      "Epoch: 11, Samples: 5696/5760, Loss: 0.16634616255760193\n",
      "Epoch: 11, Samples: 5728/5760, Loss: 1.6332019567489624\n",
      "\n",
      "Epoch: 11\n",
      "Training set: Average loss: 0.1089\n",
      "Validation set: Average loss: 0.3498, Accuracy: 764/818 (93%)\n",
      "Saving model (epoch 11) with lowest validation loss: 0.34983888039222133\n",
      "Epoch: 12, Samples: 0/5760, Loss: 0.07537055015563965\n",
      "Epoch: 12, Samples: 32/5760, Loss: 0.06612098217010498\n",
      "Epoch: 12, Samples: 64/5760, Loss: 0.053196221590042114\n",
      "Epoch: 12, Samples: 96/5760, Loss: 0.06623822450637817\n",
      "Epoch: 12, Samples: 128/5760, Loss: 0.13597522675991058\n",
      "Epoch: 12, Samples: 160/5760, Loss: 0.10574620962142944\n",
      "Epoch: 12, Samples: 192/5760, Loss: 0.09465339779853821\n",
      "Epoch: 12, Samples: 224/5760, Loss: 0.07531440258026123\n",
      "Epoch: 12, Samples: 256/5760, Loss: 0.08559451997280121\n",
      "Epoch: 12, Samples: 288/5760, Loss: 0.08765895664691925\n",
      "Epoch: 12, Samples: 320/5760, Loss: 0.08192239701747894\n",
      "Epoch: 12, Samples: 352/5760, Loss: 0.10154630243778229\n",
      "Epoch: 12, Samples: 384/5760, Loss: 0.13511112332344055\n",
      "Epoch: 12, Samples: 416/5760, Loss: 0.16112308204174042\n",
      "Epoch: 12, Samples: 448/5760, Loss: 0.10717584192752838\n",
      "Epoch: 12, Samples: 480/5760, Loss: 0.06952720880508423\n",
      "Epoch: 12, Samples: 512/5760, Loss: 0.06271062791347504\n",
      "Epoch: 12, Samples: 544/5760, Loss: 0.08991177380084991\n",
      "Epoch: 12, Samples: 576/5760, Loss: 0.09205473959445953\n",
      "Epoch: 12, Samples: 608/5760, Loss: 0.14034293591976166\n",
      "Epoch: 12, Samples: 640/5760, Loss: 0.06536030769348145\n",
      "Epoch: 12, Samples: 672/5760, Loss: 0.07039041817188263\n",
      "Epoch: 12, Samples: 704/5760, Loss: 0.06909091770648956\n",
      "Epoch: 12, Samples: 736/5760, Loss: 0.047834739089012146\n",
      "Epoch: 12, Samples: 768/5760, Loss: 0.06669360399246216\n",
      "Epoch: 12, Samples: 800/5760, Loss: 0.10473127663135529\n",
      "Epoch: 12, Samples: 832/5760, Loss: 0.06805866956710815\n",
      "Epoch: 12, Samples: 864/5760, Loss: 0.11537393927574158\n",
      "Epoch: 12, Samples: 896/5760, Loss: 0.04561176896095276\n",
      "Epoch: 12, Samples: 928/5760, Loss: 0.06519649922847748\n",
      "Epoch: 12, Samples: 960/5760, Loss: 0.05674378573894501\n",
      "Epoch: 12, Samples: 992/5760, Loss: 0.07809674739837646\n",
      "Epoch: 12, Samples: 1024/5760, Loss: 0.15494349598884583\n",
      "Epoch: 12, Samples: 1056/5760, Loss: 0.13021112978458405\n",
      "Epoch: 12, Samples: 1088/5760, Loss: 0.07850965857505798\n",
      "Epoch: 12, Samples: 1120/5760, Loss: 0.04449214041233063\n",
      "Epoch: 12, Samples: 1152/5760, Loss: 0.09907984733581543\n",
      "Epoch: 12, Samples: 1184/5760, Loss: 0.11155156791210175\n",
      "Epoch: 12, Samples: 1216/5760, Loss: 0.04669824242591858\n",
      "Epoch: 12, Samples: 1248/5760, Loss: 0.1596234291791916\n",
      "Epoch: 12, Samples: 1280/5760, Loss: 0.09318336844444275\n",
      "Epoch: 12, Samples: 1312/5760, Loss: 0.07352302968502045\n",
      "Epoch: 12, Samples: 1344/5760, Loss: 0.10518261790275574\n",
      "Epoch: 12, Samples: 1376/5760, Loss: 0.11224973201751709\n",
      "Epoch: 12, Samples: 1408/5760, Loss: 0.09960818290710449\n",
      "Epoch: 12, Samples: 1440/5760, Loss: 0.1295735090970993\n",
      "Epoch: 12, Samples: 1472/5760, Loss: 0.06287705898284912\n",
      "Epoch: 12, Samples: 1504/5760, Loss: 0.09189441800117493\n",
      "Epoch: 12, Samples: 1536/5760, Loss: 0.0720130205154419\n",
      "Epoch: 12, Samples: 1568/5760, Loss: 0.13595592975616455\n",
      "Epoch: 12, Samples: 1600/5760, Loss: 0.14662694931030273\n",
      "Epoch: 12, Samples: 1632/5760, Loss: 0.07754456996917725\n",
      "Epoch: 12, Samples: 1664/5760, Loss: 0.12498800456523895\n",
      "Epoch: 12, Samples: 1696/5760, Loss: 0.053760915994644165\n",
      "Epoch: 12, Samples: 1728/5760, Loss: 0.05245852470397949\n",
      "Epoch: 12, Samples: 1760/5760, Loss: 0.08728267252445221\n",
      "Epoch: 12, Samples: 1792/5760, Loss: 0.18642492592334747\n",
      "Epoch: 12, Samples: 1824/5760, Loss: 0.054981544613838196\n",
      "Epoch: 12, Samples: 1856/5760, Loss: 0.10751742124557495\n",
      "Epoch: 12, Samples: 1888/5760, Loss: 0.07770371437072754\n",
      "Epoch: 12, Samples: 1920/5760, Loss: 0.13470087945461273\n",
      "Epoch: 12, Samples: 1952/5760, Loss: 0.10906004905700684\n",
      "Epoch: 12, Samples: 1984/5760, Loss: 0.04442363977432251\n",
      "Epoch: 12, Samples: 2016/5760, Loss: 0.05518564581871033\n",
      "Epoch: 12, Samples: 2048/5760, Loss: 0.17957399785518646\n",
      "Epoch: 12, Samples: 2080/5760, Loss: 0.09115329384803772\n",
      "Epoch: 12, Samples: 2112/5760, Loss: 0.05791802704334259\n",
      "Epoch: 12, Samples: 2144/5760, Loss: 0.046505630016326904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Samples: 2176/5760, Loss: 0.06713055074214935\n",
      "Epoch: 12, Samples: 2208/5760, Loss: 0.03550627827644348\n",
      "Epoch: 12, Samples: 2240/5760, Loss: 0.05243200063705444\n",
      "Epoch: 12, Samples: 2272/5760, Loss: 0.07413475215435028\n",
      "Epoch: 12, Samples: 2304/5760, Loss: 0.09703060984611511\n",
      "Epoch: 12, Samples: 2336/5760, Loss: 0.061816394329071045\n",
      "Epoch: 12, Samples: 2368/5760, Loss: 0.045688197016716\n",
      "Epoch: 12, Samples: 2400/5760, Loss: 0.07205472886562347\n",
      "Epoch: 12, Samples: 2432/5760, Loss: 0.072712242603302\n",
      "Epoch: 12, Samples: 2464/5760, Loss: 0.11780250072479248\n",
      "Epoch: 12, Samples: 2496/5760, Loss: 0.06299805641174316\n",
      "Epoch: 12, Samples: 2528/5760, Loss: 0.06038016080856323\n",
      "Epoch: 12, Samples: 2560/5760, Loss: 0.15312370657920837\n",
      "Epoch: 12, Samples: 2592/5760, Loss: 0.054661065340042114\n",
      "Epoch: 12, Samples: 2624/5760, Loss: 0.038816899061203\n",
      "Epoch: 12, Samples: 2656/5760, Loss: 0.14272698760032654\n",
      "Epoch: 12, Samples: 2688/5760, Loss: 0.10904043912887573\n",
      "Epoch: 12, Samples: 2720/5760, Loss: 0.07264390587806702\n",
      "Epoch: 12, Samples: 2752/5760, Loss: 0.06245015561580658\n",
      "Epoch: 12, Samples: 2784/5760, Loss: 0.11114297807216644\n",
      "Epoch: 12, Samples: 2816/5760, Loss: 0.072484090924263\n",
      "Epoch: 12, Samples: 2848/5760, Loss: 0.07628530263900757\n",
      "Epoch: 12, Samples: 2880/5760, Loss: 0.06787778437137604\n",
      "Epoch: 12, Samples: 2912/5760, Loss: 0.041002631187438965\n",
      "Epoch: 12, Samples: 2944/5760, Loss: 0.07068906724452972\n",
      "Epoch: 12, Samples: 2976/5760, Loss: 0.17124247550964355\n",
      "Epoch: 12, Samples: 3008/5760, Loss: 0.03281986713409424\n",
      "Epoch: 12, Samples: 3040/5760, Loss: 0.054389357566833496\n",
      "Epoch: 12, Samples: 3072/5760, Loss: 0.061114802956581116\n",
      "Epoch: 12, Samples: 3104/5760, Loss: 0.07840482890605927\n",
      "Epoch: 12, Samples: 3136/5760, Loss: 0.05092722177505493\n",
      "Epoch: 12, Samples: 3168/5760, Loss: 0.10404135286808014\n",
      "Epoch: 12, Samples: 3200/5760, Loss: 0.06602410972118378\n",
      "Epoch: 12, Samples: 3232/5760, Loss: 0.06411014497280121\n",
      "Epoch: 12, Samples: 3264/5760, Loss: 0.08783459663391113\n",
      "Epoch: 12, Samples: 3296/5760, Loss: 0.10983452200889587\n",
      "Epoch: 12, Samples: 3328/5760, Loss: 0.13363806903362274\n",
      "Epoch: 12, Samples: 3360/5760, Loss: 0.07589174807071686\n",
      "Epoch: 12, Samples: 3392/5760, Loss: 0.053221046924591064\n",
      "Epoch: 12, Samples: 3424/5760, Loss: 0.09194843471050262\n",
      "Epoch: 12, Samples: 3456/5760, Loss: 0.08407565951347351\n",
      "Epoch: 12, Samples: 3488/5760, Loss: 0.0818200409412384\n",
      "Epoch: 12, Samples: 3520/5760, Loss: 0.06973952054977417\n",
      "Epoch: 12, Samples: 3552/5760, Loss: 0.054385751485824585\n",
      "Epoch: 12, Samples: 3584/5760, Loss: 0.08534859120845795\n",
      "Epoch: 12, Samples: 3616/5760, Loss: 0.1351446509361267\n",
      "Epoch: 12, Samples: 3648/5760, Loss: 0.0722995400428772\n",
      "Epoch: 12, Samples: 3680/5760, Loss: 0.08857835829257965\n",
      "Epoch: 12, Samples: 3712/5760, Loss: 0.10023121535778046\n",
      "Epoch: 12, Samples: 3744/5760, Loss: 0.13750223815441132\n",
      "Epoch: 12, Samples: 3776/5760, Loss: 0.07135920226573944\n",
      "Epoch: 12, Samples: 3808/5760, Loss: 0.07426641881465912\n",
      "Epoch: 12, Samples: 3840/5760, Loss: 0.062002554535865784\n",
      "Epoch: 12, Samples: 3872/5760, Loss: 0.09631961584091187\n",
      "Epoch: 12, Samples: 3904/5760, Loss: 0.08316828310489655\n",
      "Epoch: 12, Samples: 3936/5760, Loss: 0.07839789986610413\n",
      "Epoch: 12, Samples: 3968/5760, Loss: 0.11208294332027435\n",
      "Epoch: 12, Samples: 4000/5760, Loss: 0.048723384737968445\n",
      "Epoch: 12, Samples: 4032/5760, Loss: 0.04529649019241333\n",
      "Epoch: 12, Samples: 4064/5760, Loss: 0.087842658162117\n",
      "Epoch: 12, Samples: 4096/5760, Loss: 0.09108927845954895\n",
      "Epoch: 12, Samples: 4128/5760, Loss: 0.15147508680820465\n",
      "Epoch: 12, Samples: 4160/5760, Loss: 0.1059187650680542\n",
      "Epoch: 12, Samples: 4192/5760, Loss: 0.11067764461040497\n",
      "Epoch: 12, Samples: 4224/5760, Loss: 0.07513229548931122\n",
      "Epoch: 12, Samples: 4256/5760, Loss: 0.12658268213272095\n",
      "Epoch: 12, Samples: 4288/5760, Loss: 0.15330982208251953\n",
      "Epoch: 12, Samples: 4320/5760, Loss: 0.06589359045028687\n",
      "Epoch: 12, Samples: 4352/5760, Loss: 0.10369060933589935\n",
      "Epoch: 12, Samples: 4384/5760, Loss: 0.049374938011169434\n",
      "Epoch: 12, Samples: 4416/5760, Loss: 0.10809993743896484\n",
      "Epoch: 12, Samples: 4448/5760, Loss: 0.09299999475479126\n",
      "Epoch: 12, Samples: 4480/5760, Loss: 0.033533960580825806\n",
      "Epoch: 12, Samples: 4512/5760, Loss: 0.0991491973400116\n",
      "Epoch: 12, Samples: 4544/5760, Loss: 0.08305354416370392\n",
      "Epoch: 12, Samples: 4576/5760, Loss: 0.07742120325565338\n",
      "Epoch: 12, Samples: 4608/5760, Loss: 0.09213744103908539\n",
      "Epoch: 12, Samples: 4640/5760, Loss: 0.07452820241451263\n",
      "Epoch: 12, Samples: 4672/5760, Loss: 0.044130340218544006\n",
      "Epoch: 12, Samples: 4704/5760, Loss: 0.06061939895153046\n",
      "Epoch: 12, Samples: 4736/5760, Loss: 0.09090864658355713\n",
      "Epoch: 12, Samples: 4768/5760, Loss: 0.1173044741153717\n",
      "Epoch: 12, Samples: 4800/5760, Loss: 0.14635813236236572\n",
      "Epoch: 12, Samples: 4832/5760, Loss: 0.06709963083267212\n",
      "Epoch: 12, Samples: 4864/5760, Loss: 0.14856235682964325\n",
      "Epoch: 12, Samples: 4896/5760, Loss: 0.06991203129291534\n",
      "Epoch: 12, Samples: 4928/5760, Loss: 0.09511390328407288\n",
      "Epoch: 12, Samples: 4960/5760, Loss: 0.09971234202384949\n",
      "Epoch: 12, Samples: 4992/5760, Loss: 0.0610249787569046\n",
      "Epoch: 12, Samples: 5024/5760, Loss: 0.07390350103378296\n",
      "Epoch: 12, Samples: 5056/5760, Loss: 0.04380759596824646\n",
      "Epoch: 12, Samples: 5088/5760, Loss: 0.08778677880764008\n",
      "Epoch: 12, Samples: 5120/5760, Loss: 0.03982748091220856\n",
      "Epoch: 12, Samples: 5152/5760, Loss: 0.0549808144569397\n",
      "Epoch: 12, Samples: 5184/5760, Loss: 0.08770628273487091\n",
      "Epoch: 12, Samples: 5216/5760, Loss: 0.17775870859622955\n",
      "Epoch: 12, Samples: 5248/5760, Loss: 0.14779618382453918\n",
      "Epoch: 12, Samples: 5280/5760, Loss: 0.061837539076805115\n",
      "Epoch: 12, Samples: 5312/5760, Loss: 0.06264247000217438\n",
      "Epoch: 12, Samples: 5344/5760, Loss: 0.07573135197162628\n",
      "Epoch: 12, Samples: 5376/5760, Loss: 0.08026833832263947\n",
      "Epoch: 12, Samples: 5408/5760, Loss: 0.07075762748718262\n",
      "Epoch: 12, Samples: 5440/5760, Loss: 0.07498571276664734\n",
      "Epoch: 12, Samples: 5472/5760, Loss: 0.04605330526828766\n",
      "Epoch: 12, Samples: 5504/5760, Loss: 0.03504921495914459\n",
      "Epoch: 12, Samples: 5536/5760, Loss: 0.0805552750825882\n",
      "Epoch: 12, Samples: 5568/5760, Loss: 0.04896143078804016\n",
      "Epoch: 12, Samples: 5600/5760, Loss: 0.07389862835407257\n",
      "Epoch: 12, Samples: 5632/5760, Loss: 0.043989211320877075\n",
      "Epoch: 12, Samples: 5664/5760, Loss: 0.10034342110157013\n",
      "Epoch: 12, Samples: 5696/5760, Loss: 0.05877752602100372\n",
      "Epoch: 12, Samples: 5728/5760, Loss: 0.6705381870269775\n",
      "\n",
      "Epoch: 12\n",
      "Training set: Average loss: 0.0890\n",
      "Validation set: Average loss: 0.3555, Accuracy: 756/818 (92%)\n",
      "Epoch: 13, Samples: 0/5760, Loss: 0.07184739410877228\n",
      "Epoch: 13, Samples: 32/5760, Loss: 0.04459366202354431\n",
      "Epoch: 13, Samples: 64/5760, Loss: 0.16031211614608765\n",
      "Epoch: 13, Samples: 96/5760, Loss: 0.06815917789936066\n",
      "Epoch: 13, Samples: 128/5760, Loss: 0.035757943987846375\n",
      "Epoch: 13, Samples: 160/5760, Loss: 0.04374025762081146\n",
      "Epoch: 13, Samples: 192/5760, Loss: 0.06794114410877228\n",
      "Epoch: 13, Samples: 224/5760, Loss: 0.06958898901939392\n",
      "Epoch: 13, Samples: 256/5760, Loss: 0.05455423891544342\n",
      "Epoch: 13, Samples: 288/5760, Loss: 0.06516331434249878\n",
      "Epoch: 13, Samples: 320/5760, Loss: 0.08294203877449036\n",
      "Epoch: 13, Samples: 352/5760, Loss: 0.1216912567615509\n",
      "Epoch: 13, Samples: 384/5760, Loss: 0.0578160285949707\n",
      "Epoch: 13, Samples: 416/5760, Loss: 0.05125623941421509\n",
      "Epoch: 13, Samples: 448/5760, Loss: 0.05096547305583954\n",
      "Epoch: 13, Samples: 480/5760, Loss: 0.11238247156143188\n",
      "Epoch: 13, Samples: 512/5760, Loss: 0.07844333350658417\n",
      "Epoch: 13, Samples: 544/5760, Loss: 0.08745437860488892\n",
      "Epoch: 13, Samples: 576/5760, Loss: 0.05502691864967346\n",
      "Epoch: 13, Samples: 608/5760, Loss: 0.09447301924228668\n",
      "Epoch: 13, Samples: 640/5760, Loss: 0.05497242510318756\n",
      "Epoch: 13, Samples: 672/5760, Loss: 0.08420999348163605\n",
      "Epoch: 13, Samples: 704/5760, Loss: 0.08981858193874359\n",
      "Epoch: 13, Samples: 736/5760, Loss: 0.08298127353191376\n",
      "Epoch: 13, Samples: 768/5760, Loss: 0.07110311090946198\n",
      "Epoch: 13, Samples: 800/5760, Loss: 0.06543782353401184\n",
      "Epoch: 13, Samples: 832/5760, Loss: 0.08524695038795471\n",
      "Epoch: 13, Samples: 864/5760, Loss: 0.09064483642578125\n",
      "Epoch: 13, Samples: 896/5760, Loss: 0.13753367960453033\n",
      "Epoch: 13, Samples: 928/5760, Loss: 0.1092783510684967\n",
      "Epoch: 13, Samples: 960/5760, Loss: 0.09573318064212799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Samples: 992/5760, Loss: 0.07383483648300171\n",
      "Epoch: 13, Samples: 1024/5760, Loss: 0.0823897272348404\n",
      "Epoch: 13, Samples: 1056/5760, Loss: 0.06949789822101593\n",
      "Epoch: 13, Samples: 1088/5760, Loss: 0.04895015060901642\n",
      "Epoch: 13, Samples: 1120/5760, Loss: 0.041682541370391846\n",
      "Epoch: 13, Samples: 1152/5760, Loss: 0.06307250261306763\n",
      "Epoch: 13, Samples: 1184/5760, Loss: 0.06070666015148163\n",
      "Epoch: 13, Samples: 1216/5760, Loss: 0.09530894458293915\n",
      "Epoch: 13, Samples: 1248/5760, Loss: 0.05822652578353882\n",
      "Epoch: 13, Samples: 1280/5760, Loss: 0.08745963871479034\n",
      "Epoch: 13, Samples: 1312/5760, Loss: 0.07380671799182892\n",
      "Epoch: 13, Samples: 1344/5760, Loss: 0.04413576424121857\n",
      "Epoch: 13, Samples: 1376/5760, Loss: 0.07125526666641235\n",
      "Epoch: 13, Samples: 1408/5760, Loss: 0.05188557505607605\n",
      "Epoch: 13, Samples: 1440/5760, Loss: 0.068735271692276\n",
      "Epoch: 13, Samples: 1472/5760, Loss: 0.047585681080818176\n",
      "Epoch: 13, Samples: 1504/5760, Loss: 0.06542667746543884\n",
      "Epoch: 13, Samples: 1536/5760, Loss: 0.05794528126716614\n",
      "Epoch: 13, Samples: 1568/5760, Loss: 0.05104568600654602\n",
      "Epoch: 13, Samples: 1600/5760, Loss: 0.16285520792007446\n",
      "Epoch: 13, Samples: 1632/5760, Loss: 0.04620805382728577\n",
      "Epoch: 13, Samples: 1664/5760, Loss: 0.06513738632202148\n",
      "Epoch: 13, Samples: 1696/5760, Loss: 0.05922587215900421\n",
      "Epoch: 13, Samples: 1728/5760, Loss: 0.044732943177223206\n",
      "Epoch: 13, Samples: 1760/5760, Loss: 0.06519149243831635\n",
      "Epoch: 13, Samples: 1792/5760, Loss: 0.04544055461883545\n",
      "Epoch: 13, Samples: 1824/5760, Loss: 0.05205172300338745\n",
      "Epoch: 13, Samples: 1856/5760, Loss: 0.05788448452949524\n",
      "Epoch: 13, Samples: 1888/5760, Loss: 0.06595046818256378\n",
      "Epoch: 13, Samples: 1920/5760, Loss: 0.06668400764465332\n",
      "Epoch: 13, Samples: 1952/5760, Loss: 0.0665566474199295\n",
      "Epoch: 13, Samples: 1984/5760, Loss: 0.07127013802528381\n",
      "Epoch: 13, Samples: 2016/5760, Loss: 0.08218622207641602\n",
      "Epoch: 13, Samples: 2048/5760, Loss: 0.08736005425453186\n",
      "Epoch: 13, Samples: 2080/5760, Loss: 0.10018675029277802\n",
      "Epoch: 13, Samples: 2112/5760, Loss: 0.06473490595817566\n",
      "Epoch: 13, Samples: 2144/5760, Loss: 0.059185728430747986\n",
      "Epoch: 13, Samples: 2176/5760, Loss: 0.05368399620056152\n",
      "Epoch: 13, Samples: 2208/5760, Loss: 0.10209967195987701\n",
      "Epoch: 13, Samples: 2240/5760, Loss: 0.04752010107040405\n",
      "Epoch: 13, Samples: 2272/5760, Loss: 0.1177368015050888\n",
      "Epoch: 13, Samples: 2304/5760, Loss: 0.06911608576774597\n",
      "Epoch: 13, Samples: 2336/5760, Loss: 0.06330718100070953\n",
      "Epoch: 13, Samples: 2368/5760, Loss: 0.06587788462638855\n",
      "Epoch: 13, Samples: 2400/5760, Loss: 0.12917491793632507\n",
      "Epoch: 13, Samples: 2432/5760, Loss: 0.09023250639438629\n",
      "Epoch: 13, Samples: 2464/5760, Loss: 0.06159411370754242\n",
      "Epoch: 13, Samples: 2496/5760, Loss: 0.06699688732624054\n",
      "Epoch: 13, Samples: 2528/5760, Loss: 0.06530612707138062\n",
      "Epoch: 13, Samples: 2560/5760, Loss: 0.15142688155174255\n",
      "Epoch: 13, Samples: 2592/5760, Loss: 0.057444751262664795\n",
      "Epoch: 13, Samples: 2624/5760, Loss: 0.043393731117248535\n",
      "Epoch: 13, Samples: 2656/5760, Loss: 0.062307074666023254\n",
      "Epoch: 13, Samples: 2688/5760, Loss: 0.06416071951389313\n",
      "Epoch: 13, Samples: 2720/5760, Loss: 0.07985202968120575\n",
      "Epoch: 13, Samples: 2752/5760, Loss: 0.0537111759185791\n",
      "Epoch: 13, Samples: 2784/5760, Loss: 0.07568037509918213\n",
      "Epoch: 13, Samples: 2816/5760, Loss: 0.08941465616226196\n",
      "Epoch: 13, Samples: 2848/5760, Loss: 0.09798771142959595\n",
      "Epoch: 13, Samples: 2880/5760, Loss: 0.04462364315986633\n",
      "Epoch: 13, Samples: 2912/5760, Loss: 0.07303929328918457\n",
      "Epoch: 13, Samples: 2944/5760, Loss: 0.050985127687454224\n",
      "Epoch: 13, Samples: 2976/5760, Loss: 0.0927075445652008\n",
      "Epoch: 13, Samples: 3008/5760, Loss: 0.06942744553089142\n",
      "Epoch: 13, Samples: 3040/5760, Loss: 0.09772516787052155\n",
      "Epoch: 13, Samples: 3072/5760, Loss: 0.053613439202308655\n",
      "Epoch: 13, Samples: 3104/5760, Loss: 0.04877333343029022\n",
      "Epoch: 13, Samples: 3136/5760, Loss: 0.0384581983089447\n",
      "Epoch: 13, Samples: 3168/5760, Loss: 0.11801223456859589\n",
      "Epoch: 13, Samples: 3200/5760, Loss: 0.07830804586410522\n",
      "Epoch: 13, Samples: 3232/5760, Loss: 0.06721603870391846\n",
      "Epoch: 13, Samples: 3264/5760, Loss: 0.08251979947090149\n",
      "Epoch: 13, Samples: 3296/5760, Loss: 0.0683702826499939\n",
      "Epoch: 13, Samples: 3328/5760, Loss: 0.2040417343378067\n",
      "Epoch: 13, Samples: 3360/5760, Loss: 0.06683488190174103\n",
      "Epoch: 13, Samples: 3392/5760, Loss: 0.03428363800048828\n",
      "Epoch: 13, Samples: 3424/5760, Loss: 0.07747417688369751\n",
      "Epoch: 13, Samples: 3456/5760, Loss: 0.1023927628993988\n",
      "Epoch: 13, Samples: 3488/5760, Loss: 0.046969443559646606\n",
      "Epoch: 13, Samples: 3520/5760, Loss: 0.05811929702758789\n",
      "Epoch: 13, Samples: 3552/5760, Loss: 0.055275216698646545\n",
      "Epoch: 13, Samples: 3584/5760, Loss: 0.12013432383537292\n",
      "Epoch: 13, Samples: 3616/5760, Loss: 0.07953622937202454\n",
      "Epoch: 13, Samples: 3648/5760, Loss: 0.06557795405387878\n",
      "Epoch: 13, Samples: 3680/5760, Loss: 0.06330060958862305\n",
      "Epoch: 13, Samples: 3712/5760, Loss: 0.12260301411151886\n",
      "Epoch: 13, Samples: 3744/5760, Loss: 0.058838412165641785\n",
      "Epoch: 13, Samples: 3776/5760, Loss: 0.07545478641986847\n",
      "Epoch: 13, Samples: 3808/5760, Loss: 0.08068563044071198\n",
      "Epoch: 13, Samples: 3840/5760, Loss: 0.10333371162414551\n",
      "Epoch: 13, Samples: 3872/5760, Loss: 0.09288442134857178\n",
      "Epoch: 13, Samples: 3904/5760, Loss: 0.05633100867271423\n",
      "Epoch: 13, Samples: 3936/5760, Loss: 0.06215190887451172\n",
      "Epoch: 13, Samples: 3968/5760, Loss: 0.06842599809169769\n",
      "Epoch: 13, Samples: 4000/5760, Loss: 0.06936362385749817\n",
      "Epoch: 13, Samples: 4032/5760, Loss: 0.08622553944587708\n",
      "Epoch: 13, Samples: 4064/5760, Loss: 0.10690326988697052\n",
      "Epoch: 13, Samples: 4096/5760, Loss: 0.06868685781955719\n",
      "Epoch: 13, Samples: 4128/5760, Loss: 0.061202481389045715\n",
      "Epoch: 13, Samples: 4160/5760, Loss: 0.05225381255149841\n",
      "Epoch: 13, Samples: 4192/5760, Loss: 0.047149837017059326\n",
      "Epoch: 13, Samples: 4224/5760, Loss: 0.02682521939277649\n",
      "Epoch: 13, Samples: 4256/5760, Loss: 0.04587462544441223\n",
      "Epoch: 13, Samples: 4288/5760, Loss: 0.0658850222826004\n",
      "Epoch: 13, Samples: 4320/5760, Loss: 0.05707964301109314\n",
      "Epoch: 13, Samples: 4352/5760, Loss: 0.07034021615982056\n",
      "Epoch: 13, Samples: 4384/5760, Loss: 0.06608106195926666\n",
      "Epoch: 13, Samples: 4416/5760, Loss: 0.08719085156917572\n",
      "Epoch: 13, Samples: 4448/5760, Loss: 0.030913740396499634\n",
      "Epoch: 13, Samples: 4480/5760, Loss: 0.06264418363571167\n",
      "Epoch: 13, Samples: 4512/5760, Loss: 0.06947319209575653\n",
      "Epoch: 13, Samples: 4544/5760, Loss: 0.04879266023635864\n",
      "Epoch: 13, Samples: 4576/5760, Loss: 0.07754865288734436\n",
      "Epoch: 13, Samples: 4608/5760, Loss: 0.04019598662853241\n",
      "Epoch: 13, Samples: 4640/5760, Loss: 0.07995782792568207\n",
      "Epoch: 13, Samples: 4672/5760, Loss: 0.08513236045837402\n",
      "Epoch: 13, Samples: 4704/5760, Loss: 0.0346103310585022\n",
      "Epoch: 13, Samples: 4736/5760, Loss: 0.06817255914211273\n",
      "Epoch: 13, Samples: 4768/5760, Loss: 0.10838356614112854\n",
      "Epoch: 13, Samples: 4800/5760, Loss: 0.059312283992767334\n",
      "Epoch: 13, Samples: 4832/5760, Loss: 0.05971147119998932\n",
      "Epoch: 13, Samples: 4864/5760, Loss: 0.1017453521490097\n",
      "Epoch: 13, Samples: 4896/5760, Loss: 0.09998264908790588\n",
      "Epoch: 13, Samples: 4928/5760, Loss: 0.07659667730331421\n",
      "Epoch: 13, Samples: 4960/5760, Loss: 0.03978651762008667\n",
      "Epoch: 13, Samples: 4992/5760, Loss: 0.055126726627349854\n",
      "Epoch: 13, Samples: 5024/5760, Loss: 0.06506085395812988\n",
      "Epoch: 13, Samples: 5056/5760, Loss: 0.09199400246143341\n",
      "Epoch: 13, Samples: 5088/5760, Loss: 0.06959635019302368\n",
      "Epoch: 13, Samples: 5120/5760, Loss: 0.08350156247615814\n",
      "Epoch: 13, Samples: 5152/5760, Loss: 0.053594768047332764\n",
      "Epoch: 13, Samples: 5184/5760, Loss: 0.0560624897480011\n",
      "Epoch: 13, Samples: 5216/5760, Loss: 0.0711556226015091\n",
      "Epoch: 13, Samples: 5248/5760, Loss: 0.04379437863826752\n",
      "Epoch: 13, Samples: 5280/5760, Loss: 0.059488311409950256\n",
      "Epoch: 13, Samples: 5312/5760, Loss: 0.06462201476097107\n",
      "Epoch: 13, Samples: 5344/5760, Loss: 0.09933146834373474\n",
      "Epoch: 13, Samples: 5376/5760, Loss: 0.055496856570243835\n",
      "Epoch: 13, Samples: 5408/5760, Loss: 0.08881308138370514\n",
      "Epoch: 13, Samples: 5440/5760, Loss: 0.07804745435714722\n",
      "Epoch: 13, Samples: 5472/5760, Loss: 0.048028364777565\n",
      "Epoch: 13, Samples: 5504/5760, Loss: 0.040370553731918335\n",
      "Epoch: 13, Samples: 5536/5760, Loss: 0.0482410192489624\n",
      "Epoch: 13, Samples: 5568/5760, Loss: 0.045111283659935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Samples: 5600/5760, Loss: 0.11015938222408295\n",
      "Epoch: 13, Samples: 5632/5760, Loss: 0.07739381492137909\n",
      "Epoch: 13, Samples: 5664/5760, Loss: 0.11267164349555969\n",
      "Epoch: 13, Samples: 5696/5760, Loss: 0.029019713401794434\n",
      "Epoch: 13, Samples: 5728/5760, Loss: 0.8167370557785034\n",
      "\n",
      "Epoch: 13\n",
      "Training set: Average loss: 0.0762\n",
      "Validation set: Average loss: 0.3358, Accuracy: 757/818 (93%)\n",
      "Saving model (epoch 13) with lowest validation loss: 0.335828005694426\n",
      "Epoch: 14, Samples: 0/5760, Loss: 0.0728921890258789\n",
      "Epoch: 14, Samples: 32/5760, Loss: 0.04144415259361267\n",
      "Epoch: 14, Samples: 64/5760, Loss: 0.04003074765205383\n",
      "Epoch: 14, Samples: 96/5760, Loss: 0.05770152807235718\n",
      "Epoch: 14, Samples: 128/5760, Loss: 0.06288303434848785\n",
      "Epoch: 14, Samples: 160/5760, Loss: 0.04578763246536255\n",
      "Epoch: 14, Samples: 192/5760, Loss: 0.06556856632232666\n",
      "Epoch: 14, Samples: 224/5760, Loss: 0.06621196866035461\n",
      "Epoch: 14, Samples: 256/5760, Loss: 0.09798245131969452\n",
      "Epoch: 14, Samples: 288/5760, Loss: 0.07228001952171326\n",
      "Epoch: 14, Samples: 320/5760, Loss: 0.08907277882099152\n",
      "Epoch: 14, Samples: 352/5760, Loss: 0.14418229460716248\n",
      "Epoch: 14, Samples: 384/5760, Loss: 0.07848720252513885\n",
      "Epoch: 14, Samples: 416/5760, Loss: 0.08178156614303589\n",
      "Epoch: 14, Samples: 448/5760, Loss: 0.07777495682239532\n",
      "Epoch: 14, Samples: 480/5760, Loss: 0.07183027267456055\n",
      "Epoch: 14, Samples: 512/5760, Loss: 0.08473122119903564\n",
      "Epoch: 14, Samples: 544/5760, Loss: 0.034671276807785034\n",
      "Epoch: 14, Samples: 576/5760, Loss: 0.09376122057437897\n",
      "Epoch: 14, Samples: 608/5760, Loss: 0.04878649115562439\n",
      "Epoch: 14, Samples: 640/5760, Loss: 0.046747058629989624\n",
      "Epoch: 14, Samples: 672/5760, Loss: 0.060890063643455505\n",
      "Epoch: 14, Samples: 704/5760, Loss: 0.049781933426856995\n",
      "Epoch: 14, Samples: 736/5760, Loss: 0.05804915726184845\n",
      "Epoch: 14, Samples: 768/5760, Loss: 0.04879312217235565\n",
      "Epoch: 14, Samples: 800/5760, Loss: 0.05772186815738678\n",
      "Epoch: 14, Samples: 832/5760, Loss: 0.07119739055633545\n",
      "Epoch: 14, Samples: 864/5760, Loss: 0.056673258543014526\n",
      "Epoch: 14, Samples: 896/5760, Loss: 0.0570942759513855\n",
      "Epoch: 14, Samples: 928/5760, Loss: 0.07270993292331696\n",
      "Epoch: 14, Samples: 960/5760, Loss: 0.045131146907806396\n",
      "Epoch: 14, Samples: 992/5760, Loss: 0.05947527289390564\n",
      "Epoch: 14, Samples: 1024/5760, Loss: 0.05670061707496643\n",
      "Epoch: 14, Samples: 1056/5760, Loss: 0.07289206981658936\n",
      "Epoch: 14, Samples: 1088/5760, Loss: 0.10742083191871643\n",
      "Epoch: 14, Samples: 1120/5760, Loss: 0.05960465967655182\n",
      "Epoch: 14, Samples: 1152/5760, Loss: 0.11299024522304535\n",
      "Epoch: 14, Samples: 1184/5760, Loss: 0.07000257074832916\n",
      "Epoch: 14, Samples: 1216/5760, Loss: 0.056321293115615845\n",
      "Epoch: 14, Samples: 1248/5760, Loss: 0.03872083127498627\n",
      "Epoch: 14, Samples: 1280/5760, Loss: 0.042434290051460266\n",
      "Epoch: 14, Samples: 1312/5760, Loss: 0.07742802798748016\n",
      "Epoch: 14, Samples: 1344/5760, Loss: 0.043560177087783813\n",
      "Epoch: 14, Samples: 1376/5760, Loss: 0.06865954399108887\n",
      "Epoch: 14, Samples: 1408/5760, Loss: 0.10505445301532745\n",
      "Epoch: 14, Samples: 1440/5760, Loss: 0.07782045006752014\n",
      "Epoch: 14, Samples: 1472/5760, Loss: 0.12755897641181946\n",
      "Epoch: 14, Samples: 1504/5760, Loss: 0.04194347560405731\n",
      "Epoch: 14, Samples: 1536/5760, Loss: 0.07457225024700165\n",
      "Epoch: 14, Samples: 1568/5760, Loss: 0.1027459055185318\n",
      "Epoch: 14, Samples: 1600/5760, Loss: 0.12682276964187622\n",
      "Epoch: 14, Samples: 1632/5760, Loss: 0.043094754219055176\n",
      "Epoch: 14, Samples: 1664/5760, Loss: 0.039374202489852905\n",
      "Epoch: 14, Samples: 1696/5760, Loss: 0.026145756244659424\n",
      "Epoch: 14, Samples: 1728/5760, Loss: 0.07939964532852173\n",
      "Epoch: 14, Samples: 1760/5760, Loss: 0.07732675969600677\n",
      "Epoch: 14, Samples: 1792/5760, Loss: 0.06955176591873169\n",
      "Epoch: 14, Samples: 1824/5760, Loss: 0.05259212851524353\n",
      "Epoch: 14, Samples: 1856/5760, Loss: 0.037740692496299744\n",
      "Epoch: 14, Samples: 1888/5760, Loss: 0.052352532744407654\n",
      "Epoch: 14, Samples: 1920/5760, Loss: 0.06604564189910889\n",
      "Epoch: 14, Samples: 1952/5760, Loss: 0.08652172982692719\n",
      "Epoch: 14, Samples: 1984/5760, Loss: 0.048232123255729675\n",
      "Epoch: 14, Samples: 2016/5760, Loss: 0.040242552757263184\n",
      "Epoch: 14, Samples: 2048/5760, Loss: 0.03724512457847595\n",
      "Epoch: 14, Samples: 2080/5760, Loss: 0.07953310012817383\n",
      "Epoch: 14, Samples: 2112/5760, Loss: 0.03337445855140686\n",
      "Epoch: 14, Samples: 2144/5760, Loss: 0.04430411756038666\n",
      "Epoch: 14, Samples: 2176/5760, Loss: 0.05249752104282379\n",
      "Epoch: 14, Samples: 2208/5760, Loss: 0.054389700293540955\n",
      "Epoch: 14, Samples: 2240/5760, Loss: 0.08033044636249542\n",
      "Epoch: 14, Samples: 2272/5760, Loss: 0.04233326017856598\n",
      "Epoch: 14, Samples: 2304/5760, Loss: 0.04518648982048035\n",
      "Epoch: 14, Samples: 2336/5760, Loss: 0.04699191451072693\n",
      "Epoch: 14, Samples: 2368/5760, Loss: 0.06393331289291382\n",
      "Epoch: 14, Samples: 2400/5760, Loss: 0.05047297477722168\n",
      "Epoch: 14, Samples: 2432/5760, Loss: 0.0768592357635498\n",
      "Epoch: 14, Samples: 2464/5760, Loss: 0.06709800660610199\n",
      "Epoch: 14, Samples: 2496/5760, Loss: 0.06121864914894104\n",
      "Epoch: 14, Samples: 2528/5760, Loss: 0.04703532159328461\n",
      "Epoch: 14, Samples: 2560/5760, Loss: 0.10316696763038635\n",
      "Epoch: 14, Samples: 2592/5760, Loss: 0.039267703890800476\n",
      "Epoch: 14, Samples: 2624/5760, Loss: 0.04028892517089844\n",
      "Epoch: 14, Samples: 2656/5760, Loss: 0.09294542670249939\n",
      "Epoch: 14, Samples: 2688/5760, Loss: 0.03602899610996246\n",
      "Epoch: 14, Samples: 2720/5760, Loss: 0.07922255992889404\n",
      "Epoch: 14, Samples: 2752/5760, Loss: 0.08963842689990997\n",
      "Epoch: 14, Samples: 2784/5760, Loss: 0.06607697904109955\n",
      "Epoch: 14, Samples: 2816/5760, Loss: 0.05680780112743378\n",
      "Epoch: 14, Samples: 2848/5760, Loss: 0.054478541016578674\n",
      "Epoch: 14, Samples: 2880/5760, Loss: 0.04843874275684357\n",
      "Epoch: 14, Samples: 2912/5760, Loss: 0.05291035771369934\n",
      "Epoch: 14, Samples: 2944/5760, Loss: 0.055626899003982544\n",
      "Epoch: 14, Samples: 2976/5760, Loss: 0.07122132182121277\n",
      "Epoch: 14, Samples: 3008/5760, Loss: 0.07056164741516113\n",
      "Epoch: 14, Samples: 3040/5760, Loss: 0.07160350680351257\n",
      "Epoch: 14, Samples: 3072/5760, Loss: 0.04552198946475983\n",
      "Epoch: 14, Samples: 3104/5760, Loss: 0.05227223038673401\n",
      "Epoch: 14, Samples: 3136/5760, Loss: 0.05050404369831085\n",
      "Epoch: 14, Samples: 3168/5760, Loss: 0.04220716655254364\n",
      "Epoch: 14, Samples: 3200/5760, Loss: 0.04994696378707886\n",
      "Epoch: 14, Samples: 3232/5760, Loss: 0.05793331563472748\n",
      "Epoch: 14, Samples: 3264/5760, Loss: 0.04252363741397858\n",
      "Epoch: 14, Samples: 3296/5760, Loss: 0.06342780590057373\n",
      "Epoch: 14, Samples: 3328/5760, Loss: 0.03517037630081177\n",
      "Epoch: 14, Samples: 3360/5760, Loss: 0.08405669033527374\n",
      "Epoch: 14, Samples: 3392/5760, Loss: 0.05504791438579559\n",
      "Epoch: 14, Samples: 3424/5760, Loss: 0.06029781699180603\n",
      "Epoch: 14, Samples: 3456/5760, Loss: 0.10099421441555023\n",
      "Epoch: 14, Samples: 3488/5760, Loss: 0.05902819335460663\n",
      "Epoch: 14, Samples: 3520/5760, Loss: 0.052431702613830566\n",
      "Epoch: 14, Samples: 3552/5760, Loss: 0.06964543461799622\n",
      "Epoch: 14, Samples: 3584/5760, Loss: 0.0360126793384552\n",
      "Epoch: 14, Samples: 3616/5760, Loss: 0.05049346387386322\n",
      "Epoch: 14, Samples: 3648/5760, Loss: 0.04046916961669922\n",
      "Epoch: 14, Samples: 3680/5760, Loss: 0.03081190586090088\n",
      "Epoch: 14, Samples: 3712/5760, Loss: 0.05254390835762024\n",
      "Epoch: 14, Samples: 3744/5760, Loss: 0.09362487494945526\n",
      "Epoch: 14, Samples: 3776/5760, Loss: 0.07208937406539917\n",
      "Epoch: 14, Samples: 3808/5760, Loss: 0.055176809430122375\n",
      "Epoch: 14, Samples: 3840/5760, Loss: 0.048713743686676025\n",
      "Epoch: 14, Samples: 3872/5760, Loss: 0.04706239700317383\n",
      "Epoch: 14, Samples: 3904/5760, Loss: 0.0404333621263504\n",
      "Epoch: 14, Samples: 3936/5760, Loss: 0.045483872294425964\n",
      "Epoch: 14, Samples: 3968/5760, Loss: 0.06178739666938782\n",
      "Epoch: 14, Samples: 4000/5760, Loss: 0.08812548220157623\n",
      "Epoch: 14, Samples: 4032/5760, Loss: 0.06338544189929962\n",
      "Epoch: 14, Samples: 4064/5760, Loss: 0.06345570087432861\n",
      "Epoch: 14, Samples: 4096/5760, Loss: 0.13984113931655884\n",
      "Epoch: 14, Samples: 4128/5760, Loss: 0.062227874994277954\n",
      "Epoch: 14, Samples: 4160/5760, Loss: 0.0885862410068512\n",
      "Epoch: 14, Samples: 4192/5760, Loss: 0.03641015291213989\n",
      "Epoch: 14, Samples: 4224/5760, Loss: 0.033877789974212646\n",
      "Epoch: 14, Samples: 4256/5760, Loss: 0.09565086662769318\n",
      "Epoch: 14, Samples: 4288/5760, Loss: 0.03230936825275421\n",
      "Epoch: 14, Samples: 4320/5760, Loss: 0.050536081194877625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Samples: 4352/5760, Loss: 0.05575290322303772\n",
      "Epoch: 14, Samples: 4384/5760, Loss: 0.07735064625740051\n",
      "Epoch: 14, Samples: 4416/5760, Loss: 0.08877164125442505\n",
      "Epoch: 14, Samples: 4448/5760, Loss: 0.04671965539455414\n",
      "Epoch: 14, Samples: 4480/5760, Loss: 0.054878830909729004\n",
      "Epoch: 14, Samples: 4512/5760, Loss: 0.07600808143615723\n",
      "Epoch: 14, Samples: 4544/5760, Loss: 0.07351362705230713\n",
      "Epoch: 14, Samples: 4576/5760, Loss: 0.069307342171669\n",
      "Epoch: 14, Samples: 4608/5760, Loss: 0.049985021352767944\n",
      "Epoch: 14, Samples: 4640/5760, Loss: 0.02569866180419922\n",
      "Epoch: 14, Samples: 4672/5760, Loss: 0.04635840654373169\n",
      "Epoch: 14, Samples: 4704/5760, Loss: 0.03172159194946289\n",
      "Epoch: 14, Samples: 4736/5760, Loss: 0.09098698198795319\n",
      "Epoch: 14, Samples: 4768/5760, Loss: 0.048231735825538635\n",
      "Epoch: 14, Samples: 4800/5760, Loss: 0.04542507231235504\n",
      "Epoch: 14, Samples: 4832/5760, Loss: 0.031970828771591187\n",
      "Epoch: 14, Samples: 4864/5760, Loss: 0.03824153542518616\n",
      "Epoch: 14, Samples: 4896/5760, Loss: 0.10219857096672058\n",
      "Epoch: 14, Samples: 4928/5760, Loss: 0.03707060217857361\n",
      "Epoch: 14, Samples: 4960/5760, Loss: 0.038606688380241394\n",
      "Epoch: 14, Samples: 4992/5760, Loss: 0.04766489565372467\n",
      "Epoch: 14, Samples: 5024/5760, Loss: 0.054890409111976624\n",
      "Epoch: 14, Samples: 5056/5760, Loss: 0.04606439173221588\n",
      "Epoch: 14, Samples: 5088/5760, Loss: 0.05338297784328461\n",
      "Epoch: 14, Samples: 5120/5760, Loss: 0.046764031052589417\n",
      "Epoch: 14, Samples: 5152/5760, Loss: 0.0687853991985321\n",
      "Epoch: 14, Samples: 5184/5760, Loss: 0.049945756793022156\n",
      "Epoch: 14, Samples: 5216/5760, Loss: 0.07711857557296753\n",
      "Epoch: 14, Samples: 5248/5760, Loss: 0.08311246335506439\n",
      "Epoch: 14, Samples: 5280/5760, Loss: 0.06816832721233368\n",
      "Epoch: 14, Samples: 5312/5760, Loss: 0.04257677495479584\n",
      "Epoch: 14, Samples: 5344/5760, Loss: 0.04096853733062744\n",
      "Epoch: 14, Samples: 5376/5760, Loss: 0.05512373149394989\n",
      "Epoch: 14, Samples: 5408/5760, Loss: 0.040430694818496704\n",
      "Epoch: 14, Samples: 5440/5760, Loss: 0.036121666431427\n",
      "Epoch: 14, Samples: 5472/5760, Loss: 0.03524847328662872\n",
      "Epoch: 14, Samples: 5504/5760, Loss: 0.02945762872695923\n",
      "Epoch: 14, Samples: 5536/5760, Loss: 0.06402389705181122\n",
      "Epoch: 14, Samples: 5568/5760, Loss: 0.058251604437828064\n",
      "Epoch: 14, Samples: 5600/5760, Loss: 0.03729924559593201\n",
      "Epoch: 14, Samples: 5632/5760, Loss: 0.04368580877780914\n",
      "Epoch: 14, Samples: 5664/5760, Loss: 0.09567415714263916\n",
      "Epoch: 14, Samples: 5696/5760, Loss: 0.08947280049324036\n",
      "Epoch: 14, Samples: 5728/5760, Loss: 1.3385536670684814\n",
      "\n",
      "Epoch: 14\n",
      "Training set: Average loss: 0.0683\n",
      "Validation set: Average loss: 0.3449, Accuracy: 758/818 (93%)\n",
      "Epoch: 15, Samples: 0/5760, Loss: 0.036032676696777344\n",
      "Epoch: 15, Samples: 32/5760, Loss: 0.06062859296798706\n",
      "Epoch: 15, Samples: 64/5760, Loss: 0.03880947828292847\n",
      "Epoch: 15, Samples: 96/5760, Loss: 0.04121434688568115\n",
      "Epoch: 15, Samples: 128/5760, Loss: 0.07325159013271332\n",
      "Epoch: 15, Samples: 160/5760, Loss: 0.04028266668319702\n",
      "Epoch: 15, Samples: 192/5760, Loss: 0.053110525012016296\n",
      "Epoch: 15, Samples: 224/5760, Loss: 0.055242910981178284\n",
      "Epoch: 15, Samples: 256/5760, Loss: 0.031215190887451172\n",
      "Epoch: 15, Samples: 288/5760, Loss: 0.052957236766815186\n",
      "Epoch: 15, Samples: 320/5760, Loss: 0.047708019614219666\n",
      "Epoch: 15, Samples: 352/5760, Loss: 0.06763763725757599\n",
      "Epoch: 15, Samples: 384/5760, Loss: 0.03864970803260803\n",
      "Epoch: 15, Samples: 416/5760, Loss: 0.055737078189849854\n",
      "Epoch: 15, Samples: 448/5760, Loss: 0.08389823138713837\n",
      "Epoch: 15, Samples: 480/5760, Loss: 0.04853774607181549\n",
      "Epoch: 15, Samples: 512/5760, Loss: 0.04411771893501282\n",
      "Epoch: 15, Samples: 544/5760, Loss: 0.06327857077121735\n",
      "Epoch: 15, Samples: 576/5760, Loss: 0.11580555140972137\n",
      "Epoch: 15, Samples: 608/5760, Loss: 0.07496067881584167\n",
      "Epoch: 15, Samples: 640/5760, Loss: 0.039272308349609375\n",
      "Epoch: 15, Samples: 672/5760, Loss: 0.0310458242893219\n",
      "Epoch: 15, Samples: 704/5760, Loss: 0.06076011061668396\n",
      "Epoch: 15, Samples: 736/5760, Loss: 0.0725531280040741\n",
      "Epoch: 15, Samples: 768/5760, Loss: 0.04345349967479706\n",
      "Epoch: 15, Samples: 800/5760, Loss: 0.05798777937889099\n",
      "Epoch: 15, Samples: 832/5760, Loss: 0.07885286211967468\n",
      "Epoch: 15, Samples: 864/5760, Loss: 0.0682680606842041\n",
      "Epoch: 15, Samples: 896/5760, Loss: 0.05254045128822327\n",
      "Epoch: 15, Samples: 928/5760, Loss: 0.09972746670246124\n",
      "Epoch: 15, Samples: 960/5760, Loss: 0.052483752369880676\n",
      "Epoch: 15, Samples: 992/5760, Loss: 0.06415325403213501\n",
      "Epoch: 15, Samples: 1024/5760, Loss: 0.06343571841716766\n",
      "Epoch: 15, Samples: 1056/5760, Loss: 0.02785465121269226\n",
      "Epoch: 15, Samples: 1088/5760, Loss: 0.0653228610754013\n",
      "Epoch: 15, Samples: 1120/5760, Loss: 0.03618474304676056\n",
      "Epoch: 15, Samples: 1152/5760, Loss: 0.03198346495628357\n",
      "Epoch: 15, Samples: 1184/5760, Loss: 0.044711485505104065\n",
      "Epoch: 15, Samples: 1216/5760, Loss: 0.05538061261177063\n",
      "Epoch: 15, Samples: 1248/5760, Loss: 0.08386656641960144\n",
      "Epoch: 15, Samples: 1280/5760, Loss: 0.05837337672710419\n",
      "Epoch: 15, Samples: 1312/5760, Loss: 0.062150001525878906\n",
      "Epoch: 15, Samples: 1344/5760, Loss: 0.10623768717050552\n",
      "Epoch: 15, Samples: 1376/5760, Loss: 0.05016942322254181\n",
      "Epoch: 15, Samples: 1408/5760, Loss: 0.03937108814716339\n",
      "Epoch: 15, Samples: 1440/5760, Loss: 0.03874430060386658\n",
      "Epoch: 15, Samples: 1472/5760, Loss: 0.04480147361755371\n",
      "Epoch: 15, Samples: 1504/5760, Loss: 0.07086536288261414\n",
      "Epoch: 15, Samples: 1536/5760, Loss: 0.05151943862438202\n",
      "Epoch: 15, Samples: 1568/5760, Loss: 0.03221753239631653\n",
      "Epoch: 15, Samples: 1600/5760, Loss: 0.0475308895111084\n",
      "Epoch: 15, Samples: 1632/5760, Loss: 0.03317277133464813\n",
      "Epoch: 15, Samples: 1664/5760, Loss: 0.03204697370529175\n",
      "Epoch: 15, Samples: 1696/5760, Loss: 0.06872709095478058\n",
      "Epoch: 15, Samples: 1728/5760, Loss: 0.03799539804458618\n",
      "Epoch: 15, Samples: 1760/5760, Loss: 0.12531164288520813\n",
      "Epoch: 15, Samples: 1792/5760, Loss: 0.06225427985191345\n",
      "Epoch: 15, Samples: 1824/5760, Loss: 0.07076455652713776\n",
      "Epoch: 15, Samples: 1856/5760, Loss: 0.039338916540145874\n",
      "Epoch: 15, Samples: 1888/5760, Loss: 0.029889792203903198\n",
      "Epoch: 15, Samples: 1920/5760, Loss: 0.03310215473175049\n",
      "Epoch: 15, Samples: 1952/5760, Loss: 0.09453120827674866\n",
      "Epoch: 15, Samples: 1984/5760, Loss: 0.051500216126441956\n",
      "Epoch: 15, Samples: 2016/5760, Loss: 0.10871726274490356\n",
      "Epoch: 15, Samples: 2048/5760, Loss: 0.09954728186130524\n",
      "Epoch: 15, Samples: 2080/5760, Loss: 0.05556894838809967\n",
      "Epoch: 15, Samples: 2112/5760, Loss: 0.05742625892162323\n",
      "Epoch: 15, Samples: 2144/5760, Loss: 0.0381641685962677\n",
      "Epoch: 15, Samples: 2176/5760, Loss: 0.07557199895381927\n",
      "Epoch: 15, Samples: 2208/5760, Loss: 0.0729113221168518\n",
      "Epoch: 15, Samples: 2240/5760, Loss: 0.09406720846891403\n",
      "Epoch: 15, Samples: 2272/5760, Loss: 0.052937448024749756\n",
      "Epoch: 15, Samples: 2304/5760, Loss: 0.10951344668865204\n",
      "Epoch: 15, Samples: 2336/5760, Loss: 0.08989116549491882\n",
      "Epoch: 15, Samples: 2368/5760, Loss: 0.038031503558158875\n",
      "Epoch: 15, Samples: 2400/5760, Loss: 0.04498812556266785\n",
      "Epoch: 15, Samples: 2432/5760, Loss: 0.05535288155078888\n",
      "Epoch: 15, Samples: 2464/5760, Loss: 0.056612610816955566\n",
      "Epoch: 15, Samples: 2496/5760, Loss: 0.059196338057518005\n",
      "Epoch: 15, Samples: 2528/5760, Loss: 0.04421618580818176\n",
      "Epoch: 15, Samples: 2560/5760, Loss: 0.06083124876022339\n",
      "Epoch: 15, Samples: 2592/5760, Loss: 0.03849175572395325\n",
      "Epoch: 15, Samples: 2624/5760, Loss: 0.042689889669418335\n",
      "Epoch: 15, Samples: 2656/5760, Loss: 0.03266569972038269\n",
      "Epoch: 15, Samples: 2688/5760, Loss: 0.053199782967567444\n",
      "Epoch: 15, Samples: 2720/5760, Loss: 0.05898544192314148\n",
      "Epoch: 15, Samples: 2752/5760, Loss: 0.024757564067840576\n",
      "Epoch: 15, Samples: 2784/5760, Loss: 0.05221286416053772\n",
      "Epoch: 15, Samples: 2816/5760, Loss: 0.0365946888923645\n",
      "Epoch: 15, Samples: 2848/5760, Loss: 0.044390007853507996\n",
      "Epoch: 15, Samples: 2880/5760, Loss: 0.03470304608345032\n",
      "Epoch: 15, Samples: 2912/5760, Loss: 0.06324969232082367\n",
      "Epoch: 15, Samples: 2944/5760, Loss: 0.039677828550338745\n",
      "Epoch: 15, Samples: 2976/5760, Loss: 0.041624248027801514\n",
      "Epoch: 15, Samples: 3008/5760, Loss: 0.0450984388589859\n",
      "Epoch: 15, Samples: 3040/5760, Loss: 0.038140714168548584\n",
      "Epoch: 15, Samples: 3072/5760, Loss: 0.05221566557884216\n",
      "Epoch: 15, Samples: 3104/5760, Loss: 0.05487814545631409\n",
      "Epoch: 15, Samples: 3136/5760, Loss: 0.040450096130371094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Samples: 3168/5760, Loss: 0.062168121337890625\n",
      "Epoch: 15, Samples: 3200/5760, Loss: 0.026006340980529785\n",
      "Epoch: 15, Samples: 3232/5760, Loss: 0.03417457640171051\n",
      "Epoch: 15, Samples: 3264/5760, Loss: 0.05001409351825714\n",
      "Epoch: 15, Samples: 3296/5760, Loss: 0.05532808601856232\n",
      "Epoch: 15, Samples: 3328/5760, Loss: 0.0640917420387268\n",
      "Epoch: 15, Samples: 3360/5760, Loss: 0.04352028667926788\n",
      "Epoch: 15, Samples: 3392/5760, Loss: 0.03806889057159424\n",
      "Epoch: 15, Samples: 3424/5760, Loss: 0.031128481030464172\n",
      "Epoch: 15, Samples: 3456/5760, Loss: 0.042075932025909424\n",
      "Epoch: 15, Samples: 3488/5760, Loss: 0.08234700560569763\n",
      "Epoch: 15, Samples: 3520/5760, Loss: 0.03590981662273407\n",
      "Epoch: 15, Samples: 3552/5760, Loss: 0.03641106188297272\n",
      "Epoch: 15, Samples: 3584/5760, Loss: 0.050596192479133606\n",
      "Epoch: 15, Samples: 3616/5760, Loss: 0.04491338133811951\n",
      "Epoch: 15, Samples: 3648/5760, Loss: 0.055038273334503174\n",
      "Epoch: 15, Samples: 3680/5760, Loss: 0.04186798632144928\n",
      "Epoch: 15, Samples: 3712/5760, Loss: 0.030256003141403198\n",
      "Epoch: 15, Samples: 3744/5760, Loss: 0.07043446600437164\n",
      "Epoch: 15, Samples: 3776/5760, Loss: 0.05153895914554596\n",
      "Epoch: 15, Samples: 3808/5760, Loss: 0.05092082917690277\n",
      "Epoch: 15, Samples: 3840/5760, Loss: 0.03233957290649414\n",
      "Epoch: 15, Samples: 3872/5760, Loss: 0.046496257185935974\n",
      "Epoch: 15, Samples: 3904/5760, Loss: 0.03483805060386658\n",
      "Epoch: 15, Samples: 3936/5760, Loss: 0.03405013680458069\n",
      "Epoch: 15, Samples: 3968/5760, Loss: 0.04301074147224426\n",
      "Epoch: 15, Samples: 4000/5760, Loss: 0.08986949920654297\n",
      "Epoch: 15, Samples: 4032/5760, Loss: 0.02503955364227295\n",
      "Epoch: 15, Samples: 4064/5760, Loss: 0.04428592324256897\n",
      "Epoch: 15, Samples: 4096/5760, Loss: 0.02941650152206421\n",
      "Epoch: 15, Samples: 4128/5760, Loss: 0.04477475583553314\n",
      "Epoch: 15, Samples: 4160/5760, Loss: 0.049348875880241394\n",
      "Epoch: 15, Samples: 4192/5760, Loss: 0.09206327795982361\n",
      "Epoch: 15, Samples: 4224/5760, Loss: 0.03900764882564545\n",
      "Epoch: 15, Samples: 4256/5760, Loss: 0.03262120485305786\n",
      "Epoch: 15, Samples: 4288/5760, Loss: 0.05796514451503754\n",
      "Epoch: 15, Samples: 4320/5760, Loss: 0.08833028376102448\n",
      "Epoch: 15, Samples: 4352/5760, Loss: 0.06190986931324005\n",
      "Epoch: 15, Samples: 4384/5760, Loss: 0.07234595715999603\n",
      "Epoch: 15, Samples: 4416/5760, Loss: 0.03403198719024658\n",
      "Epoch: 15, Samples: 4448/5760, Loss: 0.04636561870574951\n",
      "Epoch: 15, Samples: 4480/5760, Loss: 0.041552960872650146\n",
      "Epoch: 15, Samples: 4512/5760, Loss: 0.046346515417099\n",
      "Epoch: 15, Samples: 4544/5760, Loss: 0.04502125084400177\n",
      "Epoch: 15, Samples: 4576/5760, Loss: 0.0242154598236084\n",
      "Epoch: 15, Samples: 4608/5760, Loss: 0.034255385398864746\n",
      "Epoch: 15, Samples: 4640/5760, Loss: 0.0535062700510025\n",
      "Epoch: 15, Samples: 4672/5760, Loss: 0.037596195936203\n",
      "Epoch: 15, Samples: 4704/5760, Loss: 0.04783482849597931\n",
      "Epoch: 15, Samples: 4736/5760, Loss: 0.024318203330039978\n",
      "Epoch: 15, Samples: 4768/5760, Loss: 0.03176566958427429\n",
      "Epoch: 15, Samples: 4800/5760, Loss: 0.03109946846961975\n",
      "Epoch: 15, Samples: 4832/5760, Loss: 0.06549715995788574\n",
      "Epoch: 15, Samples: 4864/5760, Loss: 0.07860888540744781\n",
      "Epoch: 15, Samples: 4896/5760, Loss: 0.0994051843881607\n",
      "Epoch: 15, Samples: 4928/5760, Loss: 0.028415769338607788\n",
      "Epoch: 15, Samples: 4960/5760, Loss: 0.02463938295841217\n",
      "Epoch: 15, Samples: 4992/5760, Loss: 0.10145877301692963\n",
      "Epoch: 15, Samples: 5024/5760, Loss: 0.030881285667419434\n",
      "Epoch: 15, Samples: 5056/5760, Loss: 0.03772038221359253\n",
      "Epoch: 15, Samples: 5088/5760, Loss: 0.11498600244522095\n",
      "Epoch: 15, Samples: 5120/5760, Loss: 0.0456850528717041\n",
      "Epoch: 15, Samples: 5152/5760, Loss: 0.04412798583507538\n",
      "Epoch: 15, Samples: 5184/5760, Loss: 0.03171117603778839\n",
      "Epoch: 15, Samples: 5216/5760, Loss: 0.038104861974716187\n",
      "Epoch: 15, Samples: 5248/5760, Loss: 0.035972967743873596\n",
      "Epoch: 15, Samples: 5280/5760, Loss: 0.06624981760978699\n",
      "Epoch: 15, Samples: 5312/5760, Loss: 0.06514677405357361\n",
      "Epoch: 15, Samples: 5344/5760, Loss: 0.047585487365722656\n",
      "Epoch: 15, Samples: 5376/5760, Loss: 0.05471871793270111\n",
      "Epoch: 15, Samples: 5408/5760, Loss: 0.06884153187274933\n",
      "Epoch: 15, Samples: 5440/5760, Loss: 0.04898950457572937\n",
      "Epoch: 15, Samples: 5472/5760, Loss: 0.04744163155555725\n",
      "Epoch: 15, Samples: 5504/5760, Loss: 0.06812864542007446\n",
      "Epoch: 15, Samples: 5536/5760, Loss: 0.06869916617870331\n",
      "Epoch: 15, Samples: 5568/5760, Loss: 0.030527085065841675\n",
      "Epoch: 15, Samples: 5600/5760, Loss: 0.06957341730594635\n",
      "Epoch: 15, Samples: 5632/5760, Loss: 0.037540897727012634\n",
      "Epoch: 15, Samples: 5664/5760, Loss: 0.08363544940948486\n",
      "Epoch: 15, Samples: 5696/5760, Loss: 0.07792142033576965\n",
      "Epoch: 15, Samples: 5728/5760, Loss: 1.4677950143814087\n",
      "\n",
      "Epoch: 15\n",
      "Training set: Average loss: 0.0613\n",
      "Validation set: Average loss: 0.3183, Accuracy: 757/818 (93%)\n",
      "Saving model (epoch 15) with lowest validation loss: 0.3183142620210464\n",
      "Epoch: 16, Samples: 0/5760, Loss: 0.04048846662044525\n",
      "Epoch: 16, Samples: 32/5760, Loss: 0.04236400127410889\n",
      "Epoch: 16, Samples: 64/5760, Loss: 0.026817917823791504\n",
      "Epoch: 16, Samples: 96/5760, Loss: 0.019528239965438843\n",
      "Epoch: 16, Samples: 128/5760, Loss: 0.06554870307445526\n",
      "Epoch: 16, Samples: 160/5760, Loss: 0.05042383074760437\n",
      "Epoch: 16, Samples: 192/5760, Loss: 0.02851468324661255\n",
      "Epoch: 16, Samples: 224/5760, Loss: 0.04792441427707672\n",
      "Epoch: 16, Samples: 256/5760, Loss: 0.046614259481430054\n",
      "Epoch: 16, Samples: 288/5760, Loss: 0.06841148436069489\n",
      "Epoch: 16, Samples: 320/5760, Loss: 0.03537508845329285\n",
      "Epoch: 16, Samples: 352/5760, Loss: 0.054411038756370544\n",
      "Epoch: 16, Samples: 384/5760, Loss: 0.0586090087890625\n",
      "Epoch: 16, Samples: 416/5760, Loss: 0.03639313578605652\n",
      "Epoch: 16, Samples: 448/5760, Loss: 0.0537104606628418\n",
      "Epoch: 16, Samples: 480/5760, Loss: 0.02879108488559723\n",
      "Epoch: 16, Samples: 512/5760, Loss: 0.05600301921367645\n",
      "Epoch: 16, Samples: 544/5760, Loss: 0.025927573442459106\n",
      "Epoch: 16, Samples: 576/5760, Loss: 0.04269276559352875\n",
      "Epoch: 16, Samples: 608/5760, Loss: 0.01989087462425232\n",
      "Epoch: 16, Samples: 640/5760, Loss: 0.04746723175048828\n",
      "Epoch: 16, Samples: 672/5760, Loss: 0.030901610851287842\n",
      "Epoch: 16, Samples: 704/5760, Loss: 0.07752975821495056\n",
      "Epoch: 16, Samples: 736/5760, Loss: 0.03314831852912903\n",
      "Epoch: 16, Samples: 768/5760, Loss: 0.0702546089887619\n",
      "Epoch: 16, Samples: 800/5760, Loss: 0.045172616839408875\n",
      "Epoch: 16, Samples: 832/5760, Loss: 0.031241849064826965\n",
      "Epoch: 16, Samples: 864/5760, Loss: 0.03659825026988983\n",
      "Epoch: 16, Samples: 896/5760, Loss: 0.054545313119888306\n",
      "Epoch: 16, Samples: 928/5760, Loss: 0.030941292643547058\n",
      "Epoch: 16, Samples: 960/5760, Loss: 0.07196645438671112\n",
      "Epoch: 16, Samples: 992/5760, Loss: 0.05200923979282379\n",
      "Epoch: 16, Samples: 1024/5760, Loss: 0.06985041499137878\n",
      "Epoch: 16, Samples: 1056/5760, Loss: 0.047061607241630554\n",
      "Epoch: 16, Samples: 1088/5760, Loss: 0.06734861433506012\n",
      "Epoch: 16, Samples: 1120/5760, Loss: 0.04751312732696533\n",
      "Epoch: 16, Samples: 1152/5760, Loss: 0.06330336630344391\n",
      "Epoch: 16, Samples: 1184/5760, Loss: 0.03807330131530762\n",
      "Epoch: 16, Samples: 1216/5760, Loss: 0.04363492131233215\n",
      "Epoch: 16, Samples: 1248/5760, Loss: 0.06262880563735962\n",
      "Epoch: 16, Samples: 1280/5760, Loss: 0.04727734625339508\n",
      "Epoch: 16, Samples: 1312/5760, Loss: 0.04193298518657684\n",
      "Epoch: 16, Samples: 1344/5760, Loss: 0.07216182351112366\n",
      "Epoch: 16, Samples: 1376/5760, Loss: 0.048048168420791626\n",
      "Epoch: 16, Samples: 1408/5760, Loss: 0.03626170754432678\n",
      "Epoch: 16, Samples: 1440/5760, Loss: 0.03617584705352783\n",
      "Epoch: 16, Samples: 1472/5760, Loss: 0.04904429614543915\n",
      "Epoch: 16, Samples: 1504/5760, Loss: 0.04182994365692139\n",
      "Epoch: 16, Samples: 1536/5760, Loss: 0.03050866723060608\n",
      "Epoch: 16, Samples: 1568/5760, Loss: 0.06713077425956726\n",
      "Epoch: 16, Samples: 1600/5760, Loss: 0.044613927602767944\n",
      "Epoch: 16, Samples: 1632/5760, Loss: 0.09971669316291809\n",
      "Epoch: 16, Samples: 1664/5760, Loss: 0.09406386315822601\n",
      "Epoch: 16, Samples: 1696/5760, Loss: 0.05859215557575226\n",
      "Epoch: 16, Samples: 1728/5760, Loss: 0.05408237874507904\n",
      "Epoch: 16, Samples: 1760/5760, Loss: 0.030179977416992188\n",
      "Epoch: 16, Samples: 1792/5760, Loss: 0.08310553431510925\n",
      "Epoch: 16, Samples: 1824/5760, Loss: 0.03568969666957855\n",
      "Epoch: 16, Samples: 1856/5760, Loss: 0.05225631594657898\n",
      "Epoch: 16, Samples: 1888/5760, Loss: 0.050482332706451416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Samples: 1920/5760, Loss: 0.0604180246591568\n",
      "Epoch: 16, Samples: 1952/5760, Loss: 0.03746199607849121\n",
      "Epoch: 16, Samples: 1984/5760, Loss: 0.06478942930698395\n",
      "Epoch: 16, Samples: 2016/5760, Loss: 0.04136662185192108\n",
      "Epoch: 16, Samples: 2048/5760, Loss: 0.03720635175704956\n",
      "Epoch: 16, Samples: 2080/5760, Loss: 0.05316196382045746\n",
      "Epoch: 16, Samples: 2112/5760, Loss: 0.029827460646629333\n",
      "Epoch: 16, Samples: 2144/5760, Loss: 0.10794764757156372\n",
      "Epoch: 16, Samples: 2176/5760, Loss: 0.03779922425746918\n",
      "Epoch: 16, Samples: 2208/5760, Loss: 0.03192511200904846\n",
      "Epoch: 16, Samples: 2240/5760, Loss: 0.025755763053894043\n",
      "Epoch: 16, Samples: 2272/5760, Loss: 0.06439203023910522\n",
      "Epoch: 16, Samples: 2304/5760, Loss: 0.057728126645088196\n",
      "Epoch: 16, Samples: 2336/5760, Loss: 0.044507384300231934\n",
      "Epoch: 16, Samples: 2368/5760, Loss: 0.04308600723743439\n",
      "Epoch: 16, Samples: 2400/5760, Loss: 0.035399436950683594\n",
      "Epoch: 16, Samples: 2432/5760, Loss: 0.023186147212982178\n",
      "Epoch: 16, Samples: 2464/5760, Loss: 0.09714874625205994\n",
      "Epoch: 16, Samples: 2496/5760, Loss: 0.02373790740966797\n",
      "Epoch: 16, Samples: 2528/5760, Loss: 0.03547997772693634\n",
      "Epoch: 16, Samples: 2560/5760, Loss: 0.03336822986602783\n",
      "Epoch: 16, Samples: 2592/5760, Loss: 0.043826669454574585\n",
      "Epoch: 16, Samples: 2624/5760, Loss: 0.04412484169006348\n",
      "Epoch: 16, Samples: 2656/5760, Loss: 0.020676910877227783\n",
      "Epoch: 16, Samples: 2688/5760, Loss: 0.028974175453186035\n",
      "Epoch: 16, Samples: 2720/5760, Loss: 0.057030096650123596\n",
      "Epoch: 16, Samples: 2752/5760, Loss: 0.03143751621246338\n",
      "Epoch: 16, Samples: 2784/5760, Loss: 0.139109268784523\n",
      "Epoch: 16, Samples: 2816/5760, Loss: 0.025462806224822998\n",
      "Epoch: 16, Samples: 2848/5760, Loss: 0.02973639965057373\n",
      "Epoch: 16, Samples: 2880/5760, Loss: 0.04563821852207184\n",
      "Epoch: 16, Samples: 2912/5760, Loss: 0.09047488868236542\n",
      "Epoch: 16, Samples: 2944/5760, Loss: 0.03742218017578125\n",
      "Epoch: 16, Samples: 2976/5760, Loss: 0.050204113125801086\n",
      "Epoch: 16, Samples: 3008/5760, Loss: 0.025511890649795532\n",
      "Epoch: 16, Samples: 3040/5760, Loss: 0.0383073091506958\n",
      "Epoch: 16, Samples: 3072/5760, Loss: 0.06630381941795349\n",
      "Epoch: 16, Samples: 3104/5760, Loss: 0.04865206778049469\n",
      "Epoch: 16, Samples: 3136/5760, Loss: 0.05671294033527374\n",
      "Epoch: 16, Samples: 3168/5760, Loss: 0.052652567625045776\n",
      "Epoch: 16, Samples: 3200/5760, Loss: 0.045715004205703735\n",
      "Epoch: 16, Samples: 3232/5760, Loss: 0.09286583960056305\n",
      "Epoch: 16, Samples: 3264/5760, Loss: 0.034886717796325684\n",
      "Epoch: 16, Samples: 3296/5760, Loss: 0.09211510419845581\n",
      "Epoch: 16, Samples: 3328/5760, Loss: 0.04197824001312256\n",
      "Epoch: 16, Samples: 3360/5760, Loss: 0.03426384925842285\n",
      "Epoch: 16, Samples: 3392/5760, Loss: 0.03333503007888794\n",
      "Epoch: 16, Samples: 3424/5760, Loss: 0.040559932589530945\n",
      "Epoch: 16, Samples: 3456/5760, Loss: 0.06389407813549042\n",
      "Epoch: 16, Samples: 3488/5760, Loss: 0.06267218291759491\n",
      "Epoch: 16, Samples: 3520/5760, Loss: 0.07585665583610535\n",
      "Epoch: 16, Samples: 3552/5760, Loss: 0.05591242015361786\n",
      "Epoch: 16, Samples: 3584/5760, Loss: 0.060298457741737366\n",
      "Epoch: 16, Samples: 3616/5760, Loss: 0.03862982988357544\n",
      "Epoch: 16, Samples: 3648/5760, Loss: 0.03553827106952667\n",
      "Epoch: 16, Samples: 3680/5760, Loss: 0.04203328490257263\n",
      "Epoch: 16, Samples: 3712/5760, Loss: 0.0428696870803833\n",
      "Epoch: 16, Samples: 3744/5760, Loss: 0.06532342731952667\n",
      "Epoch: 16, Samples: 3776/5760, Loss: 0.10275153815746307\n",
      "Epoch: 16, Samples: 3808/5760, Loss: 0.035404354333877563\n",
      "Epoch: 16, Samples: 3840/5760, Loss: 0.041192322969436646\n",
      "Epoch: 16, Samples: 3872/5760, Loss: 0.07082527875900269\n",
      "Epoch: 16, Samples: 3904/5760, Loss: 0.03398212790489197\n",
      "Epoch: 16, Samples: 3936/5760, Loss: 0.05562324821949005\n",
      "Epoch: 16, Samples: 3968/5760, Loss: 0.026580065488815308\n",
      "Epoch: 16, Samples: 4000/5760, Loss: 0.055685386061668396\n",
      "Epoch: 16, Samples: 4032/5760, Loss: 0.0566730797290802\n",
      "Epoch: 16, Samples: 4064/5760, Loss: 0.05372467637062073\n",
      "Epoch: 16, Samples: 4096/5760, Loss: 0.044920727610588074\n",
      "Epoch: 16, Samples: 4128/5760, Loss: 0.0319691002368927\n",
      "Epoch: 16, Samples: 4160/5760, Loss: 0.026777982711791992\n",
      "Epoch: 16, Samples: 4192/5760, Loss: 0.041082218289375305\n",
      "Epoch: 16, Samples: 4224/5760, Loss: 0.03565472364425659\n",
      "Epoch: 16, Samples: 4256/5760, Loss: 0.056445300579071045\n",
      "Epoch: 16, Samples: 4288/5760, Loss: 0.047320395708084106\n",
      "Epoch: 16, Samples: 4320/5760, Loss: 0.03604605793952942\n",
      "Epoch: 16, Samples: 4352/5760, Loss: 0.050937265157699585\n",
      "Epoch: 16, Samples: 4384/5760, Loss: 0.05113878846168518\n",
      "Epoch: 16, Samples: 4416/5760, Loss: 0.034042924642562866\n",
      "Epoch: 16, Samples: 4448/5760, Loss: 0.03076775372028351\n",
      "Epoch: 16, Samples: 4480/5760, Loss: 0.03576311469078064\n",
      "Epoch: 16, Samples: 4512/5760, Loss: 0.048324018716812134\n",
      "Epoch: 16, Samples: 4544/5760, Loss: 0.022842198610305786\n",
      "Epoch: 16, Samples: 4576/5760, Loss: 0.06464006006717682\n",
      "Epoch: 16, Samples: 4608/5760, Loss: 0.05063644051551819\n",
      "Epoch: 16, Samples: 4640/5760, Loss: 0.036101728677749634\n",
      "Epoch: 16, Samples: 4672/5760, Loss: 0.05149520933628082\n",
      "Epoch: 16, Samples: 4704/5760, Loss: 0.058828458189964294\n",
      "Epoch: 16, Samples: 4736/5760, Loss: 0.045102596282958984\n",
      "Epoch: 16, Samples: 4768/5760, Loss: 0.04589337110519409\n",
      "Epoch: 16, Samples: 4800/5760, Loss: 0.037714362144470215\n",
      "Epoch: 16, Samples: 4832/5760, Loss: 0.05948793888092041\n",
      "Epoch: 16, Samples: 4864/5760, Loss: 0.04205642640590668\n",
      "Epoch: 16, Samples: 4896/5760, Loss: 0.05679279565811157\n",
      "Epoch: 16, Samples: 4928/5760, Loss: 0.05200423300266266\n",
      "Epoch: 16, Samples: 4960/5760, Loss: 0.0458124577999115\n",
      "Epoch: 16, Samples: 4992/5760, Loss: 0.039673447608947754\n",
      "Epoch: 16, Samples: 5024/5760, Loss: 0.05475199222564697\n",
      "Epoch: 16, Samples: 5056/5760, Loss: 0.05785496532917023\n",
      "Epoch: 16, Samples: 5088/5760, Loss: 0.05687558650970459\n",
      "Epoch: 16, Samples: 5120/5760, Loss: 0.025552183389663696\n",
      "Epoch: 16, Samples: 5152/5760, Loss: 0.041575491428375244\n",
      "Epoch: 16, Samples: 5184/5760, Loss: 0.09938320517539978\n",
      "Epoch: 16, Samples: 5216/5760, Loss: 0.029589220881462097\n",
      "Epoch: 16, Samples: 5248/5760, Loss: 0.04731561243534088\n",
      "Epoch: 16, Samples: 5280/5760, Loss: 0.05993485450744629\n",
      "Epoch: 16, Samples: 5312/5760, Loss: 0.04485836625099182\n",
      "Epoch: 16, Samples: 5344/5760, Loss: 0.041950374841690063\n",
      "Epoch: 16, Samples: 5376/5760, Loss: 0.07967260479927063\n",
      "Epoch: 16, Samples: 5408/5760, Loss: 0.04183302819728851\n",
      "Epoch: 16, Samples: 5440/5760, Loss: 0.05097845196723938\n",
      "Epoch: 16, Samples: 5472/5760, Loss: 0.026633024215698242\n",
      "Epoch: 16, Samples: 5504/5760, Loss: 0.04128856956958771\n",
      "Epoch: 16, Samples: 5536/5760, Loss: 0.02376362681388855\n",
      "Epoch: 16, Samples: 5568/5760, Loss: 0.05132344365119934\n",
      "Epoch: 16, Samples: 5600/5760, Loss: 0.03397989273071289\n",
      "Epoch: 16, Samples: 5632/5760, Loss: 0.04860366880893707\n",
      "Epoch: 16, Samples: 5664/5760, Loss: 0.04614856839179993\n",
      "Epoch: 16, Samples: 5696/5760, Loss: 0.031503140926361084\n",
      "Epoch: 16, Samples: 5728/5760, Loss: 1.685449481010437\n",
      "\n",
      "Epoch: 16\n",
      "Training set: Average loss: 0.0575\n",
      "Validation set: Average loss: 0.3082, Accuracy: 760/818 (93%)\n",
      "Saving model (epoch 16) with lowest validation loss: 0.3082183811527032\n",
      "Epoch: 17, Samples: 0/5760, Loss: 0.044529274106025696\n",
      "Epoch: 17, Samples: 32/5760, Loss: 0.04763945937156677\n",
      "Epoch: 17, Samples: 64/5760, Loss: 0.05023355782032013\n",
      "Epoch: 17, Samples: 96/5760, Loss: 0.030296921730041504\n",
      "Epoch: 17, Samples: 128/5760, Loss: 0.029809504747390747\n",
      "Epoch: 17, Samples: 160/5760, Loss: 0.026024192571640015\n",
      "Epoch: 17, Samples: 192/5760, Loss: 0.046639978885650635\n",
      "Epoch: 17, Samples: 224/5760, Loss: 0.03761520981788635\n",
      "Epoch: 17, Samples: 256/5760, Loss: 0.06282727420330048\n",
      "Epoch: 17, Samples: 288/5760, Loss: 0.024642378091812134\n",
      "Epoch: 17, Samples: 320/5760, Loss: 0.03415912389755249\n",
      "Epoch: 17, Samples: 352/5760, Loss: 0.07076658308506012\n",
      "Epoch: 17, Samples: 384/5760, Loss: 0.04425157606601715\n",
      "Epoch: 17, Samples: 416/5760, Loss: 0.07119262218475342\n",
      "Epoch: 17, Samples: 448/5760, Loss: 0.04481631517410278\n",
      "Epoch: 17, Samples: 480/5760, Loss: 0.047977134585380554\n",
      "Epoch: 17, Samples: 512/5760, Loss: 0.03492206335067749\n",
      "Epoch: 17, Samples: 544/5760, Loss: 0.03176376223564148\n",
      "Epoch: 17, Samples: 576/5760, Loss: 0.08240379393100739\n",
      "Epoch: 17, Samples: 608/5760, Loss: 0.03681516647338867\n",
      "Epoch: 17, Samples: 640/5760, Loss: 0.03533241152763367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Samples: 672/5760, Loss: 0.037978917360305786\n",
      "Epoch: 17, Samples: 704/5760, Loss: 0.030932635068893433\n",
      "Epoch: 17, Samples: 736/5760, Loss: 0.052291110157966614\n",
      "Epoch: 17, Samples: 768/5760, Loss: 0.08110770583152771\n",
      "Epoch: 17, Samples: 800/5760, Loss: 0.04409995675086975\n",
      "Epoch: 17, Samples: 832/5760, Loss: 0.042166873812675476\n",
      "Epoch: 17, Samples: 864/5760, Loss: 0.052081912755966187\n",
      "Epoch: 17, Samples: 896/5760, Loss: 0.045400604605674744\n",
      "Epoch: 17, Samples: 928/5760, Loss: 0.04025326669216156\n",
      "Epoch: 17, Samples: 960/5760, Loss: 0.043420106172561646\n",
      "Epoch: 17, Samples: 992/5760, Loss: 0.04272277653217316\n",
      "Epoch: 17, Samples: 1024/5760, Loss: 0.08983109891414642\n",
      "Epoch: 17, Samples: 1056/5760, Loss: 0.05496010184288025\n",
      "Epoch: 17, Samples: 1088/5760, Loss: 0.061206668615341187\n",
      "Epoch: 17, Samples: 1120/5760, Loss: 0.03929278254508972\n",
      "Epoch: 17, Samples: 1152/5760, Loss: 0.04907578229904175\n",
      "Epoch: 17, Samples: 1184/5760, Loss: 0.031118452548980713\n",
      "Epoch: 17, Samples: 1216/5760, Loss: 0.04234093427658081\n",
      "Epoch: 17, Samples: 1248/5760, Loss: 0.04284396767616272\n",
      "Epoch: 17, Samples: 1280/5760, Loss: 0.02227264642715454\n",
      "Epoch: 17, Samples: 1312/5760, Loss: 0.02725580334663391\n",
      "Epoch: 17, Samples: 1344/5760, Loss: 0.045317888259887695\n",
      "Epoch: 17, Samples: 1376/5760, Loss: 0.055751487612724304\n",
      "Epoch: 17, Samples: 1408/5760, Loss: 0.057458505034446716\n",
      "Epoch: 17, Samples: 1440/5760, Loss: 0.0255662202835083\n",
      "Epoch: 17, Samples: 1472/5760, Loss: 0.048033446073532104\n",
      "Epoch: 17, Samples: 1504/5760, Loss: 0.05558916926383972\n",
      "Epoch: 17, Samples: 1536/5760, Loss: 0.08895666897296906\n",
      "Epoch: 17, Samples: 1568/5760, Loss: 0.02999362349510193\n",
      "Epoch: 17, Samples: 1600/5760, Loss: 0.04823856055736542\n",
      "Epoch: 17, Samples: 1632/5760, Loss: 0.03503447771072388\n",
      "Epoch: 17, Samples: 1664/5760, Loss: 0.09993554651737213\n",
      "Epoch: 17, Samples: 1696/5760, Loss: 0.038119107484817505\n",
      "Epoch: 17, Samples: 1728/5760, Loss: 0.019790858030319214\n",
      "Epoch: 17, Samples: 1760/5760, Loss: 0.024922966957092285\n",
      "Epoch: 17, Samples: 1792/5760, Loss: 0.02817663550376892\n",
      "Epoch: 17, Samples: 1824/5760, Loss: 0.05143992602825165\n",
      "Epoch: 17, Samples: 1856/5760, Loss: 0.03727295994758606\n",
      "Epoch: 17, Samples: 1888/5760, Loss: 0.10138820111751556\n",
      "Epoch: 17, Samples: 1920/5760, Loss: 0.04158470034599304\n",
      "Epoch: 17, Samples: 1952/5760, Loss: 0.048024654388427734\n",
      "Epoch: 17, Samples: 1984/5760, Loss: 0.05131201446056366\n",
      "Epoch: 17, Samples: 2016/5760, Loss: 0.04002809524536133\n",
      "Epoch: 17, Samples: 2048/5760, Loss: 0.052750229835510254\n",
      "Epoch: 17, Samples: 2080/5760, Loss: 0.035600632429122925\n",
      "Epoch: 17, Samples: 2112/5760, Loss: 0.04118695855140686\n",
      "Epoch: 17, Samples: 2144/5760, Loss: 0.043760329484939575\n",
      "Epoch: 17, Samples: 2176/5760, Loss: 0.04776722192764282\n",
      "Epoch: 17, Samples: 2208/5760, Loss: 0.02950093150138855\n",
      "Epoch: 17, Samples: 2240/5760, Loss: 0.051801830530166626\n",
      "Epoch: 17, Samples: 2272/5760, Loss: 0.07403570413589478\n",
      "Epoch: 17, Samples: 2304/5760, Loss: 0.05098101496696472\n",
      "Epoch: 17, Samples: 2336/5760, Loss: 0.039311885833740234\n",
      "Epoch: 17, Samples: 2368/5760, Loss: 0.1249738335609436\n",
      "Epoch: 17, Samples: 2400/5760, Loss: 0.04533270001411438\n",
      "Epoch: 17, Samples: 2432/5760, Loss: 0.033180952072143555\n",
      "Epoch: 17, Samples: 2464/5760, Loss: 0.052981019020080566\n",
      "Epoch: 17, Samples: 2496/5760, Loss: 0.049130797386169434\n",
      "Epoch: 17, Samples: 2528/5760, Loss: 0.029211968183517456\n",
      "Epoch: 17, Samples: 2560/5760, Loss: 0.05556425452232361\n",
      "Epoch: 17, Samples: 2592/5760, Loss: 0.03749442100524902\n",
      "Epoch: 17, Samples: 2624/5760, Loss: 0.028107285499572754\n",
      "Epoch: 17, Samples: 2656/5760, Loss: 0.037997275590896606\n",
      "Epoch: 17, Samples: 2688/5760, Loss: 0.04928281903266907\n",
      "Epoch: 17, Samples: 2720/5760, Loss: 0.03266698122024536\n",
      "Epoch: 17, Samples: 2752/5760, Loss: 0.020884662866592407\n",
      "Epoch: 17, Samples: 2784/5760, Loss: 0.03255559504032135\n",
      "Epoch: 17, Samples: 2816/5760, Loss: 0.02530422806739807\n",
      "Epoch: 17, Samples: 2848/5760, Loss: 0.04206469655036926\n",
      "Epoch: 17, Samples: 2880/5760, Loss: 0.016891181468963623\n",
      "Epoch: 17, Samples: 2912/5760, Loss: 0.0547303706407547\n",
      "Epoch: 17, Samples: 2944/5760, Loss: 0.031384021043777466\n",
      "Epoch: 17, Samples: 2976/5760, Loss: 0.030029520392417908\n",
      "Epoch: 17, Samples: 3008/5760, Loss: 0.050057798624038696\n",
      "Epoch: 17, Samples: 3040/5760, Loss: 0.03453060984611511\n",
      "Epoch: 17, Samples: 3072/5760, Loss: 0.05040477216243744\n",
      "Epoch: 17, Samples: 3104/5760, Loss: 0.033872559666633606\n",
      "Epoch: 17, Samples: 3136/5760, Loss: 0.025308877229690552\n",
      "Epoch: 17, Samples: 3168/5760, Loss: 0.037446632981300354\n",
      "Epoch: 17, Samples: 3200/5760, Loss: 0.0392184853553772\n",
      "Epoch: 17, Samples: 3232/5760, Loss: 0.05143865942955017\n",
      "Epoch: 17, Samples: 3264/5760, Loss: 0.029295027256011963\n",
      "Epoch: 17, Samples: 3296/5760, Loss: 0.0501105934381485\n",
      "Epoch: 17, Samples: 3328/5760, Loss: 0.06364418566226959\n",
      "Epoch: 17, Samples: 3360/5760, Loss: 0.05967488884925842\n",
      "Epoch: 17, Samples: 3392/5760, Loss: 0.030151844024658203\n",
      "Epoch: 17, Samples: 3424/5760, Loss: 0.03262461721897125\n",
      "Epoch: 17, Samples: 3456/5760, Loss: 0.03336957097053528\n",
      "Epoch: 17, Samples: 3488/5760, Loss: 0.03477706015110016\n",
      "Epoch: 17, Samples: 3520/5760, Loss: 0.06934407353401184\n",
      "Epoch: 17, Samples: 3552/5760, Loss: 0.028530016541481018\n",
      "Epoch: 17, Samples: 3584/5760, Loss: 0.05755987763404846\n",
      "Epoch: 17, Samples: 3616/5760, Loss: 0.04124489426612854\n",
      "Epoch: 17, Samples: 3648/5760, Loss: 0.03392826020717621\n",
      "Epoch: 17, Samples: 3680/5760, Loss: 0.04784752428531647\n",
      "Epoch: 17, Samples: 3712/5760, Loss: 0.06260307133197784\n",
      "Epoch: 17, Samples: 3744/5760, Loss: 0.052129581570625305\n",
      "Epoch: 17, Samples: 3776/5760, Loss: 0.03941623866558075\n",
      "Epoch: 17, Samples: 3808/5760, Loss: 0.04993543028831482\n",
      "Epoch: 17, Samples: 3840/5760, Loss: 0.04571102559566498\n",
      "Epoch: 17, Samples: 3872/5760, Loss: 0.04009915888309479\n",
      "Epoch: 17, Samples: 3904/5760, Loss: 0.0493006706237793\n",
      "Epoch: 17, Samples: 3936/5760, Loss: 0.04836645722389221\n",
      "Epoch: 17, Samples: 3968/5760, Loss: 0.030786782503128052\n",
      "Epoch: 17, Samples: 4000/5760, Loss: 0.03320321440696716\n",
      "Epoch: 17, Samples: 4032/5760, Loss: 0.050720423460006714\n",
      "Epoch: 17, Samples: 4064/5760, Loss: 0.03592108190059662\n",
      "Epoch: 17, Samples: 4096/5760, Loss: 0.03180058300495148\n",
      "Epoch: 17, Samples: 4128/5760, Loss: 0.11067591607570648\n",
      "Epoch: 17, Samples: 4160/5760, Loss: 0.06613858044147491\n",
      "Epoch: 17, Samples: 4192/5760, Loss: 0.03538775444030762\n",
      "Epoch: 17, Samples: 4224/5760, Loss: 0.037462204694747925\n",
      "Epoch: 17, Samples: 4256/5760, Loss: 0.03869910538196564\n",
      "Epoch: 17, Samples: 4288/5760, Loss: 0.0674981027841568\n",
      "Epoch: 17, Samples: 4320/5760, Loss: 0.02750527858734131\n",
      "Epoch: 17, Samples: 4352/5760, Loss: 0.03549474477767944\n",
      "Epoch: 17, Samples: 4384/5760, Loss: 0.04589863121509552\n",
      "Epoch: 17, Samples: 4416/5760, Loss: 0.032815396785736084\n",
      "Epoch: 17, Samples: 4448/5760, Loss: 0.046556949615478516\n",
      "Epoch: 17, Samples: 4480/5760, Loss: 0.019550830125808716\n",
      "Epoch: 17, Samples: 4512/5760, Loss: 0.017821967601776123\n",
      "Epoch: 17, Samples: 4544/5760, Loss: 0.034334972500801086\n",
      "Epoch: 17, Samples: 4576/5760, Loss: 0.03469601273536682\n",
      "Epoch: 17, Samples: 4608/5760, Loss: 0.04423891007900238\n",
      "Epoch: 17, Samples: 4640/5760, Loss: 0.04417632520198822\n",
      "Epoch: 17, Samples: 4672/5760, Loss: 0.05005410313606262\n",
      "Epoch: 17, Samples: 4704/5760, Loss: 0.042273908853530884\n",
      "Epoch: 17, Samples: 4736/5760, Loss: 0.022714197635650635\n",
      "Epoch: 17, Samples: 4768/5760, Loss: 0.04392975568771362\n",
      "Epoch: 17, Samples: 4800/5760, Loss: 0.02706766128540039\n",
      "Epoch: 17, Samples: 4832/5760, Loss: 0.03524293005466461\n",
      "Epoch: 17, Samples: 4864/5760, Loss: 0.03257453441619873\n",
      "Epoch: 17, Samples: 4896/5760, Loss: 0.054632291197776794\n",
      "Epoch: 17, Samples: 4928/5760, Loss: 0.03894588351249695\n",
      "Epoch: 17, Samples: 4960/5760, Loss: 0.04626452922821045\n",
      "Epoch: 17, Samples: 4992/5760, Loss: 0.043077677488327026\n",
      "Epoch: 17, Samples: 5024/5760, Loss: 0.032465860247612\n",
      "Epoch: 17, Samples: 5056/5760, Loss: 0.022940605878829956\n",
      "Epoch: 17, Samples: 5088/5760, Loss: 0.023948803544044495\n",
      "Epoch: 17, Samples: 5120/5760, Loss: 0.03321021795272827\n",
      "Epoch: 17, Samples: 5152/5760, Loss: 0.037302225828170776\n",
      "Epoch: 17, Samples: 5184/5760, Loss: 0.034713879227638245\n",
      "Epoch: 17, Samples: 5216/5760, Loss: 0.047803252935409546\n",
      "Epoch: 17, Samples: 5248/5760, Loss: 0.048628658056259155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Samples: 5280/5760, Loss: 0.032920509576797485\n",
      "Epoch: 17, Samples: 5312/5760, Loss: 0.03977905213832855\n",
      "Epoch: 17, Samples: 5344/5760, Loss: 0.017434149980545044\n",
      "Epoch: 17, Samples: 5376/5760, Loss: 0.027142852544784546\n",
      "Epoch: 17, Samples: 5408/5760, Loss: 0.03220963478088379\n",
      "Epoch: 17, Samples: 5440/5760, Loss: 0.034006208181381226\n",
      "Epoch: 17, Samples: 5472/5760, Loss: 0.029677599668502808\n",
      "Epoch: 17, Samples: 5504/5760, Loss: 0.029246002435684204\n",
      "Epoch: 17, Samples: 5536/5760, Loss: 0.02529144287109375\n",
      "Epoch: 17, Samples: 5568/5760, Loss: 0.0426681786775589\n",
      "Epoch: 17, Samples: 5600/5760, Loss: 0.03670439124107361\n",
      "Epoch: 17, Samples: 5632/5760, Loss: 0.06736473739147186\n",
      "Epoch: 17, Samples: 5664/5760, Loss: 0.036922603845596313\n",
      "Epoch: 17, Samples: 5696/5760, Loss: 0.03884744644165039\n",
      "Epoch: 17, Samples: 5728/5760, Loss: 1.121324062347412\n",
      "\n",
      "Epoch: 17\n",
      "Training set: Average loss: 0.0492\n",
      "Validation set: Average loss: 0.3114, Accuracy: 767/818 (94%)\n",
      "Epoch: 18, Samples: 0/5760, Loss: 0.03191208839416504\n",
      "Epoch: 18, Samples: 32/5760, Loss: 0.028954148292541504\n",
      "Epoch: 18, Samples: 64/5760, Loss: 0.029340192675590515\n",
      "Epoch: 18, Samples: 96/5760, Loss: 0.03850562870502472\n",
      "Epoch: 18, Samples: 128/5760, Loss: 0.05143651366233826\n",
      "Epoch: 18, Samples: 160/5760, Loss: 0.020496070384979248\n",
      "Epoch: 18, Samples: 192/5760, Loss: 0.04595734179019928\n",
      "Epoch: 18, Samples: 224/5760, Loss: 0.02393314242362976\n",
      "Epoch: 18, Samples: 256/5760, Loss: 0.012409210205078125\n",
      "Epoch: 18, Samples: 288/5760, Loss: 0.03542785346508026\n",
      "Epoch: 18, Samples: 320/5760, Loss: 0.026770740747451782\n",
      "Epoch: 18, Samples: 352/5760, Loss: 0.032370954751968384\n",
      "Epoch: 18, Samples: 384/5760, Loss: 0.022430777549743652\n",
      "Epoch: 18, Samples: 416/5760, Loss: 0.037168845534324646\n",
      "Epoch: 18, Samples: 448/5760, Loss: 0.04258909821510315\n",
      "Epoch: 18, Samples: 480/5760, Loss: 0.07251660525798798\n",
      "Epoch: 18, Samples: 512/5760, Loss: 0.023988157510757446\n",
      "Epoch: 18, Samples: 544/5760, Loss: 0.04802240431308746\n",
      "Epoch: 18, Samples: 576/5760, Loss: 0.04885563254356384\n",
      "Epoch: 18, Samples: 608/5760, Loss: 0.02941395342350006\n",
      "Epoch: 18, Samples: 640/5760, Loss: 0.06271955370903015\n",
      "Epoch: 18, Samples: 672/5760, Loss: 0.037429168820381165\n",
      "Epoch: 18, Samples: 704/5760, Loss: 0.030151784420013428\n",
      "Epoch: 18, Samples: 736/5760, Loss: 0.03923633694648743\n",
      "Epoch: 18, Samples: 768/5760, Loss: 0.029131054878234863\n",
      "Epoch: 18, Samples: 800/5760, Loss: 0.048742592334747314\n",
      "Epoch: 18, Samples: 832/5760, Loss: 0.04369400441646576\n",
      "Epoch: 18, Samples: 864/5760, Loss: 0.037510812282562256\n",
      "Epoch: 18, Samples: 896/5760, Loss: 0.04772266745567322\n",
      "Epoch: 18, Samples: 928/5760, Loss: 0.05753593146800995\n",
      "Epoch: 18, Samples: 960/5760, Loss: 0.04856368899345398\n",
      "Epoch: 18, Samples: 992/5760, Loss: 0.023493021726608276\n",
      "Epoch: 18, Samples: 1024/5760, Loss: 0.04295492172241211\n",
      "Epoch: 18, Samples: 1056/5760, Loss: 0.028213798999786377\n",
      "Epoch: 18, Samples: 1088/5760, Loss: 0.03505980968475342\n",
      "Epoch: 18, Samples: 1120/5760, Loss: 0.03925409913063049\n",
      "Epoch: 18, Samples: 1152/5760, Loss: 0.021721959114074707\n",
      "Epoch: 18, Samples: 1184/5760, Loss: 0.04810686409473419\n",
      "Epoch: 18, Samples: 1216/5760, Loss: 0.049915894865989685\n",
      "Epoch: 18, Samples: 1248/5760, Loss: 0.023313134908676147\n",
      "Epoch: 18, Samples: 1280/5760, Loss: 0.03161945939064026\n",
      "Epoch: 18, Samples: 1312/5760, Loss: 0.02551528811454773\n",
      "Epoch: 18, Samples: 1344/5760, Loss: 0.028866231441497803\n",
      "Epoch: 18, Samples: 1376/5760, Loss: 0.02954566478729248\n",
      "Epoch: 18, Samples: 1408/5760, Loss: 0.025836050510406494\n",
      "Epoch: 18, Samples: 1440/5760, Loss: 0.048566386103630066\n",
      "Epoch: 18, Samples: 1472/5760, Loss: 0.04805353283882141\n",
      "Epoch: 18, Samples: 1504/5760, Loss: 0.02202165126800537\n",
      "Epoch: 18, Samples: 1536/5760, Loss: 0.039918676018714905\n",
      "Epoch: 18, Samples: 1568/5760, Loss: 0.025012314319610596\n",
      "Epoch: 18, Samples: 1600/5760, Loss: 0.036436259746551514\n",
      "Epoch: 18, Samples: 1632/5760, Loss: 0.030267998576164246\n",
      "Epoch: 18, Samples: 1664/5760, Loss: 0.03206557035446167\n",
      "Epoch: 18, Samples: 1696/5760, Loss: 0.0228080153465271\n",
      "Epoch: 18, Samples: 1728/5760, Loss: 0.041283443570137024\n",
      "Epoch: 18, Samples: 1760/5760, Loss: 0.030913561582565308\n",
      "Epoch: 18, Samples: 1792/5760, Loss: 0.03469209372997284\n",
      "Epoch: 18, Samples: 1824/5760, Loss: 0.02895331382751465\n",
      "Epoch: 18, Samples: 1856/5760, Loss: 0.04437777400016785\n",
      "Epoch: 18, Samples: 1888/5760, Loss: 0.06700825691223145\n",
      "Epoch: 18, Samples: 1920/5760, Loss: 0.01955127716064453\n",
      "Epoch: 18, Samples: 1952/5760, Loss: 0.029179975390434265\n",
      "Epoch: 18, Samples: 1984/5760, Loss: 0.027916014194488525\n",
      "Epoch: 18, Samples: 2016/5760, Loss: 0.02231474220752716\n",
      "Epoch: 18, Samples: 2048/5760, Loss: 0.022579342126846313\n",
      "Epoch: 18, Samples: 2080/5760, Loss: 0.045663416385650635\n",
      "Epoch: 18, Samples: 2112/5760, Loss: 0.024209946393966675\n",
      "Epoch: 18, Samples: 2144/5760, Loss: 0.030243009328842163\n",
      "Epoch: 18, Samples: 2176/5760, Loss: 0.03728461265563965\n",
      "Epoch: 18, Samples: 2208/5760, Loss: 0.025115877389907837\n",
      "Epoch: 18, Samples: 2240/5760, Loss: 0.05198417603969574\n",
      "Epoch: 18, Samples: 2272/5760, Loss: 0.05187147855758667\n",
      "Epoch: 18, Samples: 2304/5760, Loss: 0.022017449140548706\n",
      "Epoch: 18, Samples: 2336/5760, Loss: 0.026727229356765747\n",
      "Epoch: 18, Samples: 2368/5760, Loss: 0.03682370483875275\n",
      "Epoch: 18, Samples: 2400/5760, Loss: 0.022601425647735596\n",
      "Epoch: 18, Samples: 2432/5760, Loss: 0.0455450564622879\n",
      "Epoch: 18, Samples: 2464/5760, Loss: 0.04028025269508362\n",
      "Epoch: 18, Samples: 2496/5760, Loss: 0.02787196636199951\n",
      "Epoch: 18, Samples: 2528/5760, Loss: 0.030141428112983704\n",
      "Epoch: 18, Samples: 2560/5760, Loss: 0.03111499547958374\n",
      "Epoch: 18, Samples: 2592/5760, Loss: 0.053620561957359314\n",
      "Epoch: 18, Samples: 2624/5760, Loss: 0.027419298887252808\n",
      "Epoch: 18, Samples: 2656/5760, Loss: 0.04370224475860596\n",
      "Epoch: 18, Samples: 2688/5760, Loss: 0.05514903366565704\n",
      "Epoch: 18, Samples: 2720/5760, Loss: 0.02377679944038391\n",
      "Epoch: 18, Samples: 2752/5760, Loss: 0.022927045822143555\n",
      "Epoch: 18, Samples: 2784/5760, Loss: 0.04498255252838135\n",
      "Epoch: 18, Samples: 2816/5760, Loss: 0.0173550546169281\n",
      "Epoch: 18, Samples: 2848/5760, Loss: 0.07420358061790466\n",
      "Epoch: 18, Samples: 2880/5760, Loss: 0.03458203375339508\n",
      "Epoch: 18, Samples: 2912/5760, Loss: 0.027674973011016846\n",
      "Epoch: 18, Samples: 2944/5760, Loss: 0.05531151592731476\n",
      "Epoch: 18, Samples: 2976/5760, Loss: 0.0505312979221344\n",
      "Epoch: 18, Samples: 3008/5760, Loss: 0.03690934181213379\n",
      "Epoch: 18, Samples: 3040/5760, Loss: 0.03452077507972717\n",
      "Epoch: 18, Samples: 3072/5760, Loss: 0.05414476990699768\n",
      "Epoch: 18, Samples: 3104/5760, Loss: 0.024070531129837036\n",
      "Epoch: 18, Samples: 3136/5760, Loss: 0.02779504656791687\n",
      "Epoch: 18, Samples: 3168/5760, Loss: 0.02395288646221161\n",
      "Epoch: 18, Samples: 3200/5760, Loss: 0.026372089982032776\n",
      "Epoch: 18, Samples: 3232/5760, Loss: 0.056235164403915405\n",
      "Epoch: 18, Samples: 3264/5760, Loss: 0.02841787040233612\n",
      "Epoch: 18, Samples: 3296/5760, Loss: 0.020883947610855103\n",
      "Epoch: 18, Samples: 3328/5760, Loss: 0.04750213027000427\n",
      "Epoch: 18, Samples: 3360/5760, Loss: 0.019605666399002075\n",
      "Epoch: 18, Samples: 3392/5760, Loss: 0.06056065857410431\n",
      "Epoch: 18, Samples: 3424/5760, Loss: 0.031492650508880615\n",
      "Epoch: 18, Samples: 3456/5760, Loss: 0.04778231680393219\n",
      "Epoch: 18, Samples: 3488/5760, Loss: 0.023957759141921997\n",
      "Epoch: 18, Samples: 3520/5760, Loss: 0.030290603637695312\n",
      "Epoch: 18, Samples: 3552/5760, Loss: 0.03377833962440491\n",
      "Epoch: 18, Samples: 3584/5760, Loss: 0.022443920373916626\n",
      "Epoch: 18, Samples: 3616/5760, Loss: 0.02414792776107788\n",
      "Epoch: 18, Samples: 3648/5760, Loss: 0.03304240107536316\n",
      "Epoch: 18, Samples: 3680/5760, Loss: 0.04062439501285553\n",
      "Epoch: 18, Samples: 3712/5760, Loss: 0.04475802183151245\n",
      "Epoch: 18, Samples: 3744/5760, Loss: 0.04696974158287048\n",
      "Epoch: 18, Samples: 3776/5760, Loss: 0.048063039779663086\n",
      "Epoch: 18, Samples: 3808/5760, Loss: 0.040696144104003906\n",
      "Epoch: 18, Samples: 3840/5760, Loss: 0.01802399754524231\n",
      "Epoch: 18, Samples: 3872/5760, Loss: 0.022787362337112427\n",
      "Epoch: 18, Samples: 3904/5760, Loss: 0.042345523834228516\n",
      "Epoch: 18, Samples: 3936/5760, Loss: 0.024559825658798218\n",
      "Epoch: 18, Samples: 3968/5760, Loss: 0.037414997816085815\n",
      "Epoch: 18, Samples: 4000/5760, Loss: 0.03801685571670532\n",
      "Epoch: 18, Samples: 4032/5760, Loss: 0.03151965141296387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Samples: 4064/5760, Loss: 0.028527796268463135\n",
      "Epoch: 18, Samples: 4096/5760, Loss: 0.020578593015670776\n",
      "Epoch: 18, Samples: 4128/5760, Loss: 0.03323553502559662\n",
      "Epoch: 18, Samples: 4160/5760, Loss: 0.03299956023693085\n",
      "Epoch: 18, Samples: 4192/5760, Loss: 0.04093259572982788\n",
      "Epoch: 18, Samples: 4224/5760, Loss: 0.029331296682357788\n",
      "Epoch: 18, Samples: 4256/5760, Loss: 0.027948305010795593\n",
      "Epoch: 18, Samples: 4288/5760, Loss: 0.07087716460227966\n",
      "Epoch: 18, Samples: 4320/5760, Loss: 0.050413742661476135\n",
      "Epoch: 18, Samples: 4352/5760, Loss: 0.019275814294815063\n",
      "Epoch: 18, Samples: 4384/5760, Loss: 0.029808148741722107\n",
      "Epoch: 18, Samples: 4416/5760, Loss: 0.03332352638244629\n",
      "Epoch: 18, Samples: 4448/5760, Loss: 0.03460074961185455\n",
      "Epoch: 18, Samples: 4480/5760, Loss: 0.06024445593357086\n",
      "Epoch: 18, Samples: 4512/5760, Loss: 0.04452751576900482\n",
      "Epoch: 18, Samples: 4544/5760, Loss: 0.06731685996055603\n",
      "Epoch: 18, Samples: 4576/5760, Loss: 0.13017618656158447\n",
      "Epoch: 18, Samples: 4608/5760, Loss: 0.05085505545139313\n",
      "Epoch: 18, Samples: 4640/5760, Loss: 0.05358916521072388\n",
      "Epoch: 18, Samples: 4672/5760, Loss: 0.039679378271102905\n",
      "Epoch: 18, Samples: 4704/5760, Loss: 0.03699353337287903\n",
      "Epoch: 18, Samples: 4736/5760, Loss: 0.05812300741672516\n",
      "Epoch: 18, Samples: 4768/5760, Loss: 0.028165817260742188\n",
      "Epoch: 18, Samples: 4800/5760, Loss: 0.021718919277191162\n",
      "Epoch: 18, Samples: 4832/5760, Loss: 0.08865617215633392\n",
      "Epoch: 18, Samples: 4864/5760, Loss: 0.027161002159118652\n",
      "Epoch: 18, Samples: 4896/5760, Loss: 0.043079957365989685\n",
      "Epoch: 18, Samples: 4928/5760, Loss: 0.029854297637939453\n",
      "Epoch: 18, Samples: 4960/5760, Loss: 0.03390666842460632\n",
      "Epoch: 18, Samples: 4992/5760, Loss: 0.031497254967689514\n",
      "Epoch: 18, Samples: 5024/5760, Loss: 0.040689483284950256\n",
      "Epoch: 18, Samples: 5056/5760, Loss: 0.03638264536857605\n",
      "Epoch: 18, Samples: 5088/5760, Loss: 0.017928510904312134\n",
      "Epoch: 18, Samples: 5120/5760, Loss: 0.02947103977203369\n",
      "Epoch: 18, Samples: 5152/5760, Loss: 0.041697680950164795\n",
      "Epoch: 18, Samples: 5184/5760, Loss: 0.03336210548877716\n",
      "Epoch: 18, Samples: 5216/5760, Loss: 0.052734747529029846\n",
      "Epoch: 18, Samples: 5248/5760, Loss: 0.029334276914596558\n",
      "Epoch: 18, Samples: 5280/5760, Loss: 0.04551221430301666\n",
      "Epoch: 18, Samples: 5312/5760, Loss: 0.03007189929485321\n",
      "Epoch: 18, Samples: 5344/5760, Loss: 0.05792085826396942\n",
      "Epoch: 18, Samples: 5376/5760, Loss: 0.0446496307849884\n",
      "Epoch: 18, Samples: 5408/5760, Loss: 0.032607585191726685\n",
      "Epoch: 18, Samples: 5440/5760, Loss: 0.07329767942428589\n",
      "Epoch: 18, Samples: 5472/5760, Loss: 0.07669232785701752\n",
      "Epoch: 18, Samples: 5504/5760, Loss: 0.033602118492126465\n",
      "Epoch: 18, Samples: 5536/5760, Loss: 0.03568586707115173\n",
      "Epoch: 18, Samples: 5568/5760, Loss: 0.02413785457611084\n",
      "Epoch: 18, Samples: 5600/5760, Loss: 0.04310409724712372\n",
      "Epoch: 18, Samples: 5632/5760, Loss: 0.03332260251045227\n",
      "Epoch: 18, Samples: 5664/5760, Loss: 0.030853748321533203\n",
      "Epoch: 18, Samples: 5696/5760, Loss: 0.028264373540878296\n",
      "Epoch: 18, Samples: 5728/5760, Loss: 0.7593584060668945\n",
      "\n",
      "Epoch: 18\n",
      "Training set: Average loss: 0.0415\n",
      "Validation set: Average loss: 0.3115, Accuracy: 762/818 (93%)\n",
      "Epoch: 19, Samples: 0/5760, Loss: 0.058669477701187134\n",
      "Epoch: 19, Samples: 32/5760, Loss: 0.02637140452861786\n",
      "Epoch: 19, Samples: 64/5760, Loss: 0.046340882778167725\n",
      "Epoch: 19, Samples: 96/5760, Loss: 0.04243454337120056\n",
      "Epoch: 19, Samples: 128/5760, Loss: 0.04048539698123932\n",
      "Epoch: 19, Samples: 160/5760, Loss: 0.017804205417633057\n",
      "Epoch: 19, Samples: 192/5760, Loss: 0.024913698434829712\n",
      "Epoch: 19, Samples: 224/5760, Loss: 0.027964353561401367\n",
      "Epoch: 19, Samples: 256/5760, Loss: 0.08882419764995575\n",
      "Epoch: 19, Samples: 288/5760, Loss: 0.03632234036922455\n",
      "Epoch: 19, Samples: 320/5760, Loss: 0.053394854068756104\n",
      "Epoch: 19, Samples: 352/5760, Loss: 0.03205379843711853\n",
      "Epoch: 19, Samples: 384/5760, Loss: 0.026900559663772583\n",
      "Epoch: 19, Samples: 416/5760, Loss: 0.026282161474227905\n",
      "Epoch: 19, Samples: 448/5760, Loss: 0.03853048384189606\n",
      "Epoch: 19, Samples: 480/5760, Loss: 0.039484962821006775\n",
      "Epoch: 19, Samples: 512/5760, Loss: 0.03920471668243408\n",
      "Epoch: 19, Samples: 544/5760, Loss: 0.04324817657470703\n",
      "Epoch: 19, Samples: 576/5760, Loss: 0.03185504674911499\n",
      "Epoch: 19, Samples: 608/5760, Loss: 0.04624107480049133\n",
      "Epoch: 19, Samples: 640/5760, Loss: 0.04916539788246155\n",
      "Epoch: 19, Samples: 672/5760, Loss: 0.041176557540893555\n",
      "Epoch: 19, Samples: 704/5760, Loss: 0.0327526330947876\n",
      "Epoch: 19, Samples: 736/5760, Loss: 0.0269118994474411\n",
      "Epoch: 19, Samples: 768/5760, Loss: 0.03945896029472351\n",
      "Epoch: 19, Samples: 800/5760, Loss: 0.024750977754592896\n",
      "Epoch: 19, Samples: 832/5760, Loss: 0.060749128460884094\n",
      "Epoch: 19, Samples: 864/5760, Loss: 0.032369405031204224\n",
      "Epoch: 19, Samples: 896/5760, Loss: 0.02115197479724884\n",
      "Epoch: 19, Samples: 928/5760, Loss: 0.03428246080875397\n",
      "Epoch: 19, Samples: 960/5760, Loss: 0.04277978837490082\n",
      "Epoch: 19, Samples: 992/5760, Loss: 0.03415127098560333\n",
      "Epoch: 19, Samples: 1024/5760, Loss: 0.12144064903259277\n",
      "Epoch: 19, Samples: 1056/5760, Loss: 0.021926432847976685\n",
      "Epoch: 19, Samples: 1088/5760, Loss: 0.014342665672302246\n",
      "Epoch: 19, Samples: 1120/5760, Loss: 0.019691526889801025\n",
      "Epoch: 19, Samples: 1152/5760, Loss: 0.03515873849391937\n",
      "Epoch: 19, Samples: 1184/5760, Loss: 0.03260567784309387\n",
      "Epoch: 19, Samples: 1216/5760, Loss: 0.026596277952194214\n",
      "Epoch: 19, Samples: 1248/5760, Loss: 0.03583207726478577\n",
      "Epoch: 19, Samples: 1280/5760, Loss: 0.029073357582092285\n",
      "Epoch: 19, Samples: 1312/5760, Loss: 0.029695779085159302\n",
      "Epoch: 19, Samples: 1344/5760, Loss: 0.03234937787055969\n",
      "Epoch: 19, Samples: 1376/5760, Loss: 0.023949354887008667\n",
      "Epoch: 19, Samples: 1408/5760, Loss: 0.022876113653182983\n",
      "Epoch: 19, Samples: 1440/5760, Loss: 0.05189356207847595\n",
      "Epoch: 19, Samples: 1472/5760, Loss: 0.06537814438343048\n",
      "Epoch: 19, Samples: 1504/5760, Loss: 0.03322756290435791\n",
      "Epoch: 19, Samples: 1536/5760, Loss: 0.03790953755378723\n",
      "Epoch: 19, Samples: 1568/5760, Loss: 0.02882009744644165\n",
      "Epoch: 19, Samples: 1600/5760, Loss: 0.04382975399494171\n",
      "Epoch: 19, Samples: 1632/5760, Loss: 0.022552937269210815\n",
      "Epoch: 19, Samples: 1664/5760, Loss: 0.030167996883392334\n",
      "Epoch: 19, Samples: 1696/5760, Loss: 0.030507922172546387\n",
      "Epoch: 19, Samples: 1728/5760, Loss: 0.028396710753440857\n",
      "Epoch: 19, Samples: 1760/5760, Loss: 0.03404337167739868\n",
      "Epoch: 19, Samples: 1792/5760, Loss: 0.03349126875400543\n",
      "Epoch: 19, Samples: 1824/5760, Loss: 0.03732478618621826\n",
      "Epoch: 19, Samples: 1856/5760, Loss: 0.0330519825220108\n",
      "Epoch: 19, Samples: 1888/5760, Loss: 0.05029001832008362\n",
      "Epoch: 19, Samples: 1920/5760, Loss: 0.04397130012512207\n",
      "Epoch: 19, Samples: 1952/5760, Loss: 0.026127666234970093\n",
      "Epoch: 19, Samples: 1984/5760, Loss: 0.040896713733673096\n",
      "Epoch: 19, Samples: 2016/5760, Loss: 0.02552497386932373\n",
      "Epoch: 19, Samples: 2048/5760, Loss: 0.022293567657470703\n",
      "Epoch: 19, Samples: 2080/5760, Loss: 0.030440181493759155\n",
      "Epoch: 19, Samples: 2112/5760, Loss: 0.04630531370639801\n",
      "Epoch: 19, Samples: 2144/5760, Loss: 0.04500956833362579\n",
      "Epoch: 19, Samples: 2176/5760, Loss: 0.019495457410812378\n",
      "Epoch: 19, Samples: 2208/5760, Loss: 0.02728945016860962\n",
      "Epoch: 19, Samples: 2240/5760, Loss: 0.03139948844909668\n",
      "Epoch: 19, Samples: 2272/5760, Loss: 0.034486204385757446\n",
      "Epoch: 19, Samples: 2304/5760, Loss: 0.03852415084838867\n",
      "Epoch: 19, Samples: 2336/5760, Loss: 0.03584657609462738\n",
      "Epoch: 19, Samples: 2368/5760, Loss: 0.03467436134815216\n",
      "Epoch: 19, Samples: 2400/5760, Loss: 0.011740893125534058\n",
      "Epoch: 19, Samples: 2432/5760, Loss: 0.02837759256362915\n",
      "Epoch: 19, Samples: 2464/5760, Loss: 0.03560441732406616\n",
      "Epoch: 19, Samples: 2496/5760, Loss: 0.04985283315181732\n",
      "Epoch: 19, Samples: 2528/5760, Loss: 0.019457846879959106\n",
      "Epoch: 19, Samples: 2560/5760, Loss: 0.03178510069847107\n",
      "Epoch: 19, Samples: 2592/5760, Loss: 0.03813891112804413\n",
      "Epoch: 19, Samples: 2624/5760, Loss: 0.02978646755218506\n",
      "Epoch: 19, Samples: 2656/5760, Loss: 0.024374887347221375\n",
      "Epoch: 19, Samples: 2688/5760, Loss: 0.02367386221885681\n",
      "Epoch: 19, Samples: 2720/5760, Loss: 0.022857666015625\n",
      "Epoch: 19, Samples: 2752/5760, Loss: 0.03155237436294556\n",
      "Epoch: 19, Samples: 2784/5760, Loss: 0.029639631509780884\n",
      "Epoch: 19, Samples: 2816/5760, Loss: 0.017656952142715454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Samples: 2848/5760, Loss: 0.020212262868881226\n",
      "Epoch: 19, Samples: 2880/5760, Loss: 0.0270683616399765\n",
      "Epoch: 19, Samples: 2912/5760, Loss: 0.027946457266807556\n",
      "Epoch: 19, Samples: 2944/5760, Loss: 0.03344935178756714\n",
      "Epoch: 19, Samples: 2976/5760, Loss: 0.026902049779891968\n",
      "Epoch: 19, Samples: 3008/5760, Loss: 0.04265207052230835\n",
      "Epoch: 19, Samples: 3040/5760, Loss: 0.014928162097930908\n",
      "Epoch: 19, Samples: 3072/5760, Loss: 0.03426527976989746\n",
      "Epoch: 19, Samples: 3104/5760, Loss: 0.056829988956451416\n",
      "Epoch: 19, Samples: 3136/5760, Loss: 0.041836321353912354\n",
      "Epoch: 19, Samples: 3168/5760, Loss: 0.019047409296035767\n",
      "Epoch: 19, Samples: 3200/5760, Loss: 0.03830567002296448\n",
      "Epoch: 19, Samples: 3232/5760, Loss: 0.033528491854667664\n",
      "Epoch: 19, Samples: 3264/5760, Loss: 0.022642850875854492\n",
      "Epoch: 19, Samples: 3296/5760, Loss: 0.021050333976745605\n",
      "Epoch: 19, Samples: 3328/5760, Loss: 0.029899924993515015\n",
      "Epoch: 19, Samples: 3360/5760, Loss: 0.01837041974067688\n",
      "Epoch: 19, Samples: 3392/5760, Loss: 0.07129889726638794\n",
      "Epoch: 19, Samples: 3424/5760, Loss: 0.03788281977176666\n",
      "Epoch: 19, Samples: 3456/5760, Loss: 0.024502068758010864\n",
      "Epoch: 19, Samples: 3488/5760, Loss: 0.013473212718963623\n",
      "Epoch: 19, Samples: 3520/5760, Loss: 0.025801122188568115\n",
      "Epoch: 19, Samples: 3552/5760, Loss: 0.032085955142974854\n",
      "Epoch: 19, Samples: 3584/5760, Loss: 0.026653170585632324\n",
      "Epoch: 19, Samples: 3616/5760, Loss: 0.03393550217151642\n",
      "Epoch: 19, Samples: 3648/5760, Loss: 0.04278804361820221\n",
      "Epoch: 19, Samples: 3680/5760, Loss: 0.015098512172698975\n",
      "Epoch: 19, Samples: 3712/5760, Loss: 0.023498177528381348\n",
      "Epoch: 19, Samples: 3744/5760, Loss: 0.02513548731803894\n",
      "Epoch: 19, Samples: 3776/5760, Loss: 0.020604223012924194\n",
      "Epoch: 19, Samples: 3808/5760, Loss: 0.03620138764381409\n",
      "Epoch: 19, Samples: 3840/5760, Loss: 0.03257787227630615\n",
      "Epoch: 19, Samples: 3872/5760, Loss: 0.03852072358131409\n",
      "Epoch: 19, Samples: 3904/5760, Loss: 0.048093006014823914\n",
      "Epoch: 19, Samples: 3936/5760, Loss: 0.02130994200706482\n",
      "Epoch: 19, Samples: 3968/5760, Loss: 0.023502454161643982\n",
      "Epoch: 19, Samples: 4000/5760, Loss: 0.020594090223312378\n",
      "Epoch: 19, Samples: 4032/5760, Loss: 0.056471630930900574\n",
      "Epoch: 19, Samples: 4064/5760, Loss: 0.051409170031547546\n",
      "Epoch: 19, Samples: 4096/5760, Loss: 0.021145910024642944\n",
      "Epoch: 19, Samples: 4128/5760, Loss: 0.032824188470840454\n",
      "Epoch: 19, Samples: 4160/5760, Loss: 0.022885233163833618\n",
      "Epoch: 19, Samples: 4192/5760, Loss: 0.020763427019119263\n",
      "Epoch: 19, Samples: 4224/5760, Loss: 0.027150243520736694\n",
      "Epoch: 19, Samples: 4256/5760, Loss: 0.02984541654586792\n",
      "Epoch: 19, Samples: 4288/5760, Loss: 0.02912232279777527\n",
      "Epoch: 19, Samples: 4320/5760, Loss: 0.02680230140686035\n",
      "Epoch: 19, Samples: 4352/5760, Loss: 0.05941283702850342\n",
      "Epoch: 19, Samples: 4384/5760, Loss: 0.03777801990509033\n",
      "Epoch: 19, Samples: 4416/5760, Loss: 0.03555856645107269\n",
      "Epoch: 19, Samples: 4448/5760, Loss: 0.025338083505630493\n",
      "Epoch: 19, Samples: 4480/5760, Loss: 0.019203022122383118\n",
      "Epoch: 19, Samples: 4512/5760, Loss: 0.05114910006523132\n",
      "Epoch: 19, Samples: 4544/5760, Loss: 0.05315648019313812\n",
      "Epoch: 19, Samples: 4576/5760, Loss: 0.031074687838554382\n",
      "Epoch: 19, Samples: 4608/5760, Loss: 0.042735159397125244\n",
      "Epoch: 19, Samples: 4640/5760, Loss: 0.019001543521881104\n",
      "Epoch: 19, Samples: 4672/5760, Loss: 0.023947536945343018\n",
      "Epoch: 19, Samples: 4704/5760, Loss: 0.03190869092941284\n",
      "Epoch: 19, Samples: 4736/5760, Loss: 0.020958423614501953\n",
      "Epoch: 19, Samples: 4768/5760, Loss: 0.02662605047225952\n",
      "Epoch: 19, Samples: 4800/5760, Loss: 0.03363791108131409\n",
      "Epoch: 19, Samples: 4832/5760, Loss: 0.029141336679458618\n",
      "Epoch: 19, Samples: 4864/5760, Loss: 0.03975138068199158\n",
      "Epoch: 19, Samples: 4896/5760, Loss: 0.01878136396408081\n",
      "Epoch: 19, Samples: 4928/5760, Loss: 0.03601944446563721\n",
      "Epoch: 19, Samples: 4960/5760, Loss: 0.04811552166938782\n",
      "Epoch: 19, Samples: 4992/5760, Loss: 0.044661298394203186\n",
      "Epoch: 19, Samples: 5024/5760, Loss: 0.031868040561676025\n",
      "Epoch: 19, Samples: 5056/5760, Loss: 0.028453484177589417\n",
      "Epoch: 19, Samples: 5088/5760, Loss: 0.029688790440559387\n",
      "Epoch: 19, Samples: 5120/5760, Loss: 0.05801427364349365\n",
      "Epoch: 19, Samples: 5152/5760, Loss: 0.025030672550201416\n",
      "Epoch: 19, Samples: 5184/5760, Loss: 0.043393462896347046\n",
      "Epoch: 19, Samples: 5216/5760, Loss: 0.03072035312652588\n",
      "Epoch: 19, Samples: 5248/5760, Loss: 0.036360591650009155\n",
      "Epoch: 19, Samples: 5280/5760, Loss: 0.021593868732452393\n",
      "Epoch: 19, Samples: 5312/5760, Loss: 0.029874086380004883\n",
      "Epoch: 19, Samples: 5344/5760, Loss: 0.02938532829284668\n",
      "Epoch: 19, Samples: 5376/5760, Loss: 0.028841450810432434\n",
      "Epoch: 19, Samples: 5408/5760, Loss: 0.03993937373161316\n",
      "Epoch: 19, Samples: 5440/5760, Loss: 0.03972633183002472\n",
      "Epoch: 19, Samples: 5472/5760, Loss: 0.05951900780200958\n",
      "Epoch: 19, Samples: 5504/5760, Loss: 0.019202247262001038\n",
      "Epoch: 19, Samples: 5536/5760, Loss: 0.05122397840023041\n",
      "Epoch: 19, Samples: 5568/5760, Loss: 0.04639674723148346\n",
      "Epoch: 19, Samples: 5600/5760, Loss: 0.029424279928207397\n",
      "Epoch: 19, Samples: 5632/5760, Loss: 0.03734225034713745\n",
      "Epoch: 19, Samples: 5664/5760, Loss: 0.03926543891429901\n",
      "Epoch: 19, Samples: 5696/5760, Loss: 0.025995850563049316\n",
      "Epoch: 19, Samples: 5728/5760, Loss: 0.9921959042549133\n",
      "\n",
      "Epoch: 19\n",
      "Training set: Average loss: 0.0393\n",
      "Validation set: Average loss: 0.3143, Accuracy: 763/818 (93%)\n",
      "Epoch: 20, Samples: 0/5760, Loss: 0.05469153821468353\n",
      "Epoch: 20, Samples: 32/5760, Loss: 0.029787391424179077\n",
      "Epoch: 20, Samples: 64/5760, Loss: 0.03515605628490448\n",
      "Epoch: 20, Samples: 96/5760, Loss: 0.04342035949230194\n",
      "Epoch: 20, Samples: 128/5760, Loss: 0.03391030430793762\n",
      "Epoch: 20, Samples: 160/5760, Loss: 0.026339814066886902\n",
      "Epoch: 20, Samples: 192/5760, Loss: 0.037522509694099426\n",
      "Epoch: 20, Samples: 224/5760, Loss: 0.08100125193595886\n",
      "Epoch: 20, Samples: 256/5760, Loss: 0.03927120566368103\n",
      "Epoch: 20, Samples: 288/5760, Loss: 0.018839120864868164\n",
      "Epoch: 20, Samples: 320/5760, Loss: 0.0355396568775177\n",
      "Epoch: 20, Samples: 352/5760, Loss: 0.047021254897117615\n",
      "Epoch: 20, Samples: 384/5760, Loss: 0.04385752975940704\n",
      "Epoch: 20, Samples: 416/5760, Loss: 0.03300979733467102\n",
      "Epoch: 20, Samples: 448/5760, Loss: 0.02323603630065918\n",
      "Epoch: 20, Samples: 480/5760, Loss: 0.015830576419830322\n",
      "Epoch: 20, Samples: 512/5760, Loss: 0.022099584341049194\n",
      "Epoch: 20, Samples: 544/5760, Loss: 0.028324827551841736\n",
      "Epoch: 20, Samples: 576/5760, Loss: 0.014714330434799194\n",
      "Epoch: 20, Samples: 608/5760, Loss: 0.022091835737228394\n",
      "Epoch: 20, Samples: 640/5760, Loss: 0.03906822204589844\n",
      "Epoch: 20, Samples: 672/5760, Loss: 0.03798004984855652\n",
      "Epoch: 20, Samples: 704/5760, Loss: 0.025657862424850464\n",
      "Epoch: 20, Samples: 736/5760, Loss: 0.02992020547389984\n",
      "Epoch: 20, Samples: 768/5760, Loss: 0.02677607536315918\n",
      "Epoch: 20, Samples: 800/5760, Loss: 0.03834521770477295\n",
      "Epoch: 20, Samples: 832/5760, Loss: 0.019724369049072266\n",
      "Epoch: 20, Samples: 864/5760, Loss: 0.023151636123657227\n",
      "Epoch: 20, Samples: 896/5760, Loss: 0.022132128477096558\n",
      "Epoch: 20, Samples: 928/5760, Loss: 0.033778831362724304\n",
      "Epoch: 20, Samples: 960/5760, Loss: 0.02890324592590332\n",
      "Epoch: 20, Samples: 992/5760, Loss: 0.03178921341896057\n",
      "Epoch: 20, Samples: 1024/5760, Loss: 0.04749704897403717\n",
      "Epoch: 20, Samples: 1056/5760, Loss: 0.025655895471572876\n",
      "Epoch: 20, Samples: 1088/5760, Loss: 0.021139711141586304\n",
      "Epoch: 20, Samples: 1120/5760, Loss: 0.030453473329544067\n",
      "Epoch: 20, Samples: 1152/5760, Loss: 0.03457722067832947\n",
      "Epoch: 20, Samples: 1184/5760, Loss: 0.018738985061645508\n",
      "Epoch: 20, Samples: 1216/5760, Loss: 0.04605431854724884\n",
      "Epoch: 20, Samples: 1248/5760, Loss: 0.024352997541427612\n",
      "Epoch: 20, Samples: 1280/5760, Loss: 0.03645269572734833\n",
      "Epoch: 20, Samples: 1312/5760, Loss: 0.02333730459213257\n",
      "Epoch: 20, Samples: 1344/5760, Loss: 0.03480122983455658\n",
      "Epoch: 20, Samples: 1376/5760, Loss: 0.02757948637008667\n",
      "Epoch: 20, Samples: 1408/5760, Loss: 0.022744104266166687\n",
      "Epoch: 20, Samples: 1440/5760, Loss: 0.024243339896202087\n",
      "Epoch: 20, Samples: 1472/5760, Loss: 0.01907026767730713\n",
      "Epoch: 20, Samples: 1504/5760, Loss: 0.0293969064950943\n",
      "Epoch: 20, Samples: 1536/5760, Loss: 0.02542087435722351\n",
      "Epoch: 20, Samples: 1568/5760, Loss: 0.028884992003440857\n",
      "Epoch: 20, Samples: 1600/5760, Loss: 0.03527155518531799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Samples: 1632/5760, Loss: 0.04571506381034851\n",
      "Epoch: 20, Samples: 1664/5760, Loss: 0.02233409881591797\n",
      "Epoch: 20, Samples: 1696/5760, Loss: 0.019072502851486206\n",
      "Epoch: 20, Samples: 1728/5760, Loss: 0.031488776206970215\n",
      "Epoch: 20, Samples: 1760/5760, Loss: 0.02817915380001068\n",
      "Epoch: 20, Samples: 1792/5760, Loss: 0.03433296084403992\n",
      "Epoch: 20, Samples: 1824/5760, Loss: 0.028728336095809937\n",
      "Epoch: 20, Samples: 1856/5760, Loss: 0.030373916029930115\n",
      "Epoch: 20, Samples: 1888/5760, Loss: 0.020887762308120728\n",
      "Epoch: 20, Samples: 1920/5760, Loss: 0.02297651767730713\n",
      "Epoch: 20, Samples: 1952/5760, Loss: 0.02898237109184265\n",
      "Epoch: 20, Samples: 1984/5760, Loss: 0.02791425585746765\n",
      "Epoch: 20, Samples: 2016/5760, Loss: 0.07603958249092102\n",
      "Epoch: 20, Samples: 2048/5760, Loss: 0.030079960823059082\n",
      "Epoch: 20, Samples: 2080/5760, Loss: 0.021528422832489014\n",
      "Epoch: 20, Samples: 2112/5760, Loss: 0.03312969207763672\n",
      "Epoch: 20, Samples: 2144/5760, Loss: 0.02009078860282898\n",
      "Epoch: 20, Samples: 2176/5760, Loss: 0.05695618689060211\n",
      "Epoch: 20, Samples: 2208/5760, Loss: 0.028859704732894897\n",
      "Epoch: 20, Samples: 2240/5760, Loss: 0.02692355215549469\n",
      "Epoch: 20, Samples: 2272/5760, Loss: 0.0419420450925827\n",
      "Epoch: 20, Samples: 2304/5760, Loss: 0.01824018359184265\n",
      "Epoch: 20, Samples: 2336/5760, Loss: 0.014549285173416138\n",
      "Epoch: 20, Samples: 2368/5760, Loss: 0.03629472851753235\n",
      "Epoch: 20, Samples: 2400/5760, Loss: 0.026899531483650208\n",
      "Epoch: 20, Samples: 2432/5760, Loss: 0.03304961323738098\n",
      "Epoch: 20, Samples: 2464/5760, Loss: 0.028749823570251465\n",
      "Epoch: 20, Samples: 2496/5760, Loss: 0.045159876346588135\n",
      "Epoch: 20, Samples: 2528/5760, Loss: 0.026445329189300537\n",
      "Epoch: 20, Samples: 2560/5760, Loss: 0.03727401793003082\n",
      "Epoch: 20, Samples: 2592/5760, Loss: 0.011446982622146606\n",
      "Epoch: 20, Samples: 2624/5760, Loss: 0.029844343662261963\n",
      "Epoch: 20, Samples: 2656/5760, Loss: 0.017664968967437744\n",
      "Epoch: 20, Samples: 2688/5760, Loss: 0.059204548597335815\n",
      "Epoch: 20, Samples: 2720/5760, Loss: 0.024212822318077087\n",
      "Epoch: 20, Samples: 2752/5760, Loss: 0.04280631244182587\n",
      "Epoch: 20, Samples: 2784/5760, Loss: 0.01737341284751892\n",
      "Epoch: 20, Samples: 2816/5760, Loss: 0.01977744698524475\n",
      "Epoch: 20, Samples: 2848/5760, Loss: 0.021476760506629944\n",
      "Epoch: 20, Samples: 2880/5760, Loss: 0.029757648706436157\n",
      "Epoch: 20, Samples: 2912/5760, Loss: 0.013464123010635376\n",
      "Epoch: 20, Samples: 2944/5760, Loss: 0.024552851915359497\n",
      "Epoch: 20, Samples: 2976/5760, Loss: 0.07682299613952637\n",
      "Epoch: 20, Samples: 3008/5760, Loss: 0.02648356556892395\n",
      "Epoch: 20, Samples: 3040/5760, Loss: 0.0182245671749115\n",
      "Epoch: 20, Samples: 3072/5760, Loss: 0.02002677321434021\n",
      "Epoch: 20, Samples: 3104/5760, Loss: 0.0213736891746521\n",
      "Epoch: 20, Samples: 3136/5760, Loss: 0.022335439920425415\n",
      "Epoch: 20, Samples: 3168/5760, Loss: 0.030555590987205505\n",
      "Epoch: 20, Samples: 3200/5760, Loss: 0.047839611768722534\n",
      "Epoch: 20, Samples: 3232/5760, Loss: 0.04683578014373779\n",
      "Epoch: 20, Samples: 3264/5760, Loss: 0.04176910221576691\n",
      "Epoch: 20, Samples: 3296/5760, Loss: 0.023872867226600647\n",
      "Epoch: 20, Samples: 3328/5760, Loss: 0.023806393146514893\n",
      "Epoch: 20, Samples: 3360/5760, Loss: 0.0524282306432724\n",
      "Epoch: 20, Samples: 3392/5760, Loss: 0.0327301025390625\n",
      "Epoch: 20, Samples: 3424/5760, Loss: 0.028160706162452698\n",
      "Epoch: 20, Samples: 3456/5760, Loss: 0.022138744592666626\n",
      "Epoch: 20, Samples: 3488/5760, Loss: 0.047178804874420166\n",
      "Epoch: 20, Samples: 3520/5760, Loss: 0.030640989542007446\n",
      "Epoch: 20, Samples: 3552/5760, Loss: 0.07548512518405914\n",
      "Epoch: 20, Samples: 3584/5760, Loss: 0.017051726579666138\n",
      "Epoch: 20, Samples: 3616/5760, Loss: 0.04719403386116028\n",
      "Epoch: 20, Samples: 3648/5760, Loss: 0.03075164556503296\n",
      "Epoch: 20, Samples: 3680/5760, Loss: 0.029997870326042175\n",
      "Epoch: 20, Samples: 3712/5760, Loss: 0.015199452638626099\n",
      "Epoch: 20, Samples: 3744/5760, Loss: 0.05113060772418976\n",
      "Epoch: 20, Samples: 3776/5760, Loss: 0.04359598457813263\n",
      "Epoch: 20, Samples: 3808/5760, Loss: 0.0513455867767334\n",
      "Epoch: 20, Samples: 3840/5760, Loss: 0.0435875803232193\n",
      "Epoch: 20, Samples: 3872/5760, Loss: 0.031423017382621765\n",
      "Epoch: 20, Samples: 3904/5760, Loss: 0.03433161973953247\n",
      "Epoch: 20, Samples: 3936/5760, Loss: 0.030435577034950256\n",
      "Epoch: 20, Samples: 3968/5760, Loss: 0.03839275240898132\n",
      "Epoch: 20, Samples: 4000/5760, Loss: 0.03366926312446594\n",
      "Epoch: 20, Samples: 4032/5760, Loss: 0.06584516167640686\n",
      "Epoch: 20, Samples: 4064/5760, Loss: 0.04487413167953491\n",
      "Epoch: 20, Samples: 4096/5760, Loss: 0.029197782278060913\n",
      "Epoch: 20, Samples: 4128/5760, Loss: 0.028879791498184204\n",
      "Epoch: 20, Samples: 4160/5760, Loss: 0.025918960571289062\n",
      "Epoch: 20, Samples: 4192/5760, Loss: 0.025375962257385254\n",
      "Epoch: 20, Samples: 4224/5760, Loss: 0.03409618139266968\n",
      "Epoch: 20, Samples: 4256/5760, Loss: 0.04151831567287445\n",
      "Epoch: 20, Samples: 4288/5760, Loss: 0.020007789134979248\n",
      "Epoch: 20, Samples: 4320/5760, Loss: 0.05253581702709198\n",
      "Epoch: 20, Samples: 4352/5760, Loss: 0.036383867263793945\n",
      "Epoch: 20, Samples: 4384/5760, Loss: 0.03299063444137573\n",
      "Epoch: 20, Samples: 4416/5760, Loss: 0.018398404121398926\n",
      "Epoch: 20, Samples: 4448/5760, Loss: 0.021670222282409668\n",
      "Epoch: 20, Samples: 4480/5760, Loss: 0.03553840517997742\n",
      "Epoch: 20, Samples: 4512/5760, Loss: 0.024597346782684326\n",
      "Epoch: 20, Samples: 4544/5760, Loss: 0.022585421800613403\n",
      "Epoch: 20, Samples: 4576/5760, Loss: 0.03269793093204498\n",
      "Epoch: 20, Samples: 4608/5760, Loss: 0.029550015926361084\n",
      "Epoch: 20, Samples: 4640/5760, Loss: 0.02989424765110016\n",
      "Epoch: 20, Samples: 4672/5760, Loss: 0.02176600694656372\n",
      "Epoch: 20, Samples: 4704/5760, Loss: 0.03338991105556488\n",
      "Epoch: 20, Samples: 4736/5760, Loss: 0.02755376696586609\n",
      "Epoch: 20, Samples: 4768/5760, Loss: 0.048651814460754395\n",
      "Epoch: 20, Samples: 4800/5760, Loss: 0.04467669129371643\n",
      "Epoch: 20, Samples: 4832/5760, Loss: 0.02959837019443512\n",
      "Epoch: 20, Samples: 4864/5760, Loss: 0.029756292700767517\n",
      "Epoch: 20, Samples: 4896/5760, Loss: 0.016575336456298828\n",
      "Epoch: 20, Samples: 4928/5760, Loss: 0.023865342140197754\n",
      "Epoch: 20, Samples: 4960/5760, Loss: 0.02038976550102234\n",
      "Epoch: 20, Samples: 4992/5760, Loss: 0.027342364192008972\n",
      "Epoch: 20, Samples: 5024/5760, Loss: 0.035407453775405884\n",
      "Epoch: 20, Samples: 5056/5760, Loss: 0.023641735315322876\n",
      "Epoch: 20, Samples: 5088/5760, Loss: 0.0234241783618927\n",
      "Epoch: 20, Samples: 5120/5760, Loss: 0.02223023772239685\n",
      "Epoch: 20, Samples: 5152/5760, Loss: 0.04192499816417694\n",
      "Epoch: 20, Samples: 5184/5760, Loss: 0.027740269899368286\n",
      "Epoch: 20, Samples: 5216/5760, Loss: 0.027245089411735535\n",
      "Epoch: 20, Samples: 5248/5760, Loss: 0.03763715922832489\n",
      "Epoch: 20, Samples: 5280/5760, Loss: 0.025604188442230225\n",
      "Epoch: 20, Samples: 5312/5760, Loss: 0.02745148539543152\n",
      "Epoch: 20, Samples: 5344/5760, Loss: 0.026554346084594727\n",
      "Epoch: 20, Samples: 5376/5760, Loss: 0.028593972325325012\n",
      "Epoch: 20, Samples: 5408/5760, Loss: 0.019804060459136963\n",
      "Epoch: 20, Samples: 5440/5760, Loss: 0.10217821598052979\n",
      "Epoch: 20, Samples: 5472/5760, Loss: 0.02737012505531311\n",
      "Epoch: 20, Samples: 5504/5760, Loss: 0.027917534112930298\n",
      "Epoch: 20, Samples: 5536/5760, Loss: 0.029947638511657715\n",
      "Epoch: 20, Samples: 5568/5760, Loss: 0.025834426283836365\n",
      "Epoch: 20, Samples: 5600/5760, Loss: 0.021983027458190918\n",
      "Epoch: 20, Samples: 5632/5760, Loss: 0.020427316427230835\n",
      "Epoch: 20, Samples: 5664/5760, Loss: 0.03213036060333252\n",
      "Epoch: 20, Samples: 5696/5760, Loss: 0.022264212369918823\n",
      "Epoch: 20, Samples: 5728/5760, Loss: 0.2601398229598999\n",
      "\n",
      "Epoch: 20\n",
      "Training set: Average loss: 0.0331\n",
      "Validation set: Average loss: 0.2972, Accuracy: 761/818 (93%)\n",
      "Saving model (epoch 20) with lowest validation loss: 0.29718591100894487\n",
      "Epoch: 21, Samples: 0/5760, Loss: 0.020346909761428833\n",
      "Epoch: 21, Samples: 32/5760, Loss: 0.026919066905975342\n",
      "Epoch: 21, Samples: 64/5760, Loss: 0.023449867963790894\n",
      "Epoch: 21, Samples: 96/5760, Loss: 0.018118739128112793\n",
      "Epoch: 21, Samples: 128/5760, Loss: 0.06835086643695831\n",
      "Epoch: 21, Samples: 160/5760, Loss: 0.018481582403182983\n",
      "Epoch: 21, Samples: 192/5760, Loss: 0.022760778665542603\n",
      "Epoch: 21, Samples: 224/5760, Loss: 0.014973640441894531\n",
      "Epoch: 21, Samples: 256/5760, Loss: 0.014462381601333618\n",
      "Epoch: 21, Samples: 288/5760, Loss: 0.01851499080657959\n",
      "Epoch: 21, Samples: 320/5760, Loss: 0.030099838972091675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Samples: 352/5760, Loss: 0.044028282165527344\n",
      "Epoch: 21, Samples: 384/5760, Loss: 0.020819932222366333\n",
      "Epoch: 21, Samples: 416/5760, Loss: 0.027716845273971558\n",
      "Epoch: 21, Samples: 448/5760, Loss: 0.022679954767227173\n",
      "Epoch: 21, Samples: 480/5760, Loss: 0.06054069101810455\n",
      "Epoch: 21, Samples: 512/5760, Loss: 0.03257867693901062\n",
      "Epoch: 21, Samples: 544/5760, Loss: 0.05241642892360687\n",
      "Epoch: 21, Samples: 576/5760, Loss: 0.026818007230758667\n",
      "Epoch: 21, Samples: 608/5760, Loss: 0.036577463150024414\n",
      "Epoch: 21, Samples: 640/5760, Loss: 0.019119590520858765\n",
      "Epoch: 21, Samples: 672/5760, Loss: 0.04134856164455414\n",
      "Epoch: 21, Samples: 704/5760, Loss: 0.022400885820388794\n",
      "Epoch: 21, Samples: 736/5760, Loss: 0.01857846975326538\n",
      "Epoch: 21, Samples: 768/5760, Loss: 0.02710968255996704\n",
      "Epoch: 21, Samples: 800/5760, Loss: 0.018553614616394043\n",
      "Epoch: 21, Samples: 832/5760, Loss: 0.030256330966949463\n",
      "Epoch: 21, Samples: 864/5760, Loss: 0.015972018241882324\n",
      "Epoch: 21, Samples: 896/5760, Loss: 0.01468542218208313\n",
      "Epoch: 21, Samples: 928/5760, Loss: 0.03250914812088013\n",
      "Epoch: 21, Samples: 960/5760, Loss: 0.02551761269569397\n",
      "Epoch: 21, Samples: 992/5760, Loss: 0.045119643211364746\n",
      "Epoch: 21, Samples: 1024/5760, Loss: 0.01989901065826416\n",
      "Epoch: 21, Samples: 1056/5760, Loss: 0.03555577993392944\n",
      "Epoch: 21, Samples: 1088/5760, Loss: 0.02487659454345703\n",
      "Epoch: 21, Samples: 1120/5760, Loss: 0.02062171697616577\n",
      "Epoch: 21, Samples: 1152/5760, Loss: 0.04412128031253815\n",
      "Epoch: 21, Samples: 1184/5760, Loss: 0.06806699931621552\n",
      "Epoch: 21, Samples: 1216/5760, Loss: 0.04113830626010895\n",
      "Epoch: 21, Samples: 1248/5760, Loss: 0.042589232325553894\n",
      "Epoch: 21, Samples: 1280/5760, Loss: 0.030095919966697693\n",
      "Epoch: 21, Samples: 1312/5760, Loss: 0.019413799047470093\n",
      "Epoch: 21, Samples: 1344/5760, Loss: 0.03068596124649048\n",
      "Epoch: 21, Samples: 1376/5760, Loss: 0.019263118505477905\n",
      "Epoch: 21, Samples: 1408/5760, Loss: 0.01903170347213745\n",
      "Epoch: 21, Samples: 1440/5760, Loss: 0.02287408709526062\n",
      "Epoch: 21, Samples: 1472/5760, Loss: 0.030913829803466797\n",
      "Epoch: 21, Samples: 1504/5760, Loss: 0.023152470588684082\n",
      "Epoch: 21, Samples: 1536/5760, Loss: 0.025673478841781616\n",
      "Epoch: 21, Samples: 1568/5760, Loss: 0.017895400524139404\n",
      "Epoch: 21, Samples: 1600/5760, Loss: 0.022560715675354004\n",
      "Epoch: 21, Samples: 1632/5760, Loss: 0.028776049613952637\n",
      "Epoch: 21, Samples: 1664/5760, Loss: 0.030911773443222046\n",
      "Epoch: 21, Samples: 1696/5760, Loss: 0.028697237372398376\n",
      "Epoch: 21, Samples: 1728/5760, Loss: 0.029339730739593506\n",
      "Epoch: 21, Samples: 1760/5760, Loss: 0.02110007405281067\n",
      "Epoch: 21, Samples: 1792/5760, Loss: 0.021802902221679688\n",
      "Epoch: 21, Samples: 1824/5760, Loss: 0.02240583300590515\n",
      "Epoch: 21, Samples: 1856/5760, Loss: 0.019917160272598267\n",
      "Epoch: 21, Samples: 1888/5760, Loss: 0.05529853701591492\n",
      "Epoch: 21, Samples: 1920/5760, Loss: 0.01710444688796997\n",
      "Epoch: 21, Samples: 1952/5760, Loss: 0.015411317348480225\n",
      "Epoch: 21, Samples: 1984/5760, Loss: 0.03105778992176056\n",
      "Epoch: 21, Samples: 2016/5760, Loss: 0.03126266598701477\n",
      "Epoch: 21, Samples: 2048/5760, Loss: 0.04137443006038666\n",
      "Epoch: 21, Samples: 2080/5760, Loss: 0.016950547695159912\n",
      "Epoch: 21, Samples: 2112/5760, Loss: 0.022108882665634155\n",
      "Epoch: 21, Samples: 2144/5760, Loss: 0.026569679379463196\n",
      "Epoch: 21, Samples: 2176/5760, Loss: 0.015898704528808594\n",
      "Epoch: 21, Samples: 2208/5760, Loss: 0.024988502264022827\n",
      "Epoch: 21, Samples: 2240/5760, Loss: 0.0153733491897583\n",
      "Epoch: 21, Samples: 2272/5760, Loss: 0.014042973518371582\n",
      "Epoch: 21, Samples: 2304/5760, Loss: 0.02908441424369812\n",
      "Epoch: 21, Samples: 2336/5760, Loss: 0.02224862575531006\n",
      "Epoch: 21, Samples: 2368/5760, Loss: 0.061896130442619324\n",
      "Epoch: 21, Samples: 2400/5760, Loss: 0.01811310648918152\n",
      "Epoch: 21, Samples: 2432/5760, Loss: 0.024196535348892212\n",
      "Epoch: 21, Samples: 2464/5760, Loss: 0.012228220701217651\n",
      "Epoch: 21, Samples: 2496/5760, Loss: 0.03008091449737549\n",
      "Epoch: 21, Samples: 2528/5760, Loss: 0.022745609283447266\n",
      "Epoch: 21, Samples: 2560/5760, Loss: 0.03361138701438904\n",
      "Epoch: 21, Samples: 2592/5760, Loss: 0.023338526487350464\n",
      "Epoch: 21, Samples: 2624/5760, Loss: 0.03939260542392731\n",
      "Epoch: 21, Samples: 2656/5760, Loss: 0.027013912796974182\n",
      "Epoch: 21, Samples: 2688/5760, Loss: 0.023787200450897217\n",
      "Epoch: 21, Samples: 2720/5760, Loss: 0.032211244106292725\n",
      "Epoch: 21, Samples: 2752/5760, Loss: 0.023886382579803467\n",
      "Epoch: 21, Samples: 2784/5760, Loss: 0.03970655798912048\n",
      "Epoch: 21, Samples: 2816/5760, Loss: 0.019423335790634155\n",
      "Epoch: 21, Samples: 2848/5760, Loss: 0.03029569983482361\n",
      "Epoch: 21, Samples: 2880/5760, Loss: 0.024423271417617798\n",
      "Epoch: 21, Samples: 2912/5760, Loss: 0.0512567013502121\n",
      "Epoch: 21, Samples: 2944/5760, Loss: 0.026315510272979736\n",
      "Epoch: 21, Samples: 2976/5760, Loss: 0.03129775822162628\n",
      "Epoch: 21, Samples: 3008/5760, Loss: 0.04057525098323822\n",
      "Epoch: 21, Samples: 3040/5760, Loss: 0.0838744044303894\n",
      "Epoch: 21, Samples: 3072/5760, Loss: 0.020777493715286255\n",
      "Epoch: 21, Samples: 3104/5760, Loss: 0.03326897323131561\n",
      "Epoch: 21, Samples: 3136/5760, Loss: 0.04449053108692169\n",
      "Epoch: 21, Samples: 3168/5760, Loss: 0.018693089485168457\n",
      "Epoch: 21, Samples: 3200/5760, Loss: 0.03079041838645935\n",
      "Epoch: 21, Samples: 3232/5760, Loss: 0.027566328644752502\n",
      "Epoch: 21, Samples: 3264/5760, Loss: 0.016032397747039795\n",
      "Epoch: 21, Samples: 3296/5760, Loss: 0.049508363008499146\n",
      "Epoch: 21, Samples: 3328/5760, Loss: 0.020755663514137268\n",
      "Epoch: 21, Samples: 3360/5760, Loss: 0.05369940400123596\n",
      "Epoch: 21, Samples: 3392/5760, Loss: 0.0390302836894989\n",
      "Epoch: 21, Samples: 3424/5760, Loss: 0.01428273320198059\n",
      "Epoch: 21, Samples: 3456/5760, Loss: 0.024529367685317993\n",
      "Epoch: 21, Samples: 3488/5760, Loss: 0.01872965693473816\n",
      "Epoch: 21, Samples: 3520/5760, Loss: 0.03444729745388031\n",
      "Epoch: 21, Samples: 3552/5760, Loss: 0.09461233019828796\n",
      "Epoch: 21, Samples: 3584/5760, Loss: 0.020174682140350342\n",
      "Epoch: 21, Samples: 3616/5760, Loss: 0.03261104226112366\n",
      "Epoch: 21, Samples: 3648/5760, Loss: 0.014821618795394897\n",
      "Epoch: 21, Samples: 3680/5760, Loss: 0.020051956176757812\n",
      "Epoch: 21, Samples: 3712/5760, Loss: 0.019845277070999146\n",
      "Epoch: 21, Samples: 3744/5760, Loss: 0.024082481861114502\n",
      "Epoch: 21, Samples: 3776/5760, Loss: 0.0471084862947464\n",
      "Epoch: 21, Samples: 3808/5760, Loss: 0.014616280794143677\n",
      "Epoch: 21, Samples: 3840/5760, Loss: 0.04884481430053711\n",
      "Epoch: 21, Samples: 3872/5760, Loss: 0.0312628448009491\n",
      "Epoch: 21, Samples: 3904/5760, Loss: 0.01450878381729126\n",
      "Epoch: 21, Samples: 3936/5760, Loss: 0.016020983457565308\n",
      "Epoch: 21, Samples: 3968/5760, Loss: 0.034544438123703\n",
      "Epoch: 21, Samples: 4000/5760, Loss: 0.019439727067947388\n",
      "Epoch: 21, Samples: 4032/5760, Loss: 0.017256200313568115\n",
      "Epoch: 21, Samples: 4064/5760, Loss: 0.0365125834941864\n",
      "Epoch: 21, Samples: 4096/5760, Loss: 0.030576497316360474\n",
      "Epoch: 21, Samples: 4128/5760, Loss: 0.026289671659469604\n",
      "Epoch: 21, Samples: 4160/5760, Loss: 0.032856300473213196\n",
      "Epoch: 21, Samples: 4192/5760, Loss: 0.03826223313808441\n",
      "Epoch: 21, Samples: 4224/5760, Loss: 0.03563685715198517\n",
      "Epoch: 21, Samples: 4256/5760, Loss: 0.021491914987564087\n",
      "Epoch: 21, Samples: 4288/5760, Loss: 0.02648305892944336\n",
      "Epoch: 21, Samples: 4320/5760, Loss: 0.02353966236114502\n",
      "Epoch: 21, Samples: 4352/5760, Loss: 0.014028221368789673\n",
      "Epoch: 21, Samples: 4384/5760, Loss: 0.018918782472610474\n",
      "Epoch: 21, Samples: 4416/5760, Loss: 0.01644057035446167\n",
      "Epoch: 21, Samples: 4448/5760, Loss: 0.017752230167388916\n",
      "Epoch: 21, Samples: 4480/5760, Loss: 0.031747132539749146\n",
      "Epoch: 21, Samples: 4512/5760, Loss: 0.01632791757583618\n",
      "Epoch: 21, Samples: 4544/5760, Loss: 0.024387717247009277\n",
      "Epoch: 21, Samples: 4576/5760, Loss: 0.029017001390457153\n",
      "Epoch: 21, Samples: 4608/5760, Loss: 0.04339253902435303\n",
      "Epoch: 21, Samples: 4640/5760, Loss: 0.02765655517578125\n",
      "Epoch: 21, Samples: 4672/5760, Loss: 0.034373193979263306\n",
      "Epoch: 21, Samples: 4704/5760, Loss: 0.08845376968383789\n",
      "Epoch: 21, Samples: 4736/5760, Loss: 0.014629662036895752\n",
      "Epoch: 21, Samples: 4768/5760, Loss: 0.029504388570785522\n",
      "Epoch: 21, Samples: 4800/5760, Loss: 0.021463394165039062\n",
      "Epoch: 21, Samples: 4832/5760, Loss: 0.014912784099578857\n",
      "Epoch: 21, Samples: 4864/5760, Loss: 0.015482127666473389\n",
      "Epoch: 21, Samples: 4896/5760, Loss: 0.02307194471359253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Samples: 4928/5760, Loss: 0.04478956758975983\n",
      "Epoch: 21, Samples: 4960/5760, Loss: 0.0305631160736084\n",
      "Epoch: 21, Samples: 4992/5760, Loss: 0.023786097764968872\n",
      "Epoch: 21, Samples: 5024/5760, Loss: 0.02300424873828888\n",
      "Epoch: 21, Samples: 5056/5760, Loss: 0.01776394248008728\n",
      "Epoch: 21, Samples: 5088/5760, Loss: 0.015820831060409546\n",
      "Epoch: 21, Samples: 5120/5760, Loss: 0.011847883462905884\n",
      "Epoch: 21, Samples: 5152/5760, Loss: 0.0374755859375\n",
      "Epoch: 21, Samples: 5184/5760, Loss: 0.033195674419403076\n",
      "Epoch: 21, Samples: 5216/5760, Loss: 0.021608591079711914\n",
      "Epoch: 21, Samples: 5248/5760, Loss: 0.03347472846508026\n",
      "Epoch: 21, Samples: 5280/5760, Loss: 0.015185713768005371\n",
      "Epoch: 21, Samples: 5312/5760, Loss: 0.03521159291267395\n",
      "Epoch: 21, Samples: 5344/5760, Loss: 0.03396287560462952\n",
      "Epoch: 21, Samples: 5376/5760, Loss: 0.03296840190887451\n",
      "Epoch: 21, Samples: 5408/5760, Loss: 0.026222527027130127\n",
      "Epoch: 21, Samples: 5440/5760, Loss: 0.016861826181411743\n",
      "Epoch: 21, Samples: 5472/5760, Loss: 0.023977994918823242\n",
      "Epoch: 21, Samples: 5504/5760, Loss: 0.016001015901565552\n",
      "Epoch: 21, Samples: 5536/5760, Loss: 0.024045437574386597\n",
      "Epoch: 21, Samples: 5568/5760, Loss: 0.029326945543289185\n",
      "Epoch: 21, Samples: 5600/5760, Loss: 0.026479482650756836\n",
      "Epoch: 21, Samples: 5632/5760, Loss: 0.06827503442764282\n",
      "Epoch: 21, Samples: 5664/5760, Loss: 0.024513959884643555\n",
      "Epoch: 21, Samples: 5696/5760, Loss: 0.028246194124221802\n",
      "Epoch: 21, Samples: 5728/5760, Loss: 1.568315029144287\n",
      "\n",
      "Epoch: 21\n",
      "Training set: Average loss: 0.0372\n",
      "Validation set: Average loss: 0.3030, Accuracy: 761/818 (93%)\n",
      "Epoch: 22, Samples: 0/5760, Loss: 0.0260789692401886\n",
      "Epoch: 22, Samples: 32/5760, Loss: 0.04371406137943268\n",
      "Epoch: 22, Samples: 64/5760, Loss: 0.03186136484146118\n",
      "Epoch: 22, Samples: 96/5760, Loss: 0.030758455395698547\n",
      "Epoch: 22, Samples: 128/5760, Loss: 0.022436976432800293\n",
      "Epoch: 22, Samples: 160/5760, Loss: 0.01543569564819336\n",
      "Epoch: 22, Samples: 192/5760, Loss: 0.05428507924079895\n",
      "Epoch: 22, Samples: 224/5760, Loss: 0.037523508071899414\n",
      "Epoch: 22, Samples: 256/5760, Loss: 0.031712859869003296\n",
      "Epoch: 22, Samples: 288/5760, Loss: 0.0340893417596817\n",
      "Epoch: 22, Samples: 320/5760, Loss: 0.031388670206069946\n",
      "Epoch: 22, Samples: 352/5760, Loss: 0.029417067766189575\n",
      "Epoch: 22, Samples: 384/5760, Loss: 0.028088003396987915\n",
      "Epoch: 22, Samples: 416/5760, Loss: 0.027303427457809448\n",
      "Epoch: 22, Samples: 448/5760, Loss: 0.04856206476688385\n",
      "Epoch: 22, Samples: 480/5760, Loss: 0.023373156785964966\n",
      "Epoch: 22, Samples: 512/5760, Loss: 0.028828755021095276\n",
      "Epoch: 22, Samples: 544/5760, Loss: 0.051044583320617676\n",
      "Epoch: 22, Samples: 576/5760, Loss: 0.02599504590034485\n",
      "Epoch: 22, Samples: 608/5760, Loss: 0.026800870895385742\n",
      "Epoch: 22, Samples: 640/5760, Loss: 0.025453343987464905\n",
      "Epoch: 22, Samples: 672/5760, Loss: 0.025050491094589233\n",
      "Epoch: 22, Samples: 704/5760, Loss: 0.053625524044036865\n",
      "Epoch: 22, Samples: 736/5760, Loss: 0.02816307544708252\n",
      "Epoch: 22, Samples: 768/5760, Loss: 0.016718000173568726\n",
      "Epoch: 22, Samples: 800/5760, Loss: 0.018637627363204956\n",
      "Epoch: 22, Samples: 832/5760, Loss: 0.03772719204425812\n",
      "Epoch: 22, Samples: 864/5760, Loss: 0.019261091947555542\n",
      "Epoch: 22, Samples: 896/5760, Loss: 0.020429998636245728\n",
      "Epoch: 22, Samples: 928/5760, Loss: 0.043244361877441406\n",
      "Epoch: 22, Samples: 960/5760, Loss: 0.042668551206588745\n",
      "Epoch: 22, Samples: 992/5760, Loss: 0.06637462973594666\n",
      "Epoch: 22, Samples: 1024/5760, Loss: 0.025703132152557373\n",
      "Epoch: 22, Samples: 1056/5760, Loss: 0.032227009534835815\n",
      "Epoch: 22, Samples: 1088/5760, Loss: 0.027644261717796326\n",
      "Epoch: 22, Samples: 1120/5760, Loss: 0.027010232210159302\n",
      "Epoch: 22, Samples: 1152/5760, Loss: 0.016377002000808716\n",
      "Epoch: 22, Samples: 1184/5760, Loss: 0.043902769684791565\n",
      "Epoch: 22, Samples: 1216/5760, Loss: 0.020015567541122437\n",
      "Epoch: 22, Samples: 1248/5760, Loss: 0.11086635291576385\n",
      "Epoch: 22, Samples: 1280/5760, Loss: 0.025936365127563477\n",
      "Epoch: 22, Samples: 1312/5760, Loss: 0.08791182935237885\n",
      "Epoch: 22, Samples: 1344/5760, Loss: 0.028474420309066772\n",
      "Epoch: 22, Samples: 1376/5760, Loss: 0.026300519704818726\n",
      "Epoch: 22, Samples: 1408/5760, Loss: 0.030731797218322754\n",
      "Epoch: 22, Samples: 1440/5760, Loss: 0.026063382625579834\n",
      "Epoch: 22, Samples: 1472/5760, Loss: 0.03427956998348236\n",
      "Epoch: 22, Samples: 1504/5760, Loss: 0.02456754446029663\n",
      "Epoch: 22, Samples: 1536/5760, Loss: 0.026374131441116333\n",
      "Epoch: 22, Samples: 1568/5760, Loss: 0.039634525775909424\n",
      "Epoch: 22, Samples: 1600/5760, Loss: 0.03290140628814697\n",
      "Epoch: 22, Samples: 1632/5760, Loss: 0.025391623377799988\n",
      "Epoch: 22, Samples: 1664/5760, Loss: 0.02759203314781189\n",
      "Epoch: 22, Samples: 1696/5760, Loss: 0.035513997077941895\n",
      "Epoch: 22, Samples: 1728/5760, Loss: 0.04141765832901001\n",
      "Epoch: 22, Samples: 1760/5760, Loss: 0.049015969038009644\n",
      "Epoch: 22, Samples: 1792/5760, Loss: 0.044671863317489624\n",
      "Epoch: 22, Samples: 1824/5760, Loss: 0.04109291732311249\n",
      "Epoch: 22, Samples: 1856/5760, Loss: 0.032145097851753235\n",
      "Epoch: 22, Samples: 1888/5760, Loss: 0.012304455041885376\n",
      "Epoch: 22, Samples: 1920/5760, Loss: 0.041635990142822266\n",
      "Epoch: 22, Samples: 1952/5760, Loss: 0.01784387230873108\n",
      "Epoch: 22, Samples: 1984/5760, Loss: 0.026214897632598877\n",
      "Epoch: 22, Samples: 2016/5760, Loss: 0.0449872761964798\n",
      "Epoch: 22, Samples: 2048/5760, Loss: 0.02419823408126831\n",
      "Epoch: 22, Samples: 2080/5760, Loss: 0.02746918797492981\n",
      "Epoch: 22, Samples: 2112/5760, Loss: 0.02069072425365448\n",
      "Epoch: 22, Samples: 2144/5760, Loss: 0.027829617261886597\n",
      "Epoch: 22, Samples: 2176/5760, Loss: 0.025218293070793152\n",
      "Epoch: 22, Samples: 2208/5760, Loss: 0.028077661991119385\n",
      "Epoch: 22, Samples: 2240/5760, Loss: 0.0262182354927063\n",
      "Epoch: 22, Samples: 2272/5760, Loss: 0.017086178064346313\n",
      "Epoch: 22, Samples: 2304/5760, Loss: 0.01953980326652527\n",
      "Epoch: 22, Samples: 2336/5760, Loss: 0.030281618237495422\n",
      "Epoch: 22, Samples: 2368/5760, Loss: 0.02245834469795227\n",
      "Epoch: 22, Samples: 2400/5760, Loss: 0.02072635293006897\n",
      "Epoch: 22, Samples: 2432/5760, Loss: 0.03673727810382843\n",
      "Epoch: 22, Samples: 2464/5760, Loss: 0.02159750461578369\n",
      "Epoch: 22, Samples: 2496/5760, Loss: 0.04273737967014313\n",
      "Epoch: 22, Samples: 2528/5760, Loss: 0.021709799766540527\n",
      "Epoch: 22, Samples: 2560/5760, Loss: 0.03378969430923462\n",
      "Epoch: 22, Samples: 2592/5760, Loss: 0.03562316298484802\n",
      "Epoch: 22, Samples: 2624/5760, Loss: 0.02858436107635498\n",
      "Epoch: 22, Samples: 2656/5760, Loss: 0.014337033033370972\n",
      "Epoch: 22, Samples: 2688/5760, Loss: 0.023612305521965027\n",
      "Epoch: 22, Samples: 2720/5760, Loss: 0.030652672052383423\n",
      "Epoch: 22, Samples: 2752/5760, Loss: 0.037209272384643555\n",
      "Epoch: 22, Samples: 2784/5760, Loss: 0.01999950408935547\n",
      "Epoch: 22, Samples: 2816/5760, Loss: 0.02829010784626007\n",
      "Epoch: 22, Samples: 2848/5760, Loss: 0.01784375309944153\n",
      "Epoch: 22, Samples: 2880/5760, Loss: 0.025165528059005737\n",
      "Epoch: 22, Samples: 2912/5760, Loss: 0.02774474024772644\n",
      "Epoch: 22, Samples: 2944/5760, Loss: 0.012373000383377075\n",
      "Epoch: 22, Samples: 2976/5760, Loss: 0.01873767375946045\n",
      "Epoch: 22, Samples: 3008/5760, Loss: 0.025161609053611755\n",
      "Epoch: 22, Samples: 3040/5760, Loss: 0.02444632351398468\n",
      "Epoch: 22, Samples: 3072/5760, Loss: 0.02708907425403595\n",
      "Epoch: 22, Samples: 3104/5760, Loss: 0.016216278076171875\n",
      "Epoch: 22, Samples: 3136/5760, Loss: 0.03383742272853851\n",
      "Epoch: 22, Samples: 3168/5760, Loss: 0.02125924825668335\n",
      "Epoch: 22, Samples: 3200/5760, Loss: 0.02519036829471588\n",
      "Epoch: 22, Samples: 3232/5760, Loss: 0.02065977454185486\n",
      "Epoch: 22, Samples: 3264/5760, Loss: 0.03378435969352722\n",
      "Epoch: 22, Samples: 3296/5760, Loss: 0.0218508243560791\n",
      "Epoch: 22, Samples: 3328/5760, Loss: 0.024366334080696106\n",
      "Epoch: 22, Samples: 3360/5760, Loss: 0.01546218991279602\n",
      "Epoch: 22, Samples: 3392/5760, Loss: 0.026679009199142456\n",
      "Epoch: 22, Samples: 3424/5760, Loss: 0.025828182697296143\n",
      "Epoch: 22, Samples: 3456/5760, Loss: 0.02098149061203003\n",
      "Epoch: 22, Samples: 3488/5760, Loss: 0.03407144546508789\n",
      "Epoch: 22, Samples: 3520/5760, Loss: 0.03448750078678131\n",
      "Epoch: 22, Samples: 3552/5760, Loss: 0.0396122932434082\n",
      "Epoch: 22, Samples: 3584/5760, Loss: 0.05259047448635101\n",
      "Epoch: 22, Samples: 3616/5760, Loss: 0.01607099175453186\n",
      "Epoch: 22, Samples: 3648/5760, Loss: 0.027573302388191223\n",
      "Epoch: 22, Samples: 3680/5760, Loss: 0.04024651646614075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Samples: 3712/5760, Loss: 0.04951581358909607\n",
      "Epoch: 22, Samples: 3744/5760, Loss: 0.022610068321228027\n",
      "Epoch: 22, Samples: 3776/5760, Loss: 0.022551298141479492\n",
      "Epoch: 22, Samples: 3808/5760, Loss: 0.01637852191925049\n",
      "Epoch: 22, Samples: 3840/5760, Loss: 0.02910974621772766\n",
      "Epoch: 22, Samples: 3872/5760, Loss: 0.02864477038383484\n",
      "Epoch: 22, Samples: 3904/5760, Loss: 0.02453690767288208\n",
      "Epoch: 22, Samples: 3936/5760, Loss: 0.023163259029388428\n",
      "Epoch: 22, Samples: 3968/5760, Loss: 0.03984324634075165\n",
      "Epoch: 22, Samples: 4000/5760, Loss: 0.05846241116523743\n",
      "Epoch: 22, Samples: 4032/5760, Loss: 0.04990120232105255\n",
      "Epoch: 22, Samples: 4064/5760, Loss: 0.02497461438179016\n",
      "Epoch: 22, Samples: 4096/5760, Loss: 0.021189570426940918\n",
      "Epoch: 22, Samples: 4128/5760, Loss: 0.023484259843826294\n",
      "Epoch: 22, Samples: 4160/5760, Loss: 0.01368337869644165\n",
      "Epoch: 22, Samples: 4192/5760, Loss: 0.02415814995765686\n",
      "Epoch: 22, Samples: 4224/5760, Loss: 0.022382140159606934\n",
      "Epoch: 22, Samples: 4256/5760, Loss: 0.027739673852920532\n",
      "Epoch: 22, Samples: 4288/5760, Loss: 0.02382245659828186\n",
      "Epoch: 22, Samples: 4320/5760, Loss: 0.02266034483909607\n",
      "Epoch: 22, Samples: 4352/5760, Loss: 0.0401773601770401\n",
      "Epoch: 22, Samples: 4384/5760, Loss: 0.023814767599105835\n",
      "Epoch: 22, Samples: 4416/5760, Loss: 0.01960909366607666\n",
      "Epoch: 22, Samples: 4448/5760, Loss: 0.033539846539497375\n",
      "Epoch: 22, Samples: 4480/5760, Loss: 0.03472974896430969\n",
      "Epoch: 22, Samples: 4512/5760, Loss: 0.03093162178993225\n",
      "Epoch: 22, Samples: 4544/5760, Loss: 0.023712575435638428\n",
      "Epoch: 22, Samples: 4576/5760, Loss: 0.024301663041114807\n",
      "Epoch: 22, Samples: 4608/5760, Loss: 0.010803908109664917\n",
      "Epoch: 22, Samples: 4640/5760, Loss: 0.02381114661693573\n",
      "Epoch: 22, Samples: 4672/5760, Loss: 0.013292044401168823\n",
      "Epoch: 22, Samples: 4704/5760, Loss: 0.017328381538391113\n",
      "Epoch: 22, Samples: 4736/5760, Loss: 0.013549625873565674\n",
      "Epoch: 22, Samples: 4768/5760, Loss: 0.01845535635948181\n",
      "Epoch: 22, Samples: 4800/5760, Loss: 0.027860432863235474\n",
      "Epoch: 22, Samples: 4832/5760, Loss: 0.026286005973815918\n",
      "Epoch: 22, Samples: 4864/5760, Loss: 0.03711211681365967\n",
      "Epoch: 22, Samples: 4896/5760, Loss: 0.014169365167617798\n",
      "Epoch: 22, Samples: 4928/5760, Loss: 0.01809421181678772\n",
      "Epoch: 22, Samples: 4960/5760, Loss: 0.03742098808288574\n",
      "Epoch: 22, Samples: 4992/5760, Loss: 0.014740735292434692\n",
      "Epoch: 22, Samples: 5024/5760, Loss: 0.03242340683937073\n",
      "Epoch: 22, Samples: 5056/5760, Loss: 0.016918867826461792\n",
      "Epoch: 22, Samples: 5088/5760, Loss: 0.043075427412986755\n",
      "Epoch: 22, Samples: 5120/5760, Loss: 0.025506123900413513\n",
      "Epoch: 22, Samples: 5152/5760, Loss: 0.018070101737976074\n",
      "Epoch: 22, Samples: 5184/5760, Loss: 0.02951866388320923\n",
      "Epoch: 22, Samples: 5216/5760, Loss: 0.02633565664291382\n",
      "Epoch: 22, Samples: 5248/5760, Loss: 0.017014414072036743\n",
      "Epoch: 22, Samples: 5280/5760, Loss: 0.05556158721446991\n",
      "Epoch: 22, Samples: 5312/5760, Loss: 0.029935389757156372\n",
      "Epoch: 22, Samples: 5344/5760, Loss: 0.025946766138076782\n",
      "Epoch: 22, Samples: 5376/5760, Loss: 0.03133705258369446\n",
      "Epoch: 22, Samples: 5408/5760, Loss: 0.04491584002971649\n",
      "Epoch: 22, Samples: 5440/5760, Loss: 0.0194094181060791\n",
      "Epoch: 22, Samples: 5472/5760, Loss: 0.03164897859096527\n",
      "Epoch: 22, Samples: 5504/5760, Loss: 0.020085662603378296\n",
      "Epoch: 22, Samples: 5536/5760, Loss: 0.036896005272865295\n",
      "Epoch: 22, Samples: 5568/5760, Loss: 0.02573379874229431\n",
      "Epoch: 22, Samples: 5600/5760, Loss: 0.02647659182548523\n",
      "Epoch: 22, Samples: 5632/5760, Loss: 0.02447447180747986\n",
      "Epoch: 22, Samples: 5664/5760, Loss: 0.008842557668685913\n",
      "Epoch: 22, Samples: 5696/5760, Loss: 0.040579602122306824\n",
      "Epoch: 22, Samples: 5728/5760, Loss: 0.46566641330718994\n",
      "\n",
      "Epoch: 22\n",
      "Training set: Average loss: 0.0318\n",
      "Validation set: Average loss: 0.2996, Accuracy: 760/818 (93%)\n",
      "Epoch: 23, Samples: 0/5760, Loss: 0.017984092235565186\n",
      "Epoch: 23, Samples: 32/5760, Loss: 0.01780584454536438\n",
      "Epoch: 23, Samples: 64/5760, Loss: 0.036753520369529724\n",
      "Epoch: 23, Samples: 96/5760, Loss: 0.019684582948684692\n",
      "Epoch: 23, Samples: 128/5760, Loss: 0.03795473277568817\n",
      "Epoch: 23, Samples: 160/5760, Loss: 0.0184381902217865\n",
      "Epoch: 23, Samples: 192/5760, Loss: 0.0334705114364624\n",
      "Epoch: 23, Samples: 224/5760, Loss: 0.041543394327163696\n",
      "Epoch: 23, Samples: 256/5760, Loss: 0.032552286982536316\n",
      "Epoch: 23, Samples: 288/5760, Loss: 0.023393094539642334\n",
      "Epoch: 23, Samples: 320/5760, Loss: 0.015891343355178833\n",
      "Epoch: 23, Samples: 352/5760, Loss: 0.020124435424804688\n",
      "Epoch: 23, Samples: 384/5760, Loss: 0.02656307816505432\n",
      "Epoch: 23, Samples: 416/5760, Loss: 0.022898823022842407\n",
      "Epoch: 23, Samples: 448/5760, Loss: 0.012784481048583984\n",
      "Epoch: 23, Samples: 480/5760, Loss: 0.021083682775497437\n",
      "Epoch: 23, Samples: 512/5760, Loss: 0.02894669771194458\n",
      "Epoch: 23, Samples: 544/5760, Loss: 0.023669689893722534\n",
      "Epoch: 23, Samples: 576/5760, Loss: 0.02763807773590088\n",
      "Epoch: 23, Samples: 608/5760, Loss: 0.03389415144920349\n",
      "Epoch: 23, Samples: 640/5760, Loss: 0.01939469575881958\n",
      "Epoch: 23, Samples: 672/5760, Loss: 0.058474332094192505\n",
      "Epoch: 23, Samples: 704/5760, Loss: 0.035868123173713684\n",
      "Epoch: 23, Samples: 736/5760, Loss: 0.037423133850097656\n",
      "Epoch: 23, Samples: 768/5760, Loss: 0.022570326924324036\n",
      "Epoch: 23, Samples: 800/5760, Loss: 0.04089653491973877\n",
      "Epoch: 23, Samples: 832/5760, Loss: 0.03357243537902832\n",
      "Epoch: 23, Samples: 864/5760, Loss: 0.020249247550964355\n",
      "Epoch: 23, Samples: 896/5760, Loss: 0.014106065034866333\n",
      "Epoch: 23, Samples: 928/5760, Loss: 0.0293692946434021\n",
      "Epoch: 23, Samples: 960/5760, Loss: 0.026403754949569702\n",
      "Epoch: 23, Samples: 992/5760, Loss: 0.03713332116603851\n",
      "Epoch: 23, Samples: 1024/5760, Loss: 0.02572372555732727\n",
      "Epoch: 23, Samples: 1056/5760, Loss: 0.033105745911598206\n",
      "Epoch: 23, Samples: 1088/5760, Loss: 0.014704078435897827\n",
      "Epoch: 23, Samples: 1120/5760, Loss: 0.014799505472183228\n",
      "Epoch: 23, Samples: 1152/5760, Loss: 0.048120900988578796\n",
      "Epoch: 23, Samples: 1184/5760, Loss: 0.017471343278884888\n",
      "Epoch: 23, Samples: 1216/5760, Loss: 0.017451703548431396\n",
      "Epoch: 23, Samples: 1248/5760, Loss: 0.022834867238998413\n",
      "Epoch: 23, Samples: 1280/5760, Loss: 0.02485671639442444\n",
      "Epoch: 23, Samples: 1312/5760, Loss: 0.024223893880844116\n",
      "Epoch: 23, Samples: 1344/5760, Loss: 0.01837053894996643\n",
      "Epoch: 23, Samples: 1376/5760, Loss: 0.024577677249908447\n",
      "Epoch: 23, Samples: 1408/5760, Loss: 0.01470482349395752\n",
      "Epoch: 23, Samples: 1440/5760, Loss: 0.035957396030426025\n",
      "Epoch: 23, Samples: 1472/5760, Loss: 0.014926731586456299\n",
      "Epoch: 23, Samples: 1504/5760, Loss: 0.04007565975189209\n",
      "Epoch: 23, Samples: 1536/5760, Loss: 0.02307230234146118\n",
      "Epoch: 23, Samples: 1568/5760, Loss: 0.015393823385238647\n",
      "Epoch: 23, Samples: 1600/5760, Loss: 0.011446535587310791\n",
      "Epoch: 23, Samples: 1632/5760, Loss: 0.010053694248199463\n",
      "Epoch: 23, Samples: 1664/5760, Loss: 0.021238714456558228\n",
      "Epoch: 23, Samples: 1696/5760, Loss: 0.0288851261138916\n",
      "Epoch: 23, Samples: 1728/5760, Loss: 0.02273327112197876\n",
      "Epoch: 23, Samples: 1760/5760, Loss: 0.020787060260772705\n",
      "Epoch: 23, Samples: 1792/5760, Loss: 0.029126450419425964\n",
      "Epoch: 23, Samples: 1824/5760, Loss: 0.022484272718429565\n",
      "Epoch: 23, Samples: 1856/5760, Loss: 0.023173391819000244\n",
      "Epoch: 23, Samples: 1888/5760, Loss: 0.025063008069992065\n",
      "Epoch: 23, Samples: 1920/5760, Loss: 0.027863353490829468\n",
      "Epoch: 23, Samples: 1952/5760, Loss: 0.010554283857345581\n",
      "Epoch: 23, Samples: 1984/5760, Loss: 0.0361294150352478\n",
      "Epoch: 23, Samples: 2016/5760, Loss: 0.021736711263656616\n",
      "Epoch: 23, Samples: 2048/5760, Loss: 0.02227160334587097\n",
      "Epoch: 23, Samples: 2080/5760, Loss: 0.01880401372909546\n",
      "Epoch: 23, Samples: 2112/5760, Loss: 0.02066788077354431\n",
      "Epoch: 23, Samples: 2144/5760, Loss: 0.03518389165401459\n",
      "Epoch: 23, Samples: 2176/5760, Loss: 0.013527244329452515\n",
      "Epoch: 23, Samples: 2208/5760, Loss: 0.0180281400680542\n",
      "Epoch: 23, Samples: 2240/5760, Loss: 0.013582468032836914\n",
      "Epoch: 23, Samples: 2272/5760, Loss: 0.03247058391571045\n",
      "Epoch: 23, Samples: 2304/5760, Loss: 0.017115876078605652\n",
      "Epoch: 23, Samples: 2336/5760, Loss: 0.021947622299194336\n",
      "Epoch: 23, Samples: 2368/5760, Loss: 0.032269179821014404\n",
      "Epoch: 23, Samples: 2400/5760, Loss: 0.021019667387008667\n",
      "Epoch: 23, Samples: 2432/5760, Loss: 0.021115034818649292\n",
      "Epoch: 23, Samples: 2464/5760, Loss: 0.019448161125183105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Samples: 2496/5760, Loss: 0.01410454511642456\n",
      "Epoch: 23, Samples: 2528/5760, Loss: 0.03041452169418335\n",
      "Epoch: 23, Samples: 2560/5760, Loss: 0.03288203477859497\n",
      "Epoch: 23, Samples: 2592/5760, Loss: 0.03266683220863342\n",
      "Epoch: 23, Samples: 2624/5760, Loss: 0.01583734154701233\n",
      "Epoch: 23, Samples: 2656/5760, Loss: 0.03740091621875763\n",
      "Epoch: 23, Samples: 2688/5760, Loss: 0.015578657388687134\n",
      "Epoch: 23, Samples: 2720/5760, Loss: 0.01596572995185852\n",
      "Epoch: 23, Samples: 2752/5760, Loss: 0.019811540842056274\n",
      "Epoch: 23, Samples: 2784/5760, Loss: 0.02953234314918518\n",
      "Epoch: 23, Samples: 2816/5760, Loss: 0.033020228147506714\n",
      "Epoch: 23, Samples: 2848/5760, Loss: 0.021333277225494385\n",
      "Epoch: 23, Samples: 2880/5760, Loss: 0.011545270681381226\n",
      "Epoch: 23, Samples: 2912/5760, Loss: 0.01583242416381836\n",
      "Epoch: 23, Samples: 2944/5760, Loss: 0.02658092975616455\n",
      "Epoch: 23, Samples: 2976/5760, Loss: 0.01844647526741028\n",
      "Epoch: 23, Samples: 3008/5760, Loss: 0.020017623901367188\n",
      "Epoch: 23, Samples: 3040/5760, Loss: 0.033795639872550964\n",
      "Epoch: 23, Samples: 3072/5760, Loss: 0.02334989607334137\n",
      "Epoch: 23, Samples: 3104/5760, Loss: 0.014236986637115479\n",
      "Epoch: 23, Samples: 3136/5760, Loss: 0.009549617767333984\n",
      "Epoch: 23, Samples: 3168/5760, Loss: 0.03678064048290253\n",
      "Epoch: 23, Samples: 3200/5760, Loss: 0.029170751571655273\n",
      "Epoch: 23, Samples: 3232/5760, Loss: 0.010589748620986938\n",
      "Epoch: 23, Samples: 3264/5760, Loss: 0.017886430025100708\n",
      "Epoch: 23, Samples: 3296/5760, Loss: 0.02484339475631714\n",
      "Epoch: 23, Samples: 3328/5760, Loss: 0.015715688467025757\n",
      "Epoch: 23, Samples: 3360/5760, Loss: 0.052928149700164795\n",
      "Epoch: 23, Samples: 3392/5760, Loss: 0.02567201852798462\n",
      "Epoch: 23, Samples: 3424/5760, Loss: 0.021886631846427917\n",
      "Epoch: 23, Samples: 3456/5760, Loss: 0.0661529004573822\n",
      "Epoch: 23, Samples: 3488/5760, Loss: 0.029190003871917725\n",
      "Epoch: 23, Samples: 3520/5760, Loss: 0.021622657775878906\n",
      "Epoch: 23, Samples: 3552/5760, Loss: 0.012995213270187378\n",
      "Epoch: 23, Samples: 3584/5760, Loss: 0.02945917844772339\n",
      "Epoch: 23, Samples: 3616/5760, Loss: 0.022218644618988037\n",
      "Epoch: 23, Samples: 3648/5760, Loss: 0.014637559652328491\n",
      "Epoch: 23, Samples: 3680/5760, Loss: 0.031873881816864014\n",
      "Epoch: 23, Samples: 3712/5760, Loss: 0.022266864776611328\n",
      "Epoch: 23, Samples: 3744/5760, Loss: 0.012398064136505127\n",
      "Epoch: 23, Samples: 3776/5760, Loss: 0.01609131693840027\n",
      "Epoch: 23, Samples: 3808/5760, Loss: 0.015023618936538696\n",
      "Epoch: 23, Samples: 3840/5760, Loss: 0.014836609363555908\n",
      "Epoch: 23, Samples: 3872/5760, Loss: 0.02035599946975708\n",
      "Epoch: 23, Samples: 3904/5760, Loss: 0.013597667217254639\n",
      "Epoch: 23, Samples: 3936/5760, Loss: 0.03197088837623596\n",
      "Epoch: 23, Samples: 3968/5760, Loss: 0.032730042934417725\n",
      "Epoch: 23, Samples: 4000/5760, Loss: 0.05444785952568054\n",
      "Epoch: 23, Samples: 4032/5760, Loss: 0.026182115077972412\n",
      "Epoch: 23, Samples: 4064/5760, Loss: 0.023617029190063477\n",
      "Epoch: 23, Samples: 4096/5760, Loss: 0.03784152865409851\n",
      "Epoch: 23, Samples: 4128/5760, Loss: 0.027206987142562866\n",
      "Epoch: 23, Samples: 4160/5760, Loss: 0.013377368450164795\n",
      "Epoch: 23, Samples: 4192/5760, Loss: 0.027734488248825073\n",
      "Epoch: 23, Samples: 4224/5760, Loss: 0.01697954535484314\n",
      "Epoch: 23, Samples: 4256/5760, Loss: 0.02441088855266571\n",
      "Epoch: 23, Samples: 4288/5760, Loss: 0.032274216413497925\n",
      "Epoch: 23, Samples: 4320/5760, Loss: 0.027201443910598755\n",
      "Epoch: 23, Samples: 4352/5760, Loss: 0.051685988903045654\n",
      "Epoch: 23, Samples: 4384/5760, Loss: 0.017568528652191162\n",
      "Epoch: 23, Samples: 4416/5760, Loss: 0.03848198056221008\n",
      "Epoch: 23, Samples: 4448/5760, Loss: 0.02932840585708618\n",
      "Epoch: 23, Samples: 4480/5760, Loss: 0.022698283195495605\n",
      "Epoch: 23, Samples: 4512/5760, Loss: 0.025952890515327454\n",
      "Epoch: 23, Samples: 4544/5760, Loss: 0.03251224756240845\n",
      "Epoch: 23, Samples: 4576/5760, Loss: 0.009076505899429321\n",
      "Epoch: 23, Samples: 4608/5760, Loss: 0.030595600605010986\n",
      "Epoch: 23, Samples: 4640/5760, Loss: 0.015446752309799194\n",
      "Epoch: 23, Samples: 4672/5760, Loss: 0.018703311681747437\n",
      "Epoch: 23, Samples: 4704/5760, Loss: 0.011265963315963745\n",
      "Epoch: 23, Samples: 4736/5760, Loss: 0.018496036529541016\n",
      "Epoch: 23, Samples: 4768/5760, Loss: 0.024473905563354492\n",
      "Epoch: 23, Samples: 4800/5760, Loss: 0.027510762214660645\n",
      "Epoch: 23, Samples: 4832/5760, Loss: 0.01958775520324707\n",
      "Epoch: 23, Samples: 4864/5760, Loss: 0.028389394283294678\n",
      "Epoch: 23, Samples: 4896/5760, Loss: 0.015335947275161743\n",
      "Epoch: 23, Samples: 4928/5760, Loss: 0.013165950775146484\n",
      "Epoch: 23, Samples: 4960/5760, Loss: 0.02379503846168518\n",
      "Epoch: 23, Samples: 4992/5760, Loss: 0.018727421760559082\n",
      "Epoch: 23, Samples: 5024/5760, Loss: 0.015685677528381348\n",
      "Epoch: 23, Samples: 5056/5760, Loss: 0.0274408757686615\n",
      "Epoch: 23, Samples: 5088/5760, Loss: 0.019084155559539795\n",
      "Epoch: 23, Samples: 5120/5760, Loss: 0.01558801531791687\n",
      "Epoch: 23, Samples: 5152/5760, Loss: 0.015357613563537598\n",
      "Epoch: 23, Samples: 5184/5760, Loss: 0.023566842079162598\n",
      "Epoch: 23, Samples: 5216/5760, Loss: 0.02763630449771881\n",
      "Epoch: 23, Samples: 5248/5760, Loss: 0.024678528308868408\n",
      "Epoch: 23, Samples: 5280/5760, Loss: 0.030743852257728577\n",
      "Epoch: 23, Samples: 5312/5760, Loss: 0.01506030559539795\n",
      "Epoch: 23, Samples: 5344/5760, Loss: 0.02997380495071411\n",
      "Epoch: 23, Samples: 5376/5760, Loss: 0.020939290523529053\n",
      "Epoch: 23, Samples: 5408/5760, Loss: 0.015714675188064575\n",
      "Epoch: 23, Samples: 5440/5760, Loss: 0.01035115122795105\n",
      "Epoch: 23, Samples: 5472/5760, Loss: 0.018266111612319946\n",
      "Epoch: 23, Samples: 5504/5760, Loss: 0.021256059408187866\n",
      "Epoch: 23, Samples: 5536/5760, Loss: 0.02278721332550049\n",
      "Epoch: 23, Samples: 5568/5760, Loss: 0.030362635850906372\n",
      "Epoch: 23, Samples: 5600/5760, Loss: 0.02638210356235504\n",
      "Epoch: 23, Samples: 5632/5760, Loss: 0.023942142724990845\n",
      "Epoch: 23, Samples: 5664/5760, Loss: 0.01852235198020935\n",
      "Epoch: 23, Samples: 5696/5760, Loss: 0.02958369255065918\n",
      "Epoch: 23, Samples: 5728/5760, Loss: 2.469733238220215\n",
      "\n",
      "Epoch: 23\n",
      "Training set: Average loss: 0.0378\n",
      "Validation set: Average loss: 0.2823, Accuracy: 766/818 (94%)\n",
      "Saving model (epoch 23) with lowest validation loss: 0.28226651881749815\n",
      "Epoch: 24, Samples: 0/5760, Loss: 0.017023324966430664\n",
      "Epoch: 24, Samples: 32/5760, Loss: 0.027216583490371704\n",
      "Epoch: 24, Samples: 64/5760, Loss: 0.024256721138954163\n",
      "Epoch: 24, Samples: 96/5760, Loss: 0.023068636655807495\n",
      "Epoch: 24, Samples: 128/5760, Loss: 0.02091825008392334\n",
      "Epoch: 24, Samples: 160/5760, Loss: 0.03906415402889252\n",
      "Epoch: 24, Samples: 192/5760, Loss: 0.02966994047164917\n",
      "Epoch: 24, Samples: 224/5760, Loss: 0.034321948885917664\n",
      "Epoch: 24, Samples: 256/5760, Loss: 0.014953851699829102\n",
      "Epoch: 24, Samples: 288/5760, Loss: 0.020056486129760742\n",
      "Epoch: 24, Samples: 320/5760, Loss: 0.03480580449104309\n",
      "Epoch: 24, Samples: 352/5760, Loss: 0.019072219729423523\n",
      "Epoch: 24, Samples: 384/5760, Loss: 0.03514282405376434\n",
      "Epoch: 24, Samples: 416/5760, Loss: 0.023364365100860596\n",
      "Epoch: 24, Samples: 448/5760, Loss: 0.02567717432975769\n",
      "Epoch: 24, Samples: 480/5760, Loss: 0.05087435245513916\n",
      "Epoch: 24, Samples: 512/5760, Loss: 0.045416995882987976\n",
      "Epoch: 24, Samples: 544/5760, Loss: 0.04934503138065338\n",
      "Epoch: 24, Samples: 576/5760, Loss: 0.05778022110462189\n",
      "Epoch: 24, Samples: 608/5760, Loss: 0.021867424249649048\n",
      "Epoch: 24, Samples: 640/5760, Loss: 0.01431688666343689\n",
      "Epoch: 24, Samples: 672/5760, Loss: 0.024724245071411133\n",
      "Epoch: 24, Samples: 704/5760, Loss: 0.027508705854415894\n",
      "Epoch: 24, Samples: 736/5760, Loss: 0.019502252340316772\n",
      "Epoch: 24, Samples: 768/5760, Loss: 0.016958236694335938\n",
      "Epoch: 24, Samples: 800/5760, Loss: 0.078385129570961\n",
      "Epoch: 24, Samples: 832/5760, Loss: 0.06343863904476166\n",
      "Epoch: 24, Samples: 864/5760, Loss: 0.025961965322494507\n",
      "Epoch: 24, Samples: 896/5760, Loss: 0.037417635321617126\n",
      "Epoch: 24, Samples: 928/5760, Loss: 0.030087918043136597\n",
      "Epoch: 24, Samples: 960/5760, Loss: 0.03097108006477356\n",
      "Epoch: 24, Samples: 992/5760, Loss: 0.013009697198867798\n",
      "Epoch: 24, Samples: 1024/5760, Loss: 0.02392612397670746\n",
      "Epoch: 24, Samples: 1056/5760, Loss: 0.027431458234786987\n",
      "Epoch: 24, Samples: 1088/5760, Loss: 0.021619945764541626\n",
      "Epoch: 24, Samples: 1120/5760, Loss: 0.026553303003311157\n",
      "Epoch: 24, Samples: 1152/5760, Loss: 0.06131811439990997\n",
      "Epoch: 24, Samples: 1184/5760, Loss: 0.036048129200935364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Samples: 1216/5760, Loss: 0.03845392167568207\n",
      "Epoch: 24, Samples: 1248/5760, Loss: 0.02066221833229065\n",
      "Epoch: 24, Samples: 1280/5760, Loss: 0.03558287024497986\n",
      "Epoch: 24, Samples: 1312/5760, Loss: 0.05168478190898895\n",
      "Epoch: 24, Samples: 1344/5760, Loss: 0.01563096046447754\n",
      "Epoch: 24, Samples: 1376/5760, Loss: 0.0256212055683136\n",
      "Epoch: 24, Samples: 1408/5760, Loss: 0.033670783042907715\n",
      "Epoch: 24, Samples: 1440/5760, Loss: 0.046794772148132324\n",
      "Epoch: 24, Samples: 1472/5760, Loss: 0.028034120798110962\n",
      "Epoch: 24, Samples: 1504/5760, Loss: 0.035009294748306274\n",
      "Epoch: 24, Samples: 1536/5760, Loss: 0.036558598279953\n",
      "Epoch: 24, Samples: 1568/5760, Loss: 0.044239431619644165\n",
      "Epoch: 24, Samples: 1600/5760, Loss: 0.022335171699523926\n",
      "Epoch: 24, Samples: 1632/5760, Loss: 0.026540756225585938\n",
      "Epoch: 24, Samples: 1664/5760, Loss: 0.015541642904281616\n",
      "Epoch: 24, Samples: 1696/5760, Loss: 0.02290746569633484\n",
      "Epoch: 24, Samples: 1728/5760, Loss: 0.025565072894096375\n",
      "Epoch: 24, Samples: 1760/5760, Loss: 0.030579417943954468\n",
      "Epoch: 24, Samples: 1792/5760, Loss: 0.028494566679000854\n",
      "Epoch: 24, Samples: 1824/5760, Loss: 0.015569478273391724\n",
      "Epoch: 24, Samples: 1856/5760, Loss: 0.024683117866516113\n",
      "Epoch: 24, Samples: 1888/5760, Loss: 0.024516284465789795\n",
      "Epoch: 24, Samples: 1920/5760, Loss: 0.01595976948738098\n",
      "Epoch: 24, Samples: 1952/5760, Loss: 0.03209418058395386\n",
      "Epoch: 24, Samples: 1984/5760, Loss: 0.022791802883148193\n",
      "Epoch: 24, Samples: 2016/5760, Loss: 0.019431471824645996\n",
      "Epoch: 24, Samples: 2048/5760, Loss: 0.02084141969680786\n",
      "Epoch: 24, Samples: 2080/5760, Loss: 0.018063485622406006\n",
      "Epoch: 24, Samples: 2112/5760, Loss: 0.0231819748878479\n",
      "Epoch: 24, Samples: 2144/5760, Loss: 0.025313138961791992\n",
      "Epoch: 24, Samples: 2176/5760, Loss: 0.02324637770652771\n",
      "Epoch: 24, Samples: 2208/5760, Loss: 0.02747058868408203\n",
      "Epoch: 24, Samples: 2240/5760, Loss: 0.02665799856185913\n",
      "Epoch: 24, Samples: 2272/5760, Loss: 0.02601906657218933\n",
      "Epoch: 24, Samples: 2304/5760, Loss: 0.023813724517822266\n",
      "Epoch: 24, Samples: 2336/5760, Loss: 0.018722504377365112\n",
      "Epoch: 24, Samples: 2368/5760, Loss: 0.016197383403778076\n",
      "Epoch: 24, Samples: 2400/5760, Loss: 0.04439246654510498\n",
      "Epoch: 24, Samples: 2432/5760, Loss: 0.020880848169326782\n",
      "Epoch: 24, Samples: 2464/5760, Loss: 0.031069040298461914\n",
      "Epoch: 24, Samples: 2496/5760, Loss: 0.04793284833431244\n",
      "Epoch: 24, Samples: 2528/5760, Loss: 0.015865474939346313\n",
      "Epoch: 24, Samples: 2560/5760, Loss: 0.022563666105270386\n",
      "Epoch: 24, Samples: 2592/5760, Loss: 0.02123168110847473\n",
      "Epoch: 24, Samples: 2624/5760, Loss: 0.03672516345977783\n",
      "Epoch: 24, Samples: 2656/5760, Loss: 0.021298304200172424\n",
      "Epoch: 24, Samples: 2688/5760, Loss: 0.016386061906814575\n",
      "Epoch: 24, Samples: 2720/5760, Loss: 0.02337583899497986\n",
      "Epoch: 24, Samples: 2752/5760, Loss: 0.021523550152778625\n",
      "Epoch: 24, Samples: 2784/5760, Loss: 0.024915337562561035\n",
      "Epoch: 24, Samples: 2816/5760, Loss: 0.017823904752731323\n",
      "Epoch: 24, Samples: 2848/5760, Loss: 0.026121973991394043\n",
      "Epoch: 24, Samples: 2880/5760, Loss: 0.04557409882545471\n",
      "Epoch: 24, Samples: 2912/5760, Loss: 0.052918240427970886\n",
      "Epoch: 24, Samples: 2944/5760, Loss: 0.01895001530647278\n",
      "Epoch: 24, Samples: 2976/5760, Loss: 0.017041891813278198\n",
      "Epoch: 24, Samples: 3008/5760, Loss: 0.06943321228027344\n",
      "Epoch: 24, Samples: 3040/5760, Loss: 0.01589171588420868\n",
      "Epoch: 24, Samples: 3072/5760, Loss: 0.11977092921733856\n",
      "Epoch: 24, Samples: 3104/5760, Loss: 0.018857449293136597\n",
      "Epoch: 24, Samples: 3136/5760, Loss: 0.03339362144470215\n",
      "Epoch: 24, Samples: 3168/5760, Loss: 0.01938900351524353\n",
      "Epoch: 24, Samples: 3200/5760, Loss: 0.015686243772506714\n",
      "Epoch: 24, Samples: 3232/5760, Loss: 0.017425715923309326\n",
      "Epoch: 24, Samples: 3264/5760, Loss: 0.019116461277008057\n",
      "Epoch: 24, Samples: 3296/5760, Loss: 0.01799941062927246\n",
      "Epoch: 24, Samples: 3328/5760, Loss: 0.016263097524642944\n",
      "Epoch: 24, Samples: 3360/5760, Loss: 0.019557923078536987\n",
      "Epoch: 24, Samples: 3392/5760, Loss: 0.02391114830970764\n",
      "Epoch: 24, Samples: 3424/5760, Loss: 0.03848865628242493\n",
      "Epoch: 24, Samples: 3456/5760, Loss: 0.016759485006332397\n",
      "Epoch: 24, Samples: 3488/5760, Loss: 0.014086425304412842\n",
      "Epoch: 24, Samples: 3520/5760, Loss: 0.027600884437561035\n",
      "Epoch: 24, Samples: 3552/5760, Loss: 0.022309422492980957\n",
      "Epoch: 24, Samples: 3584/5760, Loss: 0.022631913423538208\n",
      "Epoch: 24, Samples: 3616/5760, Loss: 0.026754021644592285\n",
      "Epoch: 24, Samples: 3648/5760, Loss: 0.02190825343132019\n",
      "Epoch: 24, Samples: 3680/5760, Loss: 0.02332836389541626\n",
      "Epoch: 24, Samples: 3712/5760, Loss: 0.00889664888381958\n",
      "Epoch: 24, Samples: 3744/5760, Loss: 0.02250102162361145\n",
      "Epoch: 24, Samples: 3776/5760, Loss: 0.02490195631980896\n",
      "Epoch: 24, Samples: 3808/5760, Loss: 0.0200662761926651\n",
      "Epoch: 24, Samples: 3840/5760, Loss: 0.03222551941871643\n",
      "Epoch: 24, Samples: 3872/5760, Loss: 0.03256179392337799\n",
      "Epoch: 24, Samples: 3904/5760, Loss: 0.023425310850143433\n",
      "Epoch: 24, Samples: 3936/5760, Loss: 0.02375611662864685\n",
      "Epoch: 24, Samples: 3968/5760, Loss: 0.014118015766143799\n",
      "Epoch: 24, Samples: 4000/5760, Loss: 0.026000142097473145\n",
      "Epoch: 24, Samples: 4032/5760, Loss: 0.02941100299358368\n",
      "Epoch: 24, Samples: 4064/5760, Loss: 0.018291518092155457\n",
      "Epoch: 24, Samples: 4096/5760, Loss: 0.012084394693374634\n",
      "Epoch: 24, Samples: 4128/5760, Loss: 0.04034645855426788\n",
      "Epoch: 24, Samples: 4160/5760, Loss: 0.04136821627616882\n",
      "Epoch: 24, Samples: 4192/5760, Loss: 0.03912556171417236\n",
      "Epoch: 24, Samples: 4224/5760, Loss: 0.01151987910270691\n",
      "Epoch: 24, Samples: 4256/5760, Loss: 0.014757275581359863\n",
      "Epoch: 24, Samples: 4288/5760, Loss: 0.017621129751205444\n",
      "Epoch: 24, Samples: 4320/5760, Loss: 0.028143778443336487\n",
      "Epoch: 24, Samples: 4352/5760, Loss: 0.03018587827682495\n",
      "Epoch: 24, Samples: 4384/5760, Loss: 0.018872618675231934\n",
      "Epoch: 24, Samples: 4416/5760, Loss: 0.010920286178588867\n",
      "Epoch: 24, Samples: 4448/5760, Loss: 0.024064719676971436\n",
      "Epoch: 24, Samples: 4480/5760, Loss: 0.024247735738754272\n",
      "Epoch: 24, Samples: 4512/5760, Loss: 0.01561787724494934\n",
      "Epoch: 24, Samples: 4544/5760, Loss: 0.01775193214416504\n",
      "Epoch: 24, Samples: 4576/5760, Loss: 0.012685149908065796\n",
      "Epoch: 24, Samples: 4608/5760, Loss: 0.011935710906982422\n",
      "Epoch: 24, Samples: 4640/5760, Loss: 0.015817999839782715\n",
      "Epoch: 24, Samples: 4672/5760, Loss: 0.022276699542999268\n",
      "Epoch: 24, Samples: 4704/5760, Loss: 0.025092363357543945\n",
      "Epoch: 24, Samples: 4736/5760, Loss: 0.026087909936904907\n",
      "Epoch: 24, Samples: 4768/5760, Loss: 0.01907944679260254\n",
      "Epoch: 24, Samples: 4800/5760, Loss: 0.025330722332000732\n",
      "Epoch: 24, Samples: 4832/5760, Loss: 0.02419215440750122\n",
      "Epoch: 24, Samples: 4864/5760, Loss: 0.027900218963623047\n",
      "Epoch: 24, Samples: 4896/5760, Loss: 0.018864527344703674\n",
      "Epoch: 24, Samples: 4928/5760, Loss: 0.020538806915283203\n",
      "Epoch: 24, Samples: 4960/5760, Loss: 0.01563054323196411\n",
      "Epoch: 24, Samples: 4992/5760, Loss: 0.021820485591888428\n",
      "Epoch: 24, Samples: 5024/5760, Loss: 0.015607267618179321\n",
      "Epoch: 24, Samples: 5056/5760, Loss: 0.014645129442214966\n",
      "Epoch: 24, Samples: 5088/5760, Loss: 0.024975985288619995\n",
      "Epoch: 24, Samples: 5120/5760, Loss: 0.019101351499557495\n",
      "Epoch: 24, Samples: 5152/5760, Loss: 0.03141036629676819\n",
      "Epoch: 24, Samples: 5184/5760, Loss: 0.01709267497062683\n",
      "Epoch: 24, Samples: 5216/5760, Loss: 0.015965372323989868\n",
      "Epoch: 24, Samples: 5248/5760, Loss: 0.0192258358001709\n",
      "Epoch: 24, Samples: 5280/5760, Loss: 0.013912081718444824\n",
      "Epoch: 24, Samples: 5312/5760, Loss: 0.022441625595092773\n",
      "Epoch: 24, Samples: 5344/5760, Loss: 0.02046951651573181\n",
      "Epoch: 24, Samples: 5376/5760, Loss: 0.015917867422103882\n",
      "Epoch: 24, Samples: 5408/5760, Loss: 0.02273482084274292\n",
      "Epoch: 24, Samples: 5440/5760, Loss: 0.05477540194988251\n",
      "Epoch: 24, Samples: 5472/5760, Loss: 0.013529866933822632\n",
      "Epoch: 24, Samples: 5504/5760, Loss: 0.017948567867279053\n",
      "Epoch: 24, Samples: 5536/5760, Loss: 0.03177827596664429\n",
      "Epoch: 24, Samples: 5568/5760, Loss: 0.019329696893692017\n",
      "Epoch: 24, Samples: 5600/5760, Loss: 0.049223050475120544\n",
      "Epoch: 24, Samples: 5632/5760, Loss: 0.014437228441238403\n",
      "Epoch: 24, Samples: 5664/5760, Loss: 0.015662968158721924\n",
      "Epoch: 24, Samples: 5696/5760, Loss: 0.01693752408027649\n",
      "Epoch: 24, Samples: 5728/5760, Loss: 0.517067551612854\n",
      "\n",
      "Epoch: 24\n",
      "Training set: Average loss: 0.0291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss: 0.2884, Accuracy: 762/818 (93%)\n",
      "Epoch: 25, Samples: 0/5760, Loss: 0.019876748323440552\n",
      "Epoch: 25, Samples: 32/5760, Loss: 0.020071297883987427\n",
      "Epoch: 25, Samples: 64/5760, Loss: 0.016529858112335205\n",
      "Epoch: 25, Samples: 96/5760, Loss: 0.017696261405944824\n",
      "Epoch: 25, Samples: 128/5760, Loss: 0.011487096548080444\n",
      "Epoch: 25, Samples: 160/5760, Loss: 0.013324499130249023\n",
      "Epoch: 25, Samples: 192/5760, Loss: 0.015226006507873535\n",
      "Epoch: 25, Samples: 224/5760, Loss: 0.012916773557662964\n",
      "Epoch: 25, Samples: 256/5760, Loss: 0.01772800087928772\n",
      "Epoch: 25, Samples: 288/5760, Loss: 0.03800736367702484\n",
      "Epoch: 25, Samples: 320/5760, Loss: 0.029705405235290527\n",
      "Epoch: 25, Samples: 352/5760, Loss: 0.01438005268573761\n",
      "Epoch: 25, Samples: 384/5760, Loss: 0.04662008583545685\n",
      "Epoch: 25, Samples: 416/5760, Loss: 0.016463398933410645\n",
      "Epoch: 25, Samples: 448/5760, Loss: 0.017951250076293945\n",
      "Epoch: 25, Samples: 480/5760, Loss: 0.014338791370391846\n",
      "Epoch: 25, Samples: 512/5760, Loss: 0.02558019757270813\n",
      "Epoch: 25, Samples: 544/5760, Loss: 0.02602866291999817\n",
      "Epoch: 25, Samples: 576/5760, Loss: 0.03539511561393738\n",
      "Epoch: 25, Samples: 608/5760, Loss: 0.021254360675811768\n",
      "Epoch: 25, Samples: 640/5760, Loss: 0.018024682998657227\n",
      "Epoch: 25, Samples: 672/5760, Loss: 0.02062046527862549\n",
      "Epoch: 25, Samples: 704/5760, Loss: 0.02521553635597229\n",
      "Epoch: 25, Samples: 736/5760, Loss: 0.020731478929519653\n",
      "Epoch: 25, Samples: 768/5760, Loss: 0.024470806121826172\n",
      "Epoch: 25, Samples: 800/5760, Loss: 0.03096422553062439\n",
      "Epoch: 25, Samples: 832/5760, Loss: 0.02124202251434326\n",
      "Epoch: 25, Samples: 864/5760, Loss: 0.04033246636390686\n",
      "Epoch: 25, Samples: 896/5760, Loss: 0.020902812480926514\n",
      "Epoch: 25, Samples: 928/5760, Loss: 0.035199522972106934\n",
      "Epoch: 25, Samples: 960/5760, Loss: 0.021529048681259155\n",
      "Epoch: 25, Samples: 992/5760, Loss: 0.01618841290473938\n",
      "Epoch: 25, Samples: 1024/5760, Loss: 0.03070908784866333\n",
      "Epoch: 25, Samples: 1056/5760, Loss: 0.020057320594787598\n",
      "Epoch: 25, Samples: 1088/5760, Loss: 0.025411278009414673\n",
      "Epoch: 25, Samples: 1120/5760, Loss: 0.02917894721031189\n",
      "Epoch: 25, Samples: 1152/5760, Loss: 0.02766016125679016\n",
      "Epoch: 25, Samples: 1184/5760, Loss: 0.015200763940811157\n",
      "Epoch: 25, Samples: 1216/5760, Loss: 0.018354758620262146\n",
      "Epoch: 25, Samples: 1248/5760, Loss: 0.013093709945678711\n",
      "Epoch: 25, Samples: 1280/5760, Loss: 0.03678929805755615\n",
      "Epoch: 25, Samples: 1312/5760, Loss: 0.024166345596313477\n",
      "Epoch: 25, Samples: 1344/5760, Loss: 0.022304832935333252\n",
      "Epoch: 25, Samples: 1376/5760, Loss: 0.04164513945579529\n",
      "Epoch: 25, Samples: 1408/5760, Loss: 0.01364484429359436\n",
      "Epoch: 25, Samples: 1440/5760, Loss: 0.020011574029922485\n",
      "Epoch: 25, Samples: 1472/5760, Loss: 0.018375560641288757\n",
      "Epoch: 25, Samples: 1504/5760, Loss: 0.019678980112075806\n",
      "Epoch: 25, Samples: 1536/5760, Loss: 0.012209832668304443\n",
      "Epoch: 25, Samples: 1568/5760, Loss: 0.01171332597732544\n",
      "Epoch: 25, Samples: 1600/5760, Loss: 0.019445747137069702\n",
      "Epoch: 25, Samples: 1632/5760, Loss: 0.011954247951507568\n",
      "Epoch: 25, Samples: 1664/5760, Loss: 0.018624037504196167\n",
      "Epoch: 25, Samples: 1696/5760, Loss: 0.0247647762298584\n",
      "Epoch: 25, Samples: 1728/5760, Loss: 0.018542975187301636\n",
      "Epoch: 25, Samples: 1760/5760, Loss: 0.01972275972366333\n",
      "Epoch: 25, Samples: 1792/5760, Loss: 0.020383358001708984\n",
      "Epoch: 25, Samples: 1824/5760, Loss: 0.0170382559299469\n",
      "Epoch: 25, Samples: 1856/5760, Loss: 0.020809829235076904\n",
      "Epoch: 25, Samples: 1888/5760, Loss: 0.014995738863945007\n",
      "Epoch: 25, Samples: 1920/5760, Loss: 0.02701905369758606\n",
      "Epoch: 25, Samples: 1952/5760, Loss: 0.026144564151763916\n",
      "Epoch: 25, Samples: 1984/5760, Loss: 0.01957455277442932\n",
      "Epoch: 25, Samples: 2016/5760, Loss: 0.040696561336517334\n",
      "Epoch: 25, Samples: 2048/5760, Loss: 0.0167730450630188\n",
      "Epoch: 25, Samples: 2080/5760, Loss: 0.028395235538482666\n",
      "Epoch: 25, Samples: 2112/5760, Loss: 0.02197226881980896\n",
      "Epoch: 25, Samples: 2144/5760, Loss: 0.017391905188560486\n",
      "Epoch: 25, Samples: 2176/5760, Loss: 0.010047376155853271\n",
      "Epoch: 25, Samples: 2208/5760, Loss: 0.01035359501838684\n",
      "Epoch: 25, Samples: 2240/5760, Loss: 0.03086581826210022\n",
      "Epoch: 25, Samples: 2272/5760, Loss: 0.03151601552963257\n",
      "Epoch: 25, Samples: 2304/5760, Loss: 0.011565297842025757\n",
      "Epoch: 25, Samples: 2336/5760, Loss: 0.013450950384140015\n",
      "Epoch: 25, Samples: 2368/5760, Loss: 0.023925945162773132\n",
      "Epoch: 25, Samples: 2400/5760, Loss: 0.0190618634223938\n",
      "Epoch: 25, Samples: 2432/5760, Loss: 0.018483757972717285\n",
      "Epoch: 25, Samples: 2464/5760, Loss: 0.016865819692611694\n",
      "Epoch: 25, Samples: 2496/5760, Loss: 0.012205243110656738\n",
      "Epoch: 25, Samples: 2528/5760, Loss: 0.015207737684249878\n",
      "Epoch: 25, Samples: 2560/5760, Loss: 0.023747771978378296\n",
      "Epoch: 25, Samples: 2592/5760, Loss: 0.02112087607383728\n",
      "Epoch: 25, Samples: 2624/5760, Loss: 0.013700753450393677\n",
      "Epoch: 25, Samples: 2656/5760, Loss: 0.050321683287620544\n",
      "Epoch: 25, Samples: 2688/5760, Loss: 0.043020203709602356\n",
      "Epoch: 25, Samples: 2720/5760, Loss: 0.023466497659683228\n",
      "Epoch: 25, Samples: 2752/5760, Loss: 0.017387181520462036\n",
      "Epoch: 25, Samples: 2784/5760, Loss: 0.024442225694656372\n",
      "Epoch: 25, Samples: 2816/5760, Loss: 0.0190126895904541\n",
      "Epoch: 25, Samples: 2848/5760, Loss: 0.012750566005706787\n",
      "Epoch: 25, Samples: 2880/5760, Loss: 0.01934114098548889\n",
      "Epoch: 25, Samples: 2912/5760, Loss: 0.023125767707824707\n",
      "Epoch: 25, Samples: 2944/5760, Loss: 0.03676965832710266\n",
      "Epoch: 25, Samples: 2976/5760, Loss: 0.02919265627861023\n",
      "Epoch: 25, Samples: 3008/5760, Loss: 0.01657894253730774\n",
      "Epoch: 25, Samples: 3040/5760, Loss: 0.01824742555618286\n",
      "Epoch: 25, Samples: 3072/5760, Loss: 0.025119110941886902\n",
      "Epoch: 25, Samples: 3104/5760, Loss: 0.01155826449394226\n",
      "Epoch: 25, Samples: 3136/5760, Loss: 0.03450469672679901\n",
      "Epoch: 25, Samples: 3168/5760, Loss: 0.05310089886188507\n",
      "Epoch: 25, Samples: 3200/5760, Loss: 0.015507876873016357\n",
      "Epoch: 25, Samples: 3232/5760, Loss: 0.012116223573684692\n",
      "Epoch: 25, Samples: 3264/5760, Loss: 0.03051096200942993\n",
      "Epoch: 25, Samples: 3296/5760, Loss: 0.029155820608139038\n",
      "Epoch: 25, Samples: 3328/5760, Loss: 0.02120766043663025\n",
      "Epoch: 25, Samples: 3360/5760, Loss: 0.030826091766357422\n",
      "Epoch: 25, Samples: 3392/5760, Loss: 0.020839810371398926\n",
      "Epoch: 25, Samples: 3424/5760, Loss: 0.015202134847640991\n",
      "Epoch: 25, Samples: 3456/5760, Loss: 0.014529049396514893\n",
      "Epoch: 25, Samples: 3488/5760, Loss: 0.028366923332214355\n",
      "Epoch: 25, Samples: 3520/5760, Loss: 0.01922944188117981\n",
      "Epoch: 25, Samples: 3552/5760, Loss: 0.036096811294555664\n",
      "Epoch: 25, Samples: 3584/5760, Loss: 0.015771031379699707\n",
      "Epoch: 25, Samples: 3616/5760, Loss: 0.0106964111328125\n",
      "Epoch: 25, Samples: 3648/5760, Loss: 0.01895834505558014\n",
      "Epoch: 25, Samples: 3680/5760, Loss: 0.021422356367111206\n",
      "Epoch: 25, Samples: 3712/5760, Loss: 0.03306835889816284\n",
      "Epoch: 25, Samples: 3744/5760, Loss: 0.012226134538650513\n",
      "Epoch: 25, Samples: 3776/5760, Loss: 0.017845958471298218\n",
      "Epoch: 25, Samples: 3808/5760, Loss: 0.01771041750907898\n",
      "Epoch: 25, Samples: 3840/5760, Loss: 0.01553460955619812\n",
      "Epoch: 25, Samples: 3872/5760, Loss: 0.02380681037902832\n",
      "Epoch: 25, Samples: 3904/5760, Loss: 0.016344338655471802\n",
      "Epoch: 25, Samples: 3936/5760, Loss: 0.016297131776809692\n",
      "Epoch: 25, Samples: 3968/5760, Loss: 0.01870715618133545\n",
      "Epoch: 25, Samples: 4000/5760, Loss: 0.029130637645721436\n",
      "Epoch: 25, Samples: 4032/5760, Loss: 0.028975024819374084\n",
      "Epoch: 25, Samples: 4064/5760, Loss: 0.02333681285381317\n",
      "Epoch: 25, Samples: 4096/5760, Loss: 0.01618930697441101\n",
      "Epoch: 25, Samples: 4128/5760, Loss: 0.023793086409568787\n",
      "Epoch: 25, Samples: 4160/5760, Loss: 0.02486899495124817\n",
      "Epoch: 25, Samples: 4192/5760, Loss: 0.02691301703453064\n",
      "Epoch: 25, Samples: 4224/5760, Loss: 0.015398025512695312\n",
      "Epoch: 25, Samples: 4256/5760, Loss: 0.01317933201789856\n",
      "Epoch: 25, Samples: 4288/5760, Loss: 0.016997992992401123\n",
      "Epoch: 25, Samples: 4320/5760, Loss: 0.038358256220817566\n",
      "Epoch: 25, Samples: 4352/5760, Loss: 0.02175101637840271\n",
      "Epoch: 25, Samples: 4384/5760, Loss: 0.01997542381286621\n",
      "Epoch: 25, Samples: 4416/5760, Loss: 0.01751655340194702\n",
      "Epoch: 25, Samples: 4448/5760, Loss: 0.01908305287361145\n",
      "Epoch: 25, Samples: 4480/5760, Loss: 0.018585413694381714\n",
      "Epoch: 25, Samples: 4512/5760, Loss: 0.01618310809135437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Samples: 4544/5760, Loss: 0.01597285270690918\n",
      "Epoch: 25, Samples: 4576/5760, Loss: 0.05135662853717804\n",
      "Epoch: 25, Samples: 4608/5760, Loss: 0.04787696897983551\n",
      "Epoch: 25, Samples: 4640/5760, Loss: 0.012933135032653809\n",
      "Epoch: 25, Samples: 4672/5760, Loss: 0.0188940167427063\n",
      "Epoch: 25, Samples: 4704/5760, Loss: 0.022504538297653198\n",
      "Epoch: 25, Samples: 4736/5760, Loss: 0.020168915390968323\n",
      "Epoch: 25, Samples: 4768/5760, Loss: 0.020332589745521545\n",
      "Epoch: 25, Samples: 4800/5760, Loss: 0.029420316219329834\n",
      "Epoch: 25, Samples: 4832/5760, Loss: 0.023678451776504517\n",
      "Epoch: 25, Samples: 4864/5760, Loss: 0.02490466833114624\n",
      "Epoch: 25, Samples: 4896/5760, Loss: 0.038486286997795105\n",
      "Epoch: 25, Samples: 4928/5760, Loss: 0.022470995783805847\n",
      "Epoch: 25, Samples: 4960/5760, Loss: 0.025117695331573486\n",
      "Epoch: 25, Samples: 4992/5760, Loss: 0.0368061363697052\n",
      "Epoch: 25, Samples: 5024/5760, Loss: 0.013593301177024841\n",
      "Epoch: 25, Samples: 5056/5760, Loss: 0.01478886604309082\n",
      "Epoch: 25, Samples: 5088/5760, Loss: 0.032806396484375\n",
      "Epoch: 25, Samples: 5120/5760, Loss: 0.02321597933769226\n",
      "Epoch: 25, Samples: 5152/5760, Loss: 0.022285759449005127\n",
      "Epoch: 25, Samples: 5184/5760, Loss: 0.013595521450042725\n",
      "Epoch: 25, Samples: 5216/5760, Loss: 0.021998628973960876\n",
      "Epoch: 25, Samples: 5248/5760, Loss: 0.02703481912612915\n",
      "Epoch: 25, Samples: 5280/5760, Loss: 0.02078065276145935\n",
      "Epoch: 25, Samples: 5312/5760, Loss: 0.01353868842124939\n",
      "Epoch: 25, Samples: 5344/5760, Loss: 0.02057698369026184\n",
      "Epoch: 25, Samples: 5376/5760, Loss: 0.010397017002105713\n",
      "Epoch: 25, Samples: 5408/5760, Loss: 0.031965211033821106\n",
      "Epoch: 25, Samples: 5440/5760, Loss: 0.01691541075706482\n",
      "Epoch: 25, Samples: 5472/5760, Loss: 0.016429156064987183\n",
      "Epoch: 25, Samples: 5504/5760, Loss: 0.02918916940689087\n",
      "Epoch: 25, Samples: 5536/5760, Loss: 0.012236356735229492\n",
      "Epoch: 25, Samples: 5568/5760, Loss: 0.0138053297996521\n",
      "Epoch: 25, Samples: 5600/5760, Loss: 0.015721440315246582\n",
      "Epoch: 25, Samples: 5632/5760, Loss: 0.014593362808227539\n",
      "Epoch: 25, Samples: 5664/5760, Loss: 0.014693766832351685\n",
      "Epoch: 25, Samples: 5696/5760, Loss: 0.019641607999801636\n",
      "Epoch: 25, Samples: 5728/5760, Loss: 1.0940054655075073\n",
      "\n",
      "Epoch: 25\n",
      "Training set: Average loss: 0.0280\n",
      "Validation set: Average loss: 0.2920, Accuracy: 760/818 (93%)\n",
      "Epoch: 26, Samples: 0/5760, Loss: 0.03516489267349243\n",
      "Epoch: 26, Samples: 32/5760, Loss: 0.009283334016799927\n",
      "Epoch: 26, Samples: 64/5760, Loss: 0.018603205680847168\n",
      "Epoch: 26, Samples: 96/5760, Loss: 0.014177531003952026\n",
      "Epoch: 26, Samples: 128/5760, Loss: 0.01606658101081848\n",
      "Epoch: 26, Samples: 160/5760, Loss: 0.05207401514053345\n",
      "Epoch: 26, Samples: 192/5760, Loss: 0.02729851007461548\n",
      "Epoch: 26, Samples: 224/5760, Loss: 0.009929895401000977\n",
      "Epoch: 26, Samples: 256/5760, Loss: 0.021645426750183105\n",
      "Epoch: 26, Samples: 288/5760, Loss: 0.019213974475860596\n",
      "Epoch: 26, Samples: 320/5760, Loss: 0.0106467604637146\n",
      "Epoch: 26, Samples: 352/5760, Loss: 0.03282648324966431\n",
      "Epoch: 26, Samples: 384/5760, Loss: 0.030815154314041138\n",
      "Epoch: 26, Samples: 416/5760, Loss: 0.01851150393486023\n",
      "Epoch: 26, Samples: 448/5760, Loss: 0.02129104733467102\n",
      "Epoch: 26, Samples: 480/5760, Loss: 0.02541908621788025\n",
      "Epoch: 26, Samples: 512/5760, Loss: 0.01659572124481201\n",
      "Epoch: 26, Samples: 544/5760, Loss: 0.019315242767333984\n",
      "Epoch: 26, Samples: 576/5760, Loss: 0.03362852334976196\n",
      "Epoch: 26, Samples: 608/5760, Loss: 0.036468490958213806\n",
      "Epoch: 26, Samples: 640/5760, Loss: 0.04296006262302399\n",
      "Epoch: 26, Samples: 672/5760, Loss: 0.05174151062965393\n",
      "Epoch: 26, Samples: 704/5760, Loss: 0.017045944929122925\n",
      "Epoch: 26, Samples: 736/5760, Loss: 0.017909228801727295\n",
      "Epoch: 26, Samples: 768/5760, Loss: 0.018092021346092224\n",
      "Epoch: 26, Samples: 800/5760, Loss: 0.02280484139919281\n",
      "Epoch: 26, Samples: 832/5760, Loss: 0.05525901913642883\n",
      "Epoch: 26, Samples: 864/5760, Loss: 0.021797865629196167\n",
      "Epoch: 26, Samples: 896/5760, Loss: 0.025264233350753784\n",
      "Epoch: 26, Samples: 928/5760, Loss: 0.04047134518623352\n",
      "Epoch: 26, Samples: 960/5760, Loss: 0.024869590997695923\n",
      "Epoch: 26, Samples: 992/5760, Loss: 0.019482463598251343\n",
      "Epoch: 26, Samples: 1024/5760, Loss: 0.016350001096725464\n",
      "Epoch: 26, Samples: 1056/5760, Loss: 0.02386540174484253\n",
      "Epoch: 26, Samples: 1088/5760, Loss: 0.020824000239372253\n",
      "Epoch: 26, Samples: 1120/5760, Loss: 0.022296935319900513\n",
      "Epoch: 26, Samples: 1152/5760, Loss: 0.05416102707386017\n",
      "Epoch: 26, Samples: 1184/5760, Loss: 0.035670697689056396\n",
      "Epoch: 26, Samples: 1216/5760, Loss: 0.03301048278808594\n",
      "Epoch: 26, Samples: 1248/5760, Loss: 0.026195168495178223\n",
      "Epoch: 26, Samples: 1280/5760, Loss: 0.024535566568374634\n",
      "Epoch: 26, Samples: 1312/5760, Loss: 0.012339979410171509\n",
      "Epoch: 26, Samples: 1344/5760, Loss: 0.0379389226436615\n",
      "Epoch: 26, Samples: 1376/5760, Loss: 0.03175114095211029\n",
      "Epoch: 26, Samples: 1408/5760, Loss: 0.03293365240097046\n",
      "Epoch: 26, Samples: 1440/5760, Loss: 0.03898876905441284\n",
      "Epoch: 26, Samples: 1472/5760, Loss: 0.026229143142700195\n",
      "Epoch: 26, Samples: 1504/5760, Loss: 0.020527780055999756\n",
      "Epoch: 26, Samples: 1536/5760, Loss: 0.04444509744644165\n",
      "Epoch: 26, Samples: 1568/5760, Loss: 0.02375885844230652\n",
      "Epoch: 26, Samples: 1600/5760, Loss: 0.032303616404533386\n",
      "Epoch: 26, Samples: 1632/5760, Loss: 0.025607913732528687\n",
      "Epoch: 26, Samples: 1664/5760, Loss: 0.03223538398742676\n",
      "Epoch: 26, Samples: 1696/5760, Loss: 0.03279140591621399\n",
      "Epoch: 26, Samples: 1728/5760, Loss: 0.0165366530418396\n",
      "Epoch: 26, Samples: 1760/5760, Loss: 0.037003934383392334\n",
      "Epoch: 26, Samples: 1792/5760, Loss: 0.038468554615974426\n",
      "Epoch: 26, Samples: 1824/5760, Loss: 0.026437908411026\n",
      "Epoch: 26, Samples: 1856/5760, Loss: 0.018726646900177002\n",
      "Epoch: 26, Samples: 1888/5760, Loss: 0.02189156413078308\n",
      "Epoch: 26, Samples: 1920/5760, Loss: 0.02670702338218689\n",
      "Epoch: 26, Samples: 1952/5760, Loss: 0.015088021755218506\n",
      "Epoch: 26, Samples: 1984/5760, Loss: 0.0397849977016449\n",
      "Epoch: 26, Samples: 2016/5760, Loss: 0.016385436058044434\n",
      "Epoch: 26, Samples: 2048/5760, Loss: 0.014032483100891113\n",
      "Epoch: 26, Samples: 2080/5760, Loss: 0.033480048179626465\n",
      "Epoch: 26, Samples: 2112/5760, Loss: 0.026824116706848145\n",
      "Epoch: 26, Samples: 2144/5760, Loss: 0.02060878276824951\n",
      "Epoch: 26, Samples: 2176/5760, Loss: 0.022532105445861816\n",
      "Epoch: 26, Samples: 2208/5760, Loss: 0.013284683227539062\n",
      "Epoch: 26, Samples: 2240/5760, Loss: 0.014769583940505981\n",
      "Epoch: 26, Samples: 2272/5760, Loss: 0.009778708219528198\n",
      "Epoch: 26, Samples: 2304/5760, Loss: 0.014693468809127808\n",
      "Epoch: 26, Samples: 2336/5760, Loss: 0.014672577381134033\n",
      "Epoch: 26, Samples: 2368/5760, Loss: 0.014564722776412964\n",
      "Epoch: 26, Samples: 2400/5760, Loss: 0.029888436198234558\n",
      "Epoch: 26, Samples: 2432/5760, Loss: 0.017707228660583496\n",
      "Epoch: 26, Samples: 2464/5760, Loss: 0.02022060751914978\n",
      "Epoch: 26, Samples: 2496/5760, Loss: 0.016111716628074646\n",
      "Epoch: 26, Samples: 2528/5760, Loss: 0.022866815328598022\n",
      "Epoch: 26, Samples: 2560/5760, Loss: 0.03985998034477234\n",
      "Epoch: 26, Samples: 2592/5760, Loss: 0.01912897825241089\n",
      "Epoch: 26, Samples: 2624/5760, Loss: 0.017705827951431274\n",
      "Epoch: 26, Samples: 2656/5760, Loss: 0.007761359214782715\n",
      "Epoch: 26, Samples: 2688/5760, Loss: 0.029511883854866028\n",
      "Epoch: 26, Samples: 2720/5760, Loss: 0.016639113426208496\n",
      "Epoch: 26, Samples: 2752/5760, Loss: 0.03219619393348694\n",
      "Epoch: 26, Samples: 2784/5760, Loss: 0.019443392753601074\n",
      "Epoch: 26, Samples: 2816/5760, Loss: 0.019299477338790894\n",
      "Epoch: 26, Samples: 2848/5760, Loss: 0.03873775899410248\n",
      "Epoch: 26, Samples: 2880/5760, Loss: 0.01820191740989685\n",
      "Epoch: 26, Samples: 2912/5760, Loss: 0.008767813444137573\n",
      "Epoch: 26, Samples: 2944/5760, Loss: 0.013142704963684082\n",
      "Epoch: 26, Samples: 2976/5760, Loss: 0.020467668771743774\n",
      "Epoch: 26, Samples: 3008/5760, Loss: 0.01303011178970337\n",
      "Epoch: 26, Samples: 3040/5760, Loss: 0.015485316514968872\n",
      "Epoch: 26, Samples: 3072/5760, Loss: 0.013936400413513184\n",
      "Epoch: 26, Samples: 3104/5760, Loss: 0.01165589690208435\n",
      "Epoch: 26, Samples: 3136/5760, Loss: 0.018371999263763428\n",
      "Epoch: 26, Samples: 3168/5760, Loss: 0.020181357860565186\n",
      "Epoch: 26, Samples: 3200/5760, Loss: 0.02249044179916382\n",
      "Epoch: 26, Samples: 3232/5760, Loss: 0.0173046737909317\n",
      "Epoch: 26, Samples: 3264/5760, Loss: 0.011327624320983887\n",
      "Epoch: 26, Samples: 3296/5760, Loss: 0.03046548366546631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Samples: 3328/5760, Loss: 0.030921876430511475\n",
      "Epoch: 26, Samples: 3360/5760, Loss: 0.020443975925445557\n",
      "Epoch: 26, Samples: 3392/5760, Loss: 0.016919076442718506\n",
      "Epoch: 26, Samples: 3424/5760, Loss: 0.03043997287750244\n",
      "Epoch: 26, Samples: 3456/5760, Loss: 0.06406159698963165\n",
      "Epoch: 26, Samples: 3488/5760, Loss: 0.016151994466781616\n",
      "Epoch: 26, Samples: 3520/5760, Loss: 0.015600413084030151\n",
      "Epoch: 26, Samples: 3552/5760, Loss: 0.009865790605545044\n",
      "Epoch: 26, Samples: 3584/5760, Loss: 0.013388186693191528\n",
      "Epoch: 26, Samples: 3616/5760, Loss: 0.02033570408821106\n",
      "Epoch: 26, Samples: 3648/5760, Loss: 0.021492183208465576\n",
      "Epoch: 26, Samples: 3680/5760, Loss: 0.012256801128387451\n",
      "Epoch: 26, Samples: 3712/5760, Loss: 0.03078688681125641\n",
      "Epoch: 26, Samples: 3744/5760, Loss: 0.026143044233322144\n",
      "Epoch: 26, Samples: 3776/5760, Loss: 0.02688644826412201\n",
      "Epoch: 26, Samples: 3808/5760, Loss: 0.0167386531829834\n",
      "Epoch: 26, Samples: 3840/5760, Loss: 0.023694366216659546\n",
      "Epoch: 26, Samples: 3872/5760, Loss: 0.023878812789916992\n",
      "Epoch: 26, Samples: 3904/5760, Loss: 0.018944233655929565\n",
      "Epoch: 26, Samples: 3936/5760, Loss: 0.02324250340461731\n",
      "Epoch: 26, Samples: 3968/5760, Loss: 0.018231600522994995\n",
      "Epoch: 26, Samples: 4000/5760, Loss: 0.014989793300628662\n",
      "Epoch: 26, Samples: 4032/5760, Loss: 0.013451546430587769\n",
      "Epoch: 26, Samples: 4064/5760, Loss: 0.014604389667510986\n",
      "Epoch: 26, Samples: 4096/5760, Loss: 0.02460336685180664\n",
      "Epoch: 26, Samples: 4128/5760, Loss: 0.026044413447380066\n",
      "Epoch: 26, Samples: 4160/5760, Loss: 0.02878822386264801\n",
      "Epoch: 26, Samples: 4192/5760, Loss: 0.019987136125564575\n",
      "Epoch: 26, Samples: 4224/5760, Loss: 0.009084314107894897\n",
      "Epoch: 26, Samples: 4256/5760, Loss: 0.01353907585144043\n",
      "Epoch: 26, Samples: 4288/5760, Loss: 0.04957325756549835\n",
      "Epoch: 26, Samples: 4320/5760, Loss: 0.02766042947769165\n",
      "Epoch: 26, Samples: 4352/5760, Loss: 0.01805013418197632\n",
      "Epoch: 26, Samples: 4384/5760, Loss: 0.03216201066970825\n",
      "Epoch: 26, Samples: 4416/5760, Loss: 0.02005605399608612\n",
      "Epoch: 26, Samples: 4448/5760, Loss: 0.016781330108642578\n",
      "Epoch: 26, Samples: 4480/5760, Loss: 0.035981252789497375\n",
      "Epoch: 26, Samples: 4512/5760, Loss: 0.014810621738433838\n",
      "Epoch: 26, Samples: 4544/5760, Loss: 0.01688113808631897\n",
      "Epoch: 26, Samples: 4576/5760, Loss: 0.019235074520111084\n",
      "Epoch: 26, Samples: 4608/5760, Loss: 0.013608038425445557\n",
      "Epoch: 26, Samples: 4640/5760, Loss: 0.019268542528152466\n",
      "Epoch: 26, Samples: 4672/5760, Loss: 0.011348754167556763\n",
      "Epoch: 26, Samples: 4704/5760, Loss: 0.03857016563415527\n",
      "Epoch: 26, Samples: 4736/5760, Loss: 0.02272734045982361\n",
      "Epoch: 26, Samples: 4768/5760, Loss: 0.036539942026138306\n",
      "Epoch: 26, Samples: 4800/5760, Loss: 0.017440229654312134\n",
      "Epoch: 26, Samples: 4832/5760, Loss: 0.01786366105079651\n",
      "Epoch: 26, Samples: 4864/5760, Loss: 0.015551745891571045\n",
      "Epoch: 26, Samples: 4896/5760, Loss: 0.01193082332611084\n",
      "Epoch: 26, Samples: 4928/5760, Loss: 0.014347702264785767\n",
      "Epoch: 26, Samples: 4960/5760, Loss: 0.015209585428237915\n",
      "Epoch: 26, Samples: 4992/5760, Loss: 0.014378160238265991\n",
      "Epoch: 26, Samples: 5024/5760, Loss: 0.01671355962753296\n",
      "Epoch: 26, Samples: 5056/5760, Loss: 0.02374368906021118\n",
      "Epoch: 26, Samples: 5088/5760, Loss: 0.03783233463764191\n",
      "Epoch: 26, Samples: 5120/5760, Loss: 0.01610913872718811\n",
      "Epoch: 26, Samples: 5152/5760, Loss: 0.03063833713531494\n",
      "Epoch: 26, Samples: 5184/5760, Loss: 0.03022405505180359\n",
      "Epoch: 26, Samples: 5216/5760, Loss: 0.02385726571083069\n",
      "Epoch: 26, Samples: 5248/5760, Loss: 0.013766616582870483\n",
      "Epoch: 26, Samples: 5280/5760, Loss: 0.020322680473327637\n",
      "Epoch: 26, Samples: 5312/5760, Loss: 0.04624570906162262\n",
      "Epoch: 26, Samples: 5344/5760, Loss: 0.02238750457763672\n",
      "Epoch: 26, Samples: 5376/5760, Loss: 0.010517746210098267\n",
      "Epoch: 26, Samples: 5408/5760, Loss: 0.013911455869674683\n",
      "Epoch: 26, Samples: 5440/5760, Loss: 0.020200878381729126\n",
      "Epoch: 26, Samples: 5472/5760, Loss: 0.05285128951072693\n",
      "Epoch: 26, Samples: 5504/5760, Loss: 0.015086054801940918\n",
      "Epoch: 26, Samples: 5536/5760, Loss: 0.012460142374038696\n",
      "Epoch: 26, Samples: 5568/5760, Loss: 0.022084593772888184\n",
      "Epoch: 26, Samples: 5600/5760, Loss: 0.019370436668395996\n",
      "Epoch: 26, Samples: 5632/5760, Loss: 0.01117655634880066\n",
      "Epoch: 26, Samples: 5664/5760, Loss: 0.021369218826293945\n",
      "Epoch: 26, Samples: 5696/5760, Loss: 0.02048163115978241\n",
      "Epoch: 26, Samples: 5728/5760, Loss: 1.1836652755737305\n",
      "\n",
      "Epoch: 26\n",
      "Training set: Average loss: 0.0297\n",
      "Validation set: Average loss: 0.2906, Accuracy: 762/818 (93%)\n",
      "Epoch: 27, Samples: 0/5760, Loss: 0.011960595846176147\n",
      "Epoch: 27, Samples: 32/5760, Loss: 0.02435317635536194\n",
      "Epoch: 27, Samples: 64/5760, Loss: 0.009926527738571167\n",
      "Epoch: 27, Samples: 96/5760, Loss: 0.02084466814994812\n",
      "Epoch: 27, Samples: 128/5760, Loss: 0.02130994200706482\n",
      "Epoch: 27, Samples: 160/5760, Loss: 0.030917495489120483\n",
      "Epoch: 27, Samples: 192/5760, Loss: 0.03062570095062256\n",
      "Epoch: 27, Samples: 224/5760, Loss: 0.028581976890563965\n",
      "Epoch: 27, Samples: 256/5760, Loss: 0.02172379195690155\n",
      "Epoch: 27, Samples: 288/5760, Loss: 0.010712206363677979\n",
      "Epoch: 27, Samples: 320/5760, Loss: 0.022789746522903442\n",
      "Epoch: 27, Samples: 352/5760, Loss: 0.03983737528324127\n",
      "Epoch: 27, Samples: 384/5760, Loss: 0.03267204761505127\n",
      "Epoch: 27, Samples: 416/5760, Loss: 0.024761080741882324\n",
      "Epoch: 27, Samples: 448/5760, Loss: 0.029234468936920166\n",
      "Epoch: 27, Samples: 480/5760, Loss: 0.026984095573425293\n",
      "Epoch: 27, Samples: 512/5760, Loss: 0.024499326944351196\n",
      "Epoch: 27, Samples: 544/5760, Loss: 0.021486520767211914\n",
      "Epoch: 27, Samples: 576/5760, Loss: 0.0177927166223526\n",
      "Epoch: 27, Samples: 608/5760, Loss: 0.015386223793029785\n",
      "Epoch: 27, Samples: 640/5760, Loss: 0.01459568738937378\n",
      "Epoch: 27, Samples: 672/5760, Loss: 0.03218427300453186\n",
      "Epoch: 27, Samples: 704/5760, Loss: 0.018863409757614136\n",
      "Epoch: 27, Samples: 736/5760, Loss: 0.01789972186088562\n",
      "Epoch: 27, Samples: 768/5760, Loss: 0.01777869462966919\n",
      "Epoch: 27, Samples: 800/5760, Loss: 0.013226717710494995\n",
      "Epoch: 27, Samples: 832/5760, Loss: 0.013819783926010132\n",
      "Epoch: 27, Samples: 864/5760, Loss: 0.02985510230064392\n",
      "Epoch: 27, Samples: 896/5760, Loss: 0.015896886587142944\n",
      "Epoch: 27, Samples: 928/5760, Loss: 0.01998460292816162\n",
      "Epoch: 27, Samples: 960/5760, Loss: 0.013864517211914062\n",
      "Epoch: 27, Samples: 992/5760, Loss: 0.020663172006607056\n",
      "Epoch: 27, Samples: 1024/5760, Loss: 0.016516536474227905\n",
      "Epoch: 27, Samples: 1056/5760, Loss: 0.023614466190338135\n",
      "Epoch: 27, Samples: 1088/5760, Loss: 0.021198630332946777\n",
      "Epoch: 27, Samples: 1120/5760, Loss: 0.022905603051185608\n",
      "Epoch: 27, Samples: 1152/5760, Loss: 0.025083482265472412\n",
      "Epoch: 27, Samples: 1184/5760, Loss: 0.02190953493118286\n",
      "Epoch: 27, Samples: 1216/5760, Loss: 0.025780171155929565\n",
      "Epoch: 27, Samples: 1248/5760, Loss: 0.019139260053634644\n",
      "Epoch: 27, Samples: 1280/5760, Loss: 0.0324571430683136\n",
      "Epoch: 27, Samples: 1312/5760, Loss: 0.01869925856590271\n",
      "Epoch: 27, Samples: 1344/5760, Loss: 0.030269116163253784\n",
      "Epoch: 27, Samples: 1376/5760, Loss: 0.01860538125038147\n",
      "Epoch: 27, Samples: 1408/5760, Loss: 0.024925410747528076\n",
      "Epoch: 27, Samples: 1440/5760, Loss: 0.020821213722229004\n",
      "Epoch: 27, Samples: 1472/5760, Loss: 0.012231886386871338\n",
      "Epoch: 27, Samples: 1504/5760, Loss: 0.019481807947158813\n",
      "Epoch: 27, Samples: 1536/5760, Loss: 0.01266944408416748\n",
      "Epoch: 27, Samples: 1568/5760, Loss: 0.02164265513420105\n",
      "Epoch: 27, Samples: 1600/5760, Loss: 0.01483011245727539\n",
      "Epoch: 27, Samples: 1632/5760, Loss: 0.019688159227371216\n",
      "Epoch: 27, Samples: 1664/5760, Loss: 0.015595167875289917\n",
      "Epoch: 27, Samples: 1696/5760, Loss: 0.01212376356124878\n",
      "Epoch: 27, Samples: 1728/5760, Loss: 0.015792161226272583\n",
      "Epoch: 27, Samples: 1760/5760, Loss: 0.042219147086143494\n",
      "Epoch: 27, Samples: 1792/5760, Loss: 0.014307737350463867\n",
      "Epoch: 27, Samples: 1824/5760, Loss: 0.03558260202407837\n",
      "Epoch: 27, Samples: 1856/5760, Loss: 0.016951948404312134\n",
      "Epoch: 27, Samples: 1888/5760, Loss: 0.023125439882278442\n",
      "Epoch: 27, Samples: 1920/5760, Loss: 0.014462172985076904\n",
      "Epoch: 27, Samples: 1952/5760, Loss: 0.02733507752418518\n",
      "Epoch: 27, Samples: 1984/5760, Loss: 0.013833969831466675\n",
      "Epoch: 27, Samples: 2016/5760, Loss: 0.021075844764709473\n",
      "Epoch: 27, Samples: 2048/5760, Loss: 0.020454391837120056\n",
      "Epoch: 27, Samples: 2080/5760, Loss: 0.021684348583221436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Samples: 2112/5760, Loss: 0.04384414851665497\n",
      "Epoch: 27, Samples: 2144/5760, Loss: 0.022277504205703735\n",
      "Epoch: 27, Samples: 2176/5760, Loss: 0.008122295141220093\n",
      "Epoch: 27, Samples: 2208/5760, Loss: 0.03582377731800079\n",
      "Epoch: 27, Samples: 2240/5760, Loss: 0.038663625717163086\n",
      "Epoch: 27, Samples: 2272/5760, Loss: 0.010547369718551636\n",
      "Epoch: 27, Samples: 2304/5760, Loss: 0.028164058923721313\n",
      "Epoch: 27, Samples: 2336/5760, Loss: 0.015097945928573608\n",
      "Epoch: 27, Samples: 2368/5760, Loss: 0.013705521821975708\n",
      "Epoch: 27, Samples: 2400/5760, Loss: 0.02145788073539734\n",
      "Epoch: 27, Samples: 2432/5760, Loss: 0.014698505401611328\n",
      "Epoch: 27, Samples: 2464/5760, Loss: 0.020074039697647095\n",
      "Epoch: 27, Samples: 2496/5760, Loss: 0.010204136371612549\n",
      "Epoch: 27, Samples: 2528/5760, Loss: 0.02257329225540161\n",
      "Epoch: 27, Samples: 2560/5760, Loss: 0.021097376942634583\n",
      "Epoch: 27, Samples: 2592/5760, Loss: 0.011302471160888672\n",
      "Epoch: 27, Samples: 2624/5760, Loss: 0.011515617370605469\n",
      "Epoch: 27, Samples: 2656/5760, Loss: 0.031494706869125366\n",
      "Epoch: 27, Samples: 2688/5760, Loss: 0.01541823148727417\n",
      "Epoch: 27, Samples: 2720/5760, Loss: 0.010190099477767944\n",
      "Epoch: 27, Samples: 2752/5760, Loss: 0.02542167901992798\n",
      "Epoch: 27, Samples: 2784/5760, Loss: 0.02150610089302063\n",
      "Epoch: 27, Samples: 2816/5760, Loss: 0.03956879675388336\n",
      "Epoch: 27, Samples: 2848/5760, Loss: 0.015645593404769897\n",
      "Epoch: 27, Samples: 2880/5760, Loss: 0.023233562707901\n",
      "Epoch: 27, Samples: 2912/5760, Loss: 0.01411205530166626\n",
      "Epoch: 27, Samples: 2944/5760, Loss: 0.019534945487976074\n",
      "Epoch: 27, Samples: 2976/5760, Loss: 0.025829344987869263\n",
      "Epoch: 27, Samples: 3008/5760, Loss: 0.015794038772583008\n",
      "Epoch: 27, Samples: 3040/5760, Loss: 0.015293747186660767\n",
      "Epoch: 27, Samples: 3072/5760, Loss: 0.03226514160633087\n",
      "Epoch: 27, Samples: 3104/5760, Loss: 0.029284954071044922\n",
      "Epoch: 27, Samples: 3136/5760, Loss: 0.028347238898277283\n",
      "Epoch: 27, Samples: 3168/5760, Loss: 0.00775146484375\n",
      "Epoch: 27, Samples: 3200/5760, Loss: 0.018984436988830566\n",
      "Epoch: 27, Samples: 3232/5760, Loss: 0.015914618968963623\n",
      "Epoch: 27, Samples: 3264/5760, Loss: 0.022383660078048706\n",
      "Epoch: 27, Samples: 3296/5760, Loss: 0.015308976173400879\n",
      "Epoch: 27, Samples: 3328/5760, Loss: 0.010290086269378662\n",
      "Epoch: 27, Samples: 3360/5760, Loss: 0.015546947717666626\n",
      "Epoch: 27, Samples: 3392/5760, Loss: 0.01033732295036316\n",
      "Epoch: 27, Samples: 3424/5760, Loss: 0.011861413717269897\n",
      "Epoch: 27, Samples: 3456/5760, Loss: 0.018055975437164307\n",
      "Epoch: 27, Samples: 3488/5760, Loss: 0.020008593797683716\n",
      "Epoch: 27, Samples: 3520/5760, Loss: 0.014604836702346802\n",
      "Epoch: 27, Samples: 3552/5760, Loss: 0.03388844430446625\n",
      "Epoch: 27, Samples: 3584/5760, Loss: 0.023877084255218506\n",
      "Epoch: 27, Samples: 3616/5760, Loss: 0.017134368419647217\n",
      "Epoch: 27, Samples: 3648/5760, Loss: 0.011206060647964478\n",
      "Epoch: 27, Samples: 3680/5760, Loss: 0.023370474576950073\n",
      "Epoch: 27, Samples: 3712/5760, Loss: 0.02376703917980194\n",
      "Epoch: 27, Samples: 3744/5760, Loss: 0.019512146711349487\n",
      "Epoch: 27, Samples: 3776/5760, Loss: 0.03372278809547424\n",
      "Epoch: 27, Samples: 3808/5760, Loss: 0.02116107940673828\n",
      "Epoch: 27, Samples: 3840/5760, Loss: 0.013387501239776611\n",
      "Epoch: 27, Samples: 3872/5760, Loss: 0.03894796967506409\n",
      "Epoch: 27, Samples: 3904/5760, Loss: 0.019329160451889038\n",
      "Epoch: 27, Samples: 3936/5760, Loss: 0.010845750570297241\n",
      "Epoch: 27, Samples: 3968/5760, Loss: 0.019210800528526306\n",
      "Epoch: 27, Samples: 4000/5760, Loss: 0.031420618295669556\n",
      "Epoch: 27, Samples: 4032/5760, Loss: 0.02975785732269287\n",
      "Epoch: 27, Samples: 4064/5760, Loss: 0.023687154054641724\n",
      "Epoch: 27, Samples: 4096/5760, Loss: 0.024550661444664\n",
      "Epoch: 27, Samples: 4128/5760, Loss: 0.012121587991714478\n",
      "Epoch: 27, Samples: 4160/5760, Loss: 0.007878899574279785\n",
      "Epoch: 27, Samples: 4192/5760, Loss: 0.01266983151435852\n",
      "Epoch: 27, Samples: 4224/5760, Loss: 0.026621133089065552\n",
      "Epoch: 27, Samples: 4256/5760, Loss: 0.02457711100578308\n",
      "Epoch: 27, Samples: 4288/5760, Loss: 0.017630308866500854\n",
      "Epoch: 27, Samples: 4320/5760, Loss: 0.014029324054718018\n",
      "Epoch: 27, Samples: 4352/5760, Loss: 0.018725305795669556\n",
      "Epoch: 27, Samples: 4384/5760, Loss: 0.02504691481590271\n",
      "Epoch: 27, Samples: 4416/5760, Loss: 0.01921159029006958\n",
      "Epoch: 27, Samples: 4448/5760, Loss: 0.015151053667068481\n",
      "Epoch: 27, Samples: 4480/5760, Loss: 0.01839175820350647\n",
      "Epoch: 27, Samples: 4512/5760, Loss: 0.01729455590248108\n",
      "Epoch: 27, Samples: 4544/5760, Loss: 0.035527393221855164\n",
      "Epoch: 27, Samples: 4576/5760, Loss: 0.01814889907836914\n",
      "Epoch: 27, Samples: 4608/5760, Loss: 0.012204736471176147\n",
      "Epoch: 27, Samples: 4640/5760, Loss: 0.01816558837890625\n",
      "Epoch: 27, Samples: 4672/5760, Loss: 0.01937885582447052\n",
      "Epoch: 27, Samples: 4704/5760, Loss: 0.009597450494766235\n",
      "Epoch: 27, Samples: 4736/5760, Loss: 0.008291691541671753\n",
      "Epoch: 27, Samples: 4768/5760, Loss: 0.022999927401542664\n",
      "Epoch: 27, Samples: 4800/5760, Loss: 0.018902182579040527\n",
      "Epoch: 27, Samples: 4832/5760, Loss: 0.01994052529335022\n",
      "Epoch: 27, Samples: 4864/5760, Loss: 0.026330456137657166\n",
      "Epoch: 27, Samples: 4896/5760, Loss: 0.016252100467681885\n",
      "Epoch: 27, Samples: 4928/5760, Loss: 0.029769495129585266\n",
      "Epoch: 27, Samples: 4960/5760, Loss: 0.02079951763153076\n",
      "Epoch: 27, Samples: 4992/5760, Loss: 0.0185583233833313\n",
      "Epoch: 27, Samples: 5024/5760, Loss: 0.014100342988967896\n",
      "Epoch: 27, Samples: 5056/5760, Loss: 0.015298008918762207\n",
      "Epoch: 27, Samples: 5088/5760, Loss: 0.008514970541000366\n",
      "Epoch: 27, Samples: 5120/5760, Loss: 0.01870909333229065\n",
      "Epoch: 27, Samples: 5152/5760, Loss: 0.027633309364318848\n",
      "Epoch: 27, Samples: 5184/5760, Loss: 0.0172177255153656\n",
      "Epoch: 27, Samples: 5216/5760, Loss: 0.010796159505844116\n",
      "Epoch: 27, Samples: 5248/5760, Loss: 0.011970281600952148\n",
      "Epoch: 27, Samples: 5280/5760, Loss: 0.013484567403793335\n",
      "Epoch: 27, Samples: 5312/5760, Loss: 0.01036176085472107\n",
      "Epoch: 27, Samples: 5344/5760, Loss: 0.018034160137176514\n",
      "Epoch: 27, Samples: 5376/5760, Loss: 0.027050405740737915\n",
      "Epoch: 27, Samples: 5408/5760, Loss: 0.011801272630691528\n",
      "Epoch: 27, Samples: 5440/5760, Loss: 0.03126400709152222\n",
      "Epoch: 27, Samples: 5472/5760, Loss: 0.025256961584091187\n",
      "Epoch: 27, Samples: 5504/5760, Loss: 0.012653768062591553\n",
      "Epoch: 27, Samples: 5536/5760, Loss: 0.012489855289459229\n",
      "Epoch: 27, Samples: 5568/5760, Loss: 0.01130211353302002\n",
      "Epoch: 27, Samples: 5600/5760, Loss: 0.008888930082321167\n",
      "Epoch: 27, Samples: 5632/5760, Loss: 0.040922120213508606\n",
      "Epoch: 27, Samples: 5664/5760, Loss: 0.013215839862823486\n",
      "Epoch: 27, Samples: 5696/5760, Loss: 0.016524046659469604\n",
      "Epoch: 27, Samples: 5728/5760, Loss: 3.606581211090088\n",
      "\n",
      "Epoch: 27\n",
      "Training set: Average loss: 0.0402\n",
      "Validation set: Average loss: 0.2904, Accuracy: 762/818 (93%)\n",
      "Epoch: 28, Samples: 0/5760, Loss: 0.021562039852142334\n",
      "Epoch: 28, Samples: 32/5760, Loss: 0.028832942247390747\n",
      "Epoch: 28, Samples: 64/5760, Loss: 0.016985982656478882\n",
      "Epoch: 28, Samples: 96/5760, Loss: 0.017999351024627686\n",
      "Epoch: 28, Samples: 128/5760, Loss: 0.012594819068908691\n",
      "Epoch: 28, Samples: 160/5760, Loss: 0.009994685649871826\n",
      "Epoch: 28, Samples: 192/5760, Loss: 0.04108572006225586\n",
      "Epoch: 28, Samples: 224/5760, Loss: 0.020260661840438843\n",
      "Epoch: 28, Samples: 256/5760, Loss: 0.016147643327713013\n",
      "Epoch: 28, Samples: 288/5760, Loss: 0.026706218719482422\n",
      "Epoch: 28, Samples: 320/5760, Loss: 0.016645342111587524\n",
      "Epoch: 28, Samples: 352/5760, Loss: 0.017521440982818604\n",
      "Epoch: 28, Samples: 384/5760, Loss: 0.05024343729019165\n",
      "Epoch: 28, Samples: 416/5760, Loss: 0.02257232367992401\n",
      "Epoch: 28, Samples: 448/5760, Loss: 0.03130638599395752\n",
      "Epoch: 28, Samples: 480/5760, Loss: 0.0138988196849823\n",
      "Epoch: 28, Samples: 512/5760, Loss: 0.010904639959335327\n",
      "Epoch: 28, Samples: 544/5760, Loss: 0.03087964653968811\n",
      "Epoch: 28, Samples: 576/5760, Loss: 0.033225297927856445\n",
      "Epoch: 28, Samples: 608/5760, Loss: 0.027283310890197754\n",
      "Epoch: 28, Samples: 640/5760, Loss: 0.03123311698436737\n",
      "Epoch: 28, Samples: 672/5760, Loss: 0.016548603773117065\n",
      "Epoch: 28, Samples: 704/5760, Loss: 0.08898727595806122\n",
      "Epoch: 28, Samples: 736/5760, Loss: 0.031622886657714844\n",
      "Epoch: 28, Samples: 768/5760, Loss: 0.0335366427898407\n",
      "Epoch: 28, Samples: 800/5760, Loss: 0.06912633776664734\n",
      "Epoch: 28, Samples: 832/5760, Loss: 0.024877279996871948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Samples: 864/5760, Loss: 0.03047126531600952\n",
      "Epoch: 28, Samples: 896/5760, Loss: 0.016383349895477295\n",
      "Epoch: 28, Samples: 928/5760, Loss: 0.02570369839668274\n",
      "Epoch: 28, Samples: 960/5760, Loss: 0.024727478623390198\n",
      "Epoch: 28, Samples: 992/5760, Loss: 0.043614715337753296\n",
      "Epoch: 28, Samples: 1024/5760, Loss: 0.015161633491516113\n",
      "Epoch: 28, Samples: 1056/5760, Loss: 0.01930871605873108\n",
      "Epoch: 28, Samples: 1088/5760, Loss: 0.026342272758483887\n",
      "Epoch: 28, Samples: 1120/5760, Loss: 0.019702523946762085\n",
      "Epoch: 28, Samples: 1152/5760, Loss: 0.08127859234809875\n",
      "Epoch: 28, Samples: 1184/5760, Loss: 0.029151424765586853\n",
      "Epoch: 28, Samples: 1216/5760, Loss: 0.02782885730266571\n",
      "Epoch: 28, Samples: 1248/5760, Loss: 0.019338294863700867\n",
      "Epoch: 28, Samples: 1280/5760, Loss: 0.0299137681722641\n",
      "Epoch: 28, Samples: 1312/5760, Loss: 0.011278986930847168\n",
      "Epoch: 28, Samples: 1344/5760, Loss: 0.03324764966964722\n",
      "Epoch: 28, Samples: 1376/5760, Loss: 0.020855486392974854\n",
      "Epoch: 28, Samples: 1408/5760, Loss: 0.023003697395324707\n",
      "Epoch: 28, Samples: 1440/5760, Loss: 0.018368154764175415\n",
      "Epoch: 28, Samples: 1472/5760, Loss: 0.012698560953140259\n",
      "Epoch: 28, Samples: 1504/5760, Loss: 0.021244317293167114\n",
      "Epoch: 28, Samples: 1536/5760, Loss: 0.013843953609466553\n",
      "Epoch: 28, Samples: 1568/5760, Loss: 0.02370360493659973\n",
      "Epoch: 28, Samples: 1600/5760, Loss: 0.04374268651008606\n",
      "Epoch: 28, Samples: 1632/5760, Loss: 0.027322828769683838\n",
      "Epoch: 28, Samples: 1664/5760, Loss: 0.046070873737335205\n",
      "Epoch: 28, Samples: 1696/5760, Loss: 0.010681092739105225\n",
      "Epoch: 28, Samples: 1728/5760, Loss: 0.03186342120170593\n",
      "Epoch: 28, Samples: 1760/5760, Loss: 0.024008452892303467\n",
      "Epoch: 28, Samples: 1792/5760, Loss: 0.015333086252212524\n",
      "Epoch: 28, Samples: 1824/5760, Loss: 0.017628729343414307\n",
      "Epoch: 28, Samples: 1856/5760, Loss: 0.03552074730396271\n",
      "Epoch: 28, Samples: 1888/5760, Loss: 0.03135468065738678\n",
      "Epoch: 28, Samples: 1920/5760, Loss: 0.018154948949813843\n",
      "Epoch: 28, Samples: 1952/5760, Loss: 0.006476789712905884\n",
      "Epoch: 28, Samples: 1984/5760, Loss: 0.03379526734352112\n",
      "Epoch: 28, Samples: 2016/5760, Loss: 0.014250844717025757\n",
      "Epoch: 28, Samples: 2048/5760, Loss: 0.023543953895568848\n",
      "Epoch: 28, Samples: 2080/5760, Loss: 0.014838486909866333\n",
      "Epoch: 28, Samples: 2112/5760, Loss: 0.019032657146453857\n",
      "Epoch: 28, Samples: 2144/5760, Loss: 0.02211451530456543\n",
      "Epoch: 28, Samples: 2176/5760, Loss: 0.036588042974472046\n",
      "Epoch: 28, Samples: 2208/5760, Loss: 0.022748202085494995\n",
      "Epoch: 28, Samples: 2240/5760, Loss: 0.01735439896583557\n",
      "Epoch: 28, Samples: 2272/5760, Loss: 0.01961272954940796\n",
      "Epoch: 28, Samples: 2304/5760, Loss: 0.02402317523956299\n",
      "Epoch: 28, Samples: 2336/5760, Loss: 0.022859662771224976\n",
      "Epoch: 28, Samples: 2368/5760, Loss: 0.0347287654876709\n",
      "Epoch: 28, Samples: 2400/5760, Loss: 0.016414672136306763\n",
      "Epoch: 28, Samples: 2432/5760, Loss: 0.016305774450302124\n",
      "Epoch: 28, Samples: 2464/5760, Loss: 0.022896260023117065\n",
      "Epoch: 28, Samples: 2496/5760, Loss: 0.022256866097450256\n",
      "Epoch: 28, Samples: 2528/5760, Loss: 0.02886331081390381\n",
      "Epoch: 28, Samples: 2560/5760, Loss: 0.016115844249725342\n",
      "Epoch: 28, Samples: 2592/5760, Loss: 0.022832825779914856\n",
      "Epoch: 28, Samples: 2624/5760, Loss: 0.017604559659957886\n",
      "Epoch: 28, Samples: 2656/5760, Loss: 0.013842076063156128\n",
      "Epoch: 28, Samples: 2688/5760, Loss: 0.017553120851516724\n",
      "Epoch: 28, Samples: 2720/5760, Loss: 0.014356076717376709\n",
      "Epoch: 28, Samples: 2752/5760, Loss: 0.013093560934066772\n",
      "Epoch: 28, Samples: 2784/5760, Loss: 0.01842600107192993\n",
      "Epoch: 28, Samples: 2816/5760, Loss: 0.010788053274154663\n",
      "Epoch: 28, Samples: 2848/5760, Loss: 0.011460989713668823\n",
      "Epoch: 28, Samples: 2880/5760, Loss: 0.021471112966537476\n",
      "Epoch: 28, Samples: 2912/5760, Loss: 0.023297160863876343\n",
      "Epoch: 28, Samples: 2944/5760, Loss: 0.01754075288772583\n",
      "Epoch: 28, Samples: 2976/5760, Loss: 0.019124656915664673\n",
      "Epoch: 28, Samples: 3008/5760, Loss: 0.030841395258903503\n",
      "Epoch: 28, Samples: 3040/5760, Loss: 0.017208248376846313\n",
      "Epoch: 28, Samples: 3072/5760, Loss: 0.009769797325134277\n",
      "Epoch: 28, Samples: 3104/5760, Loss: 0.024080559611320496\n",
      "Epoch: 28, Samples: 3136/5760, Loss: 0.01465579867362976\n",
      "Epoch: 28, Samples: 3168/5760, Loss: 0.019186675548553467\n",
      "Epoch: 28, Samples: 3200/5760, Loss: 0.01722043752670288\n",
      "Epoch: 28, Samples: 3232/5760, Loss: 0.016029685735702515\n",
      "Epoch: 28, Samples: 3264/5760, Loss: 0.02379581332206726\n",
      "Epoch: 28, Samples: 3296/5760, Loss: 0.016434818506240845\n",
      "Epoch: 28, Samples: 3328/5760, Loss: 0.014364540576934814\n",
      "Epoch: 28, Samples: 3360/5760, Loss: 0.011046528816223145\n",
      "Epoch: 28, Samples: 3392/5760, Loss: 0.019246667623519897\n",
      "Epoch: 28, Samples: 3424/5760, Loss: 0.023834437131881714\n",
      "Epoch: 28, Samples: 3456/5760, Loss: 0.03712640702724457\n",
      "Epoch: 28, Samples: 3488/5760, Loss: 0.018709644675254822\n",
      "Epoch: 28, Samples: 3520/5760, Loss: 0.011488676071166992\n",
      "Epoch: 28, Samples: 3552/5760, Loss: 0.01232495903968811\n",
      "Epoch: 28, Samples: 3584/5760, Loss: 0.010499566793441772\n",
      "Epoch: 28, Samples: 3616/5760, Loss: 0.013963162899017334\n",
      "Epoch: 28, Samples: 3648/5760, Loss: 0.02255690097808838\n",
      "Epoch: 28, Samples: 3680/5760, Loss: 0.012060105800628662\n",
      "Epoch: 28, Samples: 3712/5760, Loss: 0.016069799661636353\n",
      "Epoch: 28, Samples: 3744/5760, Loss: 0.03335639834403992\n",
      "Epoch: 28, Samples: 3776/5760, Loss: 0.012084484100341797\n",
      "Epoch: 28, Samples: 3808/5760, Loss: 0.02259078621864319\n",
      "Epoch: 28, Samples: 3840/5760, Loss: 0.023338615894317627\n",
      "Epoch: 28, Samples: 3872/5760, Loss: 0.018477916717529297\n",
      "Epoch: 28, Samples: 3904/5760, Loss: 0.010688543319702148\n",
      "Epoch: 28, Samples: 3936/5760, Loss: 0.013277798891067505\n",
      "Epoch: 28, Samples: 3968/5760, Loss: 0.016745269298553467\n",
      "Epoch: 28, Samples: 4000/5760, Loss: 0.015073120594024658\n",
      "Epoch: 28, Samples: 4032/5760, Loss: 0.019690394401550293\n",
      "Epoch: 28, Samples: 4064/5760, Loss: 0.01894223690032959\n",
      "Epoch: 28, Samples: 4096/5760, Loss: 0.017962008714675903\n",
      "Epoch: 28, Samples: 4128/5760, Loss: 0.018366873264312744\n",
      "Epoch: 28, Samples: 4160/5760, Loss: 0.030631184577941895\n",
      "Epoch: 28, Samples: 4192/5760, Loss: 0.03281685709953308\n",
      "Epoch: 28, Samples: 4224/5760, Loss: 0.04222431778907776\n",
      "Epoch: 28, Samples: 4256/5760, Loss: 0.01957458257675171\n",
      "Epoch: 28, Samples: 4288/5760, Loss: 0.021724998950958252\n",
      "Epoch: 28, Samples: 4320/5760, Loss: 0.021152466535568237\n",
      "Epoch: 28, Samples: 4352/5760, Loss: 0.03503194451332092\n",
      "Epoch: 28, Samples: 4384/5760, Loss: 0.011159569025039673\n",
      "Epoch: 28, Samples: 4416/5760, Loss: 0.037486761808395386\n",
      "Epoch: 28, Samples: 4448/5760, Loss: 0.03319227695465088\n",
      "Epoch: 28, Samples: 4480/5760, Loss: 0.01179051399230957\n",
      "Epoch: 28, Samples: 4512/5760, Loss: 0.026133209466934204\n",
      "Epoch: 28, Samples: 4544/5760, Loss: 0.0218927264213562\n",
      "Epoch: 28, Samples: 4576/5760, Loss: 0.018068969249725342\n",
      "Epoch: 28, Samples: 4608/5760, Loss: 0.021547973155975342\n",
      "Epoch: 28, Samples: 4640/5760, Loss: 0.022417455911636353\n",
      "Epoch: 28, Samples: 4672/5760, Loss: 0.016860395669937134\n",
      "Epoch: 28, Samples: 4704/5760, Loss: 0.03808790445327759\n",
      "Epoch: 28, Samples: 4736/5760, Loss: 0.012245386838912964\n",
      "Epoch: 28, Samples: 4768/5760, Loss: 0.014180809259414673\n",
      "Epoch: 28, Samples: 4800/5760, Loss: 0.018822237849235535\n",
      "Epoch: 28, Samples: 4832/5760, Loss: 0.022883594036102295\n",
      "Epoch: 28, Samples: 4864/5760, Loss: 0.03978079557418823\n",
      "Epoch: 28, Samples: 4896/5760, Loss: 0.026779234409332275\n",
      "Epoch: 28, Samples: 4928/5760, Loss: 0.019601494073867798\n",
      "Epoch: 28, Samples: 4960/5760, Loss: 0.009558677673339844\n",
      "Epoch: 28, Samples: 4992/5760, Loss: 0.017275482416152954\n",
      "Epoch: 28, Samples: 5024/5760, Loss: 0.01677834987640381\n",
      "Epoch: 28, Samples: 5056/5760, Loss: 0.012505114078521729\n",
      "Epoch: 28, Samples: 5088/5760, Loss: 0.015419244766235352\n",
      "Epoch: 28, Samples: 5120/5760, Loss: 0.018258124589920044\n",
      "Epoch: 28, Samples: 5152/5760, Loss: 0.010998755693435669\n",
      "Epoch: 28, Samples: 5184/5760, Loss: 0.011707961559295654\n",
      "Epoch: 28, Samples: 5216/5760, Loss: 0.03533008694648743\n",
      "Epoch: 28, Samples: 5248/5760, Loss: 0.017397195100784302\n",
      "Epoch: 28, Samples: 5280/5760, Loss: 0.017520129680633545\n",
      "Epoch: 28, Samples: 5312/5760, Loss: 0.01198357343673706\n",
      "Epoch: 28, Samples: 5344/5760, Loss: 0.017735838890075684\n",
      "Epoch: 28, Samples: 5376/5760, Loss: 0.021956145763397217\n",
      "Epoch: 28, Samples: 5408/5760, Loss: 0.018181711435317993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Samples: 5440/5760, Loss: 0.01919621229171753\n",
      "Epoch: 28, Samples: 5472/5760, Loss: 0.01087719202041626\n",
      "Epoch: 28, Samples: 5504/5760, Loss: 0.015267908573150635\n",
      "Epoch: 28, Samples: 5536/5760, Loss: 0.019541114568710327\n",
      "Epoch: 28, Samples: 5568/5760, Loss: 0.010899215936660767\n",
      "Epoch: 28, Samples: 5600/5760, Loss: 0.014314353466033936\n",
      "Epoch: 28, Samples: 5632/5760, Loss: 0.026960760354995728\n",
      "Epoch: 28, Samples: 5664/5760, Loss: 0.014451056718826294\n",
      "Epoch: 28, Samples: 5696/5760, Loss: 0.029192715883255005\n",
      "Epoch: 28, Samples: 5728/5760, Loss: 0.742371678352356\n",
      "\n",
      "Epoch: 28\n",
      "Training set: Average loss: 0.0263\n",
      "Validation set: Average loss: 0.2883, Accuracy: 763/818 (93%)\n",
      "Epoch: 29, Samples: 0/5760, Loss: 0.018901437520980835\n",
      "Epoch: 29, Samples: 32/5760, Loss: 0.014508038759231567\n",
      "Epoch: 29, Samples: 64/5760, Loss: 0.012869954109191895\n",
      "Epoch: 29, Samples: 96/5760, Loss: 0.014438658952713013\n",
      "Epoch: 29, Samples: 128/5760, Loss: 0.02065691351890564\n",
      "Epoch: 29, Samples: 160/5760, Loss: 0.013177871704101562\n",
      "Epoch: 29, Samples: 192/5760, Loss: 0.0655343234539032\n",
      "Epoch: 29, Samples: 224/5760, Loss: 0.021766990423202515\n",
      "Epoch: 29, Samples: 256/5760, Loss: 0.01875770092010498\n",
      "Epoch: 29, Samples: 288/5760, Loss: 0.014198541641235352\n",
      "Epoch: 29, Samples: 320/5760, Loss: 0.015173643827438354\n",
      "Epoch: 29, Samples: 352/5760, Loss: 0.028225451707839966\n",
      "Epoch: 29, Samples: 384/5760, Loss: 0.034796059131622314\n",
      "Epoch: 29, Samples: 416/5760, Loss: 0.02335798740386963\n",
      "Epoch: 29, Samples: 448/5760, Loss: 0.016137152910232544\n",
      "Epoch: 29, Samples: 480/5760, Loss: 0.024355098605155945\n",
      "Epoch: 29, Samples: 512/5760, Loss: 0.017410844564437866\n",
      "Epoch: 29, Samples: 544/5760, Loss: 0.032965317368507385\n",
      "Epoch: 29, Samples: 576/5760, Loss: 0.020411089062690735\n",
      "Epoch: 29, Samples: 608/5760, Loss: 0.02676689624786377\n",
      "Epoch: 29, Samples: 640/5760, Loss: 0.01595616340637207\n",
      "Epoch: 29, Samples: 672/5760, Loss: 0.023936599493026733\n",
      "Epoch: 29, Samples: 704/5760, Loss: 0.020584404468536377\n",
      "Epoch: 29, Samples: 736/5760, Loss: 0.016171246767044067\n",
      "Epoch: 29, Samples: 768/5760, Loss: 0.010408610105514526\n",
      "Epoch: 29, Samples: 800/5760, Loss: 0.04781292378902435\n",
      "Epoch: 29, Samples: 832/5760, Loss: 0.014702379703521729\n",
      "Epoch: 29, Samples: 864/5760, Loss: 0.01812085509300232\n",
      "Epoch: 29, Samples: 896/5760, Loss: 0.023514285683631897\n",
      "Epoch: 29, Samples: 928/5760, Loss: 0.025873273611068726\n",
      "Epoch: 29, Samples: 960/5760, Loss: 0.03773505985736847\n",
      "Epoch: 29, Samples: 992/5760, Loss: 0.014064997434616089\n",
      "Epoch: 29, Samples: 1024/5760, Loss: 0.017318278551101685\n",
      "Epoch: 29, Samples: 1056/5760, Loss: 0.01802361011505127\n",
      "Epoch: 29, Samples: 1088/5760, Loss: 0.04441216588020325\n",
      "Epoch: 29, Samples: 1120/5760, Loss: 0.029783502221107483\n",
      "Epoch: 29, Samples: 1152/5760, Loss: 0.029928147792816162\n",
      "Epoch: 29, Samples: 1184/5760, Loss: 0.0150737464427948\n",
      "Epoch: 29, Samples: 1216/5760, Loss: 0.037347450852394104\n",
      "Epoch: 29, Samples: 1248/5760, Loss: 0.02173483371734619\n",
      "Epoch: 29, Samples: 1280/5760, Loss: 0.010837763547897339\n",
      "Epoch: 29, Samples: 1312/5760, Loss: 0.023628368973731995\n",
      "Epoch: 29, Samples: 1344/5760, Loss: 0.014849692583084106\n",
      "Epoch: 29, Samples: 1376/5760, Loss: 0.018727898597717285\n",
      "Epoch: 29, Samples: 1408/5760, Loss: 0.025266051292419434\n",
      "Epoch: 29, Samples: 1440/5760, Loss: 0.011310040950775146\n",
      "Epoch: 29, Samples: 1472/5760, Loss: 0.012205377221107483\n",
      "Epoch: 29, Samples: 1504/5760, Loss: 0.015573233366012573\n",
      "Epoch: 29, Samples: 1536/5760, Loss: 0.021186262369155884\n",
      "Epoch: 29, Samples: 1568/5760, Loss: 0.0135439932346344\n",
      "Epoch: 29, Samples: 1600/5760, Loss: 0.014326810836791992\n",
      "Epoch: 29, Samples: 1632/5760, Loss: 0.017337054014205933\n",
      "Epoch: 29, Samples: 1664/5760, Loss: 0.015906155109405518\n",
      "Epoch: 29, Samples: 1696/5760, Loss: 0.016218602657318115\n",
      "Epoch: 29, Samples: 1728/5760, Loss: 0.022251546382904053\n",
      "Epoch: 29, Samples: 1760/5760, Loss: 0.026034757494926453\n",
      "Epoch: 29, Samples: 1792/5760, Loss: 0.013818830251693726\n",
      "Epoch: 29, Samples: 1824/5760, Loss: 0.032588064670562744\n",
      "Epoch: 29, Samples: 1856/5760, Loss: 0.01635521650314331\n",
      "Epoch: 29, Samples: 1888/5760, Loss: 0.019443362951278687\n",
      "Epoch: 29, Samples: 1920/5760, Loss: 0.008261412382125854\n",
      "Epoch: 29, Samples: 1952/5760, Loss: 0.0342215895652771\n",
      "Epoch: 29, Samples: 1984/5760, Loss: 0.010180562734603882\n",
      "Epoch: 29, Samples: 2016/5760, Loss: 0.023033753037452698\n",
      "Epoch: 29, Samples: 2048/5760, Loss: 0.025184452533721924\n",
      "Epoch: 29, Samples: 2080/5760, Loss: 0.008879482746124268\n",
      "Epoch: 29, Samples: 2112/5760, Loss: 0.024053901433944702\n",
      "Epoch: 29, Samples: 2144/5760, Loss: 0.014858931303024292\n",
      "Epoch: 29, Samples: 2176/5760, Loss: 0.007884770631790161\n",
      "Epoch: 29, Samples: 2208/5760, Loss: 0.02348572015762329\n",
      "Epoch: 29, Samples: 2240/5760, Loss: 0.023698389530181885\n",
      "Epoch: 29, Samples: 2272/5760, Loss: 0.012676090002059937\n",
      "Epoch: 29, Samples: 2304/5760, Loss: 0.01639944314956665\n",
      "Epoch: 29, Samples: 2336/5760, Loss: 0.022930309176445007\n",
      "Epoch: 29, Samples: 2368/5760, Loss: 0.018328756093978882\n",
      "Epoch: 29, Samples: 2400/5760, Loss: 0.01165759563446045\n",
      "Epoch: 29, Samples: 2432/5760, Loss: 0.01524263620376587\n",
      "Epoch: 29, Samples: 2464/5760, Loss: 0.019251748919487\n",
      "Epoch: 29, Samples: 2496/5760, Loss: 0.037526726722717285\n",
      "Epoch: 29, Samples: 2528/5760, Loss: 0.0270158052444458\n",
      "Epoch: 29, Samples: 2560/5760, Loss: 0.023925602436065674\n",
      "Epoch: 29, Samples: 2592/5760, Loss: 0.03875115513801575\n",
      "Epoch: 29, Samples: 2624/5760, Loss: 0.024884894490242004\n",
      "Epoch: 29, Samples: 2656/5760, Loss: 0.030312642455101013\n",
      "Epoch: 29, Samples: 2688/5760, Loss: 0.010937154293060303\n",
      "Epoch: 29, Samples: 2720/5760, Loss: 0.020181775093078613\n",
      "Epoch: 29, Samples: 2752/5760, Loss: 0.011776149272918701\n",
      "Epoch: 29, Samples: 2784/5760, Loss: 0.015133291482925415\n",
      "Epoch: 29, Samples: 2816/5760, Loss: 0.01625761389732361\n",
      "Epoch: 29, Samples: 2848/5760, Loss: 0.01812925934791565\n",
      "Epoch: 29, Samples: 2880/5760, Loss: 0.011263102293014526\n",
      "Epoch: 29, Samples: 2912/5760, Loss: 0.015218555927276611\n",
      "Epoch: 29, Samples: 2944/5760, Loss: 0.016248703002929688\n",
      "Epoch: 29, Samples: 2976/5760, Loss: 0.012144505977630615\n",
      "Epoch: 29, Samples: 3008/5760, Loss: 0.02285856008529663\n",
      "Epoch: 29, Samples: 3040/5760, Loss: 0.011113017797470093\n",
      "Epoch: 29, Samples: 3072/5760, Loss: 0.010366618633270264\n",
      "Epoch: 29, Samples: 3104/5760, Loss: 0.014285445213317871\n",
      "Epoch: 29, Samples: 3136/5760, Loss: 0.015630900859832764\n",
      "Epoch: 29, Samples: 3168/5760, Loss: 0.03280279040336609\n",
      "Epoch: 29, Samples: 3200/5760, Loss: 0.013163268566131592\n",
      "Epoch: 29, Samples: 3232/5760, Loss: 0.021672993898391724\n",
      "Epoch: 29, Samples: 3264/5760, Loss: 0.014897525310516357\n",
      "Epoch: 29, Samples: 3296/5760, Loss: 0.013396263122558594\n",
      "Epoch: 29, Samples: 3328/5760, Loss: 0.023669421672821045\n",
      "Epoch: 29, Samples: 3360/5760, Loss: 0.0210554301738739\n",
      "Epoch: 29, Samples: 3392/5760, Loss: 0.01773172616958618\n",
      "Epoch: 29, Samples: 3424/5760, Loss: 0.02406451106071472\n",
      "Epoch: 29, Samples: 3456/5760, Loss: 0.010357975959777832\n",
      "Epoch: 29, Samples: 3488/5760, Loss: 0.02897791564464569\n",
      "Epoch: 29, Samples: 3520/5760, Loss: 0.019244298338890076\n",
      "Epoch: 29, Samples: 3552/5760, Loss: 0.025874733924865723\n",
      "Epoch: 29, Samples: 3584/5760, Loss: 0.01500675082206726\n",
      "Epoch: 29, Samples: 3616/5760, Loss: 0.011407583951950073\n",
      "Epoch: 29, Samples: 3648/5760, Loss: 0.026574507355690002\n",
      "Epoch: 29, Samples: 3680/5760, Loss: 0.011312991380691528\n",
      "Epoch: 29, Samples: 3712/5760, Loss: 0.015511542558670044\n",
      "Epoch: 29, Samples: 3744/5760, Loss: 0.019267484545707703\n",
      "Epoch: 29, Samples: 3776/5760, Loss: 0.01479482650756836\n",
      "Epoch: 29, Samples: 3808/5760, Loss: 0.03414824604988098\n",
      "Epoch: 29, Samples: 3840/5760, Loss: 0.022176280617713928\n",
      "Epoch: 29, Samples: 3872/5760, Loss: 0.015905261039733887\n",
      "Epoch: 29, Samples: 3904/5760, Loss: 0.015587106347084045\n",
      "Epoch: 29, Samples: 3936/5760, Loss: 0.013516128063201904\n",
      "Epoch: 29, Samples: 3968/5760, Loss: 0.013262897729873657\n",
      "Epoch: 29, Samples: 4000/5760, Loss: 0.016547352075576782\n",
      "Epoch: 29, Samples: 4032/5760, Loss: 0.015215575695037842\n",
      "Epoch: 29, Samples: 4064/5760, Loss: 0.017643064260482788\n",
      "Epoch: 29, Samples: 4096/5760, Loss: 0.015853047370910645\n",
      "Epoch: 29, Samples: 4128/5760, Loss: 0.016061782836914062\n",
      "Epoch: 29, Samples: 4160/5760, Loss: 0.014882922172546387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Samples: 4192/5760, Loss: 0.01100015640258789\n",
      "Epoch: 29, Samples: 4224/5760, Loss: 0.010012209415435791\n",
      "Epoch: 29, Samples: 4256/5760, Loss: 0.010855108499526978\n",
      "Epoch: 29, Samples: 4288/5760, Loss: 0.013914138078689575\n",
      "Epoch: 29, Samples: 4320/5760, Loss: 0.007686346769332886\n",
      "Epoch: 29, Samples: 4352/5760, Loss: 0.010326415300369263\n",
      "Epoch: 29, Samples: 4384/5760, Loss: 0.015422821044921875\n",
      "Epoch: 29, Samples: 4416/5760, Loss: 0.008632808923721313\n",
      "Epoch: 29, Samples: 4448/5760, Loss: 0.011484891176223755\n",
      "Epoch: 29, Samples: 4480/5760, Loss: 0.009354233741760254\n",
      "Epoch: 29, Samples: 4512/5760, Loss: 0.015602290630340576\n",
      "Epoch: 29, Samples: 4544/5760, Loss: 0.024398207664489746\n",
      "Epoch: 29, Samples: 4576/5760, Loss: 0.010614782571792603\n",
      "Epoch: 29, Samples: 4608/5760, Loss: 0.016189128160476685\n",
      "Epoch: 29, Samples: 4640/5760, Loss: 0.026562899351119995\n",
      "Epoch: 29, Samples: 4672/5760, Loss: 0.01194685697555542\n",
      "Epoch: 29, Samples: 4704/5760, Loss: 0.02279144525527954\n",
      "Epoch: 29, Samples: 4736/5760, Loss: 0.013706684112548828\n",
      "Epoch: 29, Samples: 4768/5760, Loss: 0.018430322408676147\n",
      "Epoch: 29, Samples: 4800/5760, Loss: 0.011077195405960083\n",
      "Epoch: 29, Samples: 4832/5760, Loss: 0.010423153638839722\n",
      "Epoch: 29, Samples: 4864/5760, Loss: 0.028347507119178772\n",
      "Epoch: 29, Samples: 4896/5760, Loss: 0.013226807117462158\n",
      "Epoch: 29, Samples: 4928/5760, Loss: 0.009689927101135254\n",
      "Epoch: 29, Samples: 4960/5760, Loss: 0.018863588571548462\n",
      "Epoch: 29, Samples: 4992/5760, Loss: 0.044452816247940063\n",
      "Epoch: 29, Samples: 5024/5760, Loss: 0.011270463466644287\n",
      "Epoch: 29, Samples: 5056/5760, Loss: 0.013224363327026367\n",
      "Epoch: 29, Samples: 5088/5760, Loss: 0.020109206438064575\n",
      "Epoch: 29, Samples: 5120/5760, Loss: 0.04023689031600952\n",
      "Epoch: 29, Samples: 5152/5760, Loss: 0.011579900979995728\n",
      "Epoch: 29, Samples: 5184/5760, Loss: 0.011269450187683105\n",
      "Epoch: 29, Samples: 5216/5760, Loss: 0.011612266302108765\n",
      "Epoch: 29, Samples: 5248/5760, Loss: 0.01843222975730896\n",
      "Epoch: 29, Samples: 5280/5760, Loss: 0.015187740325927734\n",
      "Epoch: 29, Samples: 5312/5760, Loss: 0.011580318212509155\n",
      "Epoch: 29, Samples: 5344/5760, Loss: 0.010441571474075317\n",
      "Epoch: 29, Samples: 5376/5760, Loss: 0.014437764883041382\n",
      "Epoch: 29, Samples: 5408/5760, Loss: 0.012910157442092896\n",
      "Epoch: 29, Samples: 5440/5760, Loss: 0.014966785907745361\n",
      "Epoch: 29, Samples: 5472/5760, Loss: 0.01330670714378357\n",
      "Epoch: 29, Samples: 5504/5760, Loss: 0.011872440576553345\n",
      "Epoch: 29, Samples: 5536/5760, Loss: 0.026083916425704956\n",
      "Epoch: 29, Samples: 5568/5760, Loss: 0.04014416038990021\n",
      "Epoch: 29, Samples: 5600/5760, Loss: 0.010816335678100586\n",
      "Epoch: 29, Samples: 5632/5760, Loss: 0.015399277210235596\n",
      "Epoch: 29, Samples: 5664/5760, Loss: 0.011050477623939514\n",
      "Epoch: 29, Samples: 5696/5760, Loss: 0.011619120836257935\n",
      "Epoch: 29, Samples: 5728/5760, Loss: 0.6154489517211914\n",
      "\n",
      "Epoch: 29\n",
      "Training set: Average loss: 0.0222\n",
      "Validation set: Average loss: 0.2877, Accuracy: 761/818 (93%)\n",
      "Epoch: 30, Samples: 0/5760, Loss: 0.011157095432281494\n",
      "Epoch: 30, Samples: 32/5760, Loss: 0.013046801090240479\n",
      "Epoch: 30, Samples: 64/5760, Loss: 0.0130825936794281\n",
      "Epoch: 30, Samples: 96/5760, Loss: 0.010937005281448364\n",
      "Epoch: 30, Samples: 128/5760, Loss: 0.010178208351135254\n",
      "Epoch: 30, Samples: 160/5760, Loss: 0.01972043514251709\n",
      "Epoch: 30, Samples: 192/5760, Loss: 0.03285977244377136\n",
      "Epoch: 30, Samples: 224/5760, Loss: 0.019503384828567505\n",
      "Epoch: 30, Samples: 256/5760, Loss: 0.013309270143508911\n",
      "Epoch: 30, Samples: 288/5760, Loss: 0.015442371368408203\n",
      "Epoch: 30, Samples: 320/5760, Loss: 0.018789440393447876\n",
      "Epoch: 30, Samples: 352/5760, Loss: 0.034738555550575256\n",
      "Epoch: 30, Samples: 384/5760, Loss: 0.02697780728340149\n",
      "Epoch: 30, Samples: 416/5760, Loss: 0.01631215214729309\n",
      "Epoch: 30, Samples: 448/5760, Loss: 0.03237569332122803\n",
      "Epoch: 30, Samples: 480/5760, Loss: 0.026666462421417236\n",
      "Epoch: 30, Samples: 512/5760, Loss: 0.020614445209503174\n",
      "Epoch: 30, Samples: 544/5760, Loss: 0.021658122539520264\n",
      "Epoch: 30, Samples: 576/5760, Loss: 0.018075406551361084\n",
      "Epoch: 30, Samples: 608/5760, Loss: 0.006363570690155029\n",
      "Epoch: 30, Samples: 640/5760, Loss: 0.015707552433013916\n",
      "Epoch: 30, Samples: 672/5760, Loss: 0.015760093927383423\n",
      "Epoch: 30, Samples: 704/5760, Loss: 0.01564866304397583\n",
      "Epoch: 30, Samples: 736/5760, Loss: 0.012592822313308716\n",
      "Epoch: 30, Samples: 768/5760, Loss: 0.022263675928115845\n",
      "Epoch: 30, Samples: 800/5760, Loss: 0.03532947599887848\n",
      "Epoch: 30, Samples: 832/5760, Loss: 0.022546887397766113\n",
      "Epoch: 30, Samples: 864/5760, Loss: 0.021425873041152954\n",
      "Epoch: 30, Samples: 896/5760, Loss: 0.010177582502365112\n",
      "Epoch: 30, Samples: 928/5760, Loss: 0.018497884273529053\n",
      "Epoch: 30, Samples: 960/5760, Loss: 0.007902413606643677\n",
      "Epoch: 30, Samples: 992/5760, Loss: 0.014413326978683472\n",
      "Epoch: 30, Samples: 1024/5760, Loss: 0.021842479705810547\n",
      "Epoch: 30, Samples: 1056/5760, Loss: 0.01174861192703247\n",
      "Epoch: 30, Samples: 1088/5760, Loss: 0.01693117618560791\n",
      "Epoch: 30, Samples: 1120/5760, Loss: 0.012536585330963135\n",
      "Epoch: 30, Samples: 1152/5760, Loss: 0.021663963794708252\n",
      "Epoch: 30, Samples: 1184/5760, Loss: 0.04403185844421387\n",
      "Epoch: 30, Samples: 1216/5760, Loss: 0.017476171255111694\n",
      "Epoch: 30, Samples: 1248/5760, Loss: 0.04782895743846893\n",
      "Epoch: 30, Samples: 1280/5760, Loss: 0.014905273914337158\n",
      "Epoch: 30, Samples: 1312/5760, Loss: 0.020154044032096863\n",
      "Epoch: 30, Samples: 1344/5760, Loss: 0.03051126003265381\n",
      "Epoch: 30, Samples: 1376/5760, Loss: 0.01867729425430298\n",
      "Epoch: 30, Samples: 1408/5760, Loss: 0.022688329219818115\n",
      "Epoch: 30, Samples: 1440/5760, Loss: 0.01885014772415161\n",
      "Epoch: 30, Samples: 1472/5760, Loss: 0.016527533531188965\n",
      "Epoch: 30, Samples: 1504/5760, Loss: 0.008749902248382568\n",
      "Epoch: 30, Samples: 1536/5760, Loss: 0.012261897325515747\n",
      "Epoch: 30, Samples: 1568/5760, Loss: 0.025302395224571228\n",
      "Epoch: 30, Samples: 1600/5760, Loss: 0.017163604497909546\n",
      "Epoch: 30, Samples: 1632/5760, Loss: 0.02193579077720642\n",
      "Epoch: 30, Samples: 1664/5760, Loss: 0.017132669687271118\n",
      "Epoch: 30, Samples: 1696/5760, Loss: 0.019278615713119507\n",
      "Epoch: 30, Samples: 1728/5760, Loss: 0.014098793268203735\n",
      "Epoch: 30, Samples: 1760/5760, Loss: 0.014254912734031677\n",
      "Epoch: 30, Samples: 1792/5760, Loss: 0.050565868616104126\n",
      "Epoch: 30, Samples: 1824/5760, Loss: 0.01100468635559082\n",
      "Epoch: 30, Samples: 1856/5760, Loss: 0.021885424852371216\n",
      "Epoch: 30, Samples: 1888/5760, Loss: 0.0194208025932312\n",
      "Epoch: 30, Samples: 1920/5760, Loss: 0.020209968090057373\n",
      "Epoch: 30, Samples: 1952/5760, Loss: 0.02477434277534485\n",
      "Epoch: 30, Samples: 1984/5760, Loss: 0.04321105778217316\n",
      "Epoch: 30, Samples: 2016/5760, Loss: 0.011951684951782227\n",
      "Epoch: 30, Samples: 2048/5760, Loss: 0.013535380363464355\n",
      "Epoch: 30, Samples: 2080/5760, Loss: 0.015719056129455566\n",
      "Epoch: 30, Samples: 2112/5760, Loss: 0.015228688716888428\n",
      "Epoch: 30, Samples: 2144/5760, Loss: 0.014974027872085571\n",
      "Epoch: 30, Samples: 2176/5760, Loss: 0.01134258508682251\n",
      "Epoch: 30, Samples: 2208/5760, Loss: 0.011472761631011963\n",
      "Epoch: 30, Samples: 2240/5760, Loss: 0.027923792600631714\n",
      "Epoch: 30, Samples: 2272/5760, Loss: 0.010137289762496948\n",
      "Epoch: 30, Samples: 2304/5760, Loss: 0.02351883053779602\n",
      "Epoch: 30, Samples: 2336/5760, Loss: 0.016961008310317993\n",
      "Epoch: 30, Samples: 2368/5760, Loss: 0.011662513017654419\n",
      "Epoch: 30, Samples: 2400/5760, Loss: 0.015056610107421875\n",
      "Epoch: 30, Samples: 2432/5760, Loss: 0.018560409545898438\n",
      "Epoch: 30, Samples: 2464/5760, Loss: 0.016074419021606445\n",
      "Epoch: 30, Samples: 2496/5760, Loss: 0.02135370671749115\n",
      "Epoch: 30, Samples: 2528/5760, Loss: 0.030146852135658264\n",
      "Epoch: 30, Samples: 2560/5760, Loss: 0.025841742753982544\n",
      "Epoch: 30, Samples: 2592/5760, Loss: 0.01292470097541809\n",
      "Epoch: 30, Samples: 2624/5760, Loss: 0.014476031064987183\n",
      "Epoch: 30, Samples: 2656/5760, Loss: 0.010589241981506348\n",
      "Epoch: 30, Samples: 2688/5760, Loss: 0.00984722375869751\n",
      "Epoch: 30, Samples: 2720/5760, Loss: 0.021626397967338562\n",
      "Epoch: 30, Samples: 2752/5760, Loss: 0.010409951210021973\n",
      "Epoch: 30, Samples: 2784/5760, Loss: 0.014209240674972534\n",
      "Epoch: 30, Samples: 2816/5760, Loss: 0.00675085186958313\n",
      "Epoch: 30, Samples: 2848/5760, Loss: 0.011439740657806396\n",
      "Epoch: 30, Samples: 2880/5760, Loss: 0.020683974027633667\n",
      "Epoch: 30, Samples: 2912/5760, Loss: 0.014323115348815918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Samples: 2944/5760, Loss: 0.008641839027404785\n",
      "Epoch: 30, Samples: 2976/5760, Loss: 0.013607442378997803\n",
      "Epoch: 30, Samples: 3008/5760, Loss: 0.013373851776123047\n",
      "Epoch: 30, Samples: 3040/5760, Loss: 0.012774050235748291\n",
      "Epoch: 30, Samples: 3072/5760, Loss: 0.016063839197158813\n",
      "Epoch: 30, Samples: 3104/5760, Loss: 0.010306179523468018\n",
      "Epoch: 30, Samples: 3136/5760, Loss: 0.02866533398628235\n",
      "Epoch: 30, Samples: 3168/5760, Loss: 0.011908233165740967\n",
      "Epoch: 30, Samples: 3200/5760, Loss: 0.02359849214553833\n",
      "Epoch: 30, Samples: 3232/5760, Loss: 0.01068270206451416\n",
      "Epoch: 30, Samples: 3264/5760, Loss: 0.016583889722824097\n",
      "Epoch: 30, Samples: 3296/5760, Loss: 0.025031983852386475\n",
      "Epoch: 30, Samples: 3328/5760, Loss: 0.06091715395450592\n",
      "Epoch: 30, Samples: 3360/5760, Loss: 0.012472599744796753\n",
      "Epoch: 30, Samples: 3392/5760, Loss: 0.016369670629501343\n",
      "Epoch: 30, Samples: 3424/5760, Loss: 0.01917889714241028\n",
      "Epoch: 30, Samples: 3456/5760, Loss: 0.01607346534729004\n",
      "Epoch: 30, Samples: 3488/5760, Loss: 0.009351402521133423\n",
      "Epoch: 30, Samples: 3520/5760, Loss: 0.012187480926513672\n",
      "Epoch: 30, Samples: 3552/5760, Loss: 0.017470061779022217\n",
      "Epoch: 30, Samples: 3584/5760, Loss: 0.022316724061965942\n",
      "Epoch: 30, Samples: 3616/5760, Loss: 0.010729730129241943\n",
      "Epoch: 30, Samples: 3648/5760, Loss: 0.015839338302612305\n",
      "Epoch: 30, Samples: 3680/5760, Loss: 0.03019222617149353\n",
      "Epoch: 30, Samples: 3712/5760, Loss: 0.008257389068603516\n",
      "Epoch: 30, Samples: 3744/5760, Loss: 0.02244475483894348\n",
      "Epoch: 30, Samples: 3776/5760, Loss: 0.018517881631851196\n",
      "Epoch: 30, Samples: 3808/5760, Loss: 0.026495546102523804\n",
      "Epoch: 30, Samples: 3840/5760, Loss: 0.012539207935333252\n",
      "Epoch: 30, Samples: 3872/5760, Loss: 0.037314966320991516\n",
      "Epoch: 30, Samples: 3904/5760, Loss: 0.02185678482055664\n",
      "Epoch: 30, Samples: 3936/5760, Loss: 0.010515481233596802\n",
      "Epoch: 30, Samples: 3968/5760, Loss: 0.015039592981338501\n",
      "Epoch: 30, Samples: 4000/5760, Loss: 0.025561243295669556\n",
      "Epoch: 30, Samples: 4032/5760, Loss: 0.009512007236480713\n",
      "Epoch: 30, Samples: 4064/5760, Loss: 0.031876832246780396\n",
      "Epoch: 30, Samples: 4096/5760, Loss: 0.013394922018051147\n",
      "Epoch: 30, Samples: 4128/5760, Loss: 0.025938212871551514\n",
      "Epoch: 30, Samples: 4160/5760, Loss: 0.017143160104751587\n",
      "Epoch: 30, Samples: 4192/5760, Loss: 0.01109611988067627\n",
      "Epoch: 30, Samples: 4224/5760, Loss: 0.014537215232849121\n",
      "Epoch: 30, Samples: 4256/5760, Loss: 0.009196221828460693\n",
      "Epoch: 30, Samples: 4288/5760, Loss: 0.01396748423576355\n",
      "Epoch: 30, Samples: 4320/5760, Loss: 0.020504295825958252\n",
      "Epoch: 30, Samples: 4352/5760, Loss: 0.022662028670310974\n",
      "Epoch: 30, Samples: 4384/5760, Loss: 0.011357694864273071\n",
      "Epoch: 30, Samples: 4416/5760, Loss: 0.016279876232147217\n",
      "Epoch: 30, Samples: 4448/5760, Loss: 0.008250266313552856\n",
      "Epoch: 30, Samples: 4480/5760, Loss: 0.014014631509780884\n",
      "Epoch: 30, Samples: 4512/5760, Loss: 0.03676499426364899\n",
      "Epoch: 30, Samples: 4544/5760, Loss: 0.012837588787078857\n",
      "Epoch: 30, Samples: 4576/5760, Loss: 0.023334771394729614\n",
      "Epoch: 30, Samples: 4608/5760, Loss: 0.007090926170349121\n",
      "Epoch: 30, Samples: 4640/5760, Loss: 0.01691916584968567\n",
      "Epoch: 30, Samples: 4672/5760, Loss: 0.017337322235107422\n",
      "Epoch: 30, Samples: 4704/5760, Loss: 0.01054614782333374\n",
      "Epoch: 30, Samples: 4736/5760, Loss: 0.028868243098258972\n",
      "Epoch: 30, Samples: 4768/5760, Loss: 0.014420866966247559\n",
      "Epoch: 30, Samples: 4800/5760, Loss: 0.016570687294006348\n",
      "Epoch: 30, Samples: 4832/5760, Loss: 0.015292078256607056\n",
      "Epoch: 30, Samples: 4864/5760, Loss: 0.019342809915542603\n",
      "Epoch: 30, Samples: 4896/5760, Loss: 0.0194176584482193\n",
      "Epoch: 30, Samples: 4928/5760, Loss: 0.02088025212287903\n",
      "Epoch: 30, Samples: 4960/5760, Loss: 0.016725659370422363\n",
      "Epoch: 30, Samples: 4992/5760, Loss: 0.01868531107902527\n",
      "Epoch: 30, Samples: 5024/5760, Loss: 0.010662108659744263\n",
      "Epoch: 30, Samples: 5056/5760, Loss: 0.012530684471130371\n",
      "Epoch: 30, Samples: 5088/5760, Loss: 0.029140278697013855\n",
      "Epoch: 30, Samples: 5120/5760, Loss: 0.010497838258743286\n",
      "Epoch: 30, Samples: 5152/5760, Loss: 0.011106997728347778\n",
      "Epoch: 30, Samples: 5184/5760, Loss: 0.016283541917800903\n",
      "Epoch: 30, Samples: 5216/5760, Loss: 0.016543149948120117\n",
      "Epoch: 30, Samples: 5248/5760, Loss: 0.017893090844154358\n",
      "Epoch: 30, Samples: 5280/5760, Loss: 0.022986173629760742\n",
      "Epoch: 30, Samples: 5312/5760, Loss: 0.0077568888664245605\n",
      "Epoch: 30, Samples: 5344/5760, Loss: 0.018552392721176147\n",
      "Epoch: 30, Samples: 5376/5760, Loss: 0.009597718715667725\n",
      "Epoch: 30, Samples: 5408/5760, Loss: 0.028523147106170654\n",
      "Epoch: 30, Samples: 5440/5760, Loss: 0.029719680547714233\n",
      "Epoch: 30, Samples: 5472/5760, Loss: 0.022105902433395386\n",
      "Epoch: 30, Samples: 5504/5760, Loss: 0.021279126405715942\n",
      "Epoch: 30, Samples: 5536/5760, Loss: 0.011084228754043579\n",
      "Epoch: 30, Samples: 5568/5760, Loss: 0.013462275266647339\n",
      "Epoch: 30, Samples: 5600/5760, Loss: 0.018027812242507935\n",
      "Epoch: 30, Samples: 5632/5760, Loss: 0.012561410665512085\n",
      "Epoch: 30, Samples: 5664/5760, Loss: 0.019415676593780518\n",
      "Epoch: 30, Samples: 5696/5760, Loss: 0.011049896478652954\n",
      "Epoch: 30, Samples: 5728/5760, Loss: 0.8718761205673218\n",
      "\n",
      "Epoch: 30\n",
      "Training set: Average loss: 0.0230\n",
      "Validation set: Average loss: 0.2906, Accuracy: 761/818 (93%)\n",
      "Epoch: 31, Samples: 0/5760, Loss: 0.012673556804656982\n",
      "Epoch: 31, Samples: 32/5760, Loss: 0.03783199191093445\n",
      "Epoch: 31, Samples: 64/5760, Loss: 0.012888878583908081\n",
      "Epoch: 31, Samples: 96/5760, Loss: 0.02540290355682373\n",
      "Epoch: 31, Samples: 128/5760, Loss: 0.013255327939987183\n",
      "Epoch: 31, Samples: 160/5760, Loss: 0.03439861536026001\n",
      "Epoch: 31, Samples: 192/5760, Loss: 0.012119054794311523\n",
      "Epoch: 31, Samples: 224/5760, Loss: 0.015941262245178223\n",
      "Epoch: 31, Samples: 256/5760, Loss: 0.03174731135368347\n",
      "Epoch: 31, Samples: 288/5760, Loss: 0.013489902019500732\n",
      "Epoch: 31, Samples: 320/5760, Loss: 0.011129558086395264\n",
      "Epoch: 31, Samples: 352/5760, Loss: 0.015846461057662964\n",
      "Epoch: 31, Samples: 384/5760, Loss: 0.01278841495513916\n",
      "Epoch: 31, Samples: 416/5760, Loss: 0.013424545526504517\n",
      "Epoch: 31, Samples: 448/5760, Loss: 0.015629231929779053\n",
      "Epoch: 31, Samples: 480/5760, Loss: 0.03416459262371063\n",
      "Epoch: 31, Samples: 512/5760, Loss: 0.013414353132247925\n",
      "Epoch: 31, Samples: 544/5760, Loss: 0.010232686996459961\n",
      "Epoch: 31, Samples: 576/5760, Loss: 0.010543763637542725\n",
      "Epoch: 31, Samples: 608/5760, Loss: 0.010594964027404785\n",
      "Epoch: 31, Samples: 640/5760, Loss: 0.020297378301620483\n",
      "Epoch: 31, Samples: 672/5760, Loss: 0.009744018316268921\n",
      "Epoch: 31, Samples: 704/5760, Loss: 0.049274101853370667\n",
      "Epoch: 31, Samples: 736/5760, Loss: 0.024541616439819336\n",
      "Epoch: 31, Samples: 768/5760, Loss: 0.02322283387184143\n",
      "Epoch: 31, Samples: 800/5760, Loss: 0.021128594875335693\n",
      "Epoch: 31, Samples: 832/5760, Loss: 0.04048159718513489\n",
      "Epoch: 31, Samples: 864/5760, Loss: 0.025678187608718872\n",
      "Epoch: 31, Samples: 896/5760, Loss: 0.021615445613861084\n",
      "Epoch: 31, Samples: 928/5760, Loss: 0.02112486958503723\n",
      "Epoch: 31, Samples: 960/5760, Loss: 0.02066853642463684\n",
      "Epoch: 31, Samples: 992/5760, Loss: 0.015025019645690918\n",
      "Epoch: 31, Samples: 1024/5760, Loss: 0.016249626874923706\n",
      "Epoch: 31, Samples: 1056/5760, Loss: 0.027995750308036804\n",
      "Epoch: 31, Samples: 1088/5760, Loss: 0.017882347106933594\n",
      "Epoch: 31, Samples: 1120/5760, Loss: 0.020875245332717896\n",
      "Epoch: 31, Samples: 1152/5760, Loss: 0.005598276853561401\n",
      "Epoch: 31, Samples: 1184/5760, Loss: 0.021445274353027344\n",
      "Epoch: 31, Samples: 1216/5760, Loss: 0.02207544445991516\n",
      "Epoch: 31, Samples: 1248/5760, Loss: 0.043612077832221985\n",
      "Epoch: 31, Samples: 1280/5760, Loss: 0.01629766821861267\n",
      "Epoch: 31, Samples: 1312/5760, Loss: 0.029425665736198425\n",
      "Epoch: 31, Samples: 1344/5760, Loss: 0.016989022493362427\n",
      "Epoch: 31, Samples: 1376/5760, Loss: 0.01664796471595764\n",
      "Epoch: 31, Samples: 1408/5760, Loss: 0.02365592122077942\n",
      "Epoch: 31, Samples: 1440/5760, Loss: 0.010535597801208496\n",
      "Epoch: 31, Samples: 1472/5760, Loss: 0.012608736753463745\n",
      "Epoch: 31, Samples: 1504/5760, Loss: 0.03400304913520813\n",
      "Epoch: 31, Samples: 1536/5760, Loss: 0.028262779116630554\n",
      "Epoch: 31, Samples: 1568/5760, Loss: 0.015896618366241455\n",
      "Epoch: 31, Samples: 1600/5760, Loss: 0.022853508591651917\n",
      "Epoch: 31, Samples: 1632/5760, Loss: 0.019417881965637207\n",
      "Epoch: 31, Samples: 1664/5760, Loss: 0.02215719223022461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31, Samples: 1696/5760, Loss: 0.013748705387115479\n",
      "Epoch: 31, Samples: 1728/5760, Loss: 0.014903470873832703\n",
      "Epoch: 31, Samples: 1760/5760, Loss: 0.011338233947753906\n",
      "Epoch: 31, Samples: 1792/5760, Loss: 0.01977601647377014\n",
      "Epoch: 31, Samples: 1824/5760, Loss: 0.030627936124801636\n",
      "Epoch: 31, Samples: 1856/5760, Loss: 0.004957377910614014\n",
      "Epoch: 31, Samples: 1888/5760, Loss: 0.017493754625320435\n",
      "Epoch: 31, Samples: 1920/5760, Loss: 0.015305280685424805\n",
      "Epoch: 31, Samples: 1952/5760, Loss: 0.015000015497207642\n",
      "Epoch: 31, Samples: 1984/5760, Loss: 0.01891501247882843\n",
      "Epoch: 31, Samples: 2016/5760, Loss: 0.010322660207748413\n",
      "Epoch: 31, Samples: 2048/5760, Loss: 0.007852792739868164\n",
      "Epoch: 31, Samples: 2080/5760, Loss: 0.012352168560028076\n",
      "Epoch: 31, Samples: 2112/5760, Loss: 0.011840134859085083\n",
      "Epoch: 31, Samples: 2144/5760, Loss: 0.00810357928276062\n",
      "Epoch: 31, Samples: 2176/5760, Loss: 0.01693752408027649\n",
      "Epoch: 31, Samples: 2208/5760, Loss: 0.01587855815887451\n",
      "Epoch: 31, Samples: 2240/5760, Loss: 0.013516604900360107\n",
      "Epoch: 31, Samples: 2272/5760, Loss: 0.011441916227340698\n",
      "Epoch: 31, Samples: 2304/5760, Loss: 0.01351216435432434\n",
      "Epoch: 31, Samples: 2336/5760, Loss: 0.026739060878753662\n",
      "Epoch: 31, Samples: 2368/5760, Loss: 0.01166146993637085\n",
      "Epoch: 31, Samples: 2400/5760, Loss: 0.018706023693084717\n",
      "Epoch: 31, Samples: 2432/5760, Loss: 0.01485392451286316\n",
      "Epoch: 31, Samples: 2464/5760, Loss: 0.01900196075439453\n",
      "Epoch: 31, Samples: 2496/5760, Loss: 0.025358140468597412\n",
      "Epoch: 31, Samples: 2528/5760, Loss: 0.011045247316360474\n",
      "Epoch: 31, Samples: 2560/5760, Loss: 0.0202312171459198\n",
      "Epoch: 31, Samples: 2592/5760, Loss: 0.017041414976119995\n",
      "Epoch: 31, Samples: 2624/5760, Loss: 0.026247084140777588\n",
      "Epoch: 31, Samples: 2656/5760, Loss: 0.011586368083953857\n",
      "Epoch: 31, Samples: 2688/5760, Loss: 0.04232703149318695\n",
      "Epoch: 31, Samples: 2720/5760, Loss: 0.020920276641845703\n",
      "Epoch: 31, Samples: 2752/5760, Loss: 0.013534009456634521\n",
      "Epoch: 31, Samples: 2784/5760, Loss: 0.014734387397766113\n",
      "Epoch: 31, Samples: 2816/5760, Loss: 0.024998784065246582\n",
      "Epoch: 31, Samples: 2848/5760, Loss: 0.014136731624603271\n",
      "Epoch: 31, Samples: 2880/5760, Loss: 0.015577465295791626\n",
      "Epoch: 31, Samples: 2912/5760, Loss: 0.013008356094360352\n",
      "Epoch: 31, Samples: 2944/5760, Loss: 0.02857103943824768\n",
      "Epoch: 31, Samples: 2976/5760, Loss: 0.015160977840423584\n",
      "Epoch: 31, Samples: 3008/5760, Loss: 0.017670422792434692\n",
      "Epoch: 31, Samples: 3040/5760, Loss: 0.011044025421142578\n",
      "Epoch: 31, Samples: 3072/5760, Loss: 0.010234624147415161\n",
      "Epoch: 31, Samples: 3104/5760, Loss: 0.013580083847045898\n",
      "Epoch: 31, Samples: 3136/5760, Loss: 0.010657578706741333\n",
      "Epoch: 31, Samples: 3168/5760, Loss: 0.012398064136505127\n",
      "Epoch: 31, Samples: 3200/5760, Loss: 0.02894499897956848\n",
      "Epoch: 31, Samples: 3232/5760, Loss: 0.011683732271194458\n",
      "Epoch: 31, Samples: 3264/5760, Loss: 0.015858441591262817\n",
      "Epoch: 31, Samples: 3296/5760, Loss: 0.022947728633880615\n",
      "Epoch: 31, Samples: 3328/5760, Loss: 0.010336577892303467\n",
      "Epoch: 31, Samples: 3360/5760, Loss: 0.01586437225341797\n",
      "Epoch: 31, Samples: 3392/5760, Loss: 0.014894694089889526\n",
      "Epoch: 31, Samples: 3424/5760, Loss: 0.0258292555809021\n",
      "Epoch: 31, Samples: 3456/5760, Loss: 0.013995438814163208\n",
      "Epoch: 31, Samples: 3488/5760, Loss: 0.027328386902809143\n",
      "Epoch: 31, Samples: 3520/5760, Loss: 0.00854751467704773\n",
      "Epoch: 31, Samples: 3552/5760, Loss: 0.010297924280166626\n",
      "Epoch: 31, Samples: 3584/5760, Loss: 0.015467345714569092\n",
      "Epoch: 31, Samples: 3616/5760, Loss: 0.013791888952255249\n",
      "Epoch: 31, Samples: 3648/5760, Loss: 0.012691348791122437\n",
      "Epoch: 31, Samples: 3680/5760, Loss: 0.020869970321655273\n",
      "Epoch: 31, Samples: 3712/5760, Loss: 0.01454538106918335\n",
      "Epoch: 31, Samples: 3744/5760, Loss: 0.009447455406188965\n",
      "Epoch: 31, Samples: 3776/5760, Loss: 0.0115603506565094\n",
      "Epoch: 31, Samples: 3808/5760, Loss: 0.010218411684036255\n",
      "Epoch: 31, Samples: 3840/5760, Loss: 0.018228143453598022\n",
      "Epoch: 31, Samples: 3872/5760, Loss: 0.010958641767501831\n",
      "Epoch: 31, Samples: 3904/5760, Loss: 0.011381834745407104\n",
      "Epoch: 31, Samples: 3936/5760, Loss: 0.008464515209197998\n",
      "Epoch: 31, Samples: 3968/5760, Loss: 0.00902712345123291\n",
      "Epoch: 31, Samples: 4000/5760, Loss: 0.012592792510986328\n",
      "Epoch: 31, Samples: 4032/5760, Loss: 0.020236492156982422\n",
      "Epoch: 31, Samples: 4064/5760, Loss: 0.012275844812393188\n",
      "Epoch: 31, Samples: 4096/5760, Loss: 0.022664815187454224\n",
      "Epoch: 31, Samples: 4128/5760, Loss: 0.018886953592300415\n",
      "Epoch: 31, Samples: 4160/5760, Loss: 0.014940887689590454\n",
      "Epoch: 31, Samples: 4192/5760, Loss: 0.012377738952636719\n",
      "Epoch: 31, Samples: 4224/5760, Loss: 0.018710941076278687\n",
      "Epoch: 31, Samples: 4256/5760, Loss: 0.018029004335403442\n",
      "Epoch: 31, Samples: 4288/5760, Loss: 0.019992679357528687\n",
      "Epoch: 31, Samples: 4320/5760, Loss: 0.01313731074333191\n",
      "Epoch: 31, Samples: 4352/5760, Loss: 0.012891411781311035\n",
      "Epoch: 31, Samples: 4384/5760, Loss: 0.012713313102722168\n",
      "Epoch: 31, Samples: 4416/5760, Loss: 0.019971609115600586\n",
      "Epoch: 31, Samples: 4448/5760, Loss: 0.022662967443466187\n",
      "Epoch: 31, Samples: 4480/5760, Loss: 0.011290669441223145\n",
      "Epoch: 31, Samples: 4512/5760, Loss: 0.011887341737747192\n",
      "Epoch: 31, Samples: 4544/5760, Loss: 0.01763024926185608\n",
      "Epoch: 31, Samples: 4576/5760, Loss: 0.014462262392044067\n",
      "Epoch: 31, Samples: 4608/5760, Loss: 0.023292571306228638\n",
      "Epoch: 31, Samples: 4640/5760, Loss: 0.013818800449371338\n",
      "Epoch: 31, Samples: 4672/5760, Loss: 0.012815773487091064\n",
      "Epoch: 31, Samples: 4704/5760, Loss: 0.020895808935165405\n",
      "Epoch: 31, Samples: 4736/5760, Loss: 0.02647438645362854\n",
      "Epoch: 31, Samples: 4768/5760, Loss: 0.02198132872581482\n",
      "Epoch: 31, Samples: 4800/5760, Loss: 0.012975156307220459\n",
      "Epoch: 31, Samples: 4832/5760, Loss: 0.016382932662963867\n",
      "Epoch: 31, Samples: 4864/5760, Loss: 0.010013401508331299\n",
      "Epoch: 31, Samples: 4896/5760, Loss: 0.019194230437278748\n",
      "Epoch: 31, Samples: 4928/5760, Loss: 0.01124429702758789\n",
      "Epoch: 31, Samples: 4960/5760, Loss: 0.017274916172027588\n",
      "Epoch: 31, Samples: 4992/5760, Loss: 0.008154064416885376\n",
      "Epoch: 31, Samples: 5024/5760, Loss: 0.01129940152168274\n",
      "Epoch: 31, Samples: 5056/5760, Loss: 0.012127578258514404\n",
      "Epoch: 31, Samples: 5088/5760, Loss: 0.00799664855003357\n",
      "Epoch: 31, Samples: 5120/5760, Loss: 0.013247251510620117\n",
      "Epoch: 31, Samples: 5152/5760, Loss: 0.011772006750106812\n",
      "Epoch: 31, Samples: 5184/5760, Loss: 0.00721469521522522\n",
      "Epoch: 31, Samples: 5216/5760, Loss: 0.008393049240112305\n",
      "Epoch: 31, Samples: 5248/5760, Loss: 0.010872632265090942\n",
      "Epoch: 31, Samples: 5280/5760, Loss: 0.010450750589370728\n",
      "Epoch: 31, Samples: 5312/5760, Loss: 0.00837630033493042\n",
      "Epoch: 31, Samples: 5344/5760, Loss: 0.009868264198303223\n",
      "Epoch: 31, Samples: 5376/5760, Loss: 0.017941966652870178\n",
      "Epoch: 31, Samples: 5408/5760, Loss: 0.03024342656135559\n",
      "Epoch: 31, Samples: 5440/5760, Loss: 0.015757739543914795\n",
      "Epoch: 31, Samples: 5472/5760, Loss: 0.0250990092754364\n",
      "Epoch: 31, Samples: 5504/5760, Loss: 0.021792709827423096\n",
      "Epoch: 31, Samples: 5536/5760, Loss: 0.014089018106460571\n",
      "Epoch: 31, Samples: 5568/5760, Loss: 0.011561065912246704\n",
      "Epoch: 31, Samples: 5600/5760, Loss: 0.013286978006362915\n",
      "Epoch: 31, Samples: 5632/5760, Loss: 0.016176432371139526\n",
      "Epoch: 31, Samples: 5664/5760, Loss: 0.0089951753616333\n",
      "Epoch: 31, Samples: 5696/5760, Loss: 0.017248988151550293\n",
      "Epoch: 31, Samples: 5728/5760, Loss: 0.18629837036132812\n",
      "\n",
      "Epoch: 31\n",
      "Training set: Average loss: 0.0180\n",
      "Validation set: Average loss: 0.2755, Accuracy: 764/818 (93%)\n",
      "Saving model (epoch 31) with lowest validation loss: 0.2755235181404994\n",
      "Epoch: 32, Samples: 0/5760, Loss: 0.013381898403167725\n",
      "Epoch: 32, Samples: 32/5760, Loss: 0.010364174842834473\n",
      "Epoch: 32, Samples: 64/5760, Loss: 0.016019463539123535\n",
      "Epoch: 32, Samples: 96/5760, Loss: 0.01828564703464508\n",
      "Epoch: 32, Samples: 128/5760, Loss: 0.009840428829193115\n",
      "Epoch: 32, Samples: 160/5760, Loss: 0.010402083396911621\n",
      "Epoch: 32, Samples: 192/5760, Loss: 0.010556578636169434\n",
      "Epoch: 32, Samples: 224/5760, Loss: 0.017115265130996704\n",
      "Epoch: 32, Samples: 256/5760, Loss: 0.0175398588180542\n",
      "Epoch: 32, Samples: 288/5760, Loss: 0.012135714292526245\n",
      "Epoch: 32, Samples: 320/5760, Loss: 0.012578696012496948\n",
      "Epoch: 32, Samples: 352/5760, Loss: 0.008940935134887695\n",
      "Epoch: 32, Samples: 384/5760, Loss: 0.01740407943725586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Samples: 416/5760, Loss: 0.014457553625106812\n",
      "Epoch: 32, Samples: 448/5760, Loss: 0.013598024845123291\n",
      "Epoch: 32, Samples: 480/5760, Loss: 0.01399320363998413\n",
      "Epoch: 32, Samples: 512/5760, Loss: 0.013671040534973145\n",
      "Epoch: 32, Samples: 544/5760, Loss: 0.023223698139190674\n",
      "Epoch: 32, Samples: 576/5760, Loss: 0.007678508758544922\n",
      "Epoch: 32, Samples: 608/5760, Loss: 0.025887608528137207\n",
      "Epoch: 32, Samples: 640/5760, Loss: 0.013572871685028076\n",
      "Epoch: 32, Samples: 672/5760, Loss: 0.009896934032440186\n",
      "Epoch: 32, Samples: 704/5760, Loss: 0.006071537733078003\n",
      "Epoch: 32, Samples: 736/5760, Loss: 0.01341921091079712\n",
      "Epoch: 32, Samples: 768/5760, Loss: 0.016229867935180664\n",
      "Epoch: 32, Samples: 800/5760, Loss: 0.009299904108047485\n",
      "Epoch: 32, Samples: 832/5760, Loss: 0.02001360058784485\n",
      "Epoch: 32, Samples: 864/5760, Loss: 0.007899343967437744\n",
      "Epoch: 32, Samples: 896/5760, Loss: 0.01626625657081604\n",
      "Epoch: 32, Samples: 928/5760, Loss: 0.015800118446350098\n",
      "Epoch: 32, Samples: 960/5760, Loss: 0.015621036291122437\n",
      "Epoch: 32, Samples: 992/5760, Loss: 0.008779257535934448\n",
      "Epoch: 32, Samples: 1024/5760, Loss: 0.0063104331493377686\n",
      "Epoch: 32, Samples: 1056/5760, Loss: 0.008844912052154541\n",
      "Epoch: 32, Samples: 1088/5760, Loss: 0.01989293098449707\n",
      "Epoch: 32, Samples: 1120/5760, Loss: 0.02365189790725708\n",
      "Epoch: 32, Samples: 1152/5760, Loss: 0.014852821826934814\n",
      "Epoch: 32, Samples: 1184/5760, Loss: 0.022087395191192627\n",
      "Epoch: 32, Samples: 1216/5760, Loss: 0.013666540384292603\n",
      "Epoch: 32, Samples: 1248/5760, Loss: 0.010081291198730469\n",
      "Epoch: 32, Samples: 1280/5760, Loss: 0.01758652925491333\n",
      "Epoch: 32, Samples: 1312/5760, Loss: 0.010689765214920044\n",
      "Epoch: 32, Samples: 1344/5760, Loss: 0.019565105438232422\n",
      "Epoch: 32, Samples: 1376/5760, Loss: 0.02305002510547638\n",
      "Epoch: 32, Samples: 1408/5760, Loss: 0.014184653759002686\n",
      "Epoch: 32, Samples: 1440/5760, Loss: 0.011613219976425171\n",
      "Epoch: 32, Samples: 1472/5760, Loss: 0.012165725231170654\n",
      "Epoch: 32, Samples: 1504/5760, Loss: 0.010411083698272705\n",
      "Epoch: 32, Samples: 1536/5760, Loss: 0.006981194019317627\n",
      "Epoch: 32, Samples: 1568/5760, Loss: 0.008167654275894165\n",
      "Epoch: 32, Samples: 1600/5760, Loss: 0.006830781698226929\n",
      "Epoch: 32, Samples: 1632/5760, Loss: 0.01914598047733307\n",
      "Epoch: 32, Samples: 1664/5760, Loss: 0.022512048482894897\n",
      "Epoch: 32, Samples: 1696/5760, Loss: 0.010735481977462769\n",
      "Epoch: 32, Samples: 1728/5760, Loss: 0.01772843301296234\n",
      "Epoch: 32, Samples: 1760/5760, Loss: 0.007759660482406616\n",
      "Epoch: 32, Samples: 1792/5760, Loss: 0.0159776508808136\n",
      "Epoch: 32, Samples: 1824/5760, Loss: 0.012496918439865112\n",
      "Epoch: 32, Samples: 1856/5760, Loss: 0.0195884108543396\n",
      "Epoch: 32, Samples: 1888/5760, Loss: 0.021663665771484375\n",
      "Epoch: 32, Samples: 1920/5760, Loss: 0.009770721197128296\n",
      "Epoch: 32, Samples: 1952/5760, Loss: 0.014490664005279541\n",
      "Epoch: 32, Samples: 1984/5760, Loss: 0.010328620672225952\n",
      "Epoch: 32, Samples: 2016/5760, Loss: 0.0134468674659729\n",
      "Epoch: 32, Samples: 2048/5760, Loss: 0.010994136333465576\n",
      "Epoch: 32, Samples: 2080/5760, Loss: 0.008091509342193604\n",
      "Epoch: 32, Samples: 2112/5760, Loss: 0.01506015658378601\n",
      "Epoch: 32, Samples: 2144/5760, Loss: 0.020191222429275513\n",
      "Epoch: 32, Samples: 2176/5760, Loss: 0.026317477226257324\n",
      "Epoch: 32, Samples: 2208/5760, Loss: 0.008578330278396606\n",
      "Epoch: 32, Samples: 2240/5760, Loss: 0.009923338890075684\n",
      "Epoch: 32, Samples: 2272/5760, Loss: 0.011763721704483032\n",
      "Epoch: 32, Samples: 2304/5760, Loss: 0.020265430212020874\n",
      "Epoch: 32, Samples: 2336/5760, Loss: 0.01769503951072693\n",
      "Epoch: 32, Samples: 2368/5760, Loss: 0.019993513822555542\n",
      "Epoch: 32, Samples: 2400/5760, Loss: 0.013386547565460205\n",
      "Epoch: 32, Samples: 2432/5760, Loss: 0.00826302170753479\n",
      "Epoch: 32, Samples: 2464/5760, Loss: 0.031781554222106934\n",
      "Epoch: 32, Samples: 2496/5760, Loss: 0.00829973816871643\n",
      "Epoch: 32, Samples: 2528/5760, Loss: 0.016424179077148438\n",
      "Epoch: 32, Samples: 2560/5760, Loss: 0.024108201265335083\n",
      "Epoch: 32, Samples: 2592/5760, Loss: 0.009717404842376709\n",
      "Epoch: 32, Samples: 2624/5760, Loss: 0.00758698582649231\n",
      "Epoch: 32, Samples: 2656/5760, Loss: 0.01592355966567993\n",
      "Epoch: 32, Samples: 2688/5760, Loss: 0.029449760913848877\n",
      "Epoch: 32, Samples: 2720/5760, Loss: 0.0112093985080719\n",
      "Epoch: 32, Samples: 2752/5760, Loss: 0.011641323566436768\n",
      "Epoch: 32, Samples: 2784/5760, Loss: 0.015332698822021484\n",
      "Epoch: 32, Samples: 2816/5760, Loss: 0.01710185408592224\n",
      "Epoch: 32, Samples: 2848/5760, Loss: 0.021031677722930908\n",
      "Epoch: 32, Samples: 2880/5760, Loss: 0.018064647912979126\n",
      "Epoch: 32, Samples: 2912/5760, Loss: 0.012225985527038574\n",
      "Epoch: 32, Samples: 2944/5760, Loss: 0.010491162538528442\n",
      "Epoch: 32, Samples: 2976/5760, Loss: 0.007146507501602173\n",
      "Epoch: 32, Samples: 3008/5760, Loss: 0.009388923645019531\n",
      "Epoch: 32, Samples: 3040/5760, Loss: 0.010244905948638916\n",
      "Epoch: 32, Samples: 3072/5760, Loss: 0.03089091181755066\n",
      "Epoch: 32, Samples: 3104/5760, Loss: 0.013186931610107422\n",
      "Epoch: 32, Samples: 3136/5760, Loss: 0.011643320322036743\n",
      "Epoch: 32, Samples: 3168/5760, Loss: 0.01571783423423767\n",
      "Epoch: 32, Samples: 3200/5760, Loss: 0.011584669351577759\n",
      "Epoch: 32, Samples: 3232/5760, Loss: 0.00925356149673462\n",
      "Epoch: 32, Samples: 3264/5760, Loss: 0.007360488176345825\n",
      "Epoch: 32, Samples: 3296/5760, Loss: 0.017815589904785156\n",
      "Epoch: 32, Samples: 3328/5760, Loss: 0.013511687517166138\n",
      "Epoch: 32, Samples: 3360/5760, Loss: 0.010882049798965454\n",
      "Epoch: 32, Samples: 3392/5760, Loss: 0.013261735439300537\n",
      "Epoch: 32, Samples: 3424/5760, Loss: 0.008416950702667236\n",
      "Epoch: 32, Samples: 3456/5760, Loss: 0.010611891746520996\n",
      "Epoch: 32, Samples: 3488/5760, Loss: 0.015246182680130005\n",
      "Epoch: 32, Samples: 3520/5760, Loss: 0.009607911109924316\n",
      "Epoch: 32, Samples: 3552/5760, Loss: 0.013739734888076782\n",
      "Epoch: 32, Samples: 3584/5760, Loss: 0.011022329330444336\n",
      "Epoch: 32, Samples: 3616/5760, Loss: 0.01530727744102478\n",
      "Epoch: 32, Samples: 3648/5760, Loss: 0.04023571312427521\n",
      "Epoch: 32, Samples: 3680/5760, Loss: 0.008377134799957275\n",
      "Epoch: 32, Samples: 3712/5760, Loss: 0.02175074815750122\n",
      "Epoch: 32, Samples: 3744/5760, Loss: 0.01572546362876892\n",
      "Epoch: 32, Samples: 3776/5760, Loss: 0.009901106357574463\n",
      "Epoch: 32, Samples: 3808/5760, Loss: 0.01266390085220337\n",
      "Epoch: 32, Samples: 3840/5760, Loss: 0.027583807706832886\n",
      "Epoch: 32, Samples: 3872/5760, Loss: 0.011398285627365112\n",
      "Epoch: 32, Samples: 3904/5760, Loss: 0.03296107053756714\n",
      "Epoch: 32, Samples: 3936/5760, Loss: 0.010037928819656372\n",
      "Epoch: 32, Samples: 3968/5760, Loss: 0.009570270776748657\n",
      "Epoch: 32, Samples: 4000/5760, Loss: 0.026436805725097656\n",
      "Epoch: 32, Samples: 4032/5760, Loss: 0.011918634176254272\n",
      "Epoch: 32, Samples: 4064/5760, Loss: 0.011738747358322144\n",
      "Epoch: 32, Samples: 4096/5760, Loss: 0.015306025743484497\n",
      "Epoch: 32, Samples: 4128/5760, Loss: 0.022742807865142822\n",
      "Epoch: 32, Samples: 4160/5760, Loss: 0.015757113695144653\n",
      "Epoch: 32, Samples: 4192/5760, Loss: 0.011876970529556274\n",
      "Epoch: 32, Samples: 4224/5760, Loss: 0.02941042184829712\n",
      "Epoch: 32, Samples: 4256/5760, Loss: 0.01342165470123291\n",
      "Epoch: 32, Samples: 4288/5760, Loss: 0.012001603841781616\n",
      "Epoch: 32, Samples: 4320/5760, Loss: 0.009523719549179077\n",
      "Epoch: 32, Samples: 4352/5760, Loss: 0.012454181909561157\n",
      "Epoch: 32, Samples: 4384/5760, Loss: 0.03280532360076904\n",
      "Epoch: 32, Samples: 4416/5760, Loss: 0.021825969219207764\n",
      "Epoch: 32, Samples: 4448/5760, Loss: 0.01808294653892517\n",
      "Epoch: 32, Samples: 4480/5760, Loss: 0.0142669677734375\n",
      "Epoch: 32, Samples: 4512/5760, Loss: 0.01387140154838562\n",
      "Epoch: 32, Samples: 4544/5760, Loss: 0.016583561897277832\n",
      "Epoch: 32, Samples: 4576/5760, Loss: 0.009379655122756958\n",
      "Epoch: 32, Samples: 4608/5760, Loss: 0.00788906216621399\n",
      "Epoch: 32, Samples: 4640/5760, Loss: 0.02131803333759308\n",
      "Epoch: 32, Samples: 4672/5760, Loss: 0.01093408465385437\n",
      "Epoch: 32, Samples: 4704/5760, Loss: 0.01481887698173523\n",
      "Epoch: 32, Samples: 4736/5760, Loss: 0.009248167276382446\n",
      "Epoch: 32, Samples: 4768/5760, Loss: 0.013854950666427612\n",
      "Epoch: 32, Samples: 4800/5760, Loss: 0.009898781776428223\n",
      "Epoch: 32, Samples: 4832/5760, Loss: 0.018788456916809082\n",
      "Epoch: 32, Samples: 4864/5760, Loss: 0.01923784613609314\n",
      "Epoch: 32, Samples: 4896/5760, Loss: 0.011195987462997437\n",
      "Epoch: 32, Samples: 4928/5760, Loss: 0.021096765995025635\n",
      "Epoch: 32, Samples: 4960/5760, Loss: 0.010427623987197876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Samples: 4992/5760, Loss: 0.009722292423248291\n",
      "Epoch: 32, Samples: 5024/5760, Loss: 0.014224648475646973\n",
      "Epoch: 32, Samples: 5056/5760, Loss: 0.01245090365409851\n",
      "Epoch: 32, Samples: 5088/5760, Loss: 0.01879829168319702\n",
      "Epoch: 32, Samples: 5120/5760, Loss: 0.007103323936462402\n",
      "Epoch: 32, Samples: 5152/5760, Loss: 0.012977421283721924\n",
      "Epoch: 32, Samples: 5184/5760, Loss: 0.009193748235702515\n",
      "Epoch: 32, Samples: 5216/5760, Loss: 0.016350895166397095\n",
      "Epoch: 32, Samples: 5248/5760, Loss: 0.016722500324249268\n",
      "Epoch: 32, Samples: 5280/5760, Loss: 0.013451248407363892\n",
      "Epoch: 32, Samples: 5312/5760, Loss: 0.008530616760253906\n",
      "Epoch: 32, Samples: 5344/5760, Loss: 0.01418238878250122\n",
      "Epoch: 32, Samples: 5376/5760, Loss: 0.016343504190444946\n",
      "Epoch: 32, Samples: 5408/5760, Loss: 0.008579850196838379\n",
      "Epoch: 32, Samples: 5440/5760, Loss: 0.01217600703239441\n",
      "Epoch: 32, Samples: 5472/5760, Loss: 0.017648771405220032\n",
      "Epoch: 32, Samples: 5504/5760, Loss: 0.007958441972732544\n",
      "Epoch: 32, Samples: 5536/5760, Loss: 0.013989239931106567\n",
      "Epoch: 32, Samples: 5568/5760, Loss: 0.009688109159469604\n",
      "Epoch: 32, Samples: 5600/5760, Loss: 0.01326817274093628\n",
      "Epoch: 32, Samples: 5632/5760, Loss: 0.00452384352684021\n",
      "Epoch: 32, Samples: 5664/5760, Loss: 0.007924318313598633\n",
      "Epoch: 32, Samples: 5696/5760, Loss: 0.022564589977264404\n",
      "Epoch: 32, Samples: 5728/5760, Loss: 0.24095678329467773\n",
      "\n",
      "Epoch: 32\n",
      "Training set: Average loss: 0.0157\n",
      "Validation set: Average loss: 0.2792, Accuracy: 766/818 (94%)\n",
      "Epoch: 33, Samples: 0/5760, Loss: 0.004872649908065796\n",
      "Epoch: 33, Samples: 32/5760, Loss: 0.009424924850463867\n",
      "Epoch: 33, Samples: 64/5760, Loss: 0.013173729181289673\n",
      "Epoch: 33, Samples: 96/5760, Loss: 0.01009330153465271\n",
      "Epoch: 33, Samples: 128/5760, Loss: 0.013882309198379517\n",
      "Epoch: 33, Samples: 160/5760, Loss: 0.008410722017288208\n",
      "Epoch: 33, Samples: 192/5760, Loss: 0.008155107498168945\n",
      "Epoch: 33, Samples: 224/5760, Loss: 0.019428342580795288\n",
      "Epoch: 33, Samples: 256/5760, Loss: 0.01653030514717102\n",
      "Epoch: 33, Samples: 288/5760, Loss: 0.014065355062484741\n",
      "Epoch: 33, Samples: 320/5760, Loss: 0.015107929706573486\n",
      "Epoch: 33, Samples: 352/5760, Loss: 0.014693140983581543\n",
      "Epoch: 33, Samples: 384/5760, Loss: 0.015257567167282104\n",
      "Epoch: 33, Samples: 416/5760, Loss: 0.018678724765777588\n",
      "Epoch: 33, Samples: 448/5760, Loss: 0.01319819688796997\n",
      "Epoch: 33, Samples: 480/5760, Loss: 0.008309513330459595\n",
      "Epoch: 33, Samples: 512/5760, Loss: 0.012248367071151733\n",
      "Epoch: 33, Samples: 544/5760, Loss: 0.018256336450576782\n",
      "Epoch: 33, Samples: 576/5760, Loss: 0.018910646438598633\n",
      "Epoch: 33, Samples: 608/5760, Loss: 0.01732712984085083\n",
      "Epoch: 33, Samples: 640/5760, Loss: 0.013392984867095947\n",
      "Epoch: 33, Samples: 672/5760, Loss: 0.022475123405456543\n",
      "Epoch: 33, Samples: 704/5760, Loss: 0.020161569118499756\n",
      "Epoch: 33, Samples: 736/5760, Loss: 0.013433218002319336\n",
      "Epoch: 33, Samples: 768/5760, Loss: 0.012941062450408936\n",
      "Epoch: 33, Samples: 800/5760, Loss: 0.017638981342315674\n",
      "Epoch: 33, Samples: 832/5760, Loss: 0.017343461513519287\n",
      "Epoch: 33, Samples: 864/5760, Loss: 0.017774641513824463\n",
      "Epoch: 33, Samples: 896/5760, Loss: 0.005031228065490723\n",
      "Epoch: 33, Samples: 928/5760, Loss: 0.031770169734954834\n",
      "Epoch: 33, Samples: 960/5760, Loss: 0.004742413759231567\n",
      "Epoch: 33, Samples: 992/5760, Loss: 0.017311885952949524\n",
      "Epoch: 33, Samples: 1024/5760, Loss: 0.021663188934326172\n",
      "Epoch: 33, Samples: 1056/5760, Loss: 0.011353105306625366\n",
      "Epoch: 33, Samples: 1088/5760, Loss: 0.02366769313812256\n",
      "Epoch: 33, Samples: 1120/5760, Loss: 0.01055610179901123\n",
      "Epoch: 33, Samples: 1152/5760, Loss: 0.012227267026901245\n",
      "Epoch: 33, Samples: 1184/5760, Loss: 0.012436598539352417\n",
      "Epoch: 33, Samples: 1216/5760, Loss: 0.012344524264335632\n",
      "Epoch: 33, Samples: 1248/5760, Loss: 0.006981581449508667\n",
      "Epoch: 33, Samples: 1280/5760, Loss: 0.010030984878540039\n",
      "Epoch: 33, Samples: 1312/5760, Loss: 0.018143892288208008\n",
      "Epoch: 33, Samples: 1344/5760, Loss: 0.010060399770736694\n",
      "Epoch: 33, Samples: 1376/5760, Loss: 0.01112857460975647\n",
      "Epoch: 33, Samples: 1408/5760, Loss: 0.014435768127441406\n",
      "Epoch: 33, Samples: 1440/5760, Loss: 0.010048598051071167\n",
      "Epoch: 33, Samples: 1472/5760, Loss: 0.015483736991882324\n",
      "Epoch: 33, Samples: 1504/5760, Loss: 0.016988903284072876\n",
      "Epoch: 33, Samples: 1536/5760, Loss: 0.008103728294372559\n",
      "Epoch: 33, Samples: 1568/5760, Loss: 0.010399878025054932\n",
      "Epoch: 33, Samples: 1600/5760, Loss: 0.00698542594909668\n",
      "Epoch: 33, Samples: 1632/5760, Loss: 0.009270668029785156\n",
      "Epoch: 33, Samples: 1664/5760, Loss: 0.012394607067108154\n",
      "Epoch: 33, Samples: 1696/5760, Loss: 0.009622782468795776\n",
      "Epoch: 33, Samples: 1728/5760, Loss: 0.015946179628372192\n",
      "Epoch: 33, Samples: 1760/5760, Loss: 0.010961771011352539\n",
      "Epoch: 33, Samples: 1792/5760, Loss: 0.013946324586868286\n",
      "Epoch: 33, Samples: 1824/5760, Loss: 0.020046979188919067\n",
      "Epoch: 33, Samples: 1856/5760, Loss: 0.015382707118988037\n",
      "Epoch: 33, Samples: 1888/5760, Loss: 0.01091817021369934\n",
      "Epoch: 33, Samples: 1920/5760, Loss: 0.007595092058181763\n",
      "Epoch: 33, Samples: 1952/5760, Loss: 0.006188064813613892\n",
      "Epoch: 33, Samples: 1984/5760, Loss: 0.0079079270362854\n",
      "Epoch: 33, Samples: 2016/5760, Loss: 0.007766067981719971\n",
      "Epoch: 33, Samples: 2048/5760, Loss: 0.015825897455215454\n",
      "Epoch: 33, Samples: 2080/5760, Loss: 0.00623670220375061\n",
      "Epoch: 33, Samples: 2112/5760, Loss: 0.013963669538497925\n",
      "Epoch: 33, Samples: 2144/5760, Loss: 0.020003825426101685\n",
      "Epoch: 33, Samples: 2176/5760, Loss: 0.010066032409667969\n",
      "Epoch: 33, Samples: 2208/5760, Loss: 0.01438876986503601\n",
      "Epoch: 33, Samples: 2240/5760, Loss: 0.00916263461112976\n",
      "Epoch: 33, Samples: 2272/5760, Loss: 0.019125699996948242\n",
      "Epoch: 33, Samples: 2304/5760, Loss: 0.021060645580291748\n",
      "Epoch: 33, Samples: 2336/5760, Loss: 0.013399064540863037\n",
      "Epoch: 33, Samples: 2368/5760, Loss: 0.01197272539138794\n",
      "Epoch: 33, Samples: 2400/5760, Loss: 0.007418513298034668\n",
      "Epoch: 33, Samples: 2432/5760, Loss: 0.012543439865112305\n",
      "Epoch: 33, Samples: 2464/5760, Loss: 0.015620589256286621\n",
      "Epoch: 33, Samples: 2496/5760, Loss: 0.01717635989189148\n",
      "Epoch: 33, Samples: 2528/5760, Loss: 0.02773284912109375\n",
      "Epoch: 33, Samples: 2560/5760, Loss: 0.011681407690048218\n",
      "Epoch: 33, Samples: 2592/5760, Loss: 0.011812418699264526\n",
      "Epoch: 33, Samples: 2624/5760, Loss: 0.016993463039398193\n",
      "Epoch: 33, Samples: 2656/5760, Loss: 0.01390141248703003\n",
      "Epoch: 33, Samples: 2688/5760, Loss: 0.014730840921401978\n",
      "Epoch: 33, Samples: 2720/5760, Loss: 0.010958731174468994\n",
      "Epoch: 33, Samples: 2752/5760, Loss: 0.007727086544036865\n",
      "Epoch: 33, Samples: 2784/5760, Loss: 0.01785382628440857\n",
      "Epoch: 33, Samples: 2816/5760, Loss: 0.00855448842048645\n",
      "Epoch: 33, Samples: 2848/5760, Loss: 0.0165557861328125\n",
      "Epoch: 33, Samples: 2880/5760, Loss: 0.006335347890853882\n",
      "Epoch: 33, Samples: 2912/5760, Loss: 0.021815016865730286\n",
      "Epoch: 33, Samples: 2944/5760, Loss: 0.014322251081466675\n",
      "Epoch: 33, Samples: 2976/5760, Loss: 0.01566833257675171\n",
      "Epoch: 33, Samples: 3008/5760, Loss: 0.012947499752044678\n",
      "Epoch: 33, Samples: 3040/5760, Loss: 0.006457209587097168\n",
      "Epoch: 33, Samples: 3072/5760, Loss: 0.015155583620071411\n",
      "Epoch: 33, Samples: 3104/5760, Loss: 0.029241710901260376\n",
      "Epoch: 33, Samples: 3136/5760, Loss: 0.010601252317428589\n",
      "Epoch: 33, Samples: 3168/5760, Loss: 0.008087635040283203\n",
      "Epoch: 33, Samples: 3200/5760, Loss: 0.010829329490661621\n",
      "Epoch: 33, Samples: 3232/5760, Loss: 0.014645308256149292\n",
      "Epoch: 33, Samples: 3264/5760, Loss: 0.010008960962295532\n",
      "Epoch: 33, Samples: 3296/5760, Loss: 0.020681068301200867\n",
      "Epoch: 33, Samples: 3328/5760, Loss: 0.00877651572227478\n",
      "Epoch: 33, Samples: 3360/5760, Loss: 0.019079774618148804\n",
      "Epoch: 33, Samples: 3392/5760, Loss: 0.0060541629791259766\n",
      "Epoch: 33, Samples: 3424/5760, Loss: 0.0073156654834747314\n",
      "Epoch: 33, Samples: 3456/5760, Loss: 0.016364216804504395\n",
      "Epoch: 33, Samples: 3488/5760, Loss: 0.011273950338363647\n",
      "Epoch: 33, Samples: 3520/5760, Loss: 0.006337195634841919\n",
      "Epoch: 33, Samples: 3552/5760, Loss: 0.015296131372451782\n",
      "Epoch: 33, Samples: 3584/5760, Loss: 0.01199275255203247\n",
      "Epoch: 33, Samples: 3616/5760, Loss: 0.007711231708526611\n",
      "Epoch: 33, Samples: 3648/5760, Loss: 0.00979498028755188\n",
      "Epoch: 33, Samples: 3680/5760, Loss: 0.012714564800262451\n",
      "Epoch: 33, Samples: 3712/5760, Loss: 0.007971584796905518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33, Samples: 3744/5760, Loss: 0.011427134275436401\n",
      "Epoch: 33, Samples: 3776/5760, Loss: 0.007317960262298584\n",
      "Epoch: 33, Samples: 3808/5760, Loss: 0.011327892541885376\n",
      "Epoch: 33, Samples: 3840/5760, Loss: 0.012286126613616943\n",
      "Epoch: 33, Samples: 3872/5760, Loss: 0.013531267642974854\n",
      "Epoch: 33, Samples: 3904/5760, Loss: 0.007781624794006348\n",
      "Epoch: 33, Samples: 3936/5760, Loss: 0.016017764806747437\n",
      "Epoch: 33, Samples: 3968/5760, Loss: 0.015505373477935791\n",
      "Epoch: 33, Samples: 4000/5760, Loss: 0.016711294651031494\n",
      "Epoch: 33, Samples: 4032/5760, Loss: 0.013733774423599243\n",
      "Epoch: 33, Samples: 4064/5760, Loss: 0.010929584503173828\n",
      "Epoch: 33, Samples: 4096/5760, Loss: 0.006677746772766113\n",
      "Epoch: 33, Samples: 4128/5760, Loss: 0.01203581690788269\n",
      "Epoch: 33, Samples: 4160/5760, Loss: 0.008447766304016113\n",
      "Epoch: 33, Samples: 4192/5760, Loss: 0.01239129900932312\n",
      "Epoch: 33, Samples: 4224/5760, Loss: 0.01381462812423706\n",
      "Epoch: 33, Samples: 4256/5760, Loss: 0.013610780239105225\n",
      "Epoch: 33, Samples: 4288/5760, Loss: 0.008981436491012573\n",
      "Epoch: 33, Samples: 4320/5760, Loss: 0.012164682149887085\n",
      "Epoch: 33, Samples: 4352/5760, Loss: 0.013284087181091309\n",
      "Epoch: 33, Samples: 4384/5760, Loss: 0.007052242755889893\n",
      "Epoch: 33, Samples: 4416/5760, Loss: 0.007429182529449463\n",
      "Epoch: 33, Samples: 4448/5760, Loss: 0.010773897171020508\n",
      "Epoch: 33, Samples: 4480/5760, Loss: 0.011251002550125122\n",
      "Epoch: 33, Samples: 4512/5760, Loss: 0.02023884654045105\n",
      "Epoch: 33, Samples: 4544/5760, Loss: 0.016028136014938354\n",
      "Epoch: 33, Samples: 4576/5760, Loss: 0.011947482824325562\n",
      "Epoch: 33, Samples: 4608/5760, Loss: 0.012511372566223145\n",
      "Epoch: 33, Samples: 4640/5760, Loss: 0.01428145170211792\n",
      "Epoch: 33, Samples: 4672/5760, Loss: 0.009482622146606445\n",
      "Epoch: 33, Samples: 4704/5760, Loss: 0.013069838285446167\n",
      "Epoch: 33, Samples: 4736/5760, Loss: 0.02591252326965332\n",
      "Epoch: 33, Samples: 4768/5760, Loss: 0.008614599704742432\n",
      "Epoch: 33, Samples: 4800/5760, Loss: 0.007464170455932617\n",
      "Epoch: 33, Samples: 4832/5760, Loss: 0.012321889400482178\n",
      "Epoch: 33, Samples: 4864/5760, Loss: 0.009660392999649048\n",
      "Epoch: 33, Samples: 4896/5760, Loss: 0.016571640968322754\n",
      "Epoch: 33, Samples: 4928/5760, Loss: 0.006533533334732056\n",
      "Epoch: 33, Samples: 4960/5760, Loss: 0.015468001365661621\n",
      "Epoch: 33, Samples: 4992/5760, Loss: 0.010177940130233765\n",
      "Epoch: 33, Samples: 5024/5760, Loss: 0.015037357807159424\n",
      "Epoch: 33, Samples: 5056/5760, Loss: 0.009656667709350586\n",
      "Epoch: 33, Samples: 5088/5760, Loss: 0.0051007866859436035\n",
      "Epoch: 33, Samples: 5120/5760, Loss: 0.014587074518203735\n",
      "Epoch: 33, Samples: 5152/5760, Loss: 0.014453679323196411\n",
      "Epoch: 33, Samples: 5184/5760, Loss: 0.006800144910812378\n",
      "Epoch: 33, Samples: 5216/5760, Loss: 0.00862690806388855\n",
      "Epoch: 33, Samples: 5248/5760, Loss: 0.007266253232955933\n",
      "Epoch: 33, Samples: 5280/5760, Loss: 0.006392478942871094\n",
      "Epoch: 33, Samples: 5312/5760, Loss: 0.013222575187683105\n",
      "Epoch: 33, Samples: 5344/5760, Loss: 0.01327425241470337\n",
      "Epoch: 33, Samples: 5376/5760, Loss: 0.013956218957901001\n",
      "Epoch: 33, Samples: 5408/5760, Loss: 0.0514431893825531\n",
      "Epoch: 33, Samples: 5440/5760, Loss: 0.017183750867843628\n",
      "Epoch: 33, Samples: 5472/5760, Loss: 0.015463411808013916\n",
      "Epoch: 33, Samples: 5504/5760, Loss: 0.02783101797103882\n",
      "Epoch: 33, Samples: 5536/5760, Loss: 0.019739538431167603\n",
      "Epoch: 33, Samples: 5568/5760, Loss: 0.010972350835800171\n",
      "Epoch: 33, Samples: 5600/5760, Loss: 0.0057523250579833984\n",
      "Epoch: 33, Samples: 5632/5760, Loss: 0.019314855337142944\n",
      "Epoch: 33, Samples: 5664/5760, Loss: 0.01016688346862793\n",
      "Epoch: 33, Samples: 5696/5760, Loss: 0.015567898750305176\n",
      "Epoch: 33, Samples: 5728/5760, Loss: 0.6311588287353516\n",
      "\n",
      "Epoch: 33\n",
      "Training set: Average loss: 0.0166\n",
      "Validation set: Average loss: 0.2724, Accuracy: 761/818 (93%)\n",
      "Saving model (epoch 33) with lowest validation loss: 0.27235904708504677\n",
      "Epoch: 34, Samples: 0/5760, Loss: 0.01229703426361084\n",
      "Epoch: 34, Samples: 32/5760, Loss: 0.012846112251281738\n",
      "Epoch: 34, Samples: 64/5760, Loss: 0.008204430341720581\n",
      "Epoch: 34, Samples: 96/5760, Loss: 0.014308720827102661\n",
      "Epoch: 34, Samples: 128/5760, Loss: 0.024290412664413452\n",
      "Epoch: 34, Samples: 160/5760, Loss: 0.009072542190551758\n",
      "Epoch: 34, Samples: 192/5760, Loss: 0.016613036394119263\n",
      "Epoch: 34, Samples: 224/5760, Loss: 0.02827492356300354\n",
      "Epoch: 34, Samples: 256/5760, Loss: 0.01443442702293396\n",
      "Epoch: 34, Samples: 288/5760, Loss: 0.014568239450454712\n",
      "Epoch: 34, Samples: 320/5760, Loss: 0.02323216199874878\n",
      "Epoch: 34, Samples: 352/5760, Loss: 0.011288195848464966\n",
      "Epoch: 34, Samples: 384/5760, Loss: 0.011312603950500488\n",
      "Epoch: 34, Samples: 416/5760, Loss: 0.05088438093662262\n",
      "Epoch: 34, Samples: 448/5760, Loss: 0.013673126697540283\n",
      "Epoch: 34, Samples: 480/5760, Loss: 0.02281758189201355\n",
      "Epoch: 34, Samples: 512/5760, Loss: 0.03348100185394287\n",
      "Epoch: 34, Samples: 544/5760, Loss: 0.024270087480545044\n",
      "Epoch: 34, Samples: 576/5760, Loss: 0.01467275619506836\n",
      "Epoch: 34, Samples: 608/5760, Loss: 0.009271681308746338\n",
      "Epoch: 34, Samples: 640/5760, Loss: 0.029338136315345764\n",
      "Epoch: 34, Samples: 672/5760, Loss: 0.007394760847091675\n",
      "Epoch: 34, Samples: 704/5760, Loss: 0.01232108473777771\n",
      "Epoch: 34, Samples: 736/5760, Loss: 0.010402709245681763\n",
      "Epoch: 34, Samples: 768/5760, Loss: 0.013366222381591797\n",
      "Epoch: 34, Samples: 800/5760, Loss: 0.010987639427185059\n",
      "Epoch: 34, Samples: 832/5760, Loss: 0.012107521295547485\n",
      "Epoch: 34, Samples: 864/5760, Loss: 0.0074194371700286865\n",
      "Epoch: 34, Samples: 896/5760, Loss: 0.00919148325920105\n",
      "Epoch: 34, Samples: 928/5760, Loss: 0.0174618661403656\n",
      "Epoch: 34, Samples: 960/5760, Loss: 0.01656419038772583\n",
      "Epoch: 34, Samples: 992/5760, Loss: 0.015585899353027344\n",
      "Epoch: 34, Samples: 1024/5760, Loss: 0.02217000722885132\n",
      "Epoch: 34, Samples: 1056/5760, Loss: 0.009943634271621704\n",
      "Epoch: 34, Samples: 1088/5760, Loss: 0.00861579179763794\n",
      "Epoch: 34, Samples: 1120/5760, Loss: 0.010600268840789795\n",
      "Epoch: 34, Samples: 1152/5760, Loss: 0.010463595390319824\n",
      "Epoch: 34, Samples: 1184/5760, Loss: 0.009926974773406982\n",
      "Epoch: 34, Samples: 1216/5760, Loss: 0.01599070429801941\n",
      "Epoch: 34, Samples: 1248/5760, Loss: 0.020324677228927612\n",
      "Epoch: 34, Samples: 1280/5760, Loss: 0.006164878606796265\n",
      "Epoch: 34, Samples: 1312/5760, Loss: 0.025159046053886414\n",
      "Epoch: 34, Samples: 1344/5760, Loss: 0.02075245976448059\n",
      "Epoch: 34, Samples: 1376/5760, Loss: 0.025254011154174805\n",
      "Epoch: 34, Samples: 1408/5760, Loss: 0.03452952206134796\n",
      "Epoch: 34, Samples: 1440/5760, Loss: 0.011948376893997192\n",
      "Epoch: 34, Samples: 1472/5760, Loss: 0.013554096221923828\n",
      "Epoch: 34, Samples: 1504/5760, Loss: 0.02687358856201172\n",
      "Epoch: 34, Samples: 1536/5760, Loss: 0.009420216083526611\n",
      "Epoch: 34, Samples: 1568/5760, Loss: 0.010386377573013306\n",
      "Epoch: 34, Samples: 1600/5760, Loss: 0.011865109205245972\n",
      "Epoch: 34, Samples: 1632/5760, Loss: 0.015990376472473145\n",
      "Epoch: 34, Samples: 1664/5760, Loss: 0.015296846628189087\n",
      "Epoch: 34, Samples: 1696/5760, Loss: 0.009331047534942627\n",
      "Epoch: 34, Samples: 1728/5760, Loss: 0.02050304412841797\n",
      "Epoch: 34, Samples: 1760/5760, Loss: 0.005205482244491577\n",
      "Epoch: 34, Samples: 1792/5760, Loss: 0.008938014507293701\n",
      "Epoch: 34, Samples: 1824/5760, Loss: 0.014575749635696411\n",
      "Epoch: 34, Samples: 1856/5760, Loss: 0.003612041473388672\n",
      "Epoch: 34, Samples: 1888/5760, Loss: 0.008518755435943604\n",
      "Epoch: 34, Samples: 1920/5760, Loss: 0.013350993394851685\n",
      "Epoch: 34, Samples: 1952/5760, Loss: 0.015585273504257202\n",
      "Epoch: 34, Samples: 1984/5760, Loss: 0.014451548457145691\n",
      "Epoch: 34, Samples: 2016/5760, Loss: 0.0286291241645813\n",
      "Epoch: 34, Samples: 2048/5760, Loss: 0.010603845119476318\n",
      "Epoch: 34, Samples: 2080/5760, Loss: 0.014845818281173706\n",
      "Epoch: 34, Samples: 2112/5760, Loss: 0.017067551612854004\n",
      "Epoch: 34, Samples: 2144/5760, Loss: 0.01628166437149048\n",
      "Epoch: 34, Samples: 2176/5760, Loss: 0.008657485246658325\n",
      "Epoch: 34, Samples: 2208/5760, Loss: 0.005609571933746338\n",
      "Epoch: 34, Samples: 2240/5760, Loss: 0.016408056020736694\n",
      "Epoch: 34, Samples: 2272/5760, Loss: 0.04921843111515045\n",
      "Epoch: 34, Samples: 2304/5760, Loss: 0.00717771053314209\n",
      "Epoch: 34, Samples: 2336/5760, Loss: 0.010840803384780884\n",
      "Epoch: 34, Samples: 2368/5760, Loss: 0.014119923114776611\n",
      "Epoch: 34, Samples: 2400/5760, Loss: 0.02156248688697815\n",
      "Epoch: 34, Samples: 2432/5760, Loss: 0.010514259338378906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Samples: 2464/5760, Loss: 0.014910280704498291\n",
      "Epoch: 34, Samples: 2496/5760, Loss: 0.018908649682998657\n",
      "Epoch: 34, Samples: 2528/5760, Loss: 0.014386236667633057\n",
      "Epoch: 34, Samples: 2560/5760, Loss: 0.005163341760635376\n",
      "Epoch: 34, Samples: 2592/5760, Loss: 0.014223992824554443\n",
      "Epoch: 34, Samples: 2624/5760, Loss: 0.01785433292388916\n",
      "Epoch: 34, Samples: 2656/5760, Loss: 0.012239992618560791\n",
      "Epoch: 34, Samples: 2688/5760, Loss: 0.014746665954589844\n",
      "Epoch: 34, Samples: 2720/5760, Loss: 0.007063120603561401\n",
      "Epoch: 34, Samples: 2752/5760, Loss: 0.009885936975479126\n",
      "Epoch: 34, Samples: 2784/5760, Loss: 0.014881610870361328\n",
      "Epoch: 34, Samples: 2816/5760, Loss: 0.010145962238311768\n",
      "Epoch: 34, Samples: 2848/5760, Loss: 0.01375851035118103\n",
      "Epoch: 34, Samples: 2880/5760, Loss: 0.013977676630020142\n",
      "Epoch: 34, Samples: 2912/5760, Loss: 0.007465630769729614\n",
      "Epoch: 34, Samples: 2944/5760, Loss: 0.006593525409698486\n",
      "Epoch: 34, Samples: 2976/5760, Loss: 0.008056610822677612\n",
      "Epoch: 34, Samples: 3008/5760, Loss: 0.01248091459274292\n",
      "Epoch: 34, Samples: 3040/5760, Loss: 0.013360977172851562\n",
      "Epoch: 34, Samples: 3072/5760, Loss: 0.017036378383636475\n",
      "Epoch: 34, Samples: 3104/5760, Loss: 0.014131784439086914\n",
      "Epoch: 34, Samples: 3136/5760, Loss: 0.011844128370285034\n",
      "Epoch: 34, Samples: 3168/5760, Loss: 0.010384559631347656\n",
      "Epoch: 34, Samples: 3200/5760, Loss: 0.018089205026626587\n",
      "Epoch: 34, Samples: 3232/5760, Loss: 0.015831947326660156\n",
      "Epoch: 34, Samples: 3264/5760, Loss: 0.006744712591171265\n",
      "Epoch: 34, Samples: 3296/5760, Loss: 0.008217155933380127\n",
      "Epoch: 34, Samples: 3328/5760, Loss: 0.0175589919090271\n",
      "Epoch: 34, Samples: 3360/5760, Loss: 0.01005217432975769\n",
      "Epoch: 34, Samples: 3392/5760, Loss: 0.01575574278831482\n",
      "Epoch: 34, Samples: 3424/5760, Loss: 0.011096864938735962\n",
      "Epoch: 34, Samples: 3456/5760, Loss: 0.022606104612350464\n",
      "Epoch: 34, Samples: 3488/5760, Loss: 0.007990598678588867\n",
      "Epoch: 34, Samples: 3520/5760, Loss: 0.010392338037490845\n",
      "Epoch: 34, Samples: 3552/5760, Loss: 0.013969868421554565\n",
      "Epoch: 34, Samples: 3584/5760, Loss: 0.013876616954803467\n",
      "Epoch: 34, Samples: 3616/5760, Loss: 0.00927242636680603\n",
      "Epoch: 34, Samples: 3648/5760, Loss: 0.01660594344139099\n",
      "Epoch: 34, Samples: 3680/5760, Loss: 0.01004248857498169\n",
      "Epoch: 34, Samples: 3712/5760, Loss: 0.024622172117233276\n",
      "Epoch: 34, Samples: 3744/5760, Loss: 0.009713619947433472\n",
      "Epoch: 34, Samples: 3776/5760, Loss: 0.01636296510696411\n",
      "Epoch: 34, Samples: 3808/5760, Loss: 0.015460401773452759\n",
      "Epoch: 34, Samples: 3840/5760, Loss: 0.02133190631866455\n",
      "Epoch: 34, Samples: 3872/5760, Loss: 0.027770861983299255\n",
      "Epoch: 34, Samples: 3904/5760, Loss: 0.016738474369049072\n",
      "Epoch: 34, Samples: 3936/5760, Loss: 0.010500162839889526\n",
      "Epoch: 34, Samples: 3968/5760, Loss: 0.01120847463607788\n",
      "Epoch: 34, Samples: 4000/5760, Loss: 0.01346537470817566\n",
      "Epoch: 34, Samples: 4032/5760, Loss: 0.00910976529121399\n",
      "Epoch: 34, Samples: 4064/5760, Loss: 0.008844703435897827\n",
      "Epoch: 34, Samples: 4096/5760, Loss: 0.010103046894073486\n",
      "Epoch: 34, Samples: 4128/5760, Loss: 0.015747666358947754\n",
      "Epoch: 34, Samples: 4160/5760, Loss: 0.05692209303379059\n",
      "Epoch: 34, Samples: 4192/5760, Loss: 0.015882328152656555\n",
      "Epoch: 34, Samples: 4224/5760, Loss: 0.014226257801055908\n",
      "Epoch: 34, Samples: 4256/5760, Loss: 0.01855480670928955\n",
      "Epoch: 34, Samples: 4288/5760, Loss: 0.01734381914138794\n",
      "Epoch: 34, Samples: 4320/5760, Loss: 0.005811929702758789\n",
      "Epoch: 34, Samples: 4352/5760, Loss: 0.008977621793746948\n",
      "Epoch: 34, Samples: 4384/5760, Loss: 0.013416051864624023\n",
      "Epoch: 34, Samples: 4416/5760, Loss: 0.012939780950546265\n",
      "Epoch: 34, Samples: 4448/5760, Loss: 0.006779491901397705\n",
      "Epoch: 34, Samples: 4480/5760, Loss: 0.00509876012802124\n",
      "Epoch: 34, Samples: 4512/5760, Loss: 0.01619619131088257\n",
      "Epoch: 34, Samples: 4544/5760, Loss: 0.008589029312133789\n",
      "Epoch: 34, Samples: 4576/5760, Loss: 0.008328646421432495\n",
      "Epoch: 34, Samples: 4608/5760, Loss: 0.02596154808998108\n",
      "Epoch: 34, Samples: 4640/5760, Loss: 0.020803004503250122\n",
      "Epoch: 34, Samples: 4672/5760, Loss: 0.018650352954864502\n",
      "Epoch: 34, Samples: 4704/5760, Loss: 0.007684081792831421\n",
      "Epoch: 34, Samples: 4736/5760, Loss: 0.006203144788742065\n",
      "Epoch: 34, Samples: 4768/5760, Loss: 0.013922333717346191\n",
      "Epoch: 34, Samples: 4800/5760, Loss: 0.013890266418457031\n",
      "Epoch: 34, Samples: 4832/5760, Loss: 0.008177638053894043\n",
      "Epoch: 34, Samples: 4864/5760, Loss: 0.014781177043914795\n",
      "Epoch: 34, Samples: 4896/5760, Loss: 0.014107376337051392\n",
      "Epoch: 34, Samples: 4928/5760, Loss: 0.01917940378189087\n",
      "Epoch: 34, Samples: 4960/5760, Loss: 0.010113775730133057\n",
      "Epoch: 34, Samples: 4992/5760, Loss: 0.009402960538864136\n",
      "Epoch: 34, Samples: 5024/5760, Loss: 0.01380947232246399\n",
      "Epoch: 34, Samples: 5056/5760, Loss: 0.014190196990966797\n",
      "Epoch: 34, Samples: 5088/5760, Loss: 0.012983381748199463\n",
      "Epoch: 34, Samples: 5120/5760, Loss: 0.014741450548171997\n",
      "Epoch: 34, Samples: 5152/5760, Loss: 0.008762717247009277\n",
      "Epoch: 34, Samples: 5184/5760, Loss: 0.010135024785995483\n",
      "Epoch: 34, Samples: 5216/5760, Loss: 0.016044139862060547\n",
      "Epoch: 34, Samples: 5248/5760, Loss: 0.01259586215019226\n",
      "Epoch: 34, Samples: 5280/5760, Loss: 0.026385754346847534\n",
      "Epoch: 34, Samples: 5312/5760, Loss: 0.01397433876991272\n",
      "Epoch: 34, Samples: 5344/5760, Loss: 0.0242922306060791\n",
      "Epoch: 34, Samples: 5376/5760, Loss: 0.008354336023330688\n",
      "Epoch: 34, Samples: 5408/5760, Loss: 0.017987608909606934\n",
      "Epoch: 34, Samples: 5440/5760, Loss: 0.013546854257583618\n",
      "Epoch: 34, Samples: 5472/5760, Loss: 0.008847206830978394\n",
      "Epoch: 34, Samples: 5504/5760, Loss: 0.01928800344467163\n",
      "Epoch: 34, Samples: 5536/5760, Loss: 0.02173815667629242\n",
      "Epoch: 34, Samples: 5568/5760, Loss: 0.010739803314208984\n",
      "Epoch: 34, Samples: 5600/5760, Loss: 0.00932389497756958\n",
      "Epoch: 34, Samples: 5632/5760, Loss: 0.012086659669876099\n",
      "Epoch: 34, Samples: 5664/5760, Loss: 0.0072855353355407715\n",
      "Epoch: 34, Samples: 5696/5760, Loss: 0.007465571165084839\n",
      "Epoch: 34, Samples: 5728/5760, Loss: 0.2778669595718384\n",
      "\n",
      "Epoch: 34\n",
      "Training set: Average loss: 0.0161\n",
      "Validation set: Average loss: 0.2799, Accuracy: 763/818 (93%)\n",
      "Epoch: 35, Samples: 0/5760, Loss: 0.010699450969696045\n",
      "Epoch: 35, Samples: 32/5760, Loss: 0.00820210576057434\n",
      "Epoch: 35, Samples: 64/5760, Loss: 0.013406157493591309\n",
      "Epoch: 35, Samples: 96/5760, Loss: 0.012905150651931763\n",
      "Epoch: 35, Samples: 128/5760, Loss: 0.010723918676376343\n",
      "Epoch: 35, Samples: 160/5760, Loss: 0.010586142539978027\n",
      "Epoch: 35, Samples: 192/5760, Loss: 0.007681399583816528\n",
      "Epoch: 35, Samples: 224/5760, Loss: 0.011415332555770874\n",
      "Epoch: 35, Samples: 256/5760, Loss: 0.009699136018753052\n",
      "Epoch: 35, Samples: 288/5760, Loss: 0.009432673454284668\n",
      "Epoch: 35, Samples: 320/5760, Loss: 0.009724527597427368\n",
      "Epoch: 35, Samples: 352/5760, Loss: 0.018282562494277954\n",
      "Epoch: 35, Samples: 384/5760, Loss: 0.020353764295578003\n",
      "Epoch: 35, Samples: 416/5760, Loss: 0.009133756160736084\n",
      "Epoch: 35, Samples: 448/5760, Loss: 0.012130051851272583\n",
      "Epoch: 35, Samples: 480/5760, Loss: 0.011186331510543823\n",
      "Epoch: 35, Samples: 512/5760, Loss: 0.00933009386062622\n",
      "Epoch: 35, Samples: 544/5760, Loss: 0.015352576971054077\n",
      "Epoch: 35, Samples: 576/5760, Loss: 0.006803661584854126\n",
      "Epoch: 35, Samples: 608/5760, Loss: 0.011131227016448975\n",
      "Epoch: 35, Samples: 640/5760, Loss: 0.006935596466064453\n",
      "Epoch: 35, Samples: 672/5760, Loss: 0.008467555046081543\n",
      "Epoch: 35, Samples: 704/5760, Loss: 0.01975145936012268\n",
      "Epoch: 35, Samples: 736/5760, Loss: 0.02764245867729187\n",
      "Epoch: 35, Samples: 768/5760, Loss: 0.0094681978225708\n",
      "Epoch: 35, Samples: 800/5760, Loss: 0.015156656503677368\n",
      "Epoch: 35, Samples: 832/5760, Loss: 0.011282414197921753\n",
      "Epoch: 35, Samples: 864/5760, Loss: 0.012956440448760986\n",
      "Epoch: 35, Samples: 896/5760, Loss: 0.007657051086425781\n",
      "Epoch: 35, Samples: 928/5760, Loss: 0.006940305233001709\n",
      "Epoch: 35, Samples: 960/5760, Loss: 0.013457953929901123\n",
      "Epoch: 35, Samples: 992/5760, Loss: 0.02241376042366028\n",
      "Epoch: 35, Samples: 1024/5760, Loss: 0.008036136627197266\n",
      "Epoch: 35, Samples: 1056/5760, Loss: 0.01044529676437378\n",
      "Epoch: 35, Samples: 1088/5760, Loss: 0.0128096342086792\n",
      "Epoch: 35, Samples: 1120/5760, Loss: 0.012417852878570557\n",
      "Epoch: 35, Samples: 1152/5760, Loss: 0.012755155563354492\n",
      "Epoch: 35, Samples: 1184/5760, Loss: 0.012705028057098389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, Samples: 1216/5760, Loss: 0.020618081092834473\n",
      "Epoch: 35, Samples: 1248/5760, Loss: 0.010385781526565552\n",
      "Epoch: 35, Samples: 1280/5760, Loss: 0.013306409120559692\n",
      "Epoch: 35, Samples: 1312/5760, Loss: 0.011184155941009521\n",
      "Epoch: 35, Samples: 1344/5760, Loss: 0.007381945848464966\n",
      "Epoch: 35, Samples: 1376/5760, Loss: 0.012170583009719849\n",
      "Epoch: 35, Samples: 1408/5760, Loss: 0.008065104484558105\n",
      "Epoch: 35, Samples: 1440/5760, Loss: 0.01579117774963379\n",
      "Epoch: 35, Samples: 1472/5760, Loss: 0.012713789939880371\n",
      "Epoch: 35, Samples: 1504/5760, Loss: 0.021474123001098633\n",
      "Epoch: 35, Samples: 1536/5760, Loss: 0.012110114097595215\n",
      "Epoch: 35, Samples: 1568/5760, Loss: 0.009464293718338013\n",
      "Epoch: 35, Samples: 1600/5760, Loss: 0.027085930109024048\n",
      "Epoch: 35, Samples: 1632/5760, Loss: 0.014019936323165894\n",
      "Epoch: 35, Samples: 1664/5760, Loss: 0.0063038170337677\n",
      "Epoch: 35, Samples: 1696/5760, Loss: 0.00882616639137268\n",
      "Epoch: 35, Samples: 1728/5760, Loss: 0.013731539249420166\n",
      "Epoch: 35, Samples: 1760/5760, Loss: 0.015245914459228516\n",
      "Epoch: 35, Samples: 1792/5760, Loss: 0.011002689599990845\n",
      "Epoch: 35, Samples: 1824/5760, Loss: 0.011639237403869629\n",
      "Epoch: 35, Samples: 1856/5760, Loss: 0.0079689621925354\n",
      "Epoch: 35, Samples: 1888/5760, Loss: 0.013248980045318604\n",
      "Epoch: 35, Samples: 1920/5760, Loss: 0.019096732139587402\n",
      "Epoch: 35, Samples: 1952/5760, Loss: 0.010025322437286377\n",
      "Epoch: 35, Samples: 1984/5760, Loss: 0.00804668664932251\n",
      "Epoch: 35, Samples: 2016/5760, Loss: 0.0105227530002594\n",
      "Epoch: 35, Samples: 2048/5760, Loss: 0.008537143468856812\n",
      "Epoch: 35, Samples: 2080/5760, Loss: 0.006538748741149902\n",
      "Epoch: 35, Samples: 2112/5760, Loss: 0.01206853985786438\n",
      "Epoch: 35, Samples: 2144/5760, Loss: 0.0140705406665802\n",
      "Epoch: 35, Samples: 2176/5760, Loss: 0.008229643106460571\n",
      "Epoch: 35, Samples: 2208/5760, Loss: 0.011077910661697388\n",
      "Epoch: 35, Samples: 2240/5760, Loss: 0.023838669061660767\n",
      "Epoch: 35, Samples: 2272/5760, Loss: 0.011417657136917114\n",
      "Epoch: 35, Samples: 2304/5760, Loss: 0.010815739631652832\n",
      "Epoch: 35, Samples: 2336/5760, Loss: 0.008690804243087769\n",
      "Epoch: 35, Samples: 2368/5760, Loss: 0.021471023559570312\n",
      "Epoch: 35, Samples: 2400/5760, Loss: 0.007083863019943237\n",
      "Epoch: 35, Samples: 2432/5760, Loss: 0.01131775975227356\n",
      "Epoch: 35, Samples: 2464/5760, Loss: 0.011364489793777466\n",
      "Epoch: 35, Samples: 2496/5760, Loss: 0.013722598552703857\n",
      "Epoch: 35, Samples: 2528/5760, Loss: 0.011503100395202637\n",
      "Epoch: 35, Samples: 2560/5760, Loss: 0.014560163021087646\n",
      "Epoch: 35, Samples: 2592/5760, Loss: 0.01007041335105896\n",
      "Epoch: 35, Samples: 2624/5760, Loss: 0.014738470315933228\n",
      "Epoch: 35, Samples: 2656/5760, Loss: 0.010093539953231812\n",
      "Epoch: 35, Samples: 2688/5760, Loss: 0.005650937557220459\n",
      "Epoch: 35, Samples: 2720/5760, Loss: 0.011319011449813843\n",
      "Epoch: 35, Samples: 2752/5760, Loss: 0.0154036283493042\n",
      "Epoch: 35, Samples: 2784/5760, Loss: 0.007783859968185425\n",
      "Epoch: 35, Samples: 2816/5760, Loss: 0.009325593709945679\n",
      "Epoch: 35, Samples: 2848/5760, Loss: 0.015767604112625122\n",
      "Epoch: 35, Samples: 2880/5760, Loss: 0.01716652512550354\n",
      "Epoch: 35, Samples: 2912/5760, Loss: 0.01304638385772705\n",
      "Epoch: 35, Samples: 2944/5760, Loss: 0.021189242601394653\n",
      "Epoch: 35, Samples: 2976/5760, Loss: 0.013864606618881226\n",
      "Epoch: 35, Samples: 3008/5760, Loss: 0.01019701361656189\n",
      "Epoch: 35, Samples: 3040/5760, Loss: 0.009550899267196655\n",
      "Epoch: 35, Samples: 3072/5760, Loss: 0.009905576705932617\n",
      "Epoch: 35, Samples: 3104/5760, Loss: 0.03202220797538757\n",
      "Epoch: 35, Samples: 3136/5760, Loss: 0.010751664638519287\n",
      "Epoch: 35, Samples: 3168/5760, Loss: 0.013226181268692017\n",
      "Epoch: 35, Samples: 3200/5760, Loss: 0.009688496589660645\n",
      "Epoch: 35, Samples: 3232/5760, Loss: 0.04151049256324768\n",
      "Epoch: 35, Samples: 3264/5760, Loss: 0.015772998332977295\n",
      "Epoch: 35, Samples: 3296/5760, Loss: 0.009324640035629272\n",
      "Epoch: 35, Samples: 3328/5760, Loss: 0.022483348846435547\n",
      "Epoch: 35, Samples: 3360/5760, Loss: 0.026303142309188843\n",
      "Epoch: 35, Samples: 3392/5760, Loss: 0.011223673820495605\n",
      "Epoch: 35, Samples: 3424/5760, Loss: 0.014751344919204712\n",
      "Epoch: 35, Samples: 3456/5760, Loss: 0.02335178852081299\n",
      "Epoch: 35, Samples: 3488/5760, Loss: 0.01820996403694153\n",
      "Epoch: 35, Samples: 3520/5760, Loss: 0.005251467227935791\n",
      "Epoch: 35, Samples: 3552/5760, Loss: 0.02225080132484436\n",
      "Epoch: 35, Samples: 3584/5760, Loss: 0.019863039255142212\n",
      "Epoch: 35, Samples: 3616/5760, Loss: 0.03225386142730713\n",
      "Epoch: 35, Samples: 3648/5760, Loss: 0.01566869020462036\n",
      "Epoch: 35, Samples: 3680/5760, Loss: 0.009879440069198608\n",
      "Epoch: 35, Samples: 3712/5760, Loss: 0.011692911386489868\n",
      "Epoch: 35, Samples: 3744/5760, Loss: 0.011069893836975098\n",
      "Epoch: 35, Samples: 3776/5760, Loss: 0.012015372514724731\n",
      "Epoch: 35, Samples: 3808/5760, Loss: 0.014871776103973389\n",
      "Epoch: 35, Samples: 3840/5760, Loss: 0.006415337324142456\n",
      "Epoch: 35, Samples: 3872/5760, Loss: 0.01049506664276123\n",
      "Epoch: 35, Samples: 3904/5760, Loss: 0.012478053569793701\n",
      "Epoch: 35, Samples: 3936/5760, Loss: 0.010216712951660156\n",
      "Epoch: 35, Samples: 3968/5760, Loss: 0.004990935325622559\n",
      "Epoch: 35, Samples: 4000/5760, Loss: 0.018985092639923096\n",
      "Epoch: 35, Samples: 4032/5760, Loss: 0.00768163800239563\n",
      "Epoch: 35, Samples: 4064/5760, Loss: 0.010631293058395386\n",
      "Epoch: 35, Samples: 4096/5760, Loss: 0.006999462842941284\n",
      "Epoch: 35, Samples: 4128/5760, Loss: 0.006349921226501465\n",
      "Epoch: 35, Samples: 4160/5760, Loss: 0.009136348962783813\n",
      "Epoch: 35, Samples: 4192/5760, Loss: 0.013901859521865845\n",
      "Epoch: 35, Samples: 4224/5760, Loss: 0.009861350059509277\n",
      "Epoch: 35, Samples: 4256/5760, Loss: 0.00644335150718689\n",
      "Epoch: 35, Samples: 4288/5760, Loss: 0.0273827463388443\n",
      "Epoch: 35, Samples: 4320/5760, Loss: 0.011924415826797485\n",
      "Epoch: 35, Samples: 4352/5760, Loss: 0.007065236568450928\n",
      "Epoch: 35, Samples: 4384/5760, Loss: 0.01223456859588623\n",
      "Epoch: 35, Samples: 4416/5760, Loss: 0.019528180360794067\n",
      "Epoch: 35, Samples: 4448/5760, Loss: 0.007923334836959839\n",
      "Epoch: 35, Samples: 4480/5760, Loss: 0.009152650833129883\n",
      "Epoch: 35, Samples: 4512/5760, Loss: 0.013019740581512451\n",
      "Epoch: 35, Samples: 4544/5760, Loss: 0.01063796877861023\n",
      "Epoch: 35, Samples: 4576/5760, Loss: 0.006524592638015747\n",
      "Epoch: 35, Samples: 4608/5760, Loss: 0.0250454843044281\n",
      "Epoch: 35, Samples: 4640/5760, Loss: 0.009133219718933105\n",
      "Epoch: 35, Samples: 4672/5760, Loss: 0.02051571011543274\n",
      "Epoch: 35, Samples: 4704/5760, Loss: 0.010448753833770752\n",
      "Epoch: 35, Samples: 4736/5760, Loss: 0.017228662967681885\n",
      "Epoch: 35, Samples: 4768/5760, Loss: 0.009555399417877197\n",
      "Epoch: 35, Samples: 4800/5760, Loss: 0.012000888586044312\n",
      "Epoch: 35, Samples: 4832/5760, Loss: 0.009378135204315186\n",
      "Epoch: 35, Samples: 4864/5760, Loss: 0.014929264783859253\n",
      "Epoch: 35, Samples: 4896/5760, Loss: 0.0072335898876190186\n",
      "Epoch: 35, Samples: 4928/5760, Loss: 0.01247328519821167\n",
      "Epoch: 35, Samples: 4960/5760, Loss: 0.006152421236038208\n",
      "Epoch: 35, Samples: 4992/5760, Loss: 0.009841352701187134\n",
      "Epoch: 35, Samples: 5024/5760, Loss: 0.010234743356704712\n",
      "Epoch: 35, Samples: 5056/5760, Loss: 0.011338233947753906\n",
      "Epoch: 35, Samples: 5088/5760, Loss: 0.007344096899032593\n",
      "Epoch: 35, Samples: 5120/5760, Loss: 0.009015917778015137\n",
      "Epoch: 35, Samples: 5152/5760, Loss: 0.012464374303817749\n",
      "Epoch: 35, Samples: 5184/5760, Loss: 0.009617894887924194\n",
      "Epoch: 35, Samples: 5216/5760, Loss: 0.019137531518936157\n",
      "Epoch: 35, Samples: 5248/5760, Loss: 0.013842880725860596\n",
      "Epoch: 35, Samples: 5280/5760, Loss: 0.005625039339065552\n",
      "Epoch: 35, Samples: 5312/5760, Loss: 0.008304029703140259\n",
      "Epoch: 35, Samples: 5344/5760, Loss: 0.015373289585113525\n",
      "Epoch: 35, Samples: 5376/5760, Loss: 0.009363025426864624\n",
      "Epoch: 35, Samples: 5408/5760, Loss: 0.006029337644577026\n",
      "Epoch: 35, Samples: 5440/5760, Loss: 0.00888049602508545\n",
      "Epoch: 35, Samples: 5472/5760, Loss: 0.005912601947784424\n",
      "Epoch: 35, Samples: 5504/5760, Loss: 0.009864360094070435\n",
      "Epoch: 35, Samples: 5536/5760, Loss: 0.010428369045257568\n",
      "Epoch: 35, Samples: 5568/5760, Loss: 0.012142539024353027\n",
      "Epoch: 35, Samples: 5600/5760, Loss: 0.019894182682037354\n",
      "Epoch: 35, Samples: 5632/5760, Loss: 0.014815747737884521\n",
      "Epoch: 35, Samples: 5664/5760, Loss: 0.008453577756881714\n",
      "Epoch: 35, Samples: 5696/5760, Loss: 0.005315184593200684\n",
      "Epoch: 35, Samples: 5728/5760, Loss: 1.7302179336547852\n",
      "\n",
      "Epoch: 35\n",
      "Training set: Average loss: 0.0221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss: 0.2770, Accuracy: 762/818 (93%)\n",
      "Epoch: 36, Samples: 0/5760, Loss: 0.011423081159591675\n",
      "Epoch: 36, Samples: 32/5760, Loss: 0.009362220764160156\n",
      "Epoch: 36, Samples: 64/5760, Loss: 0.028561800718307495\n",
      "Epoch: 36, Samples: 96/5760, Loss: 0.021631568670272827\n",
      "Epoch: 36, Samples: 128/5760, Loss: 0.023628205060958862\n",
      "Epoch: 36, Samples: 160/5760, Loss: 0.01760578155517578\n",
      "Epoch: 36, Samples: 192/5760, Loss: 0.013125866651535034\n",
      "Epoch: 36, Samples: 224/5760, Loss: 0.02245369553565979\n",
      "Epoch: 36, Samples: 256/5760, Loss: 0.012515008449554443\n",
      "Epoch: 36, Samples: 288/5760, Loss: 0.019461512565612793\n",
      "Epoch: 36, Samples: 320/5760, Loss: 0.011774301528930664\n",
      "Epoch: 36, Samples: 352/5760, Loss: 0.015181034803390503\n",
      "Epoch: 36, Samples: 384/5760, Loss: 0.008138000965118408\n",
      "Epoch: 36, Samples: 416/5760, Loss: 0.018308579921722412\n",
      "Epoch: 36, Samples: 448/5760, Loss: 0.020399481058120728\n",
      "Epoch: 36, Samples: 480/5760, Loss: 0.019954174757003784\n",
      "Epoch: 36, Samples: 512/5760, Loss: 0.016680896282196045\n",
      "Epoch: 36, Samples: 544/5760, Loss: 0.032324910163879395\n",
      "Epoch: 36, Samples: 576/5760, Loss: 0.03460268676280975\n",
      "Epoch: 36, Samples: 608/5760, Loss: 0.03860887885093689\n",
      "Epoch: 36, Samples: 640/5760, Loss: 0.03135180473327637\n",
      "Epoch: 36, Samples: 672/5760, Loss: 0.021159887313842773\n",
      "Epoch: 36, Samples: 704/5760, Loss: 0.016424834728240967\n",
      "Epoch: 36, Samples: 736/5760, Loss: 0.017417848110198975\n",
      "Epoch: 36, Samples: 768/5760, Loss: 0.01368817687034607\n",
      "Epoch: 36, Samples: 800/5760, Loss: 0.016601741313934326\n",
      "Epoch: 36, Samples: 832/5760, Loss: 0.005636543035507202\n",
      "Epoch: 36, Samples: 864/5760, Loss: 0.006680727005004883\n",
      "Epoch: 36, Samples: 896/5760, Loss: 0.016314595937728882\n",
      "Epoch: 36, Samples: 928/5760, Loss: 0.0356888473033905\n",
      "Epoch: 36, Samples: 960/5760, Loss: 0.014353692531585693\n",
      "Epoch: 36, Samples: 992/5760, Loss: 0.017201274633407593\n",
      "Epoch: 36, Samples: 1024/5760, Loss: 0.009836047887802124\n",
      "Epoch: 36, Samples: 1056/5760, Loss: 0.0334453284740448\n",
      "Epoch: 36, Samples: 1088/5760, Loss: 0.007427901029586792\n",
      "Epoch: 36, Samples: 1120/5760, Loss: 0.010463356971740723\n",
      "Epoch: 36, Samples: 1152/5760, Loss: 0.04656811058521271\n",
      "Epoch: 36, Samples: 1184/5760, Loss: 0.010265231132507324\n",
      "Epoch: 36, Samples: 1216/5760, Loss: 0.012915194034576416\n",
      "Epoch: 36, Samples: 1248/5760, Loss: 0.00835457444190979\n",
      "Epoch: 36, Samples: 1280/5760, Loss: 0.00874406099319458\n",
      "Epoch: 36, Samples: 1312/5760, Loss: 0.020624130964279175\n",
      "Epoch: 36, Samples: 1344/5760, Loss: 0.007599294185638428\n",
      "Epoch: 36, Samples: 1376/5760, Loss: 0.02563616633415222\n",
      "Epoch: 36, Samples: 1408/5760, Loss: 0.01452702283859253\n",
      "Epoch: 36, Samples: 1440/5760, Loss: 0.014291733503341675\n",
      "Epoch: 36, Samples: 1472/5760, Loss: 0.020040124654769897\n",
      "Epoch: 36, Samples: 1504/5760, Loss: 0.010253995656967163\n",
      "Epoch: 36, Samples: 1536/5760, Loss: 0.012907862663269043\n",
      "Epoch: 36, Samples: 1568/5760, Loss: 0.010186821222305298\n",
      "Epoch: 36, Samples: 1600/5760, Loss: 0.020283669233322144\n",
      "Epoch: 36, Samples: 1632/5760, Loss: 0.016946464776992798\n",
      "Epoch: 36, Samples: 1664/5760, Loss: 0.009903132915496826\n",
      "Epoch: 36, Samples: 1696/5760, Loss: 0.01312515139579773\n",
      "Epoch: 36, Samples: 1728/5760, Loss: 0.01981028914451599\n",
      "Epoch: 36, Samples: 1760/5760, Loss: 0.01019054651260376\n",
      "Epoch: 36, Samples: 1792/5760, Loss: 0.0073410868644714355\n",
      "Epoch: 36, Samples: 1824/5760, Loss: 0.011056303977966309\n",
      "Epoch: 36, Samples: 1856/5760, Loss: 0.022700995206832886\n",
      "Epoch: 36, Samples: 1888/5760, Loss: 0.010007977485656738\n",
      "Epoch: 36, Samples: 1920/5760, Loss: 0.012525230646133423\n",
      "Epoch: 36, Samples: 1952/5760, Loss: 0.016265839338302612\n",
      "Epoch: 36, Samples: 1984/5760, Loss: 0.015056371688842773\n",
      "Epoch: 36, Samples: 2016/5760, Loss: 0.025769591331481934\n",
      "Epoch: 36, Samples: 2048/5760, Loss: 0.010357439517974854\n",
      "Epoch: 36, Samples: 2080/5760, Loss: 0.010214924812316895\n",
      "Epoch: 36, Samples: 2112/5760, Loss: 0.008161455392837524\n",
      "Epoch: 36, Samples: 2144/5760, Loss: 0.01091785728931427\n",
      "Epoch: 36, Samples: 2176/5760, Loss: 0.031365230679512024\n",
      "Epoch: 36, Samples: 2208/5760, Loss: 0.008424580097198486\n",
      "Epoch: 36, Samples: 2240/5760, Loss: 0.007542610168457031\n",
      "Epoch: 36, Samples: 2272/5760, Loss: 0.03888602554798126\n",
      "Epoch: 36, Samples: 2304/5760, Loss: 0.021800756454467773\n",
      "Epoch: 36, Samples: 2336/5760, Loss: 0.007425874471664429\n",
      "Epoch: 36, Samples: 2368/5760, Loss: 0.00770452618598938\n",
      "Epoch: 36, Samples: 2400/5760, Loss: 0.008484303951263428\n",
      "Epoch: 36, Samples: 2432/5760, Loss: 0.012378156185150146\n",
      "Epoch: 36, Samples: 2464/5760, Loss: 0.02129867672920227\n",
      "Epoch: 36, Samples: 2496/5760, Loss: 0.02028033137321472\n",
      "Epoch: 36, Samples: 2528/5760, Loss: 0.020807206630706787\n",
      "Epoch: 36, Samples: 2560/5760, Loss: 0.010021239519119263\n",
      "Epoch: 36, Samples: 2592/5760, Loss: 0.013947546482086182\n",
      "Epoch: 36, Samples: 2624/5760, Loss: 0.03733047842979431\n",
      "Epoch: 36, Samples: 2656/5760, Loss: 0.013448536396026611\n",
      "Epoch: 36, Samples: 2688/5760, Loss: 0.017845571041107178\n",
      "Epoch: 36, Samples: 2720/5760, Loss: 0.02090245485305786\n",
      "Epoch: 36, Samples: 2752/5760, Loss: 0.025824636220932007\n",
      "Epoch: 36, Samples: 2784/5760, Loss: 0.004802882671356201\n",
      "Epoch: 36, Samples: 2816/5760, Loss: 0.009312659502029419\n",
      "Epoch: 36, Samples: 2848/5760, Loss: 0.012943953275680542\n",
      "Epoch: 36, Samples: 2880/5760, Loss: 0.010429978370666504\n",
      "Epoch: 36, Samples: 2912/5760, Loss: 0.009162455797195435\n",
      "Epoch: 36, Samples: 2944/5760, Loss: 0.03252878785133362\n",
      "Epoch: 36, Samples: 2976/5760, Loss: 0.012088030576705933\n",
      "Epoch: 36, Samples: 3008/5760, Loss: 0.012895196676254272\n",
      "Epoch: 36, Samples: 3040/5760, Loss: 0.01073649525642395\n",
      "Epoch: 36, Samples: 3072/5760, Loss: 0.017389565706253052\n",
      "Epoch: 36, Samples: 3104/5760, Loss: 0.010161787271499634\n",
      "Epoch: 36, Samples: 3136/5760, Loss: 0.013061761856079102\n",
      "Epoch: 36, Samples: 3168/5760, Loss: 0.015577197074890137\n",
      "Epoch: 36, Samples: 3200/5760, Loss: 0.015472233295440674\n",
      "Epoch: 36, Samples: 3232/5760, Loss: 0.016166239976882935\n",
      "Epoch: 36, Samples: 3264/5760, Loss: 0.008583366870880127\n",
      "Epoch: 36, Samples: 3296/5760, Loss: 0.004987865686416626\n",
      "Epoch: 36, Samples: 3328/5760, Loss: 0.021413475275039673\n",
      "Epoch: 36, Samples: 3360/5760, Loss: 0.007533043622970581\n",
      "Epoch: 36, Samples: 3392/5760, Loss: 0.021376222372055054\n",
      "Epoch: 36, Samples: 3424/5760, Loss: 0.009144961833953857\n",
      "Epoch: 36, Samples: 3456/5760, Loss: 0.011379927396774292\n",
      "Epoch: 36, Samples: 3488/5760, Loss: 0.02015864849090576\n",
      "Epoch: 36, Samples: 3520/5760, Loss: 0.00962403416633606\n",
      "Epoch: 36, Samples: 3552/5760, Loss: 0.01173296570777893\n",
      "Epoch: 36, Samples: 3584/5760, Loss: 0.010696977376937866\n",
      "Epoch: 36, Samples: 3616/5760, Loss: 0.016355425119400024\n",
      "Epoch: 36, Samples: 3648/5760, Loss: 0.011559367179870605\n",
      "Epoch: 36, Samples: 3680/5760, Loss: 0.009601026773452759\n",
      "Epoch: 36, Samples: 3712/5760, Loss: 0.00910496711730957\n",
      "Epoch: 36, Samples: 3744/5760, Loss: 0.01004841923713684\n",
      "Epoch: 36, Samples: 3776/5760, Loss: 0.023091137409210205\n",
      "Epoch: 36, Samples: 3808/5760, Loss: 0.010152608156204224\n",
      "Epoch: 36, Samples: 3840/5760, Loss: 0.014277905225753784\n",
      "Epoch: 36, Samples: 3872/5760, Loss: 0.008284896612167358\n",
      "Epoch: 36, Samples: 3904/5760, Loss: 0.009402304887771606\n",
      "Epoch: 36, Samples: 3936/5760, Loss: 0.016847878694534302\n",
      "Epoch: 36, Samples: 3968/5760, Loss: 0.012072831392288208\n",
      "Epoch: 36, Samples: 4000/5760, Loss: 0.007017940282821655\n",
      "Epoch: 36, Samples: 4032/5760, Loss: 0.007979631423950195\n",
      "Epoch: 36, Samples: 4064/5760, Loss: 0.008095085620880127\n",
      "Epoch: 36, Samples: 4096/5760, Loss: 0.016955241560935974\n",
      "Epoch: 36, Samples: 4128/5760, Loss: 0.00773465633392334\n",
      "Epoch: 36, Samples: 4160/5760, Loss: 0.016094893217086792\n",
      "Epoch: 36, Samples: 4192/5760, Loss: 0.010661691427230835\n",
      "Epoch: 36, Samples: 4224/5760, Loss: 0.01020464301109314\n",
      "Epoch: 36, Samples: 4256/5760, Loss: 0.010808557271957397\n",
      "Epoch: 36, Samples: 4288/5760, Loss: 0.00872620940208435\n",
      "Epoch: 36, Samples: 4320/5760, Loss: 0.006777137517929077\n",
      "Epoch: 36, Samples: 4352/5760, Loss: 0.018473953008651733\n",
      "Epoch: 36, Samples: 4384/5760, Loss: 0.017091482877731323\n",
      "Epoch: 36, Samples: 4416/5760, Loss: 0.011658191680908203\n",
      "Epoch: 36, Samples: 4448/5760, Loss: 0.009616881608963013\n",
      "Epoch: 36, Samples: 4480/5760, Loss: 0.02386564016342163\n",
      "Epoch: 36, Samples: 4512/5760, Loss: 0.006893634796142578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Samples: 4544/5760, Loss: 0.010243535041809082\n",
      "Epoch: 36, Samples: 4576/5760, Loss: 0.012946933507919312\n",
      "Epoch: 36, Samples: 4608/5760, Loss: 0.011775404214859009\n",
      "Epoch: 36, Samples: 4640/5760, Loss: 0.014634042978286743\n",
      "Epoch: 36, Samples: 4672/5760, Loss: 0.01347208023071289\n",
      "Epoch: 36, Samples: 4704/5760, Loss: 0.01927703619003296\n",
      "Epoch: 36, Samples: 4736/5760, Loss: 0.011713415384292603\n",
      "Epoch: 36, Samples: 4768/5760, Loss: 0.011059165000915527\n",
      "Epoch: 36, Samples: 4800/5760, Loss: 0.006684780120849609\n",
      "Epoch: 36, Samples: 4832/5760, Loss: 0.010678797960281372\n",
      "Epoch: 36, Samples: 4864/5760, Loss: 0.008399873971939087\n",
      "Epoch: 36, Samples: 4896/5760, Loss: 0.02973128855228424\n",
      "Epoch: 36, Samples: 4928/5760, Loss: 0.011186689138412476\n",
      "Epoch: 36, Samples: 4960/5760, Loss: 0.034263432025909424\n",
      "Epoch: 36, Samples: 4992/5760, Loss: 0.012598901987075806\n",
      "Epoch: 36, Samples: 5024/5760, Loss: 0.00700041651725769\n",
      "Epoch: 36, Samples: 5056/5760, Loss: 0.017313599586486816\n",
      "Epoch: 36, Samples: 5088/5760, Loss: 0.014076650142669678\n",
      "Epoch: 36, Samples: 5120/5760, Loss: 0.010995626449584961\n",
      "Epoch: 36, Samples: 5152/5760, Loss: 0.008977323770523071\n",
      "Epoch: 36, Samples: 5184/5760, Loss: 0.010776877403259277\n",
      "Epoch: 36, Samples: 5216/5760, Loss: 0.015759795904159546\n",
      "Epoch: 36, Samples: 5248/5760, Loss: 0.012399405241012573\n",
      "Epoch: 36, Samples: 5280/5760, Loss: 0.01781618595123291\n",
      "Epoch: 36, Samples: 5312/5760, Loss: 0.021909117698669434\n",
      "Epoch: 36, Samples: 5344/5760, Loss: 0.008536279201507568\n",
      "Epoch: 36, Samples: 5376/5760, Loss: 0.00829380750656128\n",
      "Epoch: 36, Samples: 5408/5760, Loss: 0.022668763995170593\n",
      "Epoch: 36, Samples: 5440/5760, Loss: 0.010191887617111206\n",
      "Epoch: 36, Samples: 5472/5760, Loss: 0.008049815893173218\n",
      "Epoch: 36, Samples: 5504/5760, Loss: 0.010994762182235718\n",
      "Epoch: 36, Samples: 5536/5760, Loss: 0.0062209367752075195\n",
      "Epoch: 36, Samples: 5568/5760, Loss: 0.010276734828948975\n",
      "Epoch: 36, Samples: 5600/5760, Loss: 0.02596372365951538\n",
      "Epoch: 36, Samples: 5632/5760, Loss: 0.008827388286590576\n",
      "Epoch: 36, Samples: 5664/5760, Loss: 0.007080733776092529\n",
      "Epoch: 36, Samples: 5696/5760, Loss: 0.020823746919631958\n",
      "Epoch: 36, Samples: 5728/5760, Loss: 1.5169099569320679\n",
      "\n",
      "Epoch: 36\n",
      "Training set: Average loss: 0.0233\n",
      "Validation set: Average loss: 0.2904, Accuracy: 761/818 (93%)\n",
      "Epoch: 37, Samples: 0/5760, Loss: 0.012130051851272583\n",
      "Epoch: 37, Samples: 32/5760, Loss: 0.013743937015533447\n",
      "Epoch: 37, Samples: 64/5760, Loss: 0.00937265157699585\n",
      "Epoch: 37, Samples: 96/5760, Loss: 0.007153034210205078\n",
      "Epoch: 37, Samples: 128/5760, Loss: 0.007161498069763184\n",
      "Epoch: 37, Samples: 160/5760, Loss: 0.04585741460323334\n",
      "Epoch: 37, Samples: 192/5760, Loss: 0.013133466243743896\n",
      "Epoch: 37, Samples: 224/5760, Loss: 0.012609601020812988\n",
      "Epoch: 37, Samples: 256/5760, Loss: 0.023656845092773438\n",
      "Epoch: 37, Samples: 288/5760, Loss: 0.021937936544418335\n",
      "Epoch: 37, Samples: 320/5760, Loss: 0.016956299543380737\n",
      "Epoch: 37, Samples: 352/5760, Loss: 0.02057453989982605\n",
      "Epoch: 37, Samples: 384/5760, Loss: 0.017900794744491577\n",
      "Epoch: 37, Samples: 416/5760, Loss: 0.05234023928642273\n",
      "Epoch: 37, Samples: 448/5760, Loss: 0.02057620882987976\n",
      "Epoch: 37, Samples: 480/5760, Loss: 0.017529428005218506\n",
      "Epoch: 37, Samples: 512/5760, Loss: 0.013752967119216919\n",
      "Epoch: 37, Samples: 544/5760, Loss: 0.012181788682937622\n",
      "Epoch: 37, Samples: 576/5760, Loss: 0.013383626937866211\n",
      "Epoch: 37, Samples: 608/5760, Loss: 0.022579163312911987\n",
      "Epoch: 37, Samples: 640/5760, Loss: 0.0413949191570282\n",
      "Epoch: 37, Samples: 672/5760, Loss: 0.015787988901138306\n",
      "Epoch: 37, Samples: 704/5760, Loss: 0.01498270034790039\n",
      "Epoch: 37, Samples: 736/5760, Loss: 0.012987345457077026\n",
      "Epoch: 37, Samples: 768/5760, Loss: 0.019677311182022095\n",
      "Epoch: 37, Samples: 800/5760, Loss: 0.01968151330947876\n",
      "Epoch: 37, Samples: 832/5760, Loss: 0.01197957992553711\n",
      "Epoch: 37, Samples: 864/5760, Loss: 0.012059569358825684\n",
      "Epoch: 37, Samples: 896/5760, Loss: 0.01361653208732605\n",
      "Epoch: 37, Samples: 928/5760, Loss: 0.017664223909378052\n",
      "Epoch: 37, Samples: 960/5760, Loss: 0.02192661166191101\n",
      "Epoch: 37, Samples: 992/5760, Loss: 0.013000279664993286\n",
      "Epoch: 37, Samples: 1024/5760, Loss: 0.016515016555786133\n",
      "Epoch: 37, Samples: 1056/5760, Loss: 0.014461755752563477\n",
      "Epoch: 37, Samples: 1088/5760, Loss: 0.012862354516983032\n",
      "Epoch: 37, Samples: 1120/5760, Loss: 0.006915688514709473\n",
      "Epoch: 37, Samples: 1152/5760, Loss: 0.018133968114852905\n",
      "Epoch: 37, Samples: 1184/5760, Loss: 0.01164555549621582\n",
      "Epoch: 37, Samples: 1216/5760, Loss: 0.019840151071548462\n",
      "Epoch: 37, Samples: 1248/5760, Loss: 0.01673901081085205\n",
      "Epoch: 37, Samples: 1280/5760, Loss: 0.013849973678588867\n",
      "Epoch: 37, Samples: 1312/5760, Loss: 0.014461278915405273\n",
      "Epoch: 37, Samples: 1344/5760, Loss: 0.019172877073287964\n",
      "Epoch: 37, Samples: 1376/5760, Loss: 0.00858449935913086\n",
      "Epoch: 37, Samples: 1408/5760, Loss: 0.011154472827911377\n",
      "Epoch: 37, Samples: 1440/5760, Loss: 0.018331319093704224\n",
      "Epoch: 37, Samples: 1472/5760, Loss: 0.01849120855331421\n",
      "Epoch: 37, Samples: 1504/5760, Loss: 0.01109805703163147\n",
      "Epoch: 37, Samples: 1536/5760, Loss: 0.01833227276802063\n",
      "Epoch: 37, Samples: 1568/5760, Loss: 0.0207088440656662\n",
      "Epoch: 37, Samples: 1600/5760, Loss: 0.010517239570617676\n",
      "Epoch: 37, Samples: 1632/5760, Loss: 0.010089486837387085\n",
      "Epoch: 37, Samples: 1664/5760, Loss: 0.017850473523139954\n",
      "Epoch: 37, Samples: 1696/5760, Loss: 0.01864960789680481\n",
      "Epoch: 37, Samples: 1728/5760, Loss: 0.010408401489257812\n",
      "Epoch: 37, Samples: 1760/5760, Loss: 0.018970906734466553\n",
      "Epoch: 37, Samples: 1792/5760, Loss: 0.02235502004623413\n",
      "Epoch: 37, Samples: 1824/5760, Loss: 0.019671231508255005\n",
      "Epoch: 37, Samples: 1856/5760, Loss: 0.03580273687839508\n",
      "Epoch: 37, Samples: 1888/5760, Loss: 0.01179322600364685\n",
      "Epoch: 37, Samples: 1920/5760, Loss: 0.007285088300704956\n",
      "Epoch: 37, Samples: 1952/5760, Loss: 0.027890145778656006\n",
      "Epoch: 37, Samples: 1984/5760, Loss: 0.011118471622467041\n",
      "Epoch: 37, Samples: 2016/5760, Loss: 0.009478658437728882\n",
      "Epoch: 37, Samples: 2048/5760, Loss: 0.011692225933074951\n",
      "Epoch: 37, Samples: 2080/5760, Loss: 0.01874244213104248\n",
      "Epoch: 37, Samples: 2112/5760, Loss: 0.015176773071289062\n",
      "Epoch: 37, Samples: 2144/5760, Loss: 0.015441209077835083\n",
      "Epoch: 37, Samples: 2176/5760, Loss: 0.008704334497451782\n",
      "Epoch: 37, Samples: 2208/5760, Loss: 0.019743382930755615\n",
      "Epoch: 37, Samples: 2240/5760, Loss: 0.007698148488998413\n",
      "Epoch: 37, Samples: 2272/5760, Loss: 0.00756525993347168\n",
      "Epoch: 37, Samples: 2304/5760, Loss: 0.04436469078063965\n",
      "Epoch: 37, Samples: 2336/5760, Loss: 0.011369168758392334\n",
      "Epoch: 37, Samples: 2368/5760, Loss: 0.015757232904434204\n",
      "Epoch: 37, Samples: 2400/5760, Loss: 0.030413076281547546\n",
      "Epoch: 37, Samples: 2432/5760, Loss: 0.01375553011894226\n",
      "Epoch: 37, Samples: 2464/5760, Loss: 0.014302283525466919\n",
      "Epoch: 37, Samples: 2496/5760, Loss: 0.011558085680007935\n",
      "Epoch: 37, Samples: 2528/5760, Loss: 0.06612107157707214\n",
      "Epoch: 37, Samples: 2560/5760, Loss: 0.016569465398788452\n",
      "Epoch: 37, Samples: 2592/5760, Loss: 0.01474103331565857\n",
      "Epoch: 37, Samples: 2624/5760, Loss: 0.007681667804718018\n",
      "Epoch: 37, Samples: 2656/5760, Loss: 0.01885351538658142\n",
      "Epoch: 37, Samples: 2688/5760, Loss: 0.007734149694442749\n",
      "Epoch: 37, Samples: 2720/5760, Loss: 0.014456123113632202\n",
      "Epoch: 37, Samples: 2752/5760, Loss: 0.018718749284744263\n",
      "Epoch: 37, Samples: 2784/5760, Loss: 0.01569300889968872\n",
      "Epoch: 37, Samples: 2816/5760, Loss: 0.013633161783218384\n",
      "Epoch: 37, Samples: 2848/5760, Loss: 0.012279242277145386\n",
      "Epoch: 37, Samples: 2880/5760, Loss: 0.012223154306411743\n",
      "Epoch: 37, Samples: 2912/5760, Loss: 0.012533426284790039\n",
      "Epoch: 37, Samples: 2944/5760, Loss: 0.009392797946929932\n",
      "Epoch: 37, Samples: 2976/5760, Loss: 0.009086310863494873\n",
      "Epoch: 37, Samples: 3008/5760, Loss: 0.020447373390197754\n",
      "Epoch: 37, Samples: 3040/5760, Loss: 0.010299712419509888\n",
      "Epoch: 37, Samples: 3072/5760, Loss: 0.008922010660171509\n",
      "Epoch: 37, Samples: 3104/5760, Loss: 0.012096107006072998\n",
      "Epoch: 37, Samples: 3136/5760, Loss: 0.01107814908027649\n",
      "Epoch: 37, Samples: 3168/5760, Loss: 0.007251918315887451\n",
      "Epoch: 37, Samples: 3200/5760, Loss: 0.01573622226715088\n",
      "Epoch: 37, Samples: 3232/5760, Loss: 0.01353234052658081\n",
      "Epoch: 37, Samples: 3264/5760, Loss: 0.007350951433181763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37, Samples: 3296/5760, Loss: 0.011890530586242676\n",
      "Epoch: 37, Samples: 3328/5760, Loss: 0.024030029773712158\n",
      "Epoch: 37, Samples: 3360/5760, Loss: 0.007918447256088257\n",
      "Epoch: 37, Samples: 3392/5760, Loss: 0.012593269348144531\n",
      "Epoch: 37, Samples: 3424/5760, Loss: 0.007646530866622925\n",
      "Epoch: 37, Samples: 3456/5760, Loss: 0.007494449615478516\n",
      "Epoch: 37, Samples: 3488/5760, Loss: 0.01028972864151001\n",
      "Epoch: 37, Samples: 3520/5760, Loss: 0.006281614303588867\n",
      "Epoch: 37, Samples: 3552/5760, Loss: 0.01182141900062561\n",
      "Epoch: 37, Samples: 3584/5760, Loss: 0.022412627935409546\n",
      "Epoch: 37, Samples: 3616/5760, Loss: 0.0084247887134552\n",
      "Epoch: 37, Samples: 3648/5760, Loss: 0.011896640062332153\n",
      "Epoch: 37, Samples: 3680/5760, Loss: 0.013142317533493042\n",
      "Epoch: 37, Samples: 3712/5760, Loss: 0.011164665222167969\n",
      "Epoch: 37, Samples: 3744/5760, Loss: 0.007903963327407837\n",
      "Epoch: 37, Samples: 3776/5760, Loss: 0.010290175676345825\n",
      "Epoch: 37, Samples: 3808/5760, Loss: 0.013761937618255615\n",
      "Epoch: 37, Samples: 3840/5760, Loss: 0.010869622230529785\n",
      "Epoch: 37, Samples: 3872/5760, Loss: 0.01338815689086914\n",
      "Epoch: 37, Samples: 3904/5760, Loss: 0.017851978540420532\n",
      "Epoch: 37, Samples: 3936/5760, Loss: 0.014470428228378296\n",
      "Epoch: 37, Samples: 3968/5760, Loss: 0.01906651258468628\n",
      "Epoch: 37, Samples: 4000/5760, Loss: 0.009912848472595215\n",
      "Epoch: 37, Samples: 4032/5760, Loss: 0.015076279640197754\n",
      "Epoch: 37, Samples: 4064/5760, Loss: 0.009715080261230469\n",
      "Epoch: 37, Samples: 4096/5760, Loss: 0.012977927923202515\n",
      "Epoch: 37, Samples: 4128/5760, Loss: 0.011071532964706421\n",
      "Epoch: 37, Samples: 4160/5760, Loss: 0.01250535249710083\n",
      "Epoch: 37, Samples: 4192/5760, Loss: 0.015606969594955444\n",
      "Epoch: 37, Samples: 4224/5760, Loss: 0.012215882539749146\n",
      "Epoch: 37, Samples: 4256/5760, Loss: 0.011712223291397095\n",
      "Epoch: 37, Samples: 4288/5760, Loss: 0.009311109781265259\n",
      "Epoch: 37, Samples: 4320/5760, Loss: 0.007570117712020874\n",
      "Epoch: 37, Samples: 4352/5760, Loss: 0.015797197818756104\n",
      "Epoch: 37, Samples: 4384/5760, Loss: 0.006555080413818359\n",
      "Epoch: 37, Samples: 4416/5760, Loss: 0.008823662996292114\n",
      "Epoch: 37, Samples: 4448/5760, Loss: 0.007136732339859009\n",
      "Epoch: 37, Samples: 4480/5760, Loss: 0.011226773262023926\n",
      "Epoch: 37, Samples: 4512/5760, Loss: 0.005979686975479126\n",
      "Epoch: 37, Samples: 4544/5760, Loss: 0.010217547416687012\n",
      "Epoch: 37, Samples: 4576/5760, Loss: 0.01381942629814148\n",
      "Epoch: 37, Samples: 4608/5760, Loss: 0.012931287288665771\n",
      "Epoch: 37, Samples: 4640/5760, Loss: 0.018320202827453613\n",
      "Epoch: 37, Samples: 4672/5760, Loss: 0.019869983196258545\n",
      "Epoch: 37, Samples: 4704/5760, Loss: 0.01019519567489624\n",
      "Epoch: 37, Samples: 4736/5760, Loss: 0.013858646154403687\n",
      "Epoch: 37, Samples: 4768/5760, Loss: 0.01554650068283081\n",
      "Epoch: 37, Samples: 4800/5760, Loss: 0.011639714241027832\n",
      "Epoch: 37, Samples: 4832/5760, Loss: 0.014352977275848389\n",
      "Epoch: 37, Samples: 4864/5760, Loss: 0.011915773153305054\n",
      "Epoch: 37, Samples: 4896/5760, Loss: 0.02377462387084961\n",
      "Epoch: 37, Samples: 4928/5760, Loss: 0.010073214769363403\n",
      "Epoch: 37, Samples: 4960/5760, Loss: 0.015485554933547974\n",
      "Epoch: 37, Samples: 4992/5760, Loss: 0.008852094411849976\n",
      "Epoch: 37, Samples: 5024/5760, Loss: 0.012196123600006104\n",
      "Epoch: 37, Samples: 5056/5760, Loss: 0.013213396072387695\n",
      "Epoch: 37, Samples: 5088/5760, Loss: 0.012931764125823975\n",
      "Epoch: 37, Samples: 5120/5760, Loss: 0.02031448483467102\n",
      "Epoch: 37, Samples: 5152/5760, Loss: 0.011723905801773071\n",
      "Epoch: 37, Samples: 5184/5760, Loss: 0.01088973879814148\n",
      "Epoch: 37, Samples: 5216/5760, Loss: 0.012847214937210083\n",
      "Epoch: 37, Samples: 5248/5760, Loss: 0.007372170686721802\n",
      "Epoch: 37, Samples: 5280/5760, Loss: 0.026045218110084534\n",
      "Epoch: 37, Samples: 5312/5760, Loss: 0.00842621922492981\n",
      "Epoch: 37, Samples: 5344/5760, Loss: 0.014415323734283447\n",
      "Epoch: 37, Samples: 5376/5760, Loss: 0.010317385196685791\n",
      "Epoch: 37, Samples: 5408/5760, Loss: 0.0115966796875\n",
      "Epoch: 37, Samples: 5440/5760, Loss: 0.020322978496551514\n",
      "Epoch: 37, Samples: 5472/5760, Loss: 0.011067450046539307\n",
      "Epoch: 37, Samples: 5504/5760, Loss: 0.017977505922317505\n",
      "Epoch: 37, Samples: 5536/5760, Loss: 0.01027059555053711\n",
      "Epoch: 37, Samples: 5568/5760, Loss: 0.007297694683074951\n",
      "Epoch: 37, Samples: 5600/5760, Loss: 0.014414429664611816\n",
      "Epoch: 37, Samples: 5632/5760, Loss: 0.013623654842376709\n",
      "Epoch: 37, Samples: 5664/5760, Loss: 0.012375205755233765\n",
      "Epoch: 37, Samples: 5696/5760, Loss: 0.01708430051803589\n",
      "Epoch: 37, Samples: 5728/5760, Loss: 0.427188515663147\n",
      "\n",
      "Epoch: 37\n",
      "Training set: Average loss: 0.0172\n",
      "Validation set: Average loss: 0.2757, Accuracy: 755/818 (92%)\n",
      "Epoch: 38, Samples: 0/5760, Loss: 0.01793527603149414\n",
      "Epoch: 38, Samples: 32/5760, Loss: 0.007042288780212402\n",
      "Epoch: 38, Samples: 64/5760, Loss: 0.008029550313949585\n",
      "Epoch: 38, Samples: 96/5760, Loss: 0.010159701108932495\n",
      "Epoch: 38, Samples: 128/5760, Loss: 0.01726776361465454\n",
      "Epoch: 38, Samples: 160/5760, Loss: 0.008090376853942871\n",
      "Epoch: 38, Samples: 192/5760, Loss: 0.011438757181167603\n",
      "Epoch: 38, Samples: 224/5760, Loss: 0.020962804555892944\n",
      "Epoch: 38, Samples: 256/5760, Loss: 0.008128941059112549\n",
      "Epoch: 38, Samples: 288/5760, Loss: 0.006828874349594116\n",
      "Epoch: 38, Samples: 320/5760, Loss: 0.007029026746749878\n",
      "Epoch: 38, Samples: 352/5760, Loss: 0.008233100175857544\n",
      "Epoch: 38, Samples: 384/5760, Loss: 0.013553500175476074\n",
      "Epoch: 38, Samples: 416/5760, Loss: 0.02038535475730896\n",
      "Epoch: 38, Samples: 448/5760, Loss: 0.011876493692398071\n",
      "Epoch: 38, Samples: 480/5760, Loss: 0.008248746395111084\n",
      "Epoch: 38, Samples: 512/5760, Loss: 0.014415770769119263\n",
      "Epoch: 38, Samples: 544/5760, Loss: 0.010969758033752441\n",
      "Epoch: 38, Samples: 576/5760, Loss: 0.0076697468757629395\n",
      "Epoch: 38, Samples: 608/5760, Loss: 0.012622416019439697\n",
      "Epoch: 38, Samples: 640/5760, Loss: 0.04088228940963745\n",
      "Epoch: 38, Samples: 672/5760, Loss: 0.007845848798751831\n",
      "Epoch: 38, Samples: 704/5760, Loss: 0.010699361562728882\n",
      "Epoch: 38, Samples: 736/5760, Loss: 0.013675123453140259\n",
      "Epoch: 38, Samples: 768/5760, Loss: 0.016731932759284973\n",
      "Epoch: 38, Samples: 800/5760, Loss: 0.009599775075912476\n",
      "Epoch: 38, Samples: 832/5760, Loss: 0.0145588219165802\n",
      "Epoch: 38, Samples: 864/5760, Loss: 0.010626018047332764\n",
      "Epoch: 38, Samples: 896/5760, Loss: 0.006923049688339233\n",
      "Epoch: 38, Samples: 928/5760, Loss: 0.012173324823379517\n",
      "Epoch: 38, Samples: 960/5760, Loss: 0.006632715463638306\n",
      "Epoch: 38, Samples: 992/5760, Loss: 0.01253172755241394\n",
      "Epoch: 38, Samples: 1024/5760, Loss: 0.010381549596786499\n",
      "Epoch: 38, Samples: 1056/5760, Loss: 0.011029303073883057\n",
      "Epoch: 38, Samples: 1088/5760, Loss: 0.01902523636817932\n",
      "Epoch: 38, Samples: 1120/5760, Loss: 0.011779546737670898\n",
      "Epoch: 38, Samples: 1152/5760, Loss: 0.010801494121551514\n",
      "Epoch: 38, Samples: 1184/5760, Loss: 0.00917002558708191\n",
      "Epoch: 38, Samples: 1216/5760, Loss: 0.019870012998580933\n",
      "Epoch: 38, Samples: 1248/5760, Loss: 0.007601052522659302\n",
      "Epoch: 38, Samples: 1280/5760, Loss: 0.01128685474395752\n",
      "Epoch: 38, Samples: 1312/5760, Loss: 0.03360170125961304\n",
      "Epoch: 38, Samples: 1344/5760, Loss: 0.014960497617721558\n",
      "Epoch: 38, Samples: 1376/5760, Loss: 0.006569772958755493\n",
      "Epoch: 38, Samples: 1408/5760, Loss: 0.008343994617462158\n",
      "Epoch: 38, Samples: 1440/5760, Loss: 0.010930120944976807\n",
      "Epoch: 38, Samples: 1472/5760, Loss: 0.05129384994506836\n",
      "Epoch: 38, Samples: 1504/5760, Loss: 0.005348324775695801\n",
      "Epoch: 38, Samples: 1536/5760, Loss: 0.02627575397491455\n",
      "Epoch: 38, Samples: 1568/5760, Loss: 0.009312301874160767\n",
      "Epoch: 38, Samples: 1600/5760, Loss: 0.007123321294784546\n",
      "Epoch: 38, Samples: 1632/5760, Loss: 0.009343534708023071\n",
      "Epoch: 38, Samples: 1664/5760, Loss: 0.007906705141067505\n",
      "Epoch: 38, Samples: 1696/5760, Loss: 0.028135299682617188\n",
      "Epoch: 38, Samples: 1728/5760, Loss: 0.006309032440185547\n",
      "Epoch: 38, Samples: 1760/5760, Loss: 0.006799787282943726\n",
      "Epoch: 38, Samples: 1792/5760, Loss: 0.012781292200088501\n",
      "Epoch: 38, Samples: 1824/5760, Loss: 0.023296236991882324\n",
      "Epoch: 38, Samples: 1856/5760, Loss: 0.011843860149383545\n",
      "Epoch: 38, Samples: 1888/5760, Loss: 0.025066018104553223\n",
      "Epoch: 38, Samples: 1920/5760, Loss: 0.012607157230377197\n",
      "Epoch: 38, Samples: 1952/5760, Loss: 0.016603857278823853\n",
      "Epoch: 38, Samples: 1984/5760, Loss: 0.010769158601760864\n",
      "Epoch: 38, Samples: 2016/5760, Loss: 0.007692128419876099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, Samples: 2048/5760, Loss: 0.012573108077049255\n",
      "Epoch: 38, Samples: 2080/5760, Loss: 0.008352071046829224\n",
      "Epoch: 38, Samples: 2112/5760, Loss: 0.006165206432342529\n",
      "Epoch: 38, Samples: 2144/5760, Loss: 0.010668277740478516\n",
      "Epoch: 38, Samples: 2176/5760, Loss: 0.01013609766960144\n",
      "Epoch: 38, Samples: 2208/5760, Loss: 0.012423098087310791\n",
      "Epoch: 38, Samples: 2240/5760, Loss: 0.008616983890533447\n",
      "Epoch: 38, Samples: 2272/5760, Loss: 0.01330462098121643\n",
      "Epoch: 38, Samples: 2304/5760, Loss: 0.019200295209884644\n",
      "Epoch: 38, Samples: 2336/5760, Loss: 0.009348481893539429\n",
      "Epoch: 38, Samples: 2368/5760, Loss: 0.010793536901473999\n",
      "Epoch: 38, Samples: 2400/5760, Loss: 0.007012695074081421\n",
      "Epoch: 38, Samples: 2432/5760, Loss: 0.011550337076187134\n",
      "Epoch: 38, Samples: 2464/5760, Loss: 0.014175772666931152\n",
      "Epoch: 38, Samples: 2496/5760, Loss: 0.006442427635192871\n",
      "Epoch: 38, Samples: 2528/5760, Loss: 0.01132252812385559\n",
      "Epoch: 38, Samples: 2560/5760, Loss: 0.00885993242263794\n",
      "Epoch: 38, Samples: 2592/5760, Loss: 0.00766327977180481\n",
      "Epoch: 38, Samples: 2624/5760, Loss: 0.007058233022689819\n",
      "Epoch: 38, Samples: 2656/5760, Loss: 0.013562321662902832\n",
      "Epoch: 38, Samples: 2688/5760, Loss: 0.011340916156768799\n",
      "Epoch: 38, Samples: 2720/5760, Loss: 0.007925719022750854\n",
      "Epoch: 38, Samples: 2752/5760, Loss: 0.010067254304885864\n",
      "Epoch: 38, Samples: 2784/5760, Loss: 0.009847909212112427\n",
      "Epoch: 38, Samples: 2816/5760, Loss: 0.0159243643283844\n",
      "Epoch: 38, Samples: 2848/5760, Loss: 0.011388063430786133\n",
      "Epoch: 38, Samples: 2880/5760, Loss: 0.010583251714706421\n",
      "Epoch: 38, Samples: 2912/5760, Loss: 0.011591583490371704\n",
      "Epoch: 38, Samples: 2944/5760, Loss: 0.017258554697036743\n",
      "Epoch: 38, Samples: 2976/5760, Loss: 0.022451698780059814\n",
      "Epoch: 38, Samples: 3008/5760, Loss: 0.025722861289978027\n",
      "Epoch: 38, Samples: 3040/5760, Loss: 0.00805884599685669\n",
      "Epoch: 38, Samples: 3072/5760, Loss: 0.007858127355575562\n",
      "Epoch: 38, Samples: 3104/5760, Loss: 0.011191040277481079\n",
      "Epoch: 38, Samples: 3136/5760, Loss: 0.0059697628021240234\n",
      "Epoch: 38, Samples: 3168/5760, Loss: 0.009587466716766357\n",
      "Epoch: 38, Samples: 3200/5760, Loss: 0.01567530632019043\n",
      "Epoch: 38, Samples: 3232/5760, Loss: 0.00736624002456665\n",
      "Epoch: 38, Samples: 3264/5760, Loss: 0.010777562856674194\n",
      "Epoch: 38, Samples: 3296/5760, Loss: 0.007329970598220825\n",
      "Epoch: 38, Samples: 3328/5760, Loss: 0.007184416055679321\n",
      "Epoch: 38, Samples: 3360/5760, Loss: 0.009062349796295166\n",
      "Epoch: 38, Samples: 3392/5760, Loss: 0.006108671426773071\n",
      "Epoch: 38, Samples: 3424/5760, Loss: 0.006635516881942749\n",
      "Epoch: 38, Samples: 3456/5760, Loss: 0.0077645182609558105\n",
      "Epoch: 38, Samples: 3488/5760, Loss: 0.007985800504684448\n",
      "Epoch: 38, Samples: 3520/5760, Loss: 0.02338796854019165\n",
      "Epoch: 38, Samples: 3552/5760, Loss: 0.008088797330856323\n",
      "Epoch: 38, Samples: 3584/5760, Loss: 0.013646245002746582\n",
      "Epoch: 38, Samples: 3616/5760, Loss: 0.009317070245742798\n",
      "Epoch: 38, Samples: 3648/5760, Loss: 0.022279471158981323\n",
      "Epoch: 38, Samples: 3680/5760, Loss: 0.017695456743240356\n",
      "Epoch: 38, Samples: 3712/5760, Loss: 0.015606850385665894\n",
      "Epoch: 38, Samples: 3744/5760, Loss: 0.04074949026107788\n",
      "Epoch: 38, Samples: 3776/5760, Loss: 0.00719866156578064\n",
      "Epoch: 38, Samples: 3808/5760, Loss: 0.012354642152786255\n",
      "Epoch: 38, Samples: 3840/5760, Loss: 0.013051092624664307\n",
      "Epoch: 38, Samples: 3872/5760, Loss: 0.010715097188949585\n",
      "Epoch: 38, Samples: 3904/5760, Loss: 0.031146317720413208\n",
      "Epoch: 38, Samples: 3936/5760, Loss: 0.009000301361083984\n",
      "Epoch: 38, Samples: 3968/5760, Loss: 0.007523775100708008\n",
      "Epoch: 38, Samples: 4000/5760, Loss: 0.00933539867401123\n",
      "Epoch: 38, Samples: 4032/5760, Loss: 0.011601567268371582\n",
      "Epoch: 38, Samples: 4064/5760, Loss: 0.0055027008056640625\n",
      "Epoch: 38, Samples: 4096/5760, Loss: 0.007153987884521484\n",
      "Epoch: 38, Samples: 4128/5760, Loss: 0.01470249891281128\n",
      "Epoch: 38, Samples: 4160/5760, Loss: 0.010263323783874512\n",
      "Epoch: 38, Samples: 4192/5760, Loss: 0.014882773160934448\n",
      "Epoch: 38, Samples: 4224/5760, Loss: 0.016849994659423828\n",
      "Epoch: 38, Samples: 4256/5760, Loss: 0.008416354656219482\n",
      "Epoch: 38, Samples: 4288/5760, Loss: 0.013836771249771118\n",
      "Epoch: 38, Samples: 4320/5760, Loss: 0.007779628038406372\n",
      "Epoch: 38, Samples: 4352/5760, Loss: 0.011591225862503052\n",
      "Epoch: 38, Samples: 4384/5760, Loss: 0.008912593126296997\n",
      "Epoch: 38, Samples: 4416/5760, Loss: 0.011737972497940063\n",
      "Epoch: 38, Samples: 4448/5760, Loss: 0.010665535926818848\n",
      "Epoch: 38, Samples: 4480/5760, Loss: 0.010682463645935059\n",
      "Epoch: 38, Samples: 4512/5760, Loss: 0.009627878665924072\n",
      "Epoch: 38, Samples: 4544/5760, Loss: 0.013168871402740479\n",
      "Epoch: 38, Samples: 4576/5760, Loss: 0.00984904170036316\n",
      "Epoch: 38, Samples: 4608/5760, Loss: 0.01686534285545349\n",
      "Epoch: 38, Samples: 4640/5760, Loss: 0.008367180824279785\n",
      "Epoch: 38, Samples: 4672/5760, Loss: 0.008453071117401123\n",
      "Epoch: 38, Samples: 4704/5760, Loss: 0.005212217569351196\n",
      "Epoch: 38, Samples: 4736/5760, Loss: 0.009215950965881348\n",
      "Epoch: 38, Samples: 4768/5760, Loss: 0.011521339416503906\n",
      "Epoch: 38, Samples: 4800/5760, Loss: 0.0058533549308776855\n",
      "Epoch: 38, Samples: 4832/5760, Loss: 0.021090328693389893\n",
      "Epoch: 38, Samples: 4864/5760, Loss: 0.017836004495620728\n",
      "Epoch: 38, Samples: 4896/5760, Loss: 0.009406358003616333\n",
      "Epoch: 38, Samples: 4928/5760, Loss: 0.007224380970001221\n",
      "Epoch: 38, Samples: 4960/5760, Loss: 0.0063336193561553955\n",
      "Epoch: 38, Samples: 4992/5760, Loss: 0.016086310148239136\n",
      "Epoch: 38, Samples: 5024/5760, Loss: 0.010939210653305054\n",
      "Epoch: 38, Samples: 5056/5760, Loss: 0.006269901990890503\n",
      "Epoch: 38, Samples: 5088/5760, Loss: 0.011047929525375366\n",
      "Epoch: 38, Samples: 5120/5760, Loss: 0.008826881647109985\n",
      "Epoch: 38, Samples: 5152/5760, Loss: 0.009487062692642212\n",
      "Epoch: 38, Samples: 5184/5760, Loss: 0.01511123776435852\n",
      "Epoch: 38, Samples: 5216/5760, Loss: 0.016427576541900635\n",
      "Epoch: 38, Samples: 5248/5760, Loss: 0.03261984884738922\n",
      "Epoch: 38, Samples: 5280/5760, Loss: 0.011028438806533813\n",
      "Epoch: 38, Samples: 5312/5760, Loss: 0.023173987865447998\n",
      "Epoch: 38, Samples: 5344/5760, Loss: 0.015519648790359497\n",
      "Epoch: 38, Samples: 5376/5760, Loss: 0.013159990310668945\n",
      "Epoch: 38, Samples: 5408/5760, Loss: 0.007221192121505737\n",
      "Epoch: 38, Samples: 5440/5760, Loss: 0.011390775442123413\n",
      "Epoch: 38, Samples: 5472/5760, Loss: 0.010472744703292847\n",
      "Epoch: 38, Samples: 5504/5760, Loss: 0.008588194847106934\n",
      "Epoch: 38, Samples: 5536/5760, Loss: 0.02047821879386902\n",
      "Epoch: 38, Samples: 5568/5760, Loss: 0.009517103433609009\n",
      "Epoch: 38, Samples: 5600/5760, Loss: 0.006283015012741089\n",
      "Epoch: 38, Samples: 5632/5760, Loss: 0.009805142879486084\n",
      "Epoch: 38, Samples: 5664/5760, Loss: 0.008721679449081421\n",
      "Epoch: 38, Samples: 5696/5760, Loss: 0.027880698442459106\n",
      "Epoch: 38, Samples: 5728/5760, Loss: 0.9584755301475525\n",
      "\n",
      "Epoch: 38\n",
      "Training set: Average loss: 0.0177\n",
      "Validation set: Average loss: 0.2724, Accuracy: 767/818 (94%)\n",
      "Epoch: 39, Samples: 0/5760, Loss: 0.015662193298339844\n",
      "Epoch: 39, Samples: 32/5760, Loss: 0.008271843194961548\n",
      "Epoch: 39, Samples: 64/5760, Loss: 0.007210284471511841\n",
      "Epoch: 39, Samples: 96/5760, Loss: 0.011805236339569092\n",
      "Epoch: 39, Samples: 128/5760, Loss: 0.030031174421310425\n",
      "Epoch: 39, Samples: 160/5760, Loss: 0.01089438796043396\n",
      "Epoch: 39, Samples: 192/5760, Loss: 0.02292531728744507\n",
      "Epoch: 39, Samples: 224/5760, Loss: 0.007902473211288452\n",
      "Epoch: 39, Samples: 256/5760, Loss: 0.013544678688049316\n",
      "Epoch: 39, Samples: 288/5760, Loss: 0.014072209596633911\n",
      "Epoch: 39, Samples: 320/5760, Loss: 0.009436339139938354\n",
      "Epoch: 39, Samples: 352/5760, Loss: 0.014433294534683228\n",
      "Epoch: 39, Samples: 384/5760, Loss: 0.01808944344520569\n",
      "Epoch: 39, Samples: 416/5760, Loss: 0.023013532161712646\n",
      "Epoch: 39, Samples: 448/5760, Loss: 0.012498050928115845\n",
      "Epoch: 39, Samples: 480/5760, Loss: 0.011177659034729004\n",
      "Epoch: 39, Samples: 512/5760, Loss: 0.008769750595092773\n",
      "Epoch: 39, Samples: 544/5760, Loss: 0.014801502227783203\n",
      "Epoch: 39, Samples: 576/5760, Loss: 0.012505114078521729\n",
      "Epoch: 39, Samples: 608/5760, Loss: 0.006489425897598267\n",
      "Epoch: 39, Samples: 640/5760, Loss: 0.008722156286239624\n",
      "Epoch: 39, Samples: 672/5760, Loss: 0.01811031997203827\n",
      "Epoch: 39, Samples: 704/5760, Loss: 0.011626005172729492\n",
      "Epoch: 39, Samples: 736/5760, Loss: 0.02004927396774292\n",
      "Epoch: 39, Samples: 768/5760, Loss: 0.010165870189666748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, Samples: 800/5760, Loss: 0.021064698696136475\n",
      "Epoch: 39, Samples: 832/5760, Loss: 0.007501453161239624\n",
      "Epoch: 39, Samples: 864/5760, Loss: 0.008631795644760132\n",
      "Epoch: 39, Samples: 896/5760, Loss: 0.008937358856201172\n",
      "Epoch: 39, Samples: 928/5760, Loss: 0.007752150297164917\n",
      "Epoch: 39, Samples: 960/5760, Loss: 0.010994166135787964\n",
      "Epoch: 39, Samples: 992/5760, Loss: 0.023240819573402405\n",
      "Epoch: 39, Samples: 1024/5760, Loss: 0.011573433876037598\n",
      "Epoch: 39, Samples: 1056/5760, Loss: 0.011266469955444336\n",
      "Epoch: 39, Samples: 1088/5760, Loss: 0.00906863808631897\n",
      "Epoch: 39, Samples: 1120/5760, Loss: 0.00977414846420288\n",
      "Epoch: 39, Samples: 1152/5760, Loss: 0.016833782196044922\n",
      "Epoch: 39, Samples: 1184/5760, Loss: 0.010139763355255127\n",
      "Epoch: 39, Samples: 1216/5760, Loss: 0.007557988166809082\n",
      "Epoch: 39, Samples: 1248/5760, Loss: 0.008176237344741821\n",
      "Epoch: 39, Samples: 1280/5760, Loss: 0.009911686182022095\n",
      "Epoch: 39, Samples: 1312/5760, Loss: 0.008845984935760498\n",
      "Epoch: 39, Samples: 1344/5760, Loss: 0.011190444231033325\n",
      "Epoch: 39, Samples: 1376/5760, Loss: 0.011807471513748169\n",
      "Epoch: 39, Samples: 1408/5760, Loss: 0.019337624311447144\n",
      "Epoch: 39, Samples: 1440/5760, Loss: 0.014092326164245605\n",
      "Epoch: 39, Samples: 1472/5760, Loss: 0.006312131881713867\n",
      "Epoch: 39, Samples: 1504/5760, Loss: 0.012504726648330688\n",
      "Epoch: 39, Samples: 1536/5760, Loss: 0.00838625431060791\n",
      "Epoch: 39, Samples: 1568/5760, Loss: 0.016615718603134155\n",
      "Epoch: 39, Samples: 1600/5760, Loss: 0.013558030128479004\n",
      "Epoch: 39, Samples: 1632/5760, Loss: 0.0068479180335998535\n",
      "Epoch: 39, Samples: 1664/5760, Loss: 0.01848602294921875\n",
      "Epoch: 39, Samples: 1696/5760, Loss: 0.012585163116455078\n",
      "Epoch: 39, Samples: 1728/5760, Loss: 0.009742051362991333\n",
      "Epoch: 39, Samples: 1760/5760, Loss: 0.010850369930267334\n",
      "Epoch: 39, Samples: 1792/5760, Loss: 0.009987980127334595\n",
      "Epoch: 39, Samples: 1824/5760, Loss: 0.008190453052520752\n",
      "Epoch: 39, Samples: 1856/5760, Loss: 0.017202138900756836\n",
      "Epoch: 39, Samples: 1888/5760, Loss: 0.011679798364639282\n",
      "Epoch: 39, Samples: 1920/5760, Loss: 0.02027192711830139\n",
      "Epoch: 39, Samples: 1952/5760, Loss: 0.009477943181991577\n",
      "Epoch: 39, Samples: 1984/5760, Loss: 0.014679506421089172\n",
      "Epoch: 39, Samples: 2016/5760, Loss: 0.006809145212173462\n",
      "Epoch: 39, Samples: 2048/5760, Loss: 0.008384883403778076\n",
      "Epoch: 39, Samples: 2080/5760, Loss: 0.014626532793045044\n",
      "Epoch: 39, Samples: 2112/5760, Loss: 0.01503261923789978\n",
      "Epoch: 39, Samples: 2144/5760, Loss: 0.009539693593978882\n",
      "Epoch: 39, Samples: 2176/5760, Loss: 0.012818247079849243\n",
      "Epoch: 39, Samples: 2208/5760, Loss: 0.027225762605667114\n",
      "Epoch: 39, Samples: 2240/5760, Loss: 0.004716157913208008\n",
      "Epoch: 39, Samples: 2272/5760, Loss: 0.013988107442855835\n",
      "Epoch: 39, Samples: 2304/5760, Loss: 0.00744214653968811\n",
      "Epoch: 39, Samples: 2336/5760, Loss: 0.009187877178192139\n",
      "Epoch: 39, Samples: 2368/5760, Loss: 0.017939239740371704\n",
      "Epoch: 39, Samples: 2400/5760, Loss: 0.007753640413284302\n",
      "Epoch: 39, Samples: 2432/5760, Loss: 0.011779248714447021\n",
      "Epoch: 39, Samples: 2464/5760, Loss: 0.010177701711654663\n",
      "Epoch: 39, Samples: 2496/5760, Loss: 0.011400163173675537\n",
      "Epoch: 39, Samples: 2528/5760, Loss: 0.008952081203460693\n",
      "Epoch: 39, Samples: 2560/5760, Loss: 0.008750766515731812\n",
      "Epoch: 39, Samples: 2592/5760, Loss: 0.01103624701499939\n",
      "Epoch: 39, Samples: 2624/5760, Loss: 0.015553981065750122\n",
      "Epoch: 39, Samples: 2656/5760, Loss: 0.009210407733917236\n",
      "Epoch: 39, Samples: 2688/5760, Loss: 0.007277727127075195\n",
      "Epoch: 39, Samples: 2720/5760, Loss: 0.018880605697631836\n",
      "Epoch: 39, Samples: 2752/5760, Loss: 0.011513561010360718\n",
      "Epoch: 39, Samples: 2784/5760, Loss: 0.009032249450683594\n",
      "Epoch: 39, Samples: 2816/5760, Loss: 0.01716458797454834\n",
      "Epoch: 39, Samples: 2848/5760, Loss: 0.005216419696807861\n",
      "Epoch: 39, Samples: 2880/5760, Loss: 0.014697164297103882\n",
      "Epoch: 39, Samples: 2912/5760, Loss: 0.025589793920516968\n",
      "Epoch: 39, Samples: 2944/5760, Loss: 0.010926097631454468\n",
      "Epoch: 39, Samples: 2976/5760, Loss: 0.007907062768936157\n",
      "Epoch: 39, Samples: 3008/5760, Loss: 0.016692638397216797\n",
      "Epoch: 39, Samples: 3040/5760, Loss: 0.006167709827423096\n",
      "Epoch: 39, Samples: 3072/5760, Loss: 0.013103365898132324\n",
      "Epoch: 39, Samples: 3104/5760, Loss: 0.012048035860061646\n",
      "Epoch: 39, Samples: 3136/5760, Loss: 0.010572761297225952\n",
      "Epoch: 39, Samples: 3168/5760, Loss: 0.006518691778182983\n",
      "Epoch: 39, Samples: 3200/5760, Loss: 0.01194620132446289\n",
      "Epoch: 39, Samples: 3232/5760, Loss: 0.0068409740924835205\n",
      "Epoch: 39, Samples: 3264/5760, Loss: 0.00493815541267395\n",
      "Epoch: 39, Samples: 3296/5760, Loss: 0.007465958595275879\n",
      "Epoch: 39, Samples: 3328/5760, Loss: 0.014690294861793518\n",
      "Epoch: 39, Samples: 3360/5760, Loss: 0.01166599988937378\n",
      "Epoch: 39, Samples: 3392/5760, Loss: 0.008583605289459229\n",
      "Epoch: 39, Samples: 3424/5760, Loss: 0.008097827434539795\n",
      "Epoch: 39, Samples: 3456/5760, Loss: 0.006814301013946533\n",
      "Epoch: 39, Samples: 3488/5760, Loss: 0.0035019516944885254\n",
      "Epoch: 39, Samples: 3520/5760, Loss: 0.009213238954544067\n",
      "Epoch: 39, Samples: 3552/5760, Loss: 0.01912529766559601\n",
      "Epoch: 39, Samples: 3584/5760, Loss: 0.013985678553581238\n",
      "Epoch: 39, Samples: 3616/5760, Loss: 0.00897359848022461\n",
      "Epoch: 39, Samples: 3648/5760, Loss: 0.008234947919845581\n",
      "Epoch: 39, Samples: 3680/5760, Loss: 0.006875157356262207\n",
      "Epoch: 39, Samples: 3712/5760, Loss: 0.011281847953796387\n",
      "Epoch: 39, Samples: 3744/5760, Loss: 0.018148109316825867\n",
      "Epoch: 39, Samples: 3776/5760, Loss: 0.011588990688323975\n",
      "Epoch: 39, Samples: 3808/5760, Loss: 0.008413314819335938\n",
      "Epoch: 39, Samples: 3840/5760, Loss: 0.012301146984100342\n",
      "Epoch: 39, Samples: 3872/5760, Loss: 0.04112878441810608\n",
      "Epoch: 39, Samples: 3904/5760, Loss: 0.01223766803741455\n",
      "Epoch: 39, Samples: 3936/5760, Loss: 0.008149325847625732\n",
      "Epoch: 39, Samples: 3968/5760, Loss: 0.010443001985549927\n",
      "Epoch: 39, Samples: 4000/5760, Loss: 0.008020281791687012\n",
      "Epoch: 39, Samples: 4032/5760, Loss: 0.0364329069852829\n",
      "Epoch: 39, Samples: 4064/5760, Loss: 0.006914973258972168\n",
      "Epoch: 39, Samples: 4096/5760, Loss: 0.0065628886222839355\n",
      "Epoch: 39, Samples: 4128/5760, Loss: 0.009437650442123413\n",
      "Epoch: 39, Samples: 4160/5760, Loss: 0.011103570461273193\n",
      "Epoch: 39, Samples: 4192/5760, Loss: 0.011078029870986938\n",
      "Epoch: 39, Samples: 4224/5760, Loss: 0.013456910848617554\n",
      "Epoch: 39, Samples: 4256/5760, Loss: 0.010147303342819214\n",
      "Epoch: 39, Samples: 4288/5760, Loss: 0.007850229740142822\n",
      "Epoch: 39, Samples: 4320/5760, Loss: 0.0026182830333709717\n",
      "Epoch: 39, Samples: 4352/5760, Loss: 0.006748199462890625\n",
      "Epoch: 39, Samples: 4384/5760, Loss: 0.019616320729255676\n",
      "Epoch: 39, Samples: 4416/5760, Loss: 0.00555342435836792\n",
      "Epoch: 39, Samples: 4448/5760, Loss: 0.007521927356719971\n",
      "Epoch: 39, Samples: 4480/5760, Loss: 0.01692560315132141\n",
      "Epoch: 39, Samples: 4512/5760, Loss: 0.014518260955810547\n",
      "Epoch: 39, Samples: 4544/5760, Loss: 0.008863508701324463\n",
      "Epoch: 39, Samples: 4576/5760, Loss: 0.007003664970397949\n",
      "Epoch: 39, Samples: 4608/5760, Loss: 0.008224308490753174\n",
      "Epoch: 39, Samples: 4640/5760, Loss: 0.010034412145614624\n",
      "Epoch: 39, Samples: 4672/5760, Loss: 0.010527342557907104\n",
      "Epoch: 39, Samples: 4704/5760, Loss: 0.008485913276672363\n",
      "Epoch: 39, Samples: 4736/5760, Loss: 0.008517920970916748\n",
      "Epoch: 39, Samples: 4768/5760, Loss: 0.012311398983001709\n",
      "Epoch: 39, Samples: 4800/5760, Loss: 0.012572407722473145\n",
      "Epoch: 39, Samples: 4832/5760, Loss: 0.012586265802383423\n",
      "Epoch: 39, Samples: 4864/5760, Loss: 0.02834177017211914\n",
      "Epoch: 39, Samples: 4896/5760, Loss: 0.004841744899749756\n",
      "Epoch: 39, Samples: 4928/5760, Loss: 0.018193095922470093\n",
      "Epoch: 39, Samples: 4960/5760, Loss: 0.012481480836868286\n",
      "Epoch: 39, Samples: 4992/5760, Loss: 0.006754189729690552\n",
      "Epoch: 39, Samples: 5024/5760, Loss: 0.007973968982696533\n",
      "Epoch: 39, Samples: 5056/5760, Loss: 0.008886665105819702\n",
      "Epoch: 39, Samples: 5088/5760, Loss: 0.012325853109359741\n",
      "Epoch: 39, Samples: 5120/5760, Loss: 0.004940688610076904\n",
      "Epoch: 39, Samples: 5152/5760, Loss: 0.010479718446731567\n",
      "Epoch: 39, Samples: 5184/5760, Loss: 0.02335214614868164\n",
      "Epoch: 39, Samples: 5216/5760, Loss: 0.008002310991287231\n",
      "Epoch: 39, Samples: 5248/5760, Loss: 0.006110519170761108\n",
      "Epoch: 39, Samples: 5280/5760, Loss: 0.008650869131088257\n",
      "Epoch: 39, Samples: 5312/5760, Loss: 0.01560354232788086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, Samples: 5344/5760, Loss: 0.011287808418273926\n",
      "Epoch: 39, Samples: 5376/5760, Loss: 0.011176854372024536\n",
      "Epoch: 39, Samples: 5408/5760, Loss: 0.006205886602401733\n",
      "Epoch: 39, Samples: 5440/5760, Loss: 0.008824139833450317\n",
      "Epoch: 39, Samples: 5472/5760, Loss: 0.005985051393508911\n",
      "Epoch: 39, Samples: 5504/5760, Loss: 0.010420948266983032\n",
      "Epoch: 39, Samples: 5536/5760, Loss: 0.009534776210784912\n",
      "Epoch: 39, Samples: 5568/5760, Loss: 0.010192185640335083\n",
      "Epoch: 39, Samples: 5600/5760, Loss: 0.01273566484451294\n",
      "Epoch: 39, Samples: 5632/5760, Loss: 0.012523442506790161\n",
      "Epoch: 39, Samples: 5664/5760, Loss: 0.0070619285106658936\n",
      "Epoch: 39, Samples: 5696/5760, Loss: 0.006430178880691528\n",
      "Epoch: 39, Samples: 5728/5760, Loss: 1.3989025354385376\n",
      "\n",
      "Epoch: 39\n",
      "Training set: Average loss: 0.0194\n",
      "Validation set: Average loss: 0.2779, Accuracy: 762/818 (93%)\n",
      "Epoch: 40, Samples: 0/5760, Loss: 0.010170340538024902\n",
      "Epoch: 40, Samples: 32/5760, Loss: 0.006523042917251587\n",
      "Epoch: 40, Samples: 64/5760, Loss: 0.006500691175460815\n",
      "Epoch: 40, Samples: 96/5760, Loss: 0.007303327322006226\n",
      "Epoch: 40, Samples: 128/5760, Loss: 0.011172115802764893\n",
      "Epoch: 40, Samples: 160/5760, Loss: 0.009288311004638672\n",
      "Epoch: 40, Samples: 192/5760, Loss: 0.011568129062652588\n",
      "Epoch: 40, Samples: 224/5760, Loss: 0.010056138038635254\n",
      "Epoch: 40, Samples: 256/5760, Loss: 0.021900057792663574\n",
      "Epoch: 40, Samples: 288/5760, Loss: 0.008934766054153442\n",
      "Epoch: 40, Samples: 320/5760, Loss: 0.009799271821975708\n",
      "Epoch: 40, Samples: 352/5760, Loss: 0.006751745939254761\n",
      "Epoch: 40, Samples: 384/5760, Loss: 0.011626332998275757\n",
      "Epoch: 40, Samples: 416/5760, Loss: 0.0056462883949279785\n",
      "Epoch: 40, Samples: 448/5760, Loss: 0.015824437141418457\n",
      "Epoch: 40, Samples: 480/5760, Loss: 0.00788503885269165\n",
      "Epoch: 40, Samples: 512/5760, Loss: 0.011353790760040283\n",
      "Epoch: 40, Samples: 544/5760, Loss: 0.011994242668151855\n",
      "Epoch: 40, Samples: 576/5760, Loss: 0.015909641981124878\n",
      "Epoch: 40, Samples: 608/5760, Loss: 0.011959761381149292\n",
      "Epoch: 40, Samples: 640/5760, Loss: 0.008036255836486816\n",
      "Epoch: 40, Samples: 672/5760, Loss: 0.01568359136581421\n",
      "Epoch: 40, Samples: 704/5760, Loss: 0.007890194654464722\n",
      "Epoch: 40, Samples: 736/5760, Loss: 0.01633523404598236\n",
      "Epoch: 40, Samples: 768/5760, Loss: 0.013248622417449951\n",
      "Epoch: 40, Samples: 800/5760, Loss: 0.010774314403533936\n",
      "Epoch: 40, Samples: 832/5760, Loss: 0.00845789909362793\n",
      "Epoch: 40, Samples: 864/5760, Loss: 0.007037341594696045\n",
      "Epoch: 40, Samples: 896/5760, Loss: 0.007676482200622559\n",
      "Epoch: 40, Samples: 928/5760, Loss: 0.008040815591812134\n",
      "Epoch: 40, Samples: 960/5760, Loss: 0.013251572847366333\n",
      "Epoch: 40, Samples: 992/5760, Loss: 0.014920562505722046\n",
      "Epoch: 40, Samples: 1024/5760, Loss: 0.006424903869628906\n",
      "Epoch: 40, Samples: 1056/5760, Loss: 0.015742361545562744\n",
      "Epoch: 40, Samples: 1088/5760, Loss: 0.007941782474517822\n",
      "Epoch: 40, Samples: 1120/5760, Loss: 0.008329838514328003\n",
      "Epoch: 40, Samples: 1152/5760, Loss: 0.01604446768760681\n",
      "Epoch: 40, Samples: 1184/5760, Loss: 0.015981778502464294\n",
      "Epoch: 40, Samples: 1216/5760, Loss: 0.00792926549911499\n",
      "Epoch: 40, Samples: 1248/5760, Loss: 0.007577121257781982\n",
      "Epoch: 40, Samples: 1280/5760, Loss: 0.00827181339263916\n",
      "Epoch: 40, Samples: 1312/5760, Loss: 0.01579993963241577\n",
      "Epoch: 40, Samples: 1344/5760, Loss: 0.007505923509597778\n",
      "Epoch: 40, Samples: 1376/5760, Loss: 0.010091692209243774\n",
      "Epoch: 40, Samples: 1408/5760, Loss: 0.014111846685409546\n",
      "Epoch: 40, Samples: 1440/5760, Loss: 0.005119264125823975\n",
      "Epoch: 40, Samples: 1472/5760, Loss: 0.007873177528381348\n",
      "Epoch: 40, Samples: 1504/5760, Loss: 0.016728639602661133\n",
      "Epoch: 40, Samples: 1536/5760, Loss: 0.015049397945404053\n",
      "Epoch: 40, Samples: 1568/5760, Loss: 0.008576691150665283\n",
      "Epoch: 40, Samples: 1600/5760, Loss: 0.020366042852401733\n",
      "Epoch: 40, Samples: 1632/5760, Loss: 0.010630905628204346\n",
      "Epoch: 40, Samples: 1664/5760, Loss: 0.015464872121810913\n",
      "Epoch: 40, Samples: 1696/5760, Loss: 0.012636631727218628\n",
      "Epoch: 40, Samples: 1728/5760, Loss: 0.006511986255645752\n",
      "Epoch: 40, Samples: 1760/5760, Loss: 0.007225453853607178\n",
      "Epoch: 40, Samples: 1792/5760, Loss: 0.00841417908668518\n",
      "Epoch: 40, Samples: 1824/5760, Loss: 0.014108866453170776\n",
      "Epoch: 40, Samples: 1856/5760, Loss: 0.007963299751281738\n",
      "Epoch: 40, Samples: 1888/5760, Loss: 0.00679132342338562\n",
      "Epoch: 40, Samples: 1920/5760, Loss: 0.006651133298873901\n",
      "Epoch: 40, Samples: 1952/5760, Loss: 0.012460559606552124\n",
      "Epoch: 40, Samples: 1984/5760, Loss: 0.010929286479949951\n",
      "Epoch: 40, Samples: 2016/5760, Loss: 0.013398885726928711\n",
      "Epoch: 40, Samples: 2048/5760, Loss: 0.012829601764678955\n",
      "Epoch: 40, Samples: 2080/5760, Loss: 0.008620202541351318\n",
      "Epoch: 40, Samples: 2112/5760, Loss: 0.0072747766971588135\n",
      "Epoch: 40, Samples: 2144/5760, Loss: 0.005806416273117065\n",
      "Epoch: 40, Samples: 2176/5760, Loss: 0.009463638067245483\n",
      "Epoch: 40, Samples: 2208/5760, Loss: 0.009671002626419067\n",
      "Epoch: 40, Samples: 2240/5760, Loss: 0.005253702402114868\n",
      "Epoch: 40, Samples: 2272/5760, Loss: 0.00767478346824646\n",
      "Epoch: 40, Samples: 2304/5760, Loss: 0.0076122283935546875\n",
      "Epoch: 40, Samples: 2336/5760, Loss: 0.00746878981590271\n",
      "Epoch: 40, Samples: 2368/5760, Loss: 0.005127817392349243\n",
      "Epoch: 40, Samples: 2400/5760, Loss: 0.011108994483947754\n",
      "Epoch: 40, Samples: 2432/5760, Loss: 0.014855682849884033\n",
      "Epoch: 40, Samples: 2464/5760, Loss: 0.00847133994102478\n",
      "Epoch: 40, Samples: 2496/5760, Loss: 0.010219752788543701\n",
      "Epoch: 40, Samples: 2528/5760, Loss: 0.00575605034828186\n",
      "Epoch: 40, Samples: 2560/5760, Loss: 0.005479961633682251\n",
      "Epoch: 40, Samples: 2592/5760, Loss: 0.010254710912704468\n",
      "Epoch: 40, Samples: 2624/5760, Loss: 0.012156426906585693\n",
      "Epoch: 40, Samples: 2656/5760, Loss: 0.009312212467193604\n",
      "Epoch: 40, Samples: 2688/5760, Loss: 0.011241346597671509\n",
      "Epoch: 40, Samples: 2720/5760, Loss: 0.006623893976211548\n",
      "Epoch: 40, Samples: 2752/5760, Loss: 0.004966050386428833\n",
      "Epoch: 40, Samples: 2784/5760, Loss: 0.013612300157546997\n",
      "Epoch: 40, Samples: 2816/5760, Loss: 0.012856602668762207\n",
      "Epoch: 40, Samples: 2848/5760, Loss: 0.00742870569229126\n",
      "Epoch: 40, Samples: 2880/5760, Loss: 0.027608856558799744\n",
      "Epoch: 40, Samples: 2912/5760, Loss: 0.011554956436157227\n",
      "Epoch: 40, Samples: 2944/5760, Loss: 0.014997780323028564\n",
      "Epoch: 40, Samples: 2976/5760, Loss: 0.016068920493125916\n",
      "Epoch: 40, Samples: 3008/5760, Loss: 0.011231839656829834\n",
      "Epoch: 40, Samples: 3040/5760, Loss: 0.014222651720046997\n",
      "Epoch: 40, Samples: 3072/5760, Loss: 0.006374716758728027\n",
      "Epoch: 40, Samples: 3104/5760, Loss: 0.04905420541763306\n",
      "Epoch: 40, Samples: 3136/5760, Loss: 0.006989032030105591\n",
      "Epoch: 40, Samples: 3168/5760, Loss: 0.018636107444763184\n",
      "Epoch: 40, Samples: 3200/5760, Loss: 0.014777928590774536\n",
      "Epoch: 40, Samples: 3232/5760, Loss: 0.01194104552268982\n",
      "Epoch: 40, Samples: 3264/5760, Loss: 0.013358518481254578\n",
      "Epoch: 40, Samples: 3296/5760, Loss: 0.00747913122177124\n",
      "Epoch: 40, Samples: 3328/5760, Loss: 0.006924808025360107\n",
      "Epoch: 40, Samples: 3360/5760, Loss: 0.01285967230796814\n",
      "Epoch: 40, Samples: 3392/5760, Loss: 0.01811039447784424\n",
      "Epoch: 40, Samples: 3424/5760, Loss: 0.010464280843734741\n",
      "Epoch: 40, Samples: 3456/5760, Loss: 0.012697219848632812\n",
      "Epoch: 40, Samples: 3488/5760, Loss: 0.01829671859741211\n",
      "Epoch: 40, Samples: 3520/5760, Loss: 0.008987456560134888\n",
      "Epoch: 40, Samples: 3552/5760, Loss: 0.010444939136505127\n",
      "Epoch: 40, Samples: 3584/5760, Loss: 0.0103091299533844\n",
      "Epoch: 40, Samples: 3616/5760, Loss: 0.013008296489715576\n",
      "Epoch: 40, Samples: 3648/5760, Loss: 0.008384346961975098\n",
      "Epoch: 40, Samples: 3680/5760, Loss: 0.011759966611862183\n",
      "Epoch: 40, Samples: 3712/5760, Loss: 0.00659981369972229\n",
      "Epoch: 40, Samples: 3744/5760, Loss: 0.010668694972991943\n",
      "Epoch: 40, Samples: 3776/5760, Loss: 0.013243615627288818\n",
      "Epoch: 40, Samples: 3808/5760, Loss: 0.008310049772262573\n",
      "Epoch: 40, Samples: 3840/5760, Loss: 0.012651234865188599\n",
      "Epoch: 40, Samples: 3872/5760, Loss: 0.006528228521347046\n",
      "Epoch: 40, Samples: 3904/5760, Loss: 0.008347123861312866\n",
      "Epoch: 40, Samples: 3936/5760, Loss: 0.008747011423110962\n",
      "Epoch: 40, Samples: 3968/5760, Loss: 0.01097559928894043\n",
      "Epoch: 40, Samples: 4000/5760, Loss: 0.00924748182296753\n",
      "Epoch: 40, Samples: 4032/5760, Loss: 0.01010856032371521\n",
      "Epoch: 40, Samples: 4064/5760, Loss: 0.005795776844024658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Samples: 4096/5760, Loss: 0.004630148410797119\n",
      "Epoch: 40, Samples: 4128/5760, Loss: 0.010566622018814087\n",
      "Epoch: 40, Samples: 4160/5760, Loss: 0.009544342756271362\n",
      "Epoch: 40, Samples: 4192/5760, Loss: 0.015451550483703613\n",
      "Epoch: 40, Samples: 4224/5760, Loss: 0.01703852415084839\n",
      "Epoch: 40, Samples: 4256/5760, Loss: 0.014285922050476074\n",
      "Epoch: 40, Samples: 4288/5760, Loss: 0.009115815162658691\n",
      "Epoch: 40, Samples: 4320/5760, Loss: 0.0084819495677948\n",
      "Epoch: 40, Samples: 4352/5760, Loss: 0.01052132248878479\n",
      "Epoch: 40, Samples: 4384/5760, Loss: 0.0109158456325531\n",
      "Epoch: 40, Samples: 4416/5760, Loss: 0.011982530355453491\n",
      "Epoch: 40, Samples: 4448/5760, Loss: 0.009908139705657959\n",
      "Epoch: 40, Samples: 4480/5760, Loss: 0.008927881717681885\n",
      "Epoch: 40, Samples: 4512/5760, Loss: 0.008977562189102173\n",
      "Epoch: 40, Samples: 4544/5760, Loss: 0.011930465698242188\n",
      "Epoch: 40, Samples: 4576/5760, Loss: 0.009505152702331543\n",
      "Epoch: 40, Samples: 4608/5760, Loss: 0.008004188537597656\n",
      "Epoch: 40, Samples: 4640/5760, Loss: 0.013716548681259155\n",
      "Epoch: 40, Samples: 4672/5760, Loss: 0.0311603844165802\n",
      "Epoch: 40, Samples: 4704/5760, Loss: 0.013881713151931763\n",
      "Epoch: 40, Samples: 4736/5760, Loss: 0.008901149034500122\n",
      "Epoch: 40, Samples: 4768/5760, Loss: 0.021365836262702942\n",
      "Epoch: 40, Samples: 4800/5760, Loss: 0.009596049785614014\n",
      "Epoch: 40, Samples: 4832/5760, Loss: 0.00830337405204773\n",
      "Epoch: 40, Samples: 4864/5760, Loss: 0.0063529908657073975\n",
      "Epoch: 40, Samples: 4896/5760, Loss: 0.00828579068183899\n",
      "Epoch: 40, Samples: 4928/5760, Loss: 0.018019288778305054\n",
      "Epoch: 40, Samples: 4960/5760, Loss: 0.009458869695663452\n",
      "Epoch: 40, Samples: 4992/5760, Loss: 0.008061468601226807\n",
      "Epoch: 40, Samples: 5024/5760, Loss: 0.014582544565200806\n",
      "Epoch: 40, Samples: 5056/5760, Loss: 0.007404237985610962\n",
      "Epoch: 40, Samples: 5088/5760, Loss: 0.009688794612884521\n",
      "Epoch: 40, Samples: 5120/5760, Loss: 0.008669883012771606\n",
      "Epoch: 40, Samples: 5152/5760, Loss: 0.00999489426612854\n",
      "Epoch: 40, Samples: 5184/5760, Loss: 0.013333916664123535\n",
      "Epoch: 40, Samples: 5216/5760, Loss: 0.010039806365966797\n",
      "Epoch: 40, Samples: 5248/5760, Loss: 0.010791987180709839\n",
      "Epoch: 40, Samples: 5280/5760, Loss: 0.009693413972854614\n",
      "Epoch: 40, Samples: 5312/5760, Loss: 0.01048782467842102\n",
      "Epoch: 40, Samples: 5344/5760, Loss: 0.010131299495697021\n",
      "Epoch: 40, Samples: 5376/5760, Loss: 0.006421923637390137\n",
      "Epoch: 40, Samples: 5408/5760, Loss: 0.010951519012451172\n",
      "Epoch: 40, Samples: 5440/5760, Loss: 0.00809091329574585\n",
      "Epoch: 40, Samples: 5472/5760, Loss: 0.00690072774887085\n",
      "Epoch: 40, Samples: 5504/5760, Loss: 0.00896388292312622\n",
      "Epoch: 40, Samples: 5536/5760, Loss: 0.016413867473602295\n",
      "Epoch: 40, Samples: 5568/5760, Loss: 0.010840088129043579\n",
      "Epoch: 40, Samples: 5600/5760, Loss: 0.019023984670639038\n",
      "Epoch: 40, Samples: 5632/5760, Loss: 0.013934433460235596\n",
      "Epoch: 40, Samples: 5664/5760, Loss: 0.009329050779342651\n",
      "Epoch: 40, Samples: 5696/5760, Loss: 0.012695074081420898\n",
      "Epoch: 40, Samples: 5728/5760, Loss: 1.0647164583206177\n",
      "\n",
      "Epoch: 40\n",
      "Training set: Average loss: 0.0169\n",
      "Validation set: Average loss: 0.2724, Accuracy: 767/818 (94%)\n",
      "Epoch: 41, Samples: 0/5760, Loss: 0.007414489984512329\n",
      "Epoch: 41, Samples: 32/5760, Loss: 0.009855568408966064\n",
      "Epoch: 41, Samples: 64/5760, Loss: 0.007829368114471436\n",
      "Epoch: 41, Samples: 96/5760, Loss: 0.008998841047286987\n",
      "Epoch: 41, Samples: 128/5760, Loss: 0.012047305703163147\n",
      "Epoch: 41, Samples: 160/5760, Loss: 0.012209177017211914\n",
      "Epoch: 41, Samples: 192/5760, Loss: 0.014874100685119629\n",
      "Epoch: 41, Samples: 224/5760, Loss: 0.011317789554595947\n",
      "Epoch: 41, Samples: 256/5760, Loss: 0.024169743061065674\n",
      "Epoch: 41, Samples: 288/5760, Loss: 0.017915859818458557\n",
      "Epoch: 41, Samples: 320/5760, Loss: 0.014955282211303711\n",
      "Epoch: 41, Samples: 352/5760, Loss: 0.010481834411621094\n",
      "Epoch: 41, Samples: 384/5760, Loss: 0.0147189199924469\n",
      "Epoch: 41, Samples: 416/5760, Loss: 0.014227420091629028\n",
      "Epoch: 41, Samples: 448/5760, Loss: 0.016186416149139404\n",
      "Epoch: 41, Samples: 480/5760, Loss: 0.007405579090118408\n",
      "Epoch: 41, Samples: 512/5760, Loss: 0.012292265892028809\n",
      "Epoch: 41, Samples: 544/5760, Loss: 0.013548851013183594\n",
      "Epoch: 41, Samples: 576/5760, Loss: 0.01234745979309082\n",
      "Epoch: 41, Samples: 608/5760, Loss: 0.026791930198669434\n",
      "Epoch: 41, Samples: 640/5760, Loss: 0.009637206792831421\n",
      "Epoch: 41, Samples: 672/5760, Loss: 0.012095630168914795\n",
      "Epoch: 41, Samples: 704/5760, Loss: 0.018643707036972046\n",
      "Epoch: 41, Samples: 736/5760, Loss: 0.0503784716129303\n",
      "Epoch: 41, Samples: 768/5760, Loss: 0.022219136357307434\n",
      "Epoch: 41, Samples: 800/5760, Loss: 0.013829797506332397\n",
      "Epoch: 41, Samples: 832/5760, Loss: 0.008676856756210327\n",
      "Epoch: 41, Samples: 864/5760, Loss: 0.0391889363527298\n",
      "Epoch: 41, Samples: 896/5760, Loss: 0.012297451496124268\n",
      "Epoch: 41, Samples: 928/5760, Loss: 0.02535538375377655\n",
      "Epoch: 41, Samples: 960/5760, Loss: 0.006036639213562012\n",
      "Epoch: 41, Samples: 992/5760, Loss: 0.005231589078903198\n",
      "Epoch: 41, Samples: 1024/5760, Loss: 0.01781642436981201\n",
      "Epoch: 41, Samples: 1056/5760, Loss: 0.010301977396011353\n",
      "Epoch: 41, Samples: 1088/5760, Loss: 0.014175176620483398\n",
      "Epoch: 41, Samples: 1120/5760, Loss: 0.00970393419265747\n",
      "Epoch: 41, Samples: 1152/5760, Loss: 0.013579994440078735\n",
      "Epoch: 41, Samples: 1184/5760, Loss: 0.050980716943740845\n",
      "Epoch: 41, Samples: 1216/5760, Loss: 0.011827826499938965\n",
      "Epoch: 41, Samples: 1248/5760, Loss: 0.015319466590881348\n",
      "Epoch: 41, Samples: 1280/5760, Loss: 0.008579134941101074\n",
      "Epoch: 41, Samples: 1312/5760, Loss: 0.011190563440322876\n",
      "Epoch: 41, Samples: 1344/5760, Loss: 0.022044837474822998\n",
      "Epoch: 41, Samples: 1376/5760, Loss: 0.03784447908401489\n",
      "Epoch: 41, Samples: 1408/5760, Loss: 0.009463787078857422\n",
      "Epoch: 41, Samples: 1440/5760, Loss: 0.011905401945114136\n",
      "Epoch: 41, Samples: 1472/5760, Loss: 0.006931990385055542\n",
      "Epoch: 41, Samples: 1504/5760, Loss: 0.0041304826736450195\n",
      "Epoch: 41, Samples: 1536/5760, Loss: 0.014879465103149414\n",
      "Epoch: 41, Samples: 1568/5760, Loss: 0.011287868022918701\n",
      "Epoch: 41, Samples: 1600/5760, Loss: 0.009954750537872314\n",
      "Epoch: 41, Samples: 1632/5760, Loss: 0.01234319806098938\n",
      "Epoch: 41, Samples: 1664/5760, Loss: 0.02089005708694458\n",
      "Epoch: 41, Samples: 1696/5760, Loss: 0.011382609605789185\n",
      "Epoch: 41, Samples: 1728/5760, Loss: 0.04978097975254059\n",
      "Epoch: 41, Samples: 1760/5760, Loss: 0.01851251721382141\n",
      "Epoch: 41, Samples: 1792/5760, Loss: 0.006703972816467285\n",
      "Epoch: 41, Samples: 1824/5760, Loss: 0.007315635681152344\n",
      "Epoch: 41, Samples: 1856/5760, Loss: 0.009347110986709595\n",
      "Epoch: 41, Samples: 1888/5760, Loss: 0.009703218936920166\n",
      "Epoch: 41, Samples: 1920/5760, Loss: 0.012220501899719238\n",
      "Epoch: 41, Samples: 1952/5760, Loss: 0.01622912287712097\n",
      "Epoch: 41, Samples: 1984/5760, Loss: 0.012796282768249512\n",
      "Epoch: 41, Samples: 2016/5760, Loss: 0.004732757806777954\n",
      "Epoch: 41, Samples: 2048/5760, Loss: 0.0168401300907135\n",
      "Epoch: 41, Samples: 2080/5760, Loss: 0.00999218225479126\n",
      "Epoch: 41, Samples: 2112/5760, Loss: 0.008301973342895508\n",
      "Epoch: 41, Samples: 2144/5760, Loss: 0.008220821619033813\n",
      "Epoch: 41, Samples: 2176/5760, Loss: 0.011277645826339722\n",
      "Epoch: 41, Samples: 2208/5760, Loss: 0.018589437007904053\n",
      "Epoch: 41, Samples: 2240/5760, Loss: 0.03553418815135956\n",
      "Epoch: 41, Samples: 2272/5760, Loss: 0.01002088189125061\n",
      "Epoch: 41, Samples: 2304/5760, Loss: 0.007613807916641235\n",
      "Epoch: 41, Samples: 2336/5760, Loss: 0.021352246403694153\n",
      "Epoch: 41, Samples: 2368/5760, Loss: 0.015276461839675903\n",
      "Epoch: 41, Samples: 2400/5760, Loss: 0.023632049560546875\n",
      "Epoch: 41, Samples: 2432/5760, Loss: 0.016393035650253296\n",
      "Epoch: 41, Samples: 2464/5760, Loss: 0.01100999116897583\n",
      "Epoch: 41, Samples: 2496/5760, Loss: 0.01320105791091919\n",
      "Epoch: 41, Samples: 2528/5760, Loss: 0.010495781898498535\n",
      "Epoch: 41, Samples: 2560/5760, Loss: 0.019891858100891113\n",
      "Epoch: 41, Samples: 2592/5760, Loss: 0.011025935411453247\n",
      "Epoch: 41, Samples: 2624/5760, Loss: 0.014748454093933105\n",
      "Epoch: 41, Samples: 2656/5760, Loss: 0.008498013019561768\n",
      "Epoch: 41, Samples: 2688/5760, Loss: 0.010383456945419312\n",
      "Epoch: 41, Samples: 2720/5760, Loss: 0.009294897317886353\n",
      "Epoch: 41, Samples: 2752/5760, Loss: 0.00935104489326477\n",
      "Epoch: 41, Samples: 2784/5760, Loss: 0.007194399833679199\n",
      "Epoch: 41, Samples: 2816/5760, Loss: 0.00866195559501648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Samples: 2848/5760, Loss: 0.009835034608840942\n",
      "Epoch: 41, Samples: 2880/5760, Loss: 0.010129421949386597\n",
      "Epoch: 41, Samples: 2912/5760, Loss: 0.007381260395050049\n",
      "Epoch: 41, Samples: 2944/5760, Loss: 0.008399099111557007\n",
      "Epoch: 41, Samples: 2976/5760, Loss: 0.008533358573913574\n",
      "Epoch: 41, Samples: 3008/5760, Loss: 0.009734511375427246\n",
      "Epoch: 41, Samples: 3040/5760, Loss: 0.00865665078163147\n",
      "Epoch: 41, Samples: 3072/5760, Loss: 0.014347851276397705\n",
      "Epoch: 41, Samples: 3104/5760, Loss: 0.008351683616638184\n",
      "Epoch: 41, Samples: 3136/5760, Loss: 0.008720695972442627\n",
      "Epoch: 41, Samples: 3168/5760, Loss: 0.01604306697845459\n",
      "Epoch: 41, Samples: 3200/5760, Loss: 0.019600212574005127\n",
      "Epoch: 41, Samples: 3232/5760, Loss: 0.01335170865058899\n",
      "Epoch: 41, Samples: 3264/5760, Loss: 0.006071001291275024\n",
      "Epoch: 41, Samples: 3296/5760, Loss: 0.02051568031311035\n",
      "Epoch: 41, Samples: 3328/5760, Loss: 0.011631011962890625\n",
      "Epoch: 41, Samples: 3360/5760, Loss: 0.014847427606582642\n",
      "Epoch: 41, Samples: 3392/5760, Loss: 0.010249465703964233\n",
      "Epoch: 41, Samples: 3424/5760, Loss: 0.008255869150161743\n",
      "Epoch: 41, Samples: 3456/5760, Loss: 0.00953325629234314\n",
      "Epoch: 41, Samples: 3488/5760, Loss: 0.012868344783782959\n",
      "Epoch: 41, Samples: 3520/5760, Loss: 0.013148963451385498\n",
      "Epoch: 41, Samples: 3552/5760, Loss: 0.01592901349067688\n",
      "Epoch: 41, Samples: 3584/5760, Loss: 0.009914308786392212\n",
      "Epoch: 41, Samples: 3616/5760, Loss: 0.013423919677734375\n",
      "Epoch: 41, Samples: 3648/5760, Loss: 0.01573556661605835\n",
      "Epoch: 41, Samples: 3680/5760, Loss: 0.012163728475570679\n",
      "Epoch: 41, Samples: 3712/5760, Loss: 0.007424890995025635\n",
      "Epoch: 41, Samples: 3744/5760, Loss: 0.018664270639419556\n",
      "Epoch: 41, Samples: 3776/5760, Loss: 0.005781412124633789\n",
      "Epoch: 41, Samples: 3808/5760, Loss: 0.019386887550354004\n",
      "Epoch: 41, Samples: 3840/5760, Loss: 0.008037567138671875\n",
      "Epoch: 41, Samples: 3872/5760, Loss: 0.014062255620956421\n",
      "Epoch: 41, Samples: 3904/5760, Loss: 0.011087268590927124\n",
      "Epoch: 41, Samples: 3936/5760, Loss: 0.009857326745986938\n",
      "Epoch: 41, Samples: 3968/5760, Loss: 0.01929253339767456\n",
      "Epoch: 41, Samples: 4000/5760, Loss: 0.01034274697303772\n",
      "Epoch: 41, Samples: 4032/5760, Loss: 0.012663722038269043\n",
      "Epoch: 41, Samples: 4064/5760, Loss: 0.014457732439041138\n",
      "Epoch: 41, Samples: 4096/5760, Loss: 0.003346681594848633\n",
      "Epoch: 41, Samples: 4128/5760, Loss: 0.013214200735092163\n",
      "Epoch: 41, Samples: 4160/5760, Loss: 0.008554607629776001\n",
      "Epoch: 41, Samples: 4192/5760, Loss: 0.013508707284927368\n",
      "Epoch: 41, Samples: 4224/5760, Loss: 0.006469786167144775\n",
      "Epoch: 41, Samples: 4256/5760, Loss: 0.008452028036117554\n",
      "Epoch: 41, Samples: 4288/5760, Loss: 0.008123189210891724\n",
      "Epoch: 41, Samples: 4320/5760, Loss: 0.009946495294570923\n",
      "Epoch: 41, Samples: 4352/5760, Loss: 0.015892595052719116\n",
      "Epoch: 41, Samples: 4384/5760, Loss: 0.00650438666343689\n",
      "Epoch: 41, Samples: 4416/5760, Loss: 0.009056836366653442\n",
      "Epoch: 41, Samples: 4448/5760, Loss: 0.004762321710586548\n",
      "Epoch: 41, Samples: 4480/5760, Loss: 0.014756500720977783\n",
      "Epoch: 41, Samples: 4512/5760, Loss: 0.010121464729309082\n",
      "Epoch: 41, Samples: 4544/5760, Loss: 0.008653461933135986\n",
      "Epoch: 41, Samples: 4576/5760, Loss: 0.02123752236366272\n",
      "Epoch: 41, Samples: 4608/5760, Loss: 0.003163933753967285\n",
      "Epoch: 41, Samples: 4640/5760, Loss: 0.017290890216827393\n",
      "Epoch: 41, Samples: 4672/5760, Loss: 0.0064544677734375\n",
      "Epoch: 41, Samples: 4704/5760, Loss: 0.007899999618530273\n",
      "Epoch: 41, Samples: 4736/5760, Loss: 0.013992726802825928\n",
      "Epoch: 41, Samples: 4768/5760, Loss: 0.009744524955749512\n",
      "Epoch: 41, Samples: 4800/5760, Loss: 0.009135127067565918\n",
      "Epoch: 41, Samples: 4832/5760, Loss: 0.01182103157043457\n",
      "Epoch: 41, Samples: 4864/5760, Loss: 0.011332035064697266\n",
      "Epoch: 41, Samples: 4896/5760, Loss: 0.015995681285858154\n",
      "Epoch: 41, Samples: 4928/5760, Loss: 0.015796750783920288\n",
      "Epoch: 41, Samples: 4960/5760, Loss: 0.008941680192947388\n",
      "Epoch: 41, Samples: 4992/5760, Loss: 0.009982317686080933\n",
      "Epoch: 41, Samples: 5024/5760, Loss: 0.025562196969985962\n",
      "Epoch: 41, Samples: 5056/5760, Loss: 0.010413169860839844\n",
      "Epoch: 41, Samples: 5088/5760, Loss: 0.010378420352935791\n",
      "Epoch: 41, Samples: 5120/5760, Loss: 0.005366712808609009\n",
      "Epoch: 41, Samples: 5152/5760, Loss: 0.018161892890930176\n",
      "Epoch: 41, Samples: 5184/5760, Loss: 0.008199185132980347\n",
      "Epoch: 41, Samples: 5216/5760, Loss: 0.009823828935623169\n",
      "Epoch: 41, Samples: 5248/5760, Loss: 0.005500435829162598\n",
      "Epoch: 41, Samples: 5280/5760, Loss: 0.010418474674224854\n",
      "Epoch: 41, Samples: 5312/5760, Loss: 0.01296427845954895\n",
      "Epoch: 41, Samples: 5344/5760, Loss: 0.007538467645645142\n",
      "Epoch: 41, Samples: 5376/5760, Loss: 0.008070200681686401\n",
      "Epoch: 41, Samples: 5408/5760, Loss: 0.009303271770477295\n",
      "Epoch: 41, Samples: 5440/5760, Loss: 0.009875386953353882\n",
      "Epoch: 41, Samples: 5472/5760, Loss: 0.008828938007354736\n",
      "Epoch: 41, Samples: 5504/5760, Loss: 0.01539069414138794\n",
      "Epoch: 41, Samples: 5536/5760, Loss: 0.00928419828414917\n",
      "Epoch: 41, Samples: 5568/5760, Loss: 0.007845133543014526\n",
      "Epoch: 41, Samples: 5600/5760, Loss: 0.006673723459243774\n",
      "Epoch: 41, Samples: 5632/5760, Loss: 0.01316922903060913\n",
      "Epoch: 41, Samples: 5664/5760, Loss: 0.013184159994125366\n",
      "Epoch: 41, Samples: 5696/5760, Loss: 0.012592613697052002\n",
      "Epoch: 41, Samples: 5728/5760, Loss: 0.09659624099731445\n",
      "\n",
      "Epoch: 41\n",
      "Training set: Average loss: 0.0135\n",
      "Validation set: Average loss: 0.2737, Accuracy: 769/818 (94%)\n",
      "Epoch: 42, Samples: 0/5760, Loss: 0.013749033212661743\n",
      "Epoch: 42, Samples: 32/5760, Loss: 0.009610295295715332\n",
      "Epoch: 42, Samples: 64/5760, Loss: 0.012155592441558838\n",
      "Epoch: 42, Samples: 96/5760, Loss: 0.010906338691711426\n",
      "Epoch: 42, Samples: 128/5760, Loss: 0.008149772882461548\n",
      "Epoch: 42, Samples: 160/5760, Loss: 0.016423523426055908\n",
      "Epoch: 42, Samples: 192/5760, Loss: 0.008563369512557983\n",
      "Epoch: 42, Samples: 224/5760, Loss: 0.01340186595916748\n",
      "Epoch: 42, Samples: 256/5760, Loss: 0.006707996129989624\n",
      "Epoch: 42, Samples: 288/5760, Loss: 0.009869128465652466\n",
      "Epoch: 42, Samples: 320/5760, Loss: 0.007956147193908691\n",
      "Epoch: 42, Samples: 352/5760, Loss: 0.011291444301605225\n",
      "Epoch: 42, Samples: 384/5760, Loss: 0.005668222904205322\n",
      "Epoch: 42, Samples: 416/5760, Loss: 0.01202249526977539\n",
      "Epoch: 42, Samples: 448/5760, Loss: 0.01803603768348694\n",
      "Epoch: 42, Samples: 480/5760, Loss: 0.007903456687927246\n",
      "Epoch: 42, Samples: 512/5760, Loss: 0.009230285882949829\n",
      "Epoch: 42, Samples: 544/5760, Loss: 0.009852588176727295\n",
      "Epoch: 42, Samples: 576/5760, Loss: 0.014751046895980835\n",
      "Epoch: 42, Samples: 608/5760, Loss: 0.010449737310409546\n",
      "Epoch: 42, Samples: 640/5760, Loss: 0.011608868837356567\n",
      "Epoch: 42, Samples: 672/5760, Loss: 0.010028094053268433\n",
      "Epoch: 42, Samples: 704/5760, Loss: 0.007205307483673096\n",
      "Epoch: 42, Samples: 736/5760, Loss: 0.005974858999252319\n",
      "Epoch: 42, Samples: 768/5760, Loss: 0.01044413447380066\n",
      "Epoch: 42, Samples: 800/5760, Loss: 0.007645010948181152\n",
      "Epoch: 42, Samples: 832/5760, Loss: 0.01192578673362732\n",
      "Epoch: 42, Samples: 864/5760, Loss: 0.018861591815948486\n",
      "Epoch: 42, Samples: 896/5760, Loss: 0.012160152196884155\n",
      "Epoch: 42, Samples: 928/5760, Loss: 0.010836303234100342\n",
      "Epoch: 42, Samples: 960/5760, Loss: 0.007267266511917114\n",
      "Epoch: 42, Samples: 992/5760, Loss: 0.01020592451095581\n",
      "Epoch: 42, Samples: 1024/5760, Loss: 0.022379696369171143\n",
      "Epoch: 42, Samples: 1056/5760, Loss: 0.008769780397415161\n",
      "Epoch: 42, Samples: 1088/5760, Loss: 0.008556991815567017\n",
      "Epoch: 42, Samples: 1120/5760, Loss: 0.014975160360336304\n",
      "Epoch: 42, Samples: 1152/5760, Loss: 0.010603874921798706\n",
      "Epoch: 42, Samples: 1184/5760, Loss: 0.00805547833442688\n",
      "Epoch: 42, Samples: 1216/5760, Loss: 0.010821878910064697\n",
      "Epoch: 42, Samples: 1248/5760, Loss: 0.005335718393325806\n",
      "Epoch: 42, Samples: 1280/5760, Loss: 0.010785073041915894\n",
      "Epoch: 42, Samples: 1312/5760, Loss: 0.008200109004974365\n",
      "Epoch: 42, Samples: 1344/5760, Loss: 0.006833404302597046\n",
      "Epoch: 42, Samples: 1376/5760, Loss: 0.010104984045028687\n",
      "Epoch: 42, Samples: 1408/5760, Loss: 0.010634392499923706\n",
      "Epoch: 42, Samples: 1440/5760, Loss: 0.012331008911132812\n",
      "Epoch: 42, Samples: 1472/5760, Loss: 0.00828436017036438\n",
      "Epoch: 42, Samples: 1504/5760, Loss: 0.012365758419036865\n",
      "Epoch: 42, Samples: 1536/5760, Loss: 0.007228553295135498\n",
      "Epoch: 42, Samples: 1568/5760, Loss: 0.007768154144287109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, Samples: 1600/5760, Loss: 0.01069071888923645\n",
      "Epoch: 42, Samples: 1632/5760, Loss: 0.005950808525085449\n",
      "Epoch: 42, Samples: 1664/5760, Loss: 0.012188822031021118\n",
      "Epoch: 42, Samples: 1696/5760, Loss: 0.013143062591552734\n",
      "Epoch: 42, Samples: 1728/5760, Loss: 0.012768000364303589\n",
      "Epoch: 42, Samples: 1760/5760, Loss: 0.011404335498809814\n",
      "Epoch: 42, Samples: 1792/5760, Loss: 0.006843924522399902\n",
      "Epoch: 42, Samples: 1824/5760, Loss: 0.006738424301147461\n",
      "Epoch: 42, Samples: 1856/5760, Loss: 0.010080277919769287\n",
      "Epoch: 42, Samples: 1888/5760, Loss: 0.008012861013412476\n",
      "Epoch: 42, Samples: 1920/5760, Loss: 0.008341073989868164\n",
      "Epoch: 42, Samples: 1952/5760, Loss: 0.015554696321487427\n",
      "Epoch: 42, Samples: 1984/5760, Loss: 0.012931287288665771\n",
      "Epoch: 42, Samples: 2016/5760, Loss: 0.004827946424484253\n",
      "Epoch: 42, Samples: 2048/5760, Loss: 0.0056514739990234375\n",
      "Epoch: 42, Samples: 2080/5760, Loss: 0.014437705278396606\n",
      "Epoch: 42, Samples: 2112/5760, Loss: 0.004337310791015625\n",
      "Epoch: 42, Samples: 2144/5760, Loss: 0.006510138511657715\n",
      "Epoch: 42, Samples: 2176/5760, Loss: 0.0073277950286865234\n",
      "Epoch: 42, Samples: 2208/5760, Loss: 0.0115567147731781\n",
      "Epoch: 42, Samples: 2240/5760, Loss: 0.010370641946792603\n",
      "Epoch: 42, Samples: 2272/5760, Loss: 0.0063555240631103516\n",
      "Epoch: 42, Samples: 2304/5760, Loss: 0.01835605502128601\n",
      "Epoch: 42, Samples: 2336/5760, Loss: 0.013899803161621094\n",
      "Epoch: 42, Samples: 2368/5760, Loss: 0.0113736093044281\n",
      "Epoch: 42, Samples: 2400/5760, Loss: 0.009305417537689209\n",
      "Epoch: 42, Samples: 2432/5760, Loss: 0.005870699882507324\n",
      "Epoch: 42, Samples: 2464/5760, Loss: 0.019522905349731445\n",
      "Epoch: 42, Samples: 2496/5760, Loss: 0.009571462869644165\n",
      "Epoch: 42, Samples: 2528/5760, Loss: 0.011819541454315186\n",
      "Epoch: 42, Samples: 2560/5760, Loss: 0.010009825229644775\n",
      "Epoch: 42, Samples: 2592/5760, Loss: 0.01091107726097107\n",
      "Epoch: 42, Samples: 2624/5760, Loss: 0.005929738283157349\n",
      "Epoch: 42, Samples: 2656/5760, Loss: 0.003990292549133301\n",
      "Epoch: 42, Samples: 2688/5760, Loss: 0.007697433233261108\n",
      "Epoch: 42, Samples: 2720/5760, Loss: 0.029064416885375977\n",
      "Epoch: 42, Samples: 2752/5760, Loss: 0.007112443447113037\n",
      "Epoch: 42, Samples: 2784/5760, Loss: 0.008845925331115723\n",
      "Epoch: 42, Samples: 2816/5760, Loss: 0.008405894041061401\n",
      "Epoch: 42, Samples: 2848/5760, Loss: 0.010158807039260864\n",
      "Epoch: 42, Samples: 2880/5760, Loss: 0.0084572434425354\n",
      "Epoch: 42, Samples: 2912/5760, Loss: 0.005652248859405518\n",
      "Epoch: 42, Samples: 2944/5760, Loss: 0.01845809817314148\n",
      "Epoch: 42, Samples: 2976/5760, Loss: 0.006428569555282593\n",
      "Epoch: 42, Samples: 3008/5760, Loss: 0.012119889259338379\n",
      "Epoch: 42, Samples: 3040/5760, Loss: 0.013071149587631226\n",
      "Epoch: 42, Samples: 3072/5760, Loss: 0.006919652223587036\n",
      "Epoch: 42, Samples: 3104/5760, Loss: 0.010580390691757202\n",
      "Epoch: 42, Samples: 3136/5760, Loss: 0.014131486415863037\n",
      "Epoch: 42, Samples: 3168/5760, Loss: 0.004260808229446411\n",
      "Epoch: 42, Samples: 3200/5760, Loss: 0.014913707971572876\n",
      "Epoch: 42, Samples: 3232/5760, Loss: 0.012176334857940674\n",
      "Epoch: 42, Samples: 3264/5760, Loss: 0.005993545055389404\n",
      "Epoch: 42, Samples: 3296/5760, Loss: 0.0075387656688690186\n",
      "Epoch: 42, Samples: 3328/5760, Loss: 0.004317581653594971\n",
      "Epoch: 42, Samples: 3360/5760, Loss: 0.006868153810501099\n",
      "Epoch: 42, Samples: 3392/5760, Loss: 0.009710878133773804\n",
      "Epoch: 42, Samples: 3424/5760, Loss: 0.02013319730758667\n",
      "Epoch: 42, Samples: 3456/5760, Loss: 0.006065577268600464\n",
      "Epoch: 42, Samples: 3488/5760, Loss: 0.008681118488311768\n",
      "Epoch: 42, Samples: 3520/5760, Loss: 0.0054069459438323975\n",
      "Epoch: 42, Samples: 3552/5760, Loss: 0.006082862615585327\n",
      "Epoch: 42, Samples: 3584/5760, Loss: 0.01240462064743042\n",
      "Epoch: 42, Samples: 3616/5760, Loss: 0.012671142816543579\n",
      "Epoch: 42, Samples: 3648/5760, Loss: 0.015466868877410889\n",
      "Epoch: 42, Samples: 3680/5760, Loss: 0.013901561498641968\n",
      "Epoch: 42, Samples: 3712/5760, Loss: 0.009947091341018677\n",
      "Epoch: 42, Samples: 3744/5760, Loss: 0.011403769254684448\n",
      "Epoch: 42, Samples: 3776/5760, Loss: 0.007538169622421265\n",
      "Epoch: 42, Samples: 3808/5760, Loss: 0.0038799643516540527\n",
      "Epoch: 42, Samples: 3840/5760, Loss: 0.012375891208648682\n",
      "Epoch: 42, Samples: 3872/5760, Loss: 0.014613687992095947\n",
      "Epoch: 42, Samples: 3904/5760, Loss: 0.01563483476638794\n",
      "Epoch: 42, Samples: 3936/5760, Loss: 0.007051408290863037\n",
      "Epoch: 42, Samples: 3968/5760, Loss: 0.013169854879379272\n",
      "Epoch: 42, Samples: 4000/5760, Loss: 0.010121852159500122\n",
      "Epoch: 42, Samples: 4032/5760, Loss: 0.013478726148605347\n",
      "Epoch: 42, Samples: 4064/5760, Loss: 0.008062660694122314\n",
      "Epoch: 42, Samples: 4096/5760, Loss: 0.010053634643554688\n",
      "Epoch: 42, Samples: 4128/5760, Loss: 0.007351696491241455\n",
      "Epoch: 42, Samples: 4160/5760, Loss: 0.010816216468811035\n",
      "Epoch: 42, Samples: 4192/5760, Loss: 0.014848500490188599\n",
      "Epoch: 42, Samples: 4224/5760, Loss: 0.014421075582504272\n",
      "Epoch: 42, Samples: 4256/5760, Loss: 0.004795938730239868\n",
      "Epoch: 42, Samples: 4288/5760, Loss: 0.007646381855010986\n",
      "Epoch: 42, Samples: 4320/5760, Loss: 0.008944511413574219\n",
      "Epoch: 42, Samples: 4352/5760, Loss: 0.005319356918334961\n",
      "Epoch: 42, Samples: 4384/5760, Loss: 0.010531693696975708\n",
      "Epoch: 42, Samples: 4416/5760, Loss: 0.009874820709228516\n",
      "Epoch: 42, Samples: 4448/5760, Loss: 0.014396637678146362\n",
      "Epoch: 42, Samples: 4480/5760, Loss: 0.00904873013496399\n",
      "Epoch: 42, Samples: 4512/5760, Loss: 0.011116564273834229\n",
      "Epoch: 42, Samples: 4544/5760, Loss: 0.012550503015518188\n",
      "Epoch: 42, Samples: 4576/5760, Loss: 0.004782229661941528\n",
      "Epoch: 42, Samples: 4608/5760, Loss: 0.008491665124893188\n",
      "Epoch: 42, Samples: 4640/5760, Loss: 0.02177482843399048\n",
      "Epoch: 42, Samples: 4672/5760, Loss: 0.008081406354904175\n",
      "Epoch: 42, Samples: 4704/5760, Loss: 0.009030520915985107\n",
      "Epoch: 42, Samples: 4736/5760, Loss: 0.05809532105922699\n",
      "Epoch: 42, Samples: 4768/5760, Loss: 0.005750119686126709\n",
      "Epoch: 42, Samples: 4800/5760, Loss: 0.009888619184494019\n",
      "Epoch: 42, Samples: 4832/5760, Loss: 0.005823582410812378\n",
      "Epoch: 42, Samples: 4864/5760, Loss: 0.0062284767627716064\n",
      "Epoch: 42, Samples: 4896/5760, Loss: 0.012973159551620483\n",
      "Epoch: 42, Samples: 4928/5760, Loss: 0.015300720930099487\n",
      "Epoch: 42, Samples: 4960/5760, Loss: 0.006363898515701294\n",
      "Epoch: 42, Samples: 4992/5760, Loss: 0.01327398419380188\n",
      "Epoch: 42, Samples: 5024/5760, Loss: 0.013030081987380981\n",
      "Epoch: 42, Samples: 5056/5760, Loss: 0.010235071182250977\n",
      "Epoch: 42, Samples: 5088/5760, Loss: 0.0057398974895477295\n",
      "Epoch: 42, Samples: 5120/5760, Loss: 0.009434342384338379\n",
      "Epoch: 42, Samples: 5152/5760, Loss: 0.012287944555282593\n",
      "Epoch: 42, Samples: 5184/5760, Loss: 0.007459163665771484\n",
      "Epoch: 42, Samples: 5216/5760, Loss: 0.014042466878890991\n",
      "Epoch: 42, Samples: 5248/5760, Loss: 0.008293896913528442\n",
      "Epoch: 42, Samples: 5280/5760, Loss: 0.0097026526927948\n",
      "Epoch: 42, Samples: 5312/5760, Loss: 0.006333261728286743\n",
      "Epoch: 42, Samples: 5344/5760, Loss: 0.01284724473953247\n",
      "Epoch: 42, Samples: 5376/5760, Loss: 0.00568428635597229\n",
      "Epoch: 42, Samples: 5408/5760, Loss: 0.008464574813842773\n",
      "Epoch: 42, Samples: 5440/5760, Loss: 0.010968834161758423\n",
      "Epoch: 42, Samples: 5472/5760, Loss: 0.006845355033874512\n",
      "Epoch: 42, Samples: 5504/5760, Loss: 0.012935549020767212\n",
      "Epoch: 42, Samples: 5536/5760, Loss: 0.010497748851776123\n",
      "Epoch: 42, Samples: 5568/5760, Loss: 0.013028338551521301\n",
      "Epoch: 42, Samples: 5600/5760, Loss: 0.005262523889541626\n",
      "Epoch: 42, Samples: 5632/5760, Loss: 0.00707593560218811\n",
      "Epoch: 42, Samples: 5664/5760, Loss: 0.014851599931716919\n",
      "Epoch: 42, Samples: 5696/5760, Loss: 0.005340456962585449\n",
      "Epoch: 42, Samples: 5728/5760, Loss: 1.566834568977356\n",
      "\n",
      "Epoch: 42\n",
      "Training set: Average loss: 0.0191\n",
      "Validation set: Average loss: 0.2883, Accuracy: 761/818 (93%)\n",
      "Epoch: 43, Samples: 0/5760, Loss: 0.016617566347122192\n",
      "Epoch: 43, Samples: 32/5760, Loss: 0.0056610107421875\n",
      "Epoch: 43, Samples: 64/5760, Loss: 0.008935093879699707\n",
      "Epoch: 43, Samples: 96/5760, Loss: 0.0172155499458313\n",
      "Epoch: 43, Samples: 128/5760, Loss: 0.009738355875015259\n",
      "Epoch: 43, Samples: 160/5760, Loss: 0.013503700494766235\n",
      "Epoch: 43, Samples: 192/5760, Loss: 0.011357486248016357\n",
      "Epoch: 43, Samples: 224/5760, Loss: 0.004777371883392334\n",
      "Epoch: 43, Samples: 256/5760, Loss: 0.009958386421203613\n",
      "Epoch: 43, Samples: 288/5760, Loss: 0.0034645497798919678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43, Samples: 320/5760, Loss: 0.008895635604858398\n",
      "Epoch: 43, Samples: 352/5760, Loss: 0.016331732273101807\n",
      "Epoch: 43, Samples: 384/5760, Loss: 0.012971699237823486\n",
      "Epoch: 43, Samples: 416/5760, Loss: 0.007447004318237305\n",
      "Epoch: 43, Samples: 448/5760, Loss: 0.01256754994392395\n",
      "Epoch: 43, Samples: 480/5760, Loss: 0.008886635303497314\n",
      "Epoch: 43, Samples: 512/5760, Loss: 0.006254404783248901\n",
      "Epoch: 43, Samples: 544/5760, Loss: 0.010507375001907349\n",
      "Epoch: 43, Samples: 576/5760, Loss: 0.010348409414291382\n",
      "Epoch: 43, Samples: 608/5760, Loss: 0.01455506682395935\n",
      "Epoch: 43, Samples: 640/5760, Loss: 0.00736844539642334\n",
      "Epoch: 43, Samples: 672/5760, Loss: 0.008081406354904175\n",
      "Epoch: 43, Samples: 704/5760, Loss: 0.02233988046646118\n",
      "Epoch: 43, Samples: 736/5760, Loss: 0.010726124048233032\n",
      "Epoch: 43, Samples: 768/5760, Loss: 0.00677904486656189\n",
      "Epoch: 43, Samples: 800/5760, Loss: 0.0064032673835754395\n",
      "Epoch: 43, Samples: 832/5760, Loss: 0.00859495997428894\n",
      "Epoch: 43, Samples: 864/5760, Loss: 0.018756777048110962\n",
      "Epoch: 43, Samples: 896/5760, Loss: 0.006803423166275024\n",
      "Epoch: 43, Samples: 928/5760, Loss: 0.008333265781402588\n",
      "Epoch: 43, Samples: 960/5760, Loss: 0.007931172847747803\n",
      "Epoch: 43, Samples: 992/5760, Loss: 0.01747104525566101\n",
      "Epoch: 43, Samples: 1024/5760, Loss: 0.014494091272354126\n",
      "Epoch: 43, Samples: 1056/5760, Loss: 0.010993331670761108\n",
      "Epoch: 43, Samples: 1088/5760, Loss: 0.00988161563873291\n",
      "Epoch: 43, Samples: 1120/5760, Loss: 0.004618167877197266\n",
      "Epoch: 43, Samples: 1152/5760, Loss: 0.014549314975738525\n",
      "Epoch: 43, Samples: 1184/5760, Loss: 0.006732076406478882\n",
      "Epoch: 43, Samples: 1216/5760, Loss: 0.018583357334136963\n",
      "Epoch: 43, Samples: 1248/5760, Loss: 0.012707531452178955\n",
      "Epoch: 43, Samples: 1280/5760, Loss: 0.015880227088928223\n",
      "Epoch: 43, Samples: 1312/5760, Loss: 0.007442742586135864\n",
      "Epoch: 43, Samples: 1344/5760, Loss: 0.01390334963798523\n",
      "Epoch: 43, Samples: 1376/5760, Loss: 0.02122431993484497\n",
      "Epoch: 43, Samples: 1408/5760, Loss: 0.006101042032241821\n",
      "Epoch: 43, Samples: 1440/5760, Loss: 0.008085459470748901\n",
      "Epoch: 43, Samples: 1472/5760, Loss: 0.014148026704788208\n",
      "Epoch: 43, Samples: 1504/5760, Loss: 0.007205963134765625\n",
      "Epoch: 43, Samples: 1536/5760, Loss: 0.009465545415878296\n",
      "Epoch: 43, Samples: 1568/5760, Loss: 0.012386053800582886\n",
      "Epoch: 43, Samples: 1600/5760, Loss: 0.006599843502044678\n",
      "Epoch: 43, Samples: 1632/5760, Loss: 0.019817247986793518\n",
      "Epoch: 43, Samples: 1664/5760, Loss: 0.007737487554550171\n",
      "Epoch: 43, Samples: 1696/5760, Loss: 0.015190303325653076\n",
      "Epoch: 43, Samples: 1728/5760, Loss: 0.007686972618103027\n",
      "Epoch: 43, Samples: 1760/5760, Loss: 0.005076736211776733\n",
      "Epoch: 43, Samples: 1792/5760, Loss: 0.018124982714653015\n",
      "Epoch: 43, Samples: 1824/5760, Loss: 0.020519733428955078\n",
      "Epoch: 43, Samples: 1856/5760, Loss: 0.011322230100631714\n",
      "Epoch: 43, Samples: 1888/5760, Loss: 0.015443503856658936\n",
      "Epoch: 43, Samples: 1920/5760, Loss: 0.00879397988319397\n",
      "Epoch: 43, Samples: 1952/5760, Loss: 0.012306541204452515\n",
      "Epoch: 43, Samples: 1984/5760, Loss: 0.0080641508102417\n",
      "Epoch: 43, Samples: 2016/5760, Loss: 0.012264221906661987\n",
      "Epoch: 43, Samples: 2048/5760, Loss: 0.006596386432647705\n",
      "Epoch: 43, Samples: 2080/5760, Loss: 0.007876962423324585\n",
      "Epoch: 43, Samples: 2112/5760, Loss: 0.0056247711181640625\n",
      "Epoch: 43, Samples: 2144/5760, Loss: 0.024550288915634155\n",
      "Epoch: 43, Samples: 2176/5760, Loss: 0.00957605242729187\n",
      "Epoch: 43, Samples: 2208/5760, Loss: 0.011110097169876099\n",
      "Epoch: 43, Samples: 2240/5760, Loss: 0.015206485986709595\n",
      "Epoch: 43, Samples: 2272/5760, Loss: 0.007215529680252075\n",
      "Epoch: 43, Samples: 2304/5760, Loss: 0.006059318780899048\n",
      "Epoch: 43, Samples: 2336/5760, Loss: 0.012591958045959473\n",
      "Epoch: 43, Samples: 2368/5760, Loss: 0.026338785886764526\n",
      "Epoch: 43, Samples: 2400/5760, Loss: 0.012310653924942017\n",
      "Epoch: 43, Samples: 2432/5760, Loss: 0.006467342376708984\n",
      "Epoch: 43, Samples: 2464/5760, Loss: 0.007917672395706177\n",
      "Epoch: 43, Samples: 2496/5760, Loss: 0.008965909481048584\n",
      "Epoch: 43, Samples: 2528/5760, Loss: 0.004435420036315918\n",
      "Epoch: 43, Samples: 2560/5760, Loss: 0.006224006414413452\n",
      "Epoch: 43, Samples: 2592/5760, Loss: 0.010604918003082275\n",
      "Epoch: 43, Samples: 2624/5760, Loss: 0.014809936285018921\n",
      "Epoch: 43, Samples: 2656/5760, Loss: 0.003818333148956299\n",
      "Epoch: 43, Samples: 2688/5760, Loss: 0.010845392942428589\n",
      "Epoch: 43, Samples: 2720/5760, Loss: 0.0060832202434539795\n",
      "Epoch: 43, Samples: 2752/5760, Loss: 0.01361316442489624\n",
      "Epoch: 43, Samples: 2784/5760, Loss: 0.010336577892303467\n",
      "Epoch: 43, Samples: 2816/5760, Loss: 0.009010761976242065\n",
      "Epoch: 43, Samples: 2848/5760, Loss: 0.00500139594078064\n",
      "Epoch: 43, Samples: 2880/5760, Loss: 0.006120145320892334\n",
      "Epoch: 43, Samples: 2912/5760, Loss: 0.004135936498641968\n",
      "Epoch: 43, Samples: 2944/5760, Loss: 0.005280584096908569\n",
      "Epoch: 43, Samples: 2976/5760, Loss: 0.010702162981033325\n",
      "Epoch: 43, Samples: 3008/5760, Loss: 0.00601959228515625\n",
      "Epoch: 43, Samples: 3040/5760, Loss: 0.00503876805305481\n",
      "Epoch: 43, Samples: 3072/5760, Loss: 0.013143450021743774\n",
      "Epoch: 43, Samples: 3104/5760, Loss: 0.012864738702774048\n",
      "Epoch: 43, Samples: 3136/5760, Loss: 0.00892937183380127\n",
      "Epoch: 43, Samples: 3168/5760, Loss: 0.01202896237373352\n",
      "Epoch: 43, Samples: 3200/5760, Loss: 0.009077668190002441\n",
      "Epoch: 43, Samples: 3232/5760, Loss: 0.011996477842330933\n",
      "Epoch: 43, Samples: 3264/5760, Loss: 0.008196353912353516\n",
      "Epoch: 43, Samples: 3296/5760, Loss: 0.011531442403793335\n",
      "Epoch: 43, Samples: 3328/5760, Loss: 0.008125066757202148\n",
      "Epoch: 43, Samples: 3360/5760, Loss: 0.011713981628417969\n",
      "Epoch: 43, Samples: 3392/5760, Loss: 0.00598675012588501\n",
      "Epoch: 43, Samples: 3424/5760, Loss: 0.011022806167602539\n",
      "Epoch: 43, Samples: 3456/5760, Loss: 0.008613437414169312\n",
      "Epoch: 43, Samples: 3488/5760, Loss: 0.006392478942871094\n",
      "Epoch: 43, Samples: 3520/5760, Loss: 0.011443734169006348\n",
      "Epoch: 43, Samples: 3552/5760, Loss: 0.009359270334243774\n",
      "Epoch: 43, Samples: 3584/5760, Loss: 0.011834859848022461\n",
      "Epoch: 43, Samples: 3616/5760, Loss: 0.00600704550743103\n",
      "Epoch: 43, Samples: 3648/5760, Loss: 0.005252212285995483\n",
      "Epoch: 43, Samples: 3680/5760, Loss: 0.0032238662242889404\n",
      "Epoch: 43, Samples: 3712/5760, Loss: 0.021297141909599304\n",
      "Epoch: 43, Samples: 3744/5760, Loss: 0.007344156503677368\n",
      "Epoch: 43, Samples: 3776/5760, Loss: 0.013542264699935913\n",
      "Epoch: 43, Samples: 3808/5760, Loss: 0.008238226175308228\n",
      "Epoch: 43, Samples: 3840/5760, Loss: 0.013221979141235352\n",
      "Epoch: 43, Samples: 3872/5760, Loss: 0.007753223180770874\n",
      "Epoch: 43, Samples: 3904/5760, Loss: 0.0057717859745025635\n",
      "Epoch: 43, Samples: 3936/5760, Loss: 0.008367776870727539\n",
      "Epoch: 43, Samples: 3968/5760, Loss: 0.011329889297485352\n",
      "Epoch: 43, Samples: 4000/5760, Loss: 0.007457613945007324\n",
      "Epoch: 43, Samples: 4032/5760, Loss: 0.012180358171463013\n",
      "Epoch: 43, Samples: 4064/5760, Loss: 0.008179515600204468\n",
      "Epoch: 43, Samples: 4096/5760, Loss: 0.00879889726638794\n",
      "Epoch: 43, Samples: 4128/5760, Loss: 0.012167960405349731\n",
      "Epoch: 43, Samples: 4160/5760, Loss: 0.0059202611446380615\n",
      "Epoch: 43, Samples: 4192/5760, Loss: 0.010508805513381958\n",
      "Epoch: 43, Samples: 4224/5760, Loss: 0.01663711667060852\n",
      "Epoch: 43, Samples: 4256/5760, Loss: 0.007692545652389526\n",
      "Epoch: 43, Samples: 4288/5760, Loss: 0.0068867504596710205\n",
      "Epoch: 43, Samples: 4320/5760, Loss: 0.008483320474624634\n",
      "Epoch: 43, Samples: 4352/5760, Loss: 0.011456966400146484\n",
      "Epoch: 43, Samples: 4384/5760, Loss: 0.006902188062667847\n",
      "Epoch: 43, Samples: 4416/5760, Loss: 0.004448622465133667\n",
      "Epoch: 43, Samples: 4448/5760, Loss: 0.009174704551696777\n",
      "Epoch: 43, Samples: 4480/5760, Loss: 0.007901608943939209\n",
      "Epoch: 43, Samples: 4512/5760, Loss: 0.008203715085983276\n",
      "Epoch: 43, Samples: 4544/5760, Loss: 0.017634451389312744\n",
      "Epoch: 43, Samples: 4576/5760, Loss: 0.008930861949920654\n",
      "Epoch: 43, Samples: 4608/5760, Loss: 0.011814802885055542\n",
      "Epoch: 43, Samples: 4640/5760, Loss: 0.010241895914077759\n",
      "Epoch: 43, Samples: 4672/5760, Loss: 0.00878402590751648\n",
      "Epoch: 43, Samples: 4704/5760, Loss: 0.00945371389389038\n",
      "Epoch: 43, Samples: 4736/5760, Loss: 0.01256859302520752\n",
      "Epoch: 43, Samples: 4768/5760, Loss: 0.015040427446365356\n",
      "Epoch: 43, Samples: 4800/5760, Loss: 0.007444292306900024\n",
      "Epoch: 43, Samples: 4832/5760, Loss: 0.008064806461334229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43, Samples: 4864/5760, Loss: 0.009187847375869751\n",
      "Epoch: 43, Samples: 4896/5760, Loss: 0.005302399396896362\n",
      "Epoch: 43, Samples: 4928/5760, Loss: 0.008731424808502197\n",
      "Epoch: 43, Samples: 4960/5760, Loss: 0.005993843078613281\n",
      "Epoch: 43, Samples: 4992/5760, Loss: 0.006059318780899048\n",
      "Epoch: 43, Samples: 5024/5760, Loss: 0.006086081266403198\n",
      "Epoch: 43, Samples: 5056/5760, Loss: 0.009699851274490356\n",
      "Epoch: 43, Samples: 5088/5760, Loss: 0.009552180767059326\n",
      "Epoch: 43, Samples: 5120/5760, Loss: 0.015100598335266113\n",
      "Epoch: 43, Samples: 5152/5760, Loss: 0.005390763282775879\n",
      "Epoch: 43, Samples: 5184/5760, Loss: 0.008357644081115723\n",
      "Epoch: 43, Samples: 5216/5760, Loss: 0.008555024862289429\n",
      "Epoch: 43, Samples: 5248/5760, Loss: 0.015109598636627197\n",
      "Epoch: 43, Samples: 5280/5760, Loss: 0.007832348346710205\n",
      "Epoch: 43, Samples: 5312/5760, Loss: 0.00497010350227356\n",
      "Epoch: 43, Samples: 5344/5760, Loss: 0.01380208134651184\n",
      "Epoch: 43, Samples: 5376/5760, Loss: 0.009223222732543945\n",
      "Epoch: 43, Samples: 5408/5760, Loss: 0.008673787117004395\n",
      "Epoch: 43, Samples: 5440/5760, Loss: 0.014374643564224243\n",
      "Epoch: 43, Samples: 5472/5760, Loss: 0.014576882123947144\n",
      "Epoch: 43, Samples: 5504/5760, Loss: 0.011527180671691895\n",
      "Epoch: 43, Samples: 5536/5760, Loss: 0.007277071475982666\n",
      "Epoch: 43, Samples: 5568/5760, Loss: 0.013792544603347778\n",
      "Epoch: 43, Samples: 5600/5760, Loss: 0.004854142665863037\n",
      "Epoch: 43, Samples: 5632/5760, Loss: 0.008247941732406616\n",
      "Epoch: 43, Samples: 5664/5760, Loss: 0.00717538595199585\n",
      "Epoch: 43, Samples: 5696/5760, Loss: 0.013787657022476196\n",
      "Epoch: 43, Samples: 5728/5760, Loss: 0.11374831199645996\n",
      "\n",
      "Epoch: 43\n",
      "Training set: Average loss: 0.0107\n",
      "Validation set: Average loss: 0.2609, Accuracy: 770/818 (94%)\n",
      "Saving model (epoch 43) with lowest validation loss: 0.26094281100309813\n",
      "Epoch: 44, Samples: 0/5760, Loss: 0.00670132040977478\n",
      "Epoch: 44, Samples: 32/5760, Loss: 0.0045629143714904785\n",
      "Epoch: 44, Samples: 64/5760, Loss: 0.0060835182666778564\n",
      "Epoch: 44, Samples: 96/5760, Loss: 0.005372792482376099\n",
      "Epoch: 44, Samples: 128/5760, Loss: 0.009548038244247437\n",
      "Epoch: 44, Samples: 160/5760, Loss: 0.009396016597747803\n",
      "Epoch: 44, Samples: 192/5760, Loss: 0.00935092568397522\n",
      "Epoch: 44, Samples: 224/5760, Loss: 0.004457354545593262\n",
      "Epoch: 44, Samples: 256/5760, Loss: 0.006861090660095215\n",
      "Epoch: 44, Samples: 288/5760, Loss: 0.00524333119392395\n",
      "Epoch: 44, Samples: 320/5760, Loss: 0.006025344133377075\n",
      "Epoch: 44, Samples: 352/5760, Loss: 0.01196715235710144\n",
      "Epoch: 44, Samples: 384/5760, Loss: 0.010493695735931396\n",
      "Epoch: 44, Samples: 416/5760, Loss: 0.007154792547225952\n",
      "Epoch: 44, Samples: 448/5760, Loss: 0.006599843502044678\n",
      "Epoch: 44, Samples: 480/5760, Loss: 0.01235613226890564\n",
      "Epoch: 44, Samples: 512/5760, Loss: 0.013593018054962158\n",
      "Epoch: 44, Samples: 544/5760, Loss: 0.012092173099517822\n",
      "Epoch: 44, Samples: 576/5760, Loss: 0.008581310510635376\n",
      "Epoch: 44, Samples: 608/5760, Loss: 0.014629721641540527\n",
      "Epoch: 44, Samples: 640/5760, Loss: 0.006850630044937134\n",
      "Epoch: 44, Samples: 672/5760, Loss: 0.006157189607620239\n",
      "Epoch: 44, Samples: 704/5760, Loss: 0.005381792783737183\n",
      "Epoch: 44, Samples: 736/5760, Loss: 0.010708779096603394\n",
      "Epoch: 44, Samples: 768/5760, Loss: 0.0057863593101501465\n",
      "Epoch: 44, Samples: 800/5760, Loss: 0.022122502326965332\n",
      "Epoch: 44, Samples: 832/5760, Loss: 0.00837641954421997\n",
      "Epoch: 44, Samples: 864/5760, Loss: 0.014500647783279419\n",
      "Epoch: 44, Samples: 896/5760, Loss: 0.005975812673568726\n",
      "Epoch: 44, Samples: 928/5760, Loss: 0.014559447765350342\n",
      "Epoch: 44, Samples: 960/5760, Loss: 0.012330740690231323\n",
      "Epoch: 44, Samples: 992/5760, Loss: 0.010413825511932373\n",
      "Epoch: 44, Samples: 1024/5760, Loss: 0.009109795093536377\n",
      "Epoch: 44, Samples: 1056/5760, Loss: 0.010790973901748657\n",
      "Epoch: 44, Samples: 1088/5760, Loss: 0.007195532321929932\n",
      "Epoch: 44, Samples: 1120/5760, Loss: 0.00890505313873291\n",
      "Epoch: 44, Samples: 1152/5760, Loss: 0.00926867127418518\n",
      "Epoch: 44, Samples: 1184/5760, Loss: 0.009411424398422241\n",
      "Epoch: 44, Samples: 1216/5760, Loss: 0.009801119565963745\n",
      "Epoch: 44, Samples: 1248/5760, Loss: 0.004790782928466797\n",
      "Epoch: 44, Samples: 1280/5760, Loss: 0.008579522371292114\n",
      "Epoch: 44, Samples: 1312/5760, Loss: 0.006684422492980957\n",
      "Epoch: 44, Samples: 1344/5760, Loss: 0.010725319385528564\n",
      "Epoch: 44, Samples: 1376/5760, Loss: 0.007977128028869629\n",
      "Epoch: 44, Samples: 1408/5760, Loss: 0.005785852670669556\n",
      "Epoch: 44, Samples: 1440/5760, Loss: 0.006720989942550659\n",
      "Epoch: 44, Samples: 1472/5760, Loss: 0.011869490146636963\n",
      "Epoch: 44, Samples: 1504/5760, Loss: 0.005737602710723877\n",
      "Epoch: 44, Samples: 1536/5760, Loss: 0.005763649940490723\n",
      "Epoch: 44, Samples: 1568/5760, Loss: 0.00376930832862854\n",
      "Epoch: 44, Samples: 1600/5760, Loss: 0.010942459106445312\n",
      "Epoch: 44, Samples: 1632/5760, Loss: 0.008668571710586548\n",
      "Epoch: 44, Samples: 1664/5760, Loss: 0.008175969123840332\n",
      "Epoch: 44, Samples: 1696/5760, Loss: 0.010329753160476685\n",
      "Epoch: 44, Samples: 1728/5760, Loss: 0.007932662963867188\n",
      "Epoch: 44, Samples: 1760/5760, Loss: 0.007167309522628784\n",
      "Epoch: 44, Samples: 1792/5760, Loss: 0.01533249020576477\n",
      "Epoch: 44, Samples: 1824/5760, Loss: 0.00625002384185791\n",
      "Epoch: 44, Samples: 1856/5760, Loss: 0.006820768117904663\n",
      "Epoch: 44, Samples: 1888/5760, Loss: 0.005780130624771118\n",
      "Epoch: 44, Samples: 1920/5760, Loss: 0.0038709938526153564\n",
      "Epoch: 44, Samples: 1952/5760, Loss: 0.013565421104431152\n",
      "Epoch: 44, Samples: 1984/5760, Loss: 0.009572237730026245\n",
      "Epoch: 44, Samples: 2016/5760, Loss: 0.005571901798248291\n",
      "Epoch: 44, Samples: 2048/5760, Loss: 0.005902230739593506\n",
      "Epoch: 44, Samples: 2080/5760, Loss: 0.013738662004470825\n",
      "Epoch: 44, Samples: 2112/5760, Loss: 0.007282823324203491\n",
      "Epoch: 44, Samples: 2144/5760, Loss: 0.008531630039215088\n",
      "Epoch: 44, Samples: 2176/5760, Loss: 0.005933314561843872\n",
      "Epoch: 44, Samples: 2208/5760, Loss: 0.005839258432388306\n",
      "Epoch: 44, Samples: 2240/5760, Loss: 0.028006792068481445\n",
      "Epoch: 44, Samples: 2272/5760, Loss: 0.006391793489456177\n",
      "Epoch: 44, Samples: 2304/5760, Loss: 0.01648491621017456\n",
      "Epoch: 44, Samples: 2336/5760, Loss: 0.013986572623252869\n",
      "Epoch: 44, Samples: 2368/5760, Loss: 0.011019974946975708\n",
      "Epoch: 44, Samples: 2400/5760, Loss: 0.009377628564834595\n",
      "Epoch: 44, Samples: 2432/5760, Loss: 0.00750809907913208\n",
      "Epoch: 44, Samples: 2464/5760, Loss: 0.007267117500305176\n",
      "Epoch: 44, Samples: 2496/5760, Loss: 0.008034348487854004\n",
      "Epoch: 44, Samples: 2528/5760, Loss: 0.008449465036392212\n",
      "Epoch: 44, Samples: 2560/5760, Loss: 0.006945937871932983\n",
      "Epoch: 44, Samples: 2592/5760, Loss: 0.00849074125289917\n",
      "Epoch: 44, Samples: 2624/5760, Loss: 0.0127391517162323\n",
      "Epoch: 44, Samples: 2656/5760, Loss: 0.011462777853012085\n",
      "Epoch: 44, Samples: 2688/5760, Loss: 0.01280289888381958\n",
      "Epoch: 44, Samples: 2720/5760, Loss: 0.006251931190490723\n",
      "Epoch: 44, Samples: 2752/5760, Loss: 0.004732251167297363\n",
      "Epoch: 44, Samples: 2784/5760, Loss: 0.010197728872299194\n",
      "Epoch: 44, Samples: 2816/5760, Loss: 0.008370816707611084\n",
      "Epoch: 44, Samples: 2848/5760, Loss: 0.008929133415222168\n",
      "Epoch: 44, Samples: 2880/5760, Loss: 0.006461441516876221\n",
      "Epoch: 44, Samples: 2912/5760, Loss: 0.01239997148513794\n",
      "Epoch: 44, Samples: 2944/5760, Loss: 0.007639825344085693\n",
      "Epoch: 44, Samples: 2976/5760, Loss: 0.010046064853668213\n",
      "Epoch: 44, Samples: 3008/5760, Loss: 0.013362377882003784\n",
      "Epoch: 44, Samples: 3040/5760, Loss: 0.0075356364250183105\n",
      "Epoch: 44, Samples: 3072/5760, Loss: 0.006037771701812744\n",
      "Epoch: 44, Samples: 3104/5760, Loss: 0.010037809610366821\n",
      "Epoch: 44, Samples: 3136/5760, Loss: 0.007090717554092407\n",
      "Epoch: 44, Samples: 3168/5760, Loss: 0.014056175947189331\n",
      "Epoch: 44, Samples: 3200/5760, Loss: 0.007027804851531982\n",
      "Epoch: 44, Samples: 3232/5760, Loss: 0.0035836994647979736\n",
      "Epoch: 44, Samples: 3264/5760, Loss: 0.013931989669799805\n",
      "Epoch: 44, Samples: 3296/5760, Loss: 0.00700417160987854\n",
      "Epoch: 44, Samples: 3328/5760, Loss: 0.010377734899520874\n",
      "Epoch: 44, Samples: 3360/5760, Loss: 0.006234914064407349\n",
      "Epoch: 44, Samples: 3392/5760, Loss: 0.011405855417251587\n",
      "Epoch: 44, Samples: 3424/5760, Loss: 0.005618751049041748\n",
      "Epoch: 44, Samples: 3456/5760, Loss: 0.021614104509353638\n",
      "Epoch: 44, Samples: 3488/5760, Loss: 0.005404353141784668\n",
      "Epoch: 44, Samples: 3520/5760, Loss: 0.007684290409088135\n",
      "Epoch: 44, Samples: 3552/5760, Loss: 0.005475282669067383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Samples: 3584/5760, Loss: 0.006415247917175293\n",
      "Epoch: 44, Samples: 3616/5760, Loss: 0.010636717081069946\n",
      "Epoch: 44, Samples: 3648/5760, Loss: 0.0077207982540130615\n",
      "Epoch: 44, Samples: 3680/5760, Loss: 0.006879299879074097\n",
      "Epoch: 44, Samples: 3712/5760, Loss: 0.0055262744426727295\n",
      "Epoch: 44, Samples: 3744/5760, Loss: 0.006699055433273315\n",
      "Epoch: 44, Samples: 3776/5760, Loss: 0.0072477757930755615\n",
      "Epoch: 44, Samples: 3808/5760, Loss: 0.005993306636810303\n",
      "Epoch: 44, Samples: 3840/5760, Loss: 0.00743710994720459\n",
      "Epoch: 44, Samples: 3872/5760, Loss: 0.013319283723831177\n",
      "Epoch: 44, Samples: 3904/5760, Loss: 0.0049447715282440186\n",
      "Epoch: 44, Samples: 3936/5760, Loss: 0.006214708089828491\n",
      "Epoch: 44, Samples: 3968/5760, Loss: 0.009510666131973267\n",
      "Epoch: 44, Samples: 4000/5760, Loss: 0.006969928741455078\n",
      "Epoch: 44, Samples: 4032/5760, Loss: 0.00894191861152649\n",
      "Epoch: 44, Samples: 4064/5760, Loss: 0.00905647873878479\n",
      "Epoch: 44, Samples: 4096/5760, Loss: 0.009061545133590698\n",
      "Epoch: 44, Samples: 4128/5760, Loss: 0.010487228631973267\n",
      "Epoch: 44, Samples: 4160/5760, Loss: 0.011914461851119995\n",
      "Epoch: 44, Samples: 4192/5760, Loss: 0.006776660680770874\n",
      "Epoch: 44, Samples: 4224/5760, Loss: 0.01198914647102356\n",
      "Epoch: 44, Samples: 4256/5760, Loss: 0.006828129291534424\n",
      "Epoch: 44, Samples: 4288/5760, Loss: 0.005177706480026245\n",
      "Epoch: 44, Samples: 4320/5760, Loss: 0.005902290344238281\n",
      "Epoch: 44, Samples: 4352/5760, Loss: 0.008721202611923218\n",
      "Epoch: 44, Samples: 4384/5760, Loss: 0.005775034427642822\n",
      "Epoch: 44, Samples: 4416/5760, Loss: 0.005933880805969238\n",
      "Epoch: 44, Samples: 4448/5760, Loss: 0.007915019989013672\n",
      "Epoch: 44, Samples: 4480/5760, Loss: 0.007042109966278076\n",
      "Epoch: 44, Samples: 4512/5760, Loss: 0.014571920037269592\n",
      "Epoch: 44, Samples: 4544/5760, Loss: 0.0161285400390625\n",
      "Epoch: 44, Samples: 4576/5760, Loss: 0.0049134790897369385\n",
      "Epoch: 44, Samples: 4608/5760, Loss: 0.015765488147735596\n",
      "Epoch: 44, Samples: 4640/5760, Loss: 0.007384538650512695\n",
      "Epoch: 44, Samples: 4672/5760, Loss: 0.0077957212924957275\n",
      "Epoch: 44, Samples: 4704/5760, Loss: 0.010315507650375366\n",
      "Epoch: 44, Samples: 4736/5760, Loss: 0.010490179061889648\n",
      "Epoch: 44, Samples: 4768/5760, Loss: 0.00480237603187561\n",
      "Epoch: 44, Samples: 4800/5760, Loss: 0.0037138164043426514\n",
      "Epoch: 44, Samples: 4832/5760, Loss: 0.008635878562927246\n",
      "Epoch: 44, Samples: 4864/5760, Loss: 0.007284790277481079\n",
      "Epoch: 44, Samples: 4896/5760, Loss: 0.0066995322704315186\n",
      "Epoch: 44, Samples: 4928/5760, Loss: 0.0049852728843688965\n",
      "Epoch: 44, Samples: 4960/5760, Loss: 0.010463684797286987\n",
      "Epoch: 44, Samples: 4992/5760, Loss: 0.004784613847732544\n",
      "Epoch: 44, Samples: 5024/5760, Loss: 0.0062316954135894775\n",
      "Epoch: 44, Samples: 5056/5760, Loss: 0.009143441915512085\n",
      "Epoch: 44, Samples: 5088/5760, Loss: 0.00934457778930664\n",
      "Epoch: 44, Samples: 5120/5760, Loss: 0.006901443004608154\n",
      "Epoch: 44, Samples: 5152/5760, Loss: 0.005605489015579224\n",
      "Epoch: 44, Samples: 5184/5760, Loss: 0.006101816892623901\n",
      "Epoch: 44, Samples: 5216/5760, Loss: 0.009407639503479004\n",
      "Epoch: 44, Samples: 5248/5760, Loss: 0.012995779514312744\n",
      "Epoch: 44, Samples: 5280/5760, Loss: 0.0066716670989990234\n",
      "Epoch: 44, Samples: 5312/5760, Loss: 0.007972806692123413\n",
      "Epoch: 44, Samples: 5344/5760, Loss: 0.007913768291473389\n",
      "Epoch: 44, Samples: 5376/5760, Loss: 0.006859928369522095\n",
      "Epoch: 44, Samples: 5408/5760, Loss: 0.006538063287734985\n",
      "Epoch: 44, Samples: 5440/5760, Loss: 0.007623732089996338\n",
      "Epoch: 44, Samples: 5472/5760, Loss: 0.007713735103607178\n",
      "Epoch: 44, Samples: 5504/5760, Loss: 0.006111949682235718\n",
      "Epoch: 44, Samples: 5536/5760, Loss: 0.010036855936050415\n",
      "Epoch: 44, Samples: 5568/5760, Loss: 0.010645091533660889\n",
      "Epoch: 44, Samples: 5600/5760, Loss: 0.003647059202194214\n",
      "Epoch: 44, Samples: 5632/5760, Loss: 0.007053196430206299\n",
      "Epoch: 44, Samples: 5664/5760, Loss: 0.01270022988319397\n",
      "Epoch: 44, Samples: 5696/5760, Loss: 0.007521867752075195\n",
      "Epoch: 44, Samples: 5728/5760, Loss: 0.47838830947875977\n",
      "\n",
      "Epoch: 44\n",
      "Training set: Average loss: 0.0113\n",
      "Validation set: Average loss: 0.2657, Accuracy: 766/818 (94%)\n",
      "Epoch: 45, Samples: 0/5760, Loss: 0.01070702075958252\n",
      "Epoch: 45, Samples: 32/5760, Loss: 0.0043489038944244385\n",
      "Epoch: 45, Samples: 64/5760, Loss: 0.007567375898361206\n",
      "Epoch: 45, Samples: 96/5760, Loss: 0.0037839412689208984\n",
      "Epoch: 45, Samples: 128/5760, Loss: 0.006385982036590576\n",
      "Epoch: 45, Samples: 160/5760, Loss: 0.004065334796905518\n",
      "Epoch: 45, Samples: 192/5760, Loss: 0.011105328798294067\n",
      "Epoch: 45, Samples: 224/5760, Loss: 0.010949313640594482\n",
      "Epoch: 45, Samples: 256/5760, Loss: 0.006649404764175415\n",
      "Epoch: 45, Samples: 288/5760, Loss: 0.0115395188331604\n",
      "Epoch: 45, Samples: 320/5760, Loss: 0.009618103504180908\n",
      "Epoch: 45, Samples: 352/5760, Loss: 0.020444095134735107\n",
      "Epoch: 45, Samples: 384/5760, Loss: 0.016233861446380615\n",
      "Epoch: 45, Samples: 416/5760, Loss: 0.007192283868789673\n",
      "Epoch: 45, Samples: 448/5760, Loss: 0.011028587818145752\n",
      "Epoch: 45, Samples: 480/5760, Loss: 0.01576167345046997\n",
      "Epoch: 45, Samples: 512/5760, Loss: 0.018287599086761475\n",
      "Epoch: 45, Samples: 544/5760, Loss: 0.004338085651397705\n",
      "Epoch: 45, Samples: 576/5760, Loss: 0.0093156099319458\n",
      "Epoch: 45, Samples: 608/5760, Loss: 0.006294339895248413\n",
      "Epoch: 45, Samples: 640/5760, Loss: 0.02107575535774231\n",
      "Epoch: 45, Samples: 672/5760, Loss: 0.0233689546585083\n",
      "Epoch: 45, Samples: 704/5760, Loss: 0.008060455322265625\n",
      "Epoch: 45, Samples: 736/5760, Loss: 0.006222963333129883\n",
      "Epoch: 45, Samples: 768/5760, Loss: 0.015722140669822693\n",
      "Epoch: 45, Samples: 800/5760, Loss: 0.008481591939926147\n",
      "Epoch: 45, Samples: 832/5760, Loss: 0.018009960651397705\n",
      "Epoch: 45, Samples: 864/5760, Loss: 0.008646398782730103\n",
      "Epoch: 45, Samples: 896/5760, Loss: 0.01778164505958557\n",
      "Epoch: 45, Samples: 928/5760, Loss: 0.008284330368041992\n",
      "Epoch: 45, Samples: 960/5760, Loss: 0.008456707000732422\n",
      "Epoch: 45, Samples: 992/5760, Loss: 0.006860524415969849\n",
      "Epoch: 45, Samples: 1024/5760, Loss: 0.010365396738052368\n",
      "Epoch: 45, Samples: 1056/5760, Loss: 0.006476163864135742\n",
      "Epoch: 45, Samples: 1088/5760, Loss: 0.011324793100357056\n",
      "Epoch: 45, Samples: 1120/5760, Loss: 0.01653677225112915\n",
      "Epoch: 45, Samples: 1152/5760, Loss: 0.011120051145553589\n",
      "Epoch: 45, Samples: 1184/5760, Loss: 0.011693567037582397\n",
      "Epoch: 45, Samples: 1216/5760, Loss: 0.01412111520767212\n",
      "Epoch: 45, Samples: 1248/5760, Loss: 0.01056581735610962\n",
      "Epoch: 45, Samples: 1280/5760, Loss: 0.009777814149856567\n",
      "Epoch: 45, Samples: 1312/5760, Loss: 0.003351837396621704\n",
      "Epoch: 45, Samples: 1344/5760, Loss: 0.01001933217048645\n",
      "Epoch: 45, Samples: 1376/5760, Loss: 0.004081815481185913\n",
      "Epoch: 45, Samples: 1408/5760, Loss: 0.008802562952041626\n",
      "Epoch: 45, Samples: 1440/5760, Loss: 0.008295714855194092\n",
      "Epoch: 45, Samples: 1472/5760, Loss: 0.006954938173294067\n",
      "Epoch: 45, Samples: 1504/5760, Loss: 0.008012861013412476\n",
      "Epoch: 45, Samples: 1536/5760, Loss: 0.0075528621673583984\n",
      "Epoch: 45, Samples: 1568/5760, Loss: 0.015109151601791382\n",
      "Epoch: 45, Samples: 1600/5760, Loss: 0.007492572069168091\n",
      "Epoch: 45, Samples: 1632/5760, Loss: 0.014099210500717163\n",
      "Epoch: 45, Samples: 1664/5760, Loss: 0.004363507032394409\n",
      "Epoch: 45, Samples: 1696/5760, Loss: 0.007022440433502197\n",
      "Epoch: 45, Samples: 1728/5760, Loss: 0.00606536865234375\n",
      "Epoch: 45, Samples: 1760/5760, Loss: 0.0119704008102417\n",
      "Epoch: 45, Samples: 1792/5760, Loss: 0.016027063131332397\n",
      "Epoch: 45, Samples: 1824/5760, Loss: 0.00614282488822937\n",
      "Epoch: 45, Samples: 1856/5760, Loss: 0.01364848017692566\n",
      "Epoch: 45, Samples: 1888/5760, Loss: 0.0087127685546875\n",
      "Epoch: 45, Samples: 1920/5760, Loss: 0.017773866653442383\n",
      "Epoch: 45, Samples: 1952/5760, Loss: 0.005216658115386963\n",
      "Epoch: 45, Samples: 1984/5760, Loss: 0.007559865713119507\n",
      "Epoch: 45, Samples: 2016/5760, Loss: 0.009454965591430664\n",
      "Epoch: 45, Samples: 2048/5760, Loss: 0.007482796907424927\n",
      "Epoch: 45, Samples: 2080/5760, Loss: 0.009638279676437378\n",
      "Epoch: 45, Samples: 2112/5760, Loss: 0.00820159912109375\n",
      "Epoch: 45, Samples: 2144/5760, Loss: 0.004631310701370239\n",
      "Epoch: 45, Samples: 2176/5760, Loss: 0.027960151433944702\n",
      "Epoch: 45, Samples: 2208/5760, Loss: 0.011652201414108276\n",
      "Epoch: 45, Samples: 2240/5760, Loss: 0.004821598529815674\n",
      "Epoch: 45, Samples: 2272/5760, Loss: 0.0054242610931396484\n",
      "Epoch: 45, Samples: 2304/5760, Loss: 0.00887078046798706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45, Samples: 2336/5760, Loss: 0.008191734552383423\n",
      "Epoch: 45, Samples: 2368/5760, Loss: 0.01076534390449524\n",
      "Epoch: 45, Samples: 2400/5760, Loss: 0.006118327379226685\n",
      "Epoch: 45, Samples: 2432/5760, Loss: 0.006506592035293579\n",
      "Epoch: 45, Samples: 2464/5760, Loss: 0.012061864137649536\n",
      "Epoch: 45, Samples: 2496/5760, Loss: 0.005670607089996338\n",
      "Epoch: 45, Samples: 2528/5760, Loss: 0.007739841938018799\n",
      "Epoch: 45, Samples: 2560/5760, Loss: 0.009804457426071167\n",
      "Epoch: 45, Samples: 2592/5760, Loss: 0.008015334606170654\n",
      "Epoch: 45, Samples: 2624/5760, Loss: 0.004834681749343872\n",
      "Epoch: 45, Samples: 2656/5760, Loss: 0.007418125867843628\n",
      "Epoch: 45, Samples: 2688/5760, Loss: 0.009115099906921387\n",
      "Epoch: 45, Samples: 2720/5760, Loss: 0.00754132866859436\n",
      "Epoch: 45, Samples: 2752/5760, Loss: 0.004002869129180908\n",
      "Epoch: 45, Samples: 2784/5760, Loss: 0.008238047361373901\n",
      "Epoch: 45, Samples: 2816/5760, Loss: 0.00452035665512085\n",
      "Epoch: 45, Samples: 2848/5760, Loss: 0.005842477083206177\n",
      "Epoch: 45, Samples: 2880/5760, Loss: 0.007488369941711426\n",
      "Epoch: 45, Samples: 2912/5760, Loss: 0.007851511240005493\n",
      "Epoch: 45, Samples: 2944/5760, Loss: 0.012188762426376343\n",
      "Epoch: 45, Samples: 2976/5760, Loss: 0.019076168537139893\n",
      "Epoch: 45, Samples: 3008/5760, Loss: 0.014923423528671265\n",
      "Epoch: 45, Samples: 3040/5760, Loss: 0.0076043009757995605\n",
      "Epoch: 45, Samples: 3072/5760, Loss: 0.009159177541732788\n",
      "Epoch: 45, Samples: 3104/5760, Loss: 0.006986677646636963\n",
      "Epoch: 45, Samples: 3136/5760, Loss: 0.008830100297927856\n",
      "Epoch: 45, Samples: 3168/5760, Loss: 0.01037532091140747\n",
      "Epoch: 45, Samples: 3200/5760, Loss: 0.009245038032531738\n",
      "Epoch: 45, Samples: 3232/5760, Loss: 0.006854444742202759\n",
      "Epoch: 45, Samples: 3264/5760, Loss: 0.009149521589279175\n",
      "Epoch: 45, Samples: 3296/5760, Loss: 0.013152211904525757\n",
      "Epoch: 45, Samples: 3328/5760, Loss: 0.007229655981063843\n",
      "Epoch: 45, Samples: 3360/5760, Loss: 0.009398013353347778\n",
      "Epoch: 45, Samples: 3392/5760, Loss: 0.008938521146774292\n",
      "Epoch: 45, Samples: 3424/5760, Loss: 0.0077582597732543945\n",
      "Epoch: 45, Samples: 3456/5760, Loss: 0.007550269365310669\n",
      "Epoch: 45, Samples: 3488/5760, Loss: 0.0067490339279174805\n",
      "Epoch: 45, Samples: 3520/5760, Loss: 0.009740501642227173\n",
      "Epoch: 45, Samples: 3552/5760, Loss: 0.008458167314529419\n",
      "Epoch: 45, Samples: 3584/5760, Loss: 0.011831402778625488\n",
      "Epoch: 45, Samples: 3616/5760, Loss: 0.010330915451049805\n",
      "Epoch: 45, Samples: 3648/5760, Loss: 0.01137632131576538\n",
      "Epoch: 45, Samples: 3680/5760, Loss: 0.005200892686843872\n",
      "Epoch: 45, Samples: 3712/5760, Loss: 0.01658664643764496\n",
      "Epoch: 45, Samples: 3744/5760, Loss: 0.00797969102859497\n",
      "Epoch: 45, Samples: 3776/5760, Loss: 0.008353948593139648\n",
      "Epoch: 45, Samples: 3808/5760, Loss: 0.01110348105430603\n",
      "Epoch: 45, Samples: 3840/5760, Loss: 0.01124921441078186\n",
      "Epoch: 45, Samples: 3872/5760, Loss: 0.010387808084487915\n",
      "Epoch: 45, Samples: 3904/5760, Loss: 0.013801634311676025\n",
      "Epoch: 45, Samples: 3936/5760, Loss: 0.0085010826587677\n",
      "Epoch: 45, Samples: 3968/5760, Loss: 0.0063988566398620605\n",
      "Epoch: 45, Samples: 4000/5760, Loss: 0.020925015211105347\n",
      "Epoch: 45, Samples: 4032/5760, Loss: 0.0064723193645477295\n",
      "Epoch: 45, Samples: 4064/5760, Loss: 0.008834779262542725\n",
      "Epoch: 45, Samples: 4096/5760, Loss: 0.007002502679824829\n",
      "Epoch: 45, Samples: 4128/5760, Loss: 0.006851881742477417\n",
      "Epoch: 45, Samples: 4160/5760, Loss: 0.005064815282821655\n",
      "Epoch: 45, Samples: 4192/5760, Loss: 0.009759664535522461\n",
      "Epoch: 45, Samples: 4224/5760, Loss: 0.006875157356262207\n",
      "Epoch: 45, Samples: 4256/5760, Loss: 0.016465365886688232\n",
      "Epoch: 45, Samples: 4288/5760, Loss: 0.011755615472793579\n",
      "Epoch: 45, Samples: 4320/5760, Loss: 0.005986928939819336\n",
      "Epoch: 45, Samples: 4352/5760, Loss: 0.01206880807876587\n",
      "Epoch: 45, Samples: 4384/5760, Loss: 0.008519500494003296\n",
      "Epoch: 45, Samples: 4416/5760, Loss: 0.007487267255783081\n",
      "Epoch: 45, Samples: 4448/5760, Loss: 0.008504480123519897\n",
      "Epoch: 45, Samples: 4480/5760, Loss: 0.012696653604507446\n",
      "Epoch: 45, Samples: 4512/5760, Loss: 0.013098210096359253\n",
      "Epoch: 45, Samples: 4544/5760, Loss: 0.003661215305328369\n",
      "Epoch: 45, Samples: 4576/5760, Loss: 0.005964338779449463\n",
      "Epoch: 45, Samples: 4608/5760, Loss: 0.012378469109535217\n",
      "Epoch: 45, Samples: 4640/5760, Loss: 0.0052960216999053955\n",
      "Epoch: 45, Samples: 4672/5760, Loss: 0.008620381355285645\n",
      "Epoch: 45, Samples: 4704/5760, Loss: 0.009424328804016113\n",
      "Epoch: 45, Samples: 4736/5760, Loss: 0.0037587881088256836\n",
      "Epoch: 45, Samples: 4768/5760, Loss: 0.008700162172317505\n",
      "Epoch: 45, Samples: 4800/5760, Loss: 0.011718660593032837\n",
      "Epoch: 45, Samples: 4832/5760, Loss: 0.014563366770744324\n",
      "Epoch: 45, Samples: 4864/5760, Loss: 0.006340205669403076\n",
      "Epoch: 45, Samples: 4896/5760, Loss: 0.004520446062088013\n",
      "Epoch: 45, Samples: 4928/5760, Loss: 0.014190226793289185\n",
      "Epoch: 45, Samples: 4960/5760, Loss: 0.010305911302566528\n",
      "Epoch: 45, Samples: 4992/5760, Loss: 0.006937175989151001\n",
      "Epoch: 45, Samples: 5024/5760, Loss: 0.007189154624938965\n",
      "Epoch: 45, Samples: 5056/5760, Loss: 0.005740046501159668\n",
      "Epoch: 45, Samples: 5088/5760, Loss: 0.013305366039276123\n",
      "Epoch: 45, Samples: 5120/5760, Loss: 0.005375415086746216\n",
      "Epoch: 45, Samples: 5152/5760, Loss: 0.035669222474098206\n",
      "Epoch: 45, Samples: 5184/5760, Loss: 0.008245468139648438\n",
      "Epoch: 45, Samples: 5216/5760, Loss: 0.008655130863189697\n",
      "Epoch: 45, Samples: 5248/5760, Loss: 0.005538225173950195\n",
      "Epoch: 45, Samples: 5280/5760, Loss: 0.009719759225845337\n",
      "Epoch: 45, Samples: 5312/5760, Loss: 0.006462305784225464\n",
      "Epoch: 45, Samples: 5344/5760, Loss: 0.008400529623031616\n",
      "Epoch: 45, Samples: 5376/5760, Loss: 0.005017518997192383\n",
      "Epoch: 45, Samples: 5408/5760, Loss: 0.0044197142124176025\n",
      "Epoch: 45, Samples: 5440/5760, Loss: 0.005305171012878418\n",
      "Epoch: 45, Samples: 5472/5760, Loss: 0.007507026195526123\n",
      "Epoch: 45, Samples: 5504/5760, Loss: 0.00949639081954956\n",
      "Epoch: 45, Samples: 5536/5760, Loss: 0.015708893537521362\n",
      "Epoch: 45, Samples: 5568/5760, Loss: 0.009281754493713379\n",
      "Epoch: 45, Samples: 5600/5760, Loss: 0.006080031394958496\n",
      "Epoch: 45, Samples: 5632/5760, Loss: 0.009470969438552856\n",
      "Epoch: 45, Samples: 5664/5760, Loss: 0.0078025758266448975\n",
      "Epoch: 45, Samples: 5696/5760, Loss: 0.026780754327774048\n",
      "Epoch: 45, Samples: 5728/5760, Loss: 0.8127912282943726\n",
      "\n",
      "Epoch: 45\n",
      "Training set: Average loss: 0.0141\n",
      "Validation set: Average loss: 0.2718, Accuracy: 766/818 (94%)\n",
      "Epoch: 46, Samples: 0/5760, Loss: 0.012375801801681519\n",
      "Epoch: 46, Samples: 32/5760, Loss: 0.01097339391708374\n",
      "Epoch: 46, Samples: 64/5760, Loss: 0.007196366786956787\n",
      "Epoch: 46, Samples: 96/5760, Loss: 0.020948171615600586\n",
      "Epoch: 46, Samples: 128/5760, Loss: 0.010658413171768188\n",
      "Epoch: 46, Samples: 160/5760, Loss: 0.0063758790493011475\n",
      "Epoch: 46, Samples: 192/5760, Loss: 0.006887167692184448\n",
      "Epoch: 46, Samples: 224/5760, Loss: 0.008044213056564331\n",
      "Epoch: 46, Samples: 256/5760, Loss: 0.009853363037109375\n",
      "Epoch: 46, Samples: 288/5760, Loss: 0.0134049654006958\n",
      "Epoch: 46, Samples: 320/5760, Loss: 0.012988507747650146\n",
      "Epoch: 46, Samples: 352/5760, Loss: 0.009006738662719727\n",
      "Epoch: 46, Samples: 384/5760, Loss: 0.010392695665359497\n",
      "Epoch: 46, Samples: 416/5760, Loss: 0.0075747668743133545\n",
      "Epoch: 46, Samples: 448/5760, Loss: 0.008943378925323486\n",
      "Epoch: 46, Samples: 480/5760, Loss: 0.007085025310516357\n",
      "Epoch: 46, Samples: 512/5760, Loss: 0.00549045205116272\n",
      "Epoch: 46, Samples: 544/5760, Loss: 0.009834587574005127\n",
      "Epoch: 46, Samples: 576/5760, Loss: 0.005294919013977051\n",
      "Epoch: 46, Samples: 608/5760, Loss: 0.0074787139892578125\n",
      "Epoch: 46, Samples: 640/5760, Loss: 0.008265465497970581\n",
      "Epoch: 46, Samples: 672/5760, Loss: 0.008977323770523071\n",
      "Epoch: 46, Samples: 704/5760, Loss: 0.007086306810379028\n",
      "Epoch: 46, Samples: 736/5760, Loss: 0.01881396770477295\n",
      "Epoch: 46, Samples: 768/5760, Loss: 0.007689416408538818\n",
      "Epoch: 46, Samples: 800/5760, Loss: 0.009465396404266357\n",
      "Epoch: 46, Samples: 832/5760, Loss: 0.013088643550872803\n",
      "Epoch: 46, Samples: 864/5760, Loss: 0.0066301822662353516\n",
      "Epoch: 46, Samples: 896/5760, Loss: 0.0062986016273498535\n",
      "Epoch: 46, Samples: 928/5760, Loss: 0.007660001516342163\n",
      "Epoch: 46, Samples: 960/5760, Loss: 0.009082436561584473\n",
      "Epoch: 46, Samples: 992/5760, Loss: 0.0069710612297058105\n",
      "Epoch: 46, Samples: 1024/5760, Loss: 0.0067262351512908936\n",
      "Epoch: 46, Samples: 1056/5760, Loss: 0.027858301997184753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Samples: 1088/5760, Loss: 0.005261242389678955\n",
      "Epoch: 46, Samples: 1120/5760, Loss: 0.009209215641021729\n",
      "Epoch: 46, Samples: 1152/5760, Loss: 0.009106040000915527\n",
      "Epoch: 46, Samples: 1184/5760, Loss: 0.012512266635894775\n",
      "Epoch: 46, Samples: 1216/5760, Loss: 0.01787668466567993\n",
      "Epoch: 46, Samples: 1248/5760, Loss: 0.00744318962097168\n",
      "Epoch: 46, Samples: 1280/5760, Loss: 0.00499996542930603\n",
      "Epoch: 46, Samples: 1312/5760, Loss: 0.005899012088775635\n",
      "Epoch: 46, Samples: 1344/5760, Loss: 0.009559184312820435\n",
      "Epoch: 46, Samples: 1376/5760, Loss: 0.011307328939437866\n",
      "Epoch: 46, Samples: 1408/5760, Loss: 0.016657888889312744\n",
      "Epoch: 46, Samples: 1440/5760, Loss: 0.02142941951751709\n",
      "Epoch: 46, Samples: 1472/5760, Loss: 0.012982159852981567\n",
      "Epoch: 46, Samples: 1504/5760, Loss: 0.008884221315383911\n",
      "Epoch: 46, Samples: 1536/5760, Loss: 0.005676060914993286\n",
      "Epoch: 46, Samples: 1568/5760, Loss: 0.008099108934402466\n",
      "Epoch: 46, Samples: 1600/5760, Loss: 0.006492942571640015\n",
      "Epoch: 46, Samples: 1632/5760, Loss: 0.0048818886280059814\n",
      "Epoch: 46, Samples: 1664/5760, Loss: 0.008052200078964233\n",
      "Epoch: 46, Samples: 1696/5760, Loss: 0.006086766719818115\n",
      "Epoch: 46, Samples: 1728/5760, Loss: 0.006970524787902832\n",
      "Epoch: 46, Samples: 1760/5760, Loss: 0.009315520524978638\n",
      "Epoch: 46, Samples: 1792/5760, Loss: 0.012975215911865234\n",
      "Epoch: 46, Samples: 1824/5760, Loss: 0.005471140146255493\n",
      "Epoch: 46, Samples: 1856/5760, Loss: 0.00952717661857605\n",
      "Epoch: 46, Samples: 1888/5760, Loss: 0.005688011646270752\n",
      "Epoch: 46, Samples: 1920/5760, Loss: 0.008194208145141602\n",
      "Epoch: 46, Samples: 1952/5760, Loss: 0.007260501384735107\n",
      "Epoch: 46, Samples: 1984/5760, Loss: 0.008588582277297974\n",
      "Epoch: 46, Samples: 2016/5760, Loss: 0.008467227220535278\n",
      "Epoch: 46, Samples: 2048/5760, Loss: 0.01475110650062561\n",
      "Epoch: 46, Samples: 2080/5760, Loss: 0.007826805114746094\n",
      "Epoch: 46, Samples: 2112/5760, Loss: 0.011607319116592407\n",
      "Epoch: 46, Samples: 2144/5760, Loss: 0.004704833030700684\n",
      "Epoch: 46, Samples: 2176/5760, Loss: 0.010794460773468018\n",
      "Epoch: 46, Samples: 2208/5760, Loss: 0.006493210792541504\n",
      "Epoch: 46, Samples: 2240/5760, Loss: 0.006460845470428467\n",
      "Epoch: 46, Samples: 2272/5760, Loss: 0.006774485111236572\n",
      "Epoch: 46, Samples: 2304/5760, Loss: 0.013625934720039368\n",
      "Epoch: 46, Samples: 2336/5760, Loss: 0.010833978652954102\n",
      "Epoch: 46, Samples: 2368/5760, Loss: 0.005979359149932861\n",
      "Epoch: 46, Samples: 2400/5760, Loss: 0.004772752523422241\n",
      "Epoch: 46, Samples: 2432/5760, Loss: 0.008471786975860596\n",
      "Epoch: 46, Samples: 2464/5760, Loss: 0.0083637535572052\n",
      "Epoch: 46, Samples: 2496/5760, Loss: 0.020944744348526\n",
      "Epoch: 46, Samples: 2528/5760, Loss: 0.011871248483657837\n",
      "Epoch: 46, Samples: 2560/5760, Loss: 0.015430063009262085\n",
      "Epoch: 46, Samples: 2592/5760, Loss: 0.007940739393234253\n",
      "Epoch: 46, Samples: 2624/5760, Loss: 0.00940793752670288\n",
      "Epoch: 46, Samples: 2656/5760, Loss: 0.01113671064376831\n",
      "Epoch: 46, Samples: 2688/5760, Loss: 0.004462867975234985\n",
      "Epoch: 46, Samples: 2720/5760, Loss: 0.0064133405685424805\n",
      "Epoch: 46, Samples: 2752/5760, Loss: 0.007697165012359619\n",
      "Epoch: 46, Samples: 2784/5760, Loss: 0.007680147886276245\n",
      "Epoch: 46, Samples: 2816/5760, Loss: 0.007287919521331787\n",
      "Epoch: 46, Samples: 2848/5760, Loss: 0.006115943193435669\n",
      "Epoch: 46, Samples: 2880/5760, Loss: 0.009882926940917969\n",
      "Epoch: 46, Samples: 2912/5760, Loss: 0.00427207350730896\n",
      "Epoch: 46, Samples: 2944/5760, Loss: 0.005938977003097534\n",
      "Epoch: 46, Samples: 2976/5760, Loss: 0.037209928035736084\n",
      "Epoch: 46, Samples: 3008/5760, Loss: 0.005595654249191284\n",
      "Epoch: 46, Samples: 3040/5760, Loss: 0.005045562982559204\n",
      "Epoch: 46, Samples: 3072/5760, Loss: 0.01016506552696228\n",
      "Epoch: 46, Samples: 3104/5760, Loss: 0.011302709579467773\n",
      "Epoch: 46, Samples: 3136/5760, Loss: 0.005458265542984009\n",
      "Epoch: 46, Samples: 3168/5760, Loss: 0.0038556456565856934\n",
      "Epoch: 46, Samples: 3200/5760, Loss: 0.012441664934158325\n",
      "Epoch: 46, Samples: 3232/5760, Loss: 0.018734723329544067\n",
      "Epoch: 46, Samples: 3264/5760, Loss: 0.005784213542938232\n",
      "Epoch: 46, Samples: 3296/5760, Loss: 0.009688615798950195\n",
      "Epoch: 46, Samples: 3328/5760, Loss: 0.008818447589874268\n",
      "Epoch: 46, Samples: 3360/5760, Loss: 0.005079537630081177\n",
      "Epoch: 46, Samples: 3392/5760, Loss: 0.010678291320800781\n",
      "Epoch: 46, Samples: 3424/5760, Loss: 0.005859375\n",
      "Epoch: 46, Samples: 3456/5760, Loss: 0.006528019905090332\n",
      "Epoch: 46, Samples: 3488/5760, Loss: 0.005071699619293213\n",
      "Epoch: 46, Samples: 3520/5760, Loss: 0.016018301248550415\n",
      "Epoch: 46, Samples: 3552/5760, Loss: 0.007181286811828613\n",
      "Epoch: 46, Samples: 3584/5760, Loss: 0.012192338705062866\n",
      "Epoch: 46, Samples: 3616/5760, Loss: 0.003812819719314575\n",
      "Epoch: 46, Samples: 3648/5760, Loss: 0.007739543914794922\n",
      "Epoch: 46, Samples: 3680/5760, Loss: 0.0062119364738464355\n",
      "Epoch: 46, Samples: 3712/5760, Loss: 0.008011490106582642\n",
      "Epoch: 46, Samples: 3744/5760, Loss: 0.009967893362045288\n",
      "Epoch: 46, Samples: 3776/5760, Loss: 0.006980091333389282\n",
      "Epoch: 46, Samples: 3808/5760, Loss: 0.007735550403594971\n",
      "Epoch: 46, Samples: 3840/5760, Loss: 0.00979924201965332\n",
      "Epoch: 46, Samples: 3872/5760, Loss: 0.01014205813407898\n",
      "Epoch: 46, Samples: 3904/5760, Loss: 0.009324759244918823\n",
      "Epoch: 46, Samples: 3936/5760, Loss: 0.005963534116744995\n",
      "Epoch: 46, Samples: 3968/5760, Loss: 0.005651533603668213\n",
      "Epoch: 46, Samples: 4000/5760, Loss: 0.0039092302322387695\n",
      "Epoch: 46, Samples: 4032/5760, Loss: 0.008786559104919434\n",
      "Epoch: 46, Samples: 4064/5760, Loss: 0.00621989369392395\n",
      "Epoch: 46, Samples: 4096/5760, Loss: 0.007004499435424805\n",
      "Epoch: 46, Samples: 4128/5760, Loss: 0.004551023244857788\n",
      "Epoch: 46, Samples: 4160/5760, Loss: 0.006267189979553223\n",
      "Epoch: 46, Samples: 4192/5760, Loss: 0.00604286789894104\n",
      "Epoch: 46, Samples: 4224/5760, Loss: 0.009269863367080688\n",
      "Epoch: 46, Samples: 4256/5760, Loss: 0.0061401426792144775\n",
      "Epoch: 46, Samples: 4288/5760, Loss: 0.011703908443450928\n",
      "Epoch: 46, Samples: 4320/5760, Loss: 0.0052772462368011475\n",
      "Epoch: 46, Samples: 4352/5760, Loss: 0.011148184537887573\n",
      "Epoch: 46, Samples: 4384/5760, Loss: 0.007898479700088501\n",
      "Epoch: 46, Samples: 4416/5760, Loss: 0.007614403963088989\n",
      "Epoch: 46, Samples: 4448/5760, Loss: 0.007381021976470947\n",
      "Epoch: 46, Samples: 4480/5760, Loss: 0.008130133152008057\n",
      "Epoch: 46, Samples: 4512/5760, Loss: 0.006039649248123169\n",
      "Epoch: 46, Samples: 4544/5760, Loss: 0.012768566608428955\n",
      "Epoch: 46, Samples: 4576/5760, Loss: 0.004226595163345337\n",
      "Epoch: 46, Samples: 4608/5760, Loss: 0.01987457275390625\n",
      "Epoch: 46, Samples: 4640/5760, Loss: 0.018035054206848145\n",
      "Epoch: 46, Samples: 4672/5760, Loss: 0.005072534084320068\n",
      "Epoch: 46, Samples: 4704/5760, Loss: 0.010588020086288452\n",
      "Epoch: 46, Samples: 4736/5760, Loss: 0.005585432052612305\n",
      "Epoch: 46, Samples: 4768/5760, Loss: 0.017347276210784912\n",
      "Epoch: 46, Samples: 4800/5760, Loss: 0.011098116636276245\n",
      "Epoch: 46, Samples: 4832/5760, Loss: 0.009128600358963013\n",
      "Epoch: 46, Samples: 4864/5760, Loss: 0.006436347961425781\n",
      "Epoch: 46, Samples: 4896/5760, Loss: 0.0071007609367370605\n",
      "Epoch: 46, Samples: 4928/5760, Loss: 0.005357205867767334\n",
      "Epoch: 46, Samples: 4960/5760, Loss: 0.0034793317317962646\n",
      "Epoch: 46, Samples: 4992/5760, Loss: 0.0067442357540130615\n",
      "Epoch: 46, Samples: 5024/5760, Loss: 0.010140776634216309\n",
      "Epoch: 46, Samples: 5056/5760, Loss: 0.010839223861694336\n",
      "Epoch: 46, Samples: 5088/5760, Loss: 0.02515891194343567\n",
      "Epoch: 46, Samples: 5120/5760, Loss: 0.009702563285827637\n",
      "Epoch: 46, Samples: 5152/5760, Loss: 0.005097627639770508\n",
      "Epoch: 46, Samples: 5184/5760, Loss: 0.008948445320129395\n",
      "Epoch: 46, Samples: 5216/5760, Loss: 0.006168246269226074\n",
      "Epoch: 46, Samples: 5248/5760, Loss: 0.012803882360458374\n",
      "Epoch: 46, Samples: 5280/5760, Loss: 0.00798255205154419\n",
      "Epoch: 46, Samples: 5312/5760, Loss: 0.012039124965667725\n",
      "Epoch: 46, Samples: 5344/5760, Loss: 0.005790263414382935\n",
      "Epoch: 46, Samples: 5376/5760, Loss: 0.008280038833618164\n",
      "Epoch: 46, Samples: 5408/5760, Loss: 0.010123193264007568\n",
      "Epoch: 46, Samples: 5440/5760, Loss: 0.008110612630844116\n",
      "Epoch: 46, Samples: 5472/5760, Loss: 0.019766241312026978\n",
      "Epoch: 46, Samples: 5504/5760, Loss: 0.007380068302154541\n",
      "Epoch: 46, Samples: 5536/5760, Loss: 0.007336676120758057\n",
      "Epoch: 46, Samples: 5568/5760, Loss: 0.008949130773544312\n",
      "Epoch: 46, Samples: 5600/5760, Loss: 0.005755811929702759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Samples: 5632/5760, Loss: 0.011112689971923828\n",
      "Epoch: 46, Samples: 5664/5760, Loss: 0.0061522722244262695\n",
      "Epoch: 46, Samples: 5696/5760, Loss: 0.0051419734954833984\n",
      "Epoch: 46, Samples: 5728/5760, Loss: 0.8440858125686646\n",
      "\n",
      "Epoch: 46\n",
      "Training set: Average loss: 0.0138\n",
      "Validation set: Average loss: 0.2722, Accuracy: 763/818 (93%)\n",
      "Epoch: 47, Samples: 0/5760, Loss: 0.006483316421508789\n",
      "Epoch: 47, Samples: 32/5760, Loss: 0.006003618240356445\n",
      "Epoch: 47, Samples: 64/5760, Loss: 0.023862898349761963\n",
      "Epoch: 47, Samples: 96/5760, Loss: 0.008703142404556274\n",
      "Epoch: 47, Samples: 128/5760, Loss: 0.010204315185546875\n",
      "Epoch: 47, Samples: 160/5760, Loss: 0.004775106906890869\n",
      "Epoch: 47, Samples: 192/5760, Loss: 0.006645530462265015\n",
      "Epoch: 47, Samples: 224/5760, Loss: 0.009424656629562378\n",
      "Epoch: 47, Samples: 256/5760, Loss: 0.007643282413482666\n",
      "Epoch: 47, Samples: 288/5760, Loss: 0.010960906744003296\n",
      "Epoch: 47, Samples: 320/5760, Loss: 0.02146124839782715\n",
      "Epoch: 47, Samples: 352/5760, Loss: 0.005500972270965576\n",
      "Epoch: 47, Samples: 384/5760, Loss: 0.018056929111480713\n",
      "Epoch: 47, Samples: 416/5760, Loss: 0.008962273597717285\n",
      "Epoch: 47, Samples: 448/5760, Loss: 0.01542094349861145\n",
      "Epoch: 47, Samples: 480/5760, Loss: 0.006886661052703857\n",
      "Epoch: 47, Samples: 512/5760, Loss: 0.00886690616607666\n",
      "Epoch: 47, Samples: 544/5760, Loss: 0.014560878276824951\n",
      "Epoch: 47, Samples: 576/5760, Loss: 0.01386713981628418\n",
      "Epoch: 47, Samples: 608/5760, Loss: 0.016194313764572144\n",
      "Epoch: 47, Samples: 640/5760, Loss: 0.0057891905307769775\n",
      "Epoch: 47, Samples: 672/5760, Loss: 0.008820533752441406\n",
      "Epoch: 47, Samples: 704/5760, Loss: 0.008935779333114624\n",
      "Epoch: 47, Samples: 736/5760, Loss: 0.04135619103908539\n",
      "Epoch: 47, Samples: 768/5760, Loss: 0.01431611180305481\n",
      "Epoch: 47, Samples: 800/5760, Loss: 0.011506527662277222\n",
      "Epoch: 47, Samples: 832/5760, Loss: 0.0033303797245025635\n",
      "Epoch: 47, Samples: 864/5760, Loss: 0.009545087814331055\n",
      "Epoch: 47, Samples: 896/5760, Loss: 0.00985214114189148\n",
      "Epoch: 47, Samples: 928/5760, Loss: 0.018154382705688477\n",
      "Epoch: 47, Samples: 960/5760, Loss: 0.00951296091079712\n",
      "Epoch: 47, Samples: 992/5760, Loss: 0.0116482675075531\n",
      "Epoch: 47, Samples: 1024/5760, Loss: 0.006023883819580078\n",
      "Epoch: 47, Samples: 1056/5760, Loss: 0.008543312549591064\n",
      "Epoch: 47, Samples: 1088/5760, Loss: 0.007163941860198975\n",
      "Epoch: 47, Samples: 1120/5760, Loss: 0.00938183069229126\n",
      "Epoch: 47, Samples: 1152/5760, Loss: 0.01116710901260376\n",
      "Epoch: 47, Samples: 1184/5760, Loss: 0.008517712354660034\n",
      "Epoch: 47, Samples: 1216/5760, Loss: 0.0050393640995025635\n",
      "Epoch: 47, Samples: 1248/5760, Loss: 0.013992756605148315\n",
      "Epoch: 47, Samples: 1280/5760, Loss: 0.010118216276168823\n",
      "Epoch: 47, Samples: 1312/5760, Loss: 0.006439298391342163\n",
      "Epoch: 47, Samples: 1344/5760, Loss: 0.006322592496871948\n",
      "Epoch: 47, Samples: 1376/5760, Loss: 0.008776068687438965\n",
      "Epoch: 47, Samples: 1408/5760, Loss: 0.008901000022888184\n",
      "Epoch: 47, Samples: 1440/5760, Loss: 0.012677252292633057\n",
      "Epoch: 47, Samples: 1472/5760, Loss: 0.007141768932342529\n",
      "Epoch: 47, Samples: 1504/5760, Loss: 0.03127807378768921\n",
      "Epoch: 47, Samples: 1536/5760, Loss: 0.010005772113800049\n",
      "Epoch: 47, Samples: 1568/5760, Loss: 0.011359035968780518\n",
      "Epoch: 47, Samples: 1600/5760, Loss: 0.006472587585449219\n",
      "Epoch: 47, Samples: 1632/5760, Loss: 0.011440485715866089\n",
      "Epoch: 47, Samples: 1664/5760, Loss: 0.010688960552215576\n",
      "Epoch: 47, Samples: 1696/5760, Loss: 0.01857924461364746\n",
      "Epoch: 47, Samples: 1728/5760, Loss: 0.00475001335144043\n",
      "Epoch: 47, Samples: 1760/5760, Loss: 0.11911728978157043\n",
      "Epoch: 47, Samples: 1792/5760, Loss: 0.006568014621734619\n",
      "Epoch: 47, Samples: 1824/5760, Loss: 0.022702977061271667\n",
      "Epoch: 47, Samples: 1856/5760, Loss: 0.006960272789001465\n",
      "Epoch: 47, Samples: 1888/5760, Loss: 0.006229758262634277\n",
      "Epoch: 47, Samples: 1920/5760, Loss: 0.008023619651794434\n",
      "Epoch: 47, Samples: 1952/5760, Loss: 0.009044021368026733\n",
      "Epoch: 47, Samples: 1984/5760, Loss: 0.012689739465713501\n",
      "Epoch: 47, Samples: 2016/5760, Loss: 0.007534205913543701\n",
      "Epoch: 47, Samples: 2048/5760, Loss: 0.007741153240203857\n",
      "Epoch: 47, Samples: 2080/5760, Loss: 0.009105324745178223\n",
      "Epoch: 47, Samples: 2112/5760, Loss: 0.015162885189056396\n",
      "Epoch: 47, Samples: 2144/5760, Loss: 0.00888112187385559\n",
      "Epoch: 47, Samples: 2176/5760, Loss: 0.015804260969161987\n",
      "Epoch: 47, Samples: 2208/5760, Loss: 0.010449051856994629\n",
      "Epoch: 47, Samples: 2240/5760, Loss: 0.004799157381057739\n",
      "Epoch: 47, Samples: 2272/5760, Loss: 0.016057491302490234\n",
      "Epoch: 47, Samples: 2304/5760, Loss: 0.010600954294204712\n",
      "Epoch: 47, Samples: 2336/5760, Loss: 0.008663594722747803\n",
      "Epoch: 47, Samples: 2368/5760, Loss: 0.008895576000213623\n",
      "Epoch: 47, Samples: 2400/5760, Loss: 0.012415409088134766\n",
      "Epoch: 47, Samples: 2432/5760, Loss: 0.007251381874084473\n",
      "Epoch: 47, Samples: 2464/5760, Loss: 0.006477475166320801\n",
      "Epoch: 47, Samples: 2496/5760, Loss: 0.006769955158233643\n",
      "Epoch: 47, Samples: 2528/5760, Loss: 0.012047559022903442\n",
      "Epoch: 47, Samples: 2560/5760, Loss: 0.01127314567565918\n",
      "Epoch: 47, Samples: 2592/5760, Loss: 0.008229285478591919\n",
      "Epoch: 47, Samples: 2624/5760, Loss: 0.01198124885559082\n",
      "Epoch: 47, Samples: 2656/5760, Loss: 0.00681498646736145\n",
      "Epoch: 47, Samples: 2688/5760, Loss: 0.005138486623764038\n",
      "Epoch: 47, Samples: 2720/5760, Loss: 0.03000412881374359\n",
      "Epoch: 47, Samples: 2752/5760, Loss: 0.0058189332485198975\n",
      "Epoch: 47, Samples: 2784/5760, Loss: 0.006269484758377075\n",
      "Epoch: 47, Samples: 2816/5760, Loss: 0.008241206407546997\n",
      "Epoch: 47, Samples: 2848/5760, Loss: 0.00894099473953247\n",
      "Epoch: 47, Samples: 2880/5760, Loss: 0.004117727279663086\n",
      "Epoch: 47, Samples: 2912/5760, Loss: 0.014497995376586914\n",
      "Epoch: 47, Samples: 2944/5760, Loss: 0.012946337461471558\n",
      "Epoch: 47, Samples: 2976/5760, Loss: 0.00926673412322998\n",
      "Epoch: 47, Samples: 3008/5760, Loss: 0.01236996054649353\n",
      "Epoch: 47, Samples: 3040/5760, Loss: 0.00879707932472229\n",
      "Epoch: 47, Samples: 3072/5760, Loss: 0.01185944676399231\n",
      "Epoch: 47, Samples: 3104/5760, Loss: 0.008945673704147339\n",
      "Epoch: 47, Samples: 3136/5760, Loss: 0.01457640528678894\n",
      "Epoch: 47, Samples: 3168/5760, Loss: 0.013749778270721436\n",
      "Epoch: 47, Samples: 3200/5760, Loss: 0.005225062370300293\n",
      "Epoch: 47, Samples: 3232/5760, Loss: 0.00969916582107544\n",
      "Epoch: 47, Samples: 3264/5760, Loss: 0.0033192336559295654\n",
      "Epoch: 47, Samples: 3296/5760, Loss: 0.011109501123428345\n",
      "Epoch: 47, Samples: 3328/5760, Loss: 0.005850702524185181\n",
      "Epoch: 47, Samples: 3360/5760, Loss: 0.014524787664413452\n",
      "Epoch: 47, Samples: 3392/5760, Loss: 0.01049312949180603\n",
      "Epoch: 47, Samples: 3424/5760, Loss: 0.017600014805793762\n",
      "Epoch: 47, Samples: 3456/5760, Loss: 0.013464361429214478\n",
      "Epoch: 47, Samples: 3488/5760, Loss: 0.005573064088821411\n",
      "Epoch: 47, Samples: 3520/5760, Loss: 0.015782713890075684\n",
      "Epoch: 47, Samples: 3552/5760, Loss: 0.008064448833465576\n",
      "Epoch: 47, Samples: 3584/5760, Loss: 0.012448310852050781\n",
      "Epoch: 47, Samples: 3616/5760, Loss: 0.003918766975402832\n",
      "Epoch: 47, Samples: 3648/5760, Loss: 0.011379599571228027\n",
      "Epoch: 47, Samples: 3680/5760, Loss: 0.0075928568840026855\n",
      "Epoch: 47, Samples: 3712/5760, Loss: 0.0073368847370147705\n",
      "Epoch: 47, Samples: 3744/5760, Loss: 0.007162541151046753\n",
      "Epoch: 47, Samples: 3776/5760, Loss: 0.006507575511932373\n",
      "Epoch: 47, Samples: 3808/5760, Loss: 0.0063399821519851685\n",
      "Epoch: 47, Samples: 3840/5760, Loss: 0.006187081336975098\n",
      "Epoch: 47, Samples: 3872/5760, Loss: 0.005193084478378296\n",
      "Epoch: 47, Samples: 3904/5760, Loss: 0.009218871593475342\n",
      "Epoch: 47, Samples: 3936/5760, Loss: 0.012257874011993408\n",
      "Epoch: 47, Samples: 3968/5760, Loss: 0.014839619398117065\n",
      "Epoch: 47, Samples: 4000/5760, Loss: 0.013696461915969849\n",
      "Epoch: 47, Samples: 4032/5760, Loss: 0.006292074918746948\n",
      "Epoch: 47, Samples: 4064/5760, Loss: 0.0052138566970825195\n",
      "Epoch: 47, Samples: 4096/5760, Loss: 0.014848023653030396\n",
      "Epoch: 47, Samples: 4128/5760, Loss: 0.011739641427993774\n",
      "Epoch: 47, Samples: 4160/5760, Loss: 0.007607609033584595\n",
      "Epoch: 47, Samples: 4192/5760, Loss: 0.004705518484115601\n",
      "Epoch: 47, Samples: 4224/5760, Loss: 0.006650298833847046\n",
      "Epoch: 47, Samples: 4256/5760, Loss: 0.007051914930343628\n",
      "Epoch: 47, Samples: 4288/5760, Loss: 0.015845060348510742\n",
      "Epoch: 47, Samples: 4320/5760, Loss: 0.008853167295455933\n",
      "Epoch: 47, Samples: 4352/5760, Loss: 0.007960736751556396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Samples: 4384/5760, Loss: 0.01276978850364685\n",
      "Epoch: 47, Samples: 4416/5760, Loss: 0.007819056510925293\n",
      "Epoch: 47, Samples: 4448/5760, Loss: 0.0050885677337646484\n",
      "Epoch: 47, Samples: 4480/5760, Loss: 0.011619657278060913\n",
      "Epoch: 47, Samples: 4512/5760, Loss: 0.006091952323913574\n",
      "Epoch: 47, Samples: 4544/5760, Loss: 0.007312595844268799\n",
      "Epoch: 47, Samples: 4576/5760, Loss: 0.014245778322219849\n",
      "Epoch: 47, Samples: 4608/5760, Loss: 0.0059507787227630615\n",
      "Epoch: 47, Samples: 4640/5760, Loss: 0.004423558712005615\n",
      "Epoch: 47, Samples: 4672/5760, Loss: 0.007710754871368408\n",
      "Epoch: 47, Samples: 4704/5760, Loss: 0.004688262939453125\n",
      "Epoch: 47, Samples: 4736/5760, Loss: 0.0076705217361450195\n",
      "Epoch: 47, Samples: 4768/5760, Loss: 0.009558230638504028\n",
      "Epoch: 47, Samples: 4800/5760, Loss: 0.008059829473495483\n",
      "Epoch: 47, Samples: 4832/5760, Loss: 0.005702346563339233\n",
      "Epoch: 47, Samples: 4864/5760, Loss: 0.00825682282447815\n",
      "Epoch: 47, Samples: 4896/5760, Loss: 0.011273413896560669\n",
      "Epoch: 47, Samples: 4928/5760, Loss: 0.005608320236206055\n",
      "Epoch: 47, Samples: 4960/5760, Loss: 0.007100313901901245\n",
      "Epoch: 47, Samples: 4992/5760, Loss: 0.014806628227233887\n",
      "Epoch: 47, Samples: 5024/5760, Loss: 0.009079724550247192\n",
      "Epoch: 47, Samples: 5056/5760, Loss: 0.004923015832901001\n",
      "Epoch: 47, Samples: 5088/5760, Loss: 0.022420108318328857\n",
      "Epoch: 47, Samples: 5120/5760, Loss: 0.013421356678009033\n",
      "Epoch: 47, Samples: 5152/5760, Loss: 0.007748782634735107\n",
      "Epoch: 47, Samples: 5184/5760, Loss: 0.009426236152648926\n",
      "Epoch: 47, Samples: 5216/5760, Loss: 0.008376449346542358\n",
      "Epoch: 47, Samples: 5248/5760, Loss: 0.007007688283920288\n",
      "Epoch: 47, Samples: 5280/5760, Loss: 0.004830121994018555\n",
      "Epoch: 47, Samples: 5312/5760, Loss: 0.009374618530273438\n",
      "Epoch: 47, Samples: 5344/5760, Loss: 0.005594342947006226\n",
      "Epoch: 47, Samples: 5376/5760, Loss: 0.010805875062942505\n",
      "Epoch: 47, Samples: 5408/5760, Loss: 0.006889224052429199\n",
      "Epoch: 47, Samples: 5440/5760, Loss: 0.00828254222869873\n",
      "Epoch: 47, Samples: 5472/5760, Loss: 0.009820550680160522\n",
      "Epoch: 47, Samples: 5504/5760, Loss: 0.006247907876968384\n",
      "Epoch: 47, Samples: 5536/5760, Loss: 0.004030793905258179\n",
      "Epoch: 47, Samples: 5568/5760, Loss: 0.007101863622665405\n",
      "Epoch: 47, Samples: 5600/5760, Loss: 0.004421800374984741\n",
      "Epoch: 47, Samples: 5632/5760, Loss: 0.007132768630981445\n",
      "Epoch: 47, Samples: 5664/5760, Loss: 0.00600472092628479\n",
      "Epoch: 47, Samples: 5696/5760, Loss: 0.016212597489356995\n",
      "Epoch: 47, Samples: 5728/5760, Loss: 0.3360813856124878\n",
      "\n",
      "Epoch: 47\n",
      "Training set: Average loss: 0.0123\n",
      "Validation set: Average loss: 0.2796, Accuracy: 765/818 (94%)\n",
      "Epoch: 48, Samples: 0/5760, Loss: 0.00509294867515564\n",
      "Epoch: 48, Samples: 32/5760, Loss: 0.0061352550983428955\n",
      "Epoch: 48, Samples: 64/5760, Loss: 0.005630940198898315\n",
      "Epoch: 48, Samples: 96/5760, Loss: 0.00786447525024414\n",
      "Epoch: 48, Samples: 128/5760, Loss: 0.009133189916610718\n",
      "Epoch: 48, Samples: 160/5760, Loss: 0.006911754608154297\n",
      "Epoch: 48, Samples: 192/5760, Loss: 0.010298043489456177\n",
      "Epoch: 48, Samples: 224/5760, Loss: 0.01467934250831604\n",
      "Epoch: 48, Samples: 256/5760, Loss: 0.0058394670486450195\n",
      "Epoch: 48, Samples: 288/5760, Loss: 0.014692872762680054\n",
      "Epoch: 48, Samples: 320/5760, Loss: 0.005090683698654175\n",
      "Epoch: 48, Samples: 352/5760, Loss: 0.01432773470878601\n",
      "Epoch: 48, Samples: 384/5760, Loss: 0.007143139839172363\n",
      "Epoch: 48, Samples: 416/5760, Loss: 0.012356072664260864\n",
      "Epoch: 48, Samples: 448/5760, Loss: 0.01315346360206604\n",
      "Epoch: 48, Samples: 480/5760, Loss: 0.009755909442901611\n",
      "Epoch: 48, Samples: 512/5760, Loss: 0.00935894250869751\n",
      "Epoch: 48, Samples: 544/5760, Loss: 0.004002690315246582\n",
      "Epoch: 48, Samples: 576/5760, Loss: 0.026141583919525146\n",
      "Epoch: 48, Samples: 608/5760, Loss: 0.010816484689712524\n",
      "Epoch: 48, Samples: 640/5760, Loss: 0.011650562286376953\n",
      "Epoch: 48, Samples: 672/5760, Loss: 0.0029011070728302\n",
      "Epoch: 48, Samples: 704/5760, Loss: 0.005809217691421509\n",
      "Epoch: 48, Samples: 736/5760, Loss: 0.005512893199920654\n",
      "Epoch: 48, Samples: 768/5760, Loss: 0.010892510414123535\n",
      "Epoch: 48, Samples: 800/5760, Loss: 0.01550513505935669\n",
      "Epoch: 48, Samples: 832/5760, Loss: 0.006169706583023071\n",
      "Epoch: 48, Samples: 864/5760, Loss: 0.010604441165924072\n",
      "Epoch: 48, Samples: 896/5760, Loss: 0.005403250455856323\n",
      "Epoch: 48, Samples: 928/5760, Loss: 0.007733136415481567\n",
      "Epoch: 48, Samples: 960/5760, Loss: 0.0058062076568603516\n",
      "Epoch: 48, Samples: 992/5760, Loss: 0.004840463399887085\n",
      "Epoch: 48, Samples: 1024/5760, Loss: 0.004438996315002441\n",
      "Epoch: 48, Samples: 1056/5760, Loss: 0.006805747747421265\n",
      "Epoch: 48, Samples: 1088/5760, Loss: 0.004250645637512207\n",
      "Epoch: 48, Samples: 1120/5760, Loss: 0.0051018595695495605\n",
      "Epoch: 48, Samples: 1152/5760, Loss: 0.009532809257507324\n",
      "Epoch: 48, Samples: 1184/5760, Loss: 0.01178818941116333\n",
      "Epoch: 48, Samples: 1216/5760, Loss: 0.007902860641479492\n",
      "Epoch: 48, Samples: 1248/5760, Loss: 0.0035834908485412598\n",
      "Epoch: 48, Samples: 1280/5760, Loss: 0.00539475679397583\n",
      "Epoch: 48, Samples: 1312/5760, Loss: 0.0075359344482421875\n",
      "Epoch: 48, Samples: 1344/5760, Loss: 0.008766502141952515\n",
      "Epoch: 48, Samples: 1376/5760, Loss: 0.0073438286781311035\n",
      "Epoch: 48, Samples: 1408/5760, Loss: 0.006455421447753906\n",
      "Epoch: 48, Samples: 1440/5760, Loss: 0.015548959374427795\n",
      "Epoch: 48, Samples: 1472/5760, Loss: 0.006323516368865967\n",
      "Epoch: 48, Samples: 1504/5760, Loss: 0.006400704383850098\n",
      "Epoch: 48, Samples: 1536/5760, Loss: 0.007375866174697876\n",
      "Epoch: 48, Samples: 1568/5760, Loss: 0.00844535231590271\n",
      "Epoch: 48, Samples: 1600/5760, Loss: 0.0060408711433410645\n",
      "Epoch: 48, Samples: 1632/5760, Loss: 0.013794571161270142\n",
      "Epoch: 48, Samples: 1664/5760, Loss: 0.007520496845245361\n",
      "Epoch: 48, Samples: 1696/5760, Loss: 0.006035655736923218\n",
      "Epoch: 48, Samples: 1728/5760, Loss: 0.012755602598190308\n",
      "Epoch: 48, Samples: 1760/5760, Loss: 0.005371153354644775\n",
      "Epoch: 48, Samples: 1792/5760, Loss: 0.008423328399658203\n",
      "Epoch: 48, Samples: 1824/5760, Loss: 0.007147759199142456\n",
      "Epoch: 48, Samples: 1856/5760, Loss: 0.007755011320114136\n",
      "Epoch: 48, Samples: 1888/5760, Loss: 0.003917574882507324\n",
      "Epoch: 48, Samples: 1920/5760, Loss: 0.011264592409133911\n",
      "Epoch: 48, Samples: 1952/5760, Loss: 0.007562220096588135\n",
      "Epoch: 48, Samples: 1984/5760, Loss: 0.009544432163238525\n",
      "Epoch: 48, Samples: 2016/5760, Loss: 0.00456613302230835\n",
      "Epoch: 48, Samples: 2048/5760, Loss: 0.0072702765464782715\n",
      "Epoch: 48, Samples: 2080/5760, Loss: 0.006736576557159424\n",
      "Epoch: 48, Samples: 2112/5760, Loss: 0.004921644926071167\n",
      "Epoch: 48, Samples: 2144/5760, Loss: 0.005154132843017578\n",
      "Epoch: 48, Samples: 2176/5760, Loss: 0.010108411312103271\n",
      "Epoch: 48, Samples: 2208/5760, Loss: 0.00705525279045105\n",
      "Epoch: 48, Samples: 2240/5760, Loss: 0.007867693901062012\n",
      "Epoch: 48, Samples: 2272/5760, Loss: 0.011614054441452026\n",
      "Epoch: 48, Samples: 2304/5760, Loss: 0.012276113033294678\n",
      "Epoch: 48, Samples: 2336/5760, Loss: 0.007677614688873291\n",
      "Epoch: 48, Samples: 2368/5760, Loss: 0.0068645179271698\n",
      "Epoch: 48, Samples: 2400/5760, Loss: 0.007406830787658691\n",
      "Epoch: 48, Samples: 2432/5760, Loss: 0.0055748820304870605\n",
      "Epoch: 48, Samples: 2464/5760, Loss: 0.006463289260864258\n",
      "Epoch: 48, Samples: 2496/5760, Loss: 0.014073193073272705\n",
      "Epoch: 48, Samples: 2528/5760, Loss: 0.0049825310707092285\n",
      "Epoch: 48, Samples: 2560/5760, Loss: 0.0035562515258789062\n",
      "Epoch: 48, Samples: 2592/5760, Loss: 0.005796492099761963\n",
      "Epoch: 48, Samples: 2624/5760, Loss: 0.0076507627964019775\n",
      "Epoch: 48, Samples: 2656/5760, Loss: 0.005922794342041016\n",
      "Epoch: 48, Samples: 2688/5760, Loss: 0.009379386901855469\n",
      "Epoch: 48, Samples: 2720/5760, Loss: 0.004812717437744141\n",
      "Epoch: 48, Samples: 2752/5760, Loss: 0.009197711944580078\n",
      "Epoch: 48, Samples: 2784/5760, Loss: 0.00423315167427063\n",
      "Epoch: 48, Samples: 2816/5760, Loss: 0.011116355657577515\n",
      "Epoch: 48, Samples: 2848/5760, Loss: 0.01146116852760315\n",
      "Epoch: 48, Samples: 2880/5760, Loss: 0.0051946938037872314\n",
      "Epoch: 48, Samples: 2912/5760, Loss: 0.010000467300415039\n",
      "Epoch: 48, Samples: 2944/5760, Loss: 0.007357865571975708\n",
      "Epoch: 48, Samples: 2976/5760, Loss: 0.020504504442214966\n",
      "Epoch: 48, Samples: 3008/5760, Loss: 0.007696092128753662\n",
      "Epoch: 48, Samples: 3040/5760, Loss: 0.006030410528182983\n",
      "Epoch: 48, Samples: 3072/5760, Loss: 0.0075111985206604\n",
      "Epoch: 48, Samples: 3104/5760, Loss: 0.004985600709915161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48, Samples: 3136/5760, Loss: 0.0071318745613098145\n",
      "Epoch: 48, Samples: 3168/5760, Loss: 0.011173158884048462\n",
      "Epoch: 48, Samples: 3200/5760, Loss: 0.009577006101608276\n",
      "Epoch: 48, Samples: 3232/5760, Loss: 0.01787850260734558\n",
      "Epoch: 48, Samples: 3264/5760, Loss: 0.00718274712562561\n",
      "Epoch: 48, Samples: 3296/5760, Loss: 0.025303423404693604\n",
      "Epoch: 48, Samples: 3328/5760, Loss: 0.006113797426223755\n",
      "Epoch: 48, Samples: 3360/5760, Loss: 0.009993314743041992\n",
      "Epoch: 48, Samples: 3392/5760, Loss: 0.006203711032867432\n",
      "Epoch: 48, Samples: 3424/5760, Loss: 0.004904508590698242\n",
      "Epoch: 48, Samples: 3456/5760, Loss: 0.017323553562164307\n",
      "Epoch: 48, Samples: 3488/5760, Loss: 0.009497463703155518\n",
      "Epoch: 48, Samples: 3520/5760, Loss: 0.010479211807250977\n",
      "Epoch: 48, Samples: 3552/5760, Loss: 0.007750749588012695\n",
      "Epoch: 48, Samples: 3584/5760, Loss: 0.010088562965393066\n",
      "Epoch: 48, Samples: 3616/5760, Loss: 0.0044731199741363525\n",
      "Epoch: 48, Samples: 3648/5760, Loss: 0.006776541471481323\n",
      "Epoch: 48, Samples: 3680/5760, Loss: 0.0037726163864135742\n",
      "Epoch: 48, Samples: 3712/5760, Loss: 0.00712209939956665\n",
      "Epoch: 48, Samples: 3744/5760, Loss: 0.010211795568466187\n",
      "Epoch: 48, Samples: 3776/5760, Loss: 0.005408972501754761\n",
      "Epoch: 48, Samples: 3808/5760, Loss: 0.005498707294464111\n",
      "Epoch: 48, Samples: 3840/5760, Loss: 0.010011225938796997\n",
      "Epoch: 48, Samples: 3872/5760, Loss: 0.005285412073135376\n",
      "Epoch: 48, Samples: 3904/5760, Loss: 0.008439809083938599\n",
      "Epoch: 48, Samples: 3936/5760, Loss: 0.011628299951553345\n",
      "Epoch: 48, Samples: 3968/5760, Loss: 0.005905598402023315\n",
      "Epoch: 48, Samples: 4000/5760, Loss: 0.013594508171081543\n",
      "Epoch: 48, Samples: 4032/5760, Loss: 0.009053051471710205\n",
      "Epoch: 48, Samples: 4064/5760, Loss: 0.006126523017883301\n",
      "Epoch: 48, Samples: 4096/5760, Loss: 0.012318819761276245\n",
      "Epoch: 48, Samples: 4128/5760, Loss: 0.006381213665008545\n",
      "Epoch: 48, Samples: 4160/5760, Loss: 0.011440813541412354\n",
      "Epoch: 48, Samples: 4192/5760, Loss: 0.005875229835510254\n",
      "Epoch: 48, Samples: 4224/5760, Loss: 0.004045605659484863\n",
      "Epoch: 48, Samples: 4256/5760, Loss: 0.019389241933822632\n",
      "Epoch: 48, Samples: 4288/5760, Loss: 0.008908122777938843\n",
      "Epoch: 48, Samples: 4320/5760, Loss: 0.0112190842628479\n",
      "Epoch: 48, Samples: 4352/5760, Loss: 0.006064653396606445\n",
      "Epoch: 48, Samples: 4384/5760, Loss: 0.009178221225738525\n",
      "Epoch: 48, Samples: 4416/5760, Loss: 0.011502325534820557\n",
      "Epoch: 48, Samples: 4448/5760, Loss: 0.010427415370941162\n",
      "Epoch: 48, Samples: 4480/5760, Loss: 0.0062874555587768555\n",
      "Epoch: 48, Samples: 4512/5760, Loss: 0.0085715651512146\n",
      "Epoch: 48, Samples: 4544/5760, Loss: 0.01335027813911438\n",
      "Epoch: 48, Samples: 4576/5760, Loss: 0.006788760423660278\n",
      "Epoch: 48, Samples: 4608/5760, Loss: 0.01496124267578125\n",
      "Epoch: 48, Samples: 4640/5760, Loss: 0.0061036646366119385\n",
      "Epoch: 48, Samples: 4672/5760, Loss: 0.00832068920135498\n",
      "Epoch: 48, Samples: 4704/5760, Loss: 0.010896295309066772\n",
      "Epoch: 48, Samples: 4736/5760, Loss: 0.0036208033561706543\n",
      "Epoch: 48, Samples: 4768/5760, Loss: 0.009224921464920044\n",
      "Epoch: 48, Samples: 4800/5760, Loss: 0.006100833415985107\n",
      "Epoch: 48, Samples: 4832/5760, Loss: 0.017896413803100586\n",
      "Epoch: 48, Samples: 4864/5760, Loss: 0.004012703895568848\n",
      "Epoch: 48, Samples: 4896/5760, Loss: 0.006042420864105225\n",
      "Epoch: 48, Samples: 4928/5760, Loss: 0.007758080959320068\n",
      "Epoch: 48, Samples: 4960/5760, Loss: 0.018730998039245605\n",
      "Epoch: 48, Samples: 4992/5760, Loss: 0.02228406071662903\n",
      "Epoch: 48, Samples: 5024/5760, Loss: 0.0089644193649292\n",
      "Epoch: 48, Samples: 5056/5760, Loss: 0.01273772120475769\n",
      "Epoch: 48, Samples: 5088/5760, Loss: 0.005796462297439575\n",
      "Epoch: 48, Samples: 5120/5760, Loss: 0.004910945892333984\n",
      "Epoch: 48, Samples: 5152/5760, Loss: 0.010926097631454468\n",
      "Epoch: 48, Samples: 5184/5760, Loss: 0.007300078868865967\n",
      "Epoch: 48, Samples: 5216/5760, Loss: 0.007930278778076172\n",
      "Epoch: 48, Samples: 5248/5760, Loss: 0.020170480012893677\n",
      "Epoch: 48, Samples: 5280/5760, Loss: 0.007779985666275024\n",
      "Epoch: 48, Samples: 5312/5760, Loss: 0.006884455680847168\n",
      "Epoch: 48, Samples: 5344/5760, Loss: 0.00504264235496521\n",
      "Epoch: 48, Samples: 5376/5760, Loss: 0.007761508226394653\n",
      "Epoch: 48, Samples: 5408/5760, Loss: 0.006992042064666748\n",
      "Epoch: 48, Samples: 5440/5760, Loss: 0.009012699127197266\n",
      "Epoch: 48, Samples: 5472/5760, Loss: 0.004256337881088257\n",
      "Epoch: 48, Samples: 5504/5760, Loss: 0.005501925945281982\n",
      "Epoch: 48, Samples: 5536/5760, Loss: 0.007676571607589722\n",
      "Epoch: 48, Samples: 5568/5760, Loss: 0.015850067138671875\n",
      "Epoch: 48, Samples: 5600/5760, Loss: 0.006792306900024414\n",
      "Epoch: 48, Samples: 5632/5760, Loss: 0.01638585329055786\n",
      "Epoch: 48, Samples: 5664/5760, Loss: 0.01104709506034851\n",
      "Epoch: 48, Samples: 5696/5760, Loss: 0.006485402584075928\n",
      "Epoch: 48, Samples: 5728/5760, Loss: 0.48595428466796875\n",
      "\n",
      "Epoch: 48\n",
      "Training set: Average loss: 0.0114\n",
      "Validation set: Average loss: 0.2661, Accuracy: 762/818 (93%)\n",
      "Epoch: 49, Samples: 0/5760, Loss: 0.005898594856262207\n",
      "Epoch: 49, Samples: 32/5760, Loss: 0.01305854320526123\n",
      "Epoch: 49, Samples: 64/5760, Loss: 0.007410764694213867\n",
      "Epoch: 49, Samples: 96/5760, Loss: 0.01729997992515564\n",
      "Epoch: 49, Samples: 128/5760, Loss: 0.006078064441680908\n",
      "Epoch: 49, Samples: 160/5760, Loss: 0.0030046701431274414\n",
      "Epoch: 49, Samples: 192/5760, Loss: 0.004309892654418945\n",
      "Epoch: 49, Samples: 224/5760, Loss: 0.00537380576133728\n",
      "Epoch: 49, Samples: 256/5760, Loss: 0.006715148687362671\n",
      "Epoch: 49, Samples: 288/5760, Loss: 0.007149547338485718\n",
      "Epoch: 49, Samples: 320/5760, Loss: 0.011198073625564575\n",
      "Epoch: 49, Samples: 352/5760, Loss: 0.009748488664627075\n",
      "Epoch: 49, Samples: 384/5760, Loss: 0.006867647171020508\n",
      "Epoch: 49, Samples: 416/5760, Loss: 0.010405033826828003\n",
      "Epoch: 49, Samples: 448/5760, Loss: 0.004213958978652954\n",
      "Epoch: 49, Samples: 480/5760, Loss: 0.008862525224685669\n",
      "Epoch: 49, Samples: 512/5760, Loss: 0.007852137088775635\n",
      "Epoch: 49, Samples: 544/5760, Loss: 0.014161467552185059\n",
      "Epoch: 49, Samples: 576/5760, Loss: 0.011169582605361938\n",
      "Epoch: 49, Samples: 608/5760, Loss: 0.005266696214675903\n",
      "Epoch: 49, Samples: 640/5760, Loss: 0.008705824613571167\n",
      "Epoch: 49, Samples: 672/5760, Loss: 0.005360513925552368\n",
      "Epoch: 49, Samples: 704/5760, Loss: 0.005463898181915283\n",
      "Epoch: 49, Samples: 736/5760, Loss: 0.05602139234542847\n",
      "Epoch: 49, Samples: 768/5760, Loss: 0.005037277936935425\n",
      "Epoch: 49, Samples: 800/5760, Loss: 0.004691183567047119\n",
      "Epoch: 49, Samples: 832/5760, Loss: 0.004107058048248291\n",
      "Epoch: 49, Samples: 864/5760, Loss: 0.008951306343078613\n",
      "Epoch: 49, Samples: 896/5760, Loss: 0.0032977163791656494\n",
      "Epoch: 49, Samples: 928/5760, Loss: 0.01024395227432251\n",
      "Epoch: 49, Samples: 960/5760, Loss: 0.0018796026706695557\n",
      "Epoch: 49, Samples: 992/5760, Loss: 0.02294352650642395\n",
      "Epoch: 49, Samples: 1024/5760, Loss: 0.011404752731323242\n",
      "Epoch: 49, Samples: 1056/5760, Loss: 0.009107083082199097\n",
      "Epoch: 49, Samples: 1088/5760, Loss: 0.006120800971984863\n",
      "Epoch: 49, Samples: 1120/5760, Loss: 0.006568968296051025\n",
      "Epoch: 49, Samples: 1152/5760, Loss: 0.011733949184417725\n",
      "Epoch: 49, Samples: 1184/5760, Loss: 0.006676375865936279\n",
      "Epoch: 49, Samples: 1216/5760, Loss: 0.01682940125465393\n",
      "Epoch: 49, Samples: 1248/5760, Loss: 0.0154876708984375\n",
      "Epoch: 49, Samples: 1280/5760, Loss: 0.009582310914993286\n",
      "Epoch: 49, Samples: 1312/5760, Loss: 0.00678667426109314\n",
      "Epoch: 49, Samples: 1344/5760, Loss: 0.010586053133010864\n",
      "Epoch: 49, Samples: 1376/5760, Loss: 0.007720470428466797\n",
      "Epoch: 49, Samples: 1408/5760, Loss: 0.008592784404754639\n",
      "Epoch: 49, Samples: 1440/5760, Loss: 0.006251126527786255\n",
      "Epoch: 49, Samples: 1472/5760, Loss: 0.009963840246200562\n",
      "Epoch: 49, Samples: 1504/5760, Loss: 0.012822657823562622\n",
      "Epoch: 49, Samples: 1536/5760, Loss: 0.007429689168930054\n",
      "Epoch: 49, Samples: 1568/5760, Loss: 0.004025638103485107\n",
      "Epoch: 49, Samples: 1600/5760, Loss: 0.008880972862243652\n",
      "Epoch: 49, Samples: 1632/5760, Loss: 0.006619691848754883\n",
      "Epoch: 49, Samples: 1664/5760, Loss: 0.006170034408569336\n",
      "Epoch: 49, Samples: 1696/5760, Loss: 0.0067314207553863525\n",
      "Epoch: 49, Samples: 1728/5760, Loss: 0.01888209581375122\n",
      "Epoch: 49, Samples: 1760/5760, Loss: 0.008053183555603027\n",
      "Epoch: 49, Samples: 1792/5760, Loss: 0.020963728427886963\n",
      "Epoch: 49, Samples: 1824/5760, Loss: 0.005988359451293945\n",
      "Epoch: 49, Samples: 1856/5760, Loss: 0.007070302963256836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49, Samples: 1888/5760, Loss: 0.006431013345718384\n",
      "Epoch: 49, Samples: 1920/5760, Loss: 0.010497301816940308\n",
      "Epoch: 49, Samples: 1952/5760, Loss: 0.006691604852676392\n",
      "Epoch: 49, Samples: 1984/5760, Loss: 0.010846436023712158\n",
      "Epoch: 49, Samples: 2016/5760, Loss: 0.0051055848598480225\n",
      "Epoch: 49, Samples: 2048/5760, Loss: 0.007225453853607178\n",
      "Epoch: 49, Samples: 2080/5760, Loss: 0.007082700729370117\n",
      "Epoch: 49, Samples: 2112/5760, Loss: 0.025361597537994385\n",
      "Epoch: 49, Samples: 2144/5760, Loss: 0.013745635747909546\n",
      "Epoch: 49, Samples: 2176/5760, Loss: 0.01703965663909912\n",
      "Epoch: 49, Samples: 2208/5760, Loss: 0.005430012941360474\n",
      "Epoch: 49, Samples: 2240/5760, Loss: 0.00850638747215271\n",
      "Epoch: 49, Samples: 2272/5760, Loss: 0.007506996393203735\n",
      "Epoch: 49, Samples: 2304/5760, Loss: 0.005652397871017456\n",
      "Epoch: 49, Samples: 2336/5760, Loss: 0.005252212285995483\n",
      "Epoch: 49, Samples: 2368/5760, Loss: 0.008325278759002686\n",
      "Epoch: 49, Samples: 2400/5760, Loss: 0.00978204607963562\n",
      "Epoch: 49, Samples: 2432/5760, Loss: 0.005247890949249268\n",
      "Epoch: 49, Samples: 2464/5760, Loss: 0.006690382957458496\n",
      "Epoch: 49, Samples: 2496/5760, Loss: 0.006253659725189209\n",
      "Epoch: 49, Samples: 2528/5760, Loss: 0.00600782036781311\n",
      "Epoch: 49, Samples: 2560/5760, Loss: 0.006576806306838989\n",
      "Epoch: 49, Samples: 2592/5760, Loss: 0.005333751440048218\n",
      "Epoch: 49, Samples: 2624/5760, Loss: 0.008576720952987671\n",
      "Epoch: 49, Samples: 2656/5760, Loss: 0.008306354284286499\n",
      "Epoch: 49, Samples: 2688/5760, Loss: 0.01080232858657837\n",
      "Epoch: 49, Samples: 2720/5760, Loss: 0.009292513132095337\n",
      "Epoch: 49, Samples: 2752/5760, Loss: 0.00952807068824768\n",
      "Epoch: 49, Samples: 2784/5760, Loss: 0.005751222372055054\n",
      "Epoch: 49, Samples: 2816/5760, Loss: 0.007705271244049072\n",
      "Epoch: 49, Samples: 2848/5760, Loss: 0.009025603532791138\n",
      "Epoch: 49, Samples: 2880/5760, Loss: 0.008110255002975464\n",
      "Epoch: 49, Samples: 2912/5760, Loss: 0.0073299407958984375\n",
      "Epoch: 49, Samples: 2944/5760, Loss: 0.005873292684555054\n",
      "Epoch: 49, Samples: 2976/5760, Loss: 0.008220076560974121\n",
      "Epoch: 49, Samples: 3008/5760, Loss: 0.012650847434997559\n",
      "Epoch: 49, Samples: 3040/5760, Loss: 0.007582515478134155\n",
      "Epoch: 49, Samples: 3072/5760, Loss: 0.008452355861663818\n",
      "Epoch: 49, Samples: 3104/5760, Loss: 0.008618533611297607\n",
      "Epoch: 49, Samples: 3136/5760, Loss: 0.006325900554656982\n",
      "Epoch: 49, Samples: 3168/5760, Loss: 0.008202046155929565\n",
      "Epoch: 49, Samples: 3200/5760, Loss: 0.006977766752243042\n",
      "Epoch: 49, Samples: 3232/5760, Loss: 0.007496923208236694\n",
      "Epoch: 49, Samples: 3264/5760, Loss: 0.0071565210819244385\n",
      "Epoch: 49, Samples: 3296/5760, Loss: 0.008439123630523682\n",
      "Epoch: 49, Samples: 3328/5760, Loss: 0.007072716951370239\n",
      "Epoch: 49, Samples: 3360/5760, Loss: 0.009052634239196777\n",
      "Epoch: 49, Samples: 3392/5760, Loss: 0.006494104862213135\n",
      "Epoch: 49, Samples: 3424/5760, Loss: 0.01885470747947693\n",
      "Epoch: 49, Samples: 3456/5760, Loss: 0.005572676658630371\n",
      "Epoch: 49, Samples: 3488/5760, Loss: 0.012560546398162842\n",
      "Epoch: 49, Samples: 3520/5760, Loss: 0.0063378214836120605\n",
      "Epoch: 49, Samples: 3552/5760, Loss: 0.00530242919921875\n",
      "Epoch: 49, Samples: 3584/5760, Loss: 0.008234411478042603\n",
      "Epoch: 49, Samples: 3616/5760, Loss: 0.005343705415725708\n",
      "Epoch: 49, Samples: 3648/5760, Loss: 0.011305034160614014\n",
      "Epoch: 49, Samples: 3680/5760, Loss: 0.005793243646621704\n",
      "Epoch: 49, Samples: 3712/5760, Loss: 0.005973756313323975\n",
      "Epoch: 49, Samples: 3744/5760, Loss: 0.009241998195648193\n",
      "Epoch: 49, Samples: 3776/5760, Loss: 0.014665216207504272\n",
      "Epoch: 49, Samples: 3808/5760, Loss: 0.006400436162948608\n",
      "Epoch: 49, Samples: 3840/5760, Loss: 0.00459858775138855\n",
      "Epoch: 49, Samples: 3872/5760, Loss: 0.013154357671737671\n",
      "Epoch: 49, Samples: 3904/5760, Loss: 0.009271293878555298\n",
      "Epoch: 49, Samples: 3936/5760, Loss: 0.012329936027526855\n",
      "Epoch: 49, Samples: 3968/5760, Loss: 0.009980857372283936\n",
      "Epoch: 49, Samples: 4000/5760, Loss: 0.007520228624343872\n",
      "Epoch: 49, Samples: 4032/5760, Loss: 0.007397443056106567\n",
      "Epoch: 49, Samples: 4064/5760, Loss: 0.009029477834701538\n",
      "Epoch: 49, Samples: 4096/5760, Loss: 0.006078749895095825\n",
      "Epoch: 49, Samples: 4128/5760, Loss: 0.006321996450424194\n",
      "Epoch: 49, Samples: 4160/5760, Loss: 0.008719384670257568\n",
      "Epoch: 49, Samples: 4192/5760, Loss: 0.015167325735092163\n",
      "Epoch: 49, Samples: 4224/5760, Loss: 0.006944417953491211\n",
      "Epoch: 49, Samples: 4256/5760, Loss: 0.013012170791625977\n",
      "Epoch: 49, Samples: 4288/5760, Loss: 0.012155845761299133\n",
      "Epoch: 49, Samples: 4320/5760, Loss: 0.01120847463607788\n",
      "Epoch: 49, Samples: 4352/5760, Loss: 0.009693235158920288\n",
      "Epoch: 49, Samples: 4384/5760, Loss: 0.006646215915679932\n",
      "Epoch: 49, Samples: 4416/5760, Loss: 0.004818648099899292\n",
      "Epoch: 49, Samples: 4448/5760, Loss: 0.00608140230178833\n",
      "Epoch: 49, Samples: 4480/5760, Loss: 0.008450329303741455\n",
      "Epoch: 49, Samples: 4512/5760, Loss: 0.042615920305252075\n",
      "Epoch: 49, Samples: 4544/5760, Loss: 0.005185812711715698\n",
      "Epoch: 49, Samples: 4576/5760, Loss: 0.005366116762161255\n",
      "Epoch: 49, Samples: 4608/5760, Loss: 0.012676775455474854\n",
      "Epoch: 49, Samples: 4640/5760, Loss: 0.007387340068817139\n",
      "Epoch: 49, Samples: 4672/5760, Loss: 0.005147010087966919\n",
      "Epoch: 49, Samples: 4704/5760, Loss: 0.007491558790206909\n",
      "Epoch: 49, Samples: 4736/5760, Loss: 0.01084783673286438\n",
      "Epoch: 49, Samples: 4768/5760, Loss: 0.006976038217544556\n",
      "Epoch: 49, Samples: 4800/5760, Loss: 0.004073143005371094\n",
      "Epoch: 49, Samples: 4832/5760, Loss: 0.014231652021408081\n",
      "Epoch: 49, Samples: 4864/5760, Loss: 0.008696258068084717\n",
      "Epoch: 49, Samples: 4896/5760, Loss: 0.006142228841781616\n",
      "Epoch: 49, Samples: 4928/5760, Loss: 0.005449086427688599\n",
      "Epoch: 49, Samples: 4960/5760, Loss: 0.010361850261688232\n",
      "Epoch: 49, Samples: 4992/5760, Loss: 0.0032311677932739258\n",
      "Epoch: 49, Samples: 5024/5760, Loss: 0.004097789525985718\n",
      "Epoch: 49, Samples: 5056/5760, Loss: 0.009877443313598633\n",
      "Epoch: 49, Samples: 5088/5760, Loss: 0.00516924262046814\n",
      "Epoch: 49, Samples: 5120/5760, Loss: 0.003357917070388794\n",
      "Epoch: 49, Samples: 5152/5760, Loss: 0.013100862503051758\n",
      "Epoch: 49, Samples: 5184/5760, Loss: 0.0068930983543396\n",
      "Epoch: 49, Samples: 5216/5760, Loss: 0.005343824625015259\n",
      "Epoch: 49, Samples: 5248/5760, Loss: 0.005535155534744263\n",
      "Epoch: 49, Samples: 5280/5760, Loss: 0.012798905372619629\n",
      "Epoch: 49, Samples: 5312/5760, Loss: 0.0025637447834014893\n",
      "Epoch: 49, Samples: 5344/5760, Loss: 0.008666396141052246\n",
      "Epoch: 49, Samples: 5376/5760, Loss: 0.005842715501785278\n",
      "Epoch: 49, Samples: 5408/5760, Loss: 0.010356128215789795\n",
      "Epoch: 49, Samples: 5440/5760, Loss: 0.006088972091674805\n",
      "Epoch: 49, Samples: 5472/5760, Loss: 0.015777215361595154\n",
      "Epoch: 49, Samples: 5504/5760, Loss: 0.008833378553390503\n",
      "Epoch: 49, Samples: 5536/5760, Loss: 0.008740365505218506\n",
      "Epoch: 49, Samples: 5568/5760, Loss: 0.012931019067764282\n",
      "Epoch: 49, Samples: 5600/5760, Loss: 0.0046224892139434814\n",
      "Epoch: 49, Samples: 5632/5760, Loss: 0.005823403596878052\n",
      "Epoch: 49, Samples: 5664/5760, Loss: 0.012612849473953247\n",
      "Epoch: 49, Samples: 5696/5760, Loss: 0.01701939105987549\n",
      "Epoch: 49, Samples: 5728/5760, Loss: 0.773662805557251\n",
      "\n",
      "Epoch: 49\n",
      "Training set: Average loss: 0.0132\n",
      "Validation set: Average loss: 0.2703, Accuracy: 766/818 (94%)\n",
      "Epoch: 50, Samples: 0/5760, Loss: 0.01331356167793274\n",
      "Epoch: 50, Samples: 32/5760, Loss: 0.011244624853134155\n",
      "Epoch: 50, Samples: 64/5760, Loss: 0.005034714937210083\n",
      "Epoch: 50, Samples: 96/5760, Loss: 0.0057995617389678955\n",
      "Epoch: 50, Samples: 128/5760, Loss: 0.021327078342437744\n",
      "Epoch: 50, Samples: 160/5760, Loss: 0.011997640132904053\n",
      "Epoch: 50, Samples: 192/5760, Loss: 0.011778861284255981\n",
      "Epoch: 50, Samples: 224/5760, Loss: 0.04003037512302399\n",
      "Epoch: 50, Samples: 256/5760, Loss: 0.06241053342819214\n",
      "Epoch: 50, Samples: 288/5760, Loss: 0.04781760275363922\n",
      "Epoch: 50, Samples: 320/5760, Loss: 0.00784456729888916\n",
      "Epoch: 50, Samples: 352/5760, Loss: 0.023095369338989258\n",
      "Epoch: 50, Samples: 384/5760, Loss: 0.016116589307785034\n",
      "Epoch: 50, Samples: 416/5760, Loss: 0.03519292175769806\n",
      "Epoch: 50, Samples: 448/5760, Loss: 0.011438608169555664\n",
      "Epoch: 50, Samples: 480/5760, Loss: 0.1399918496608734\n",
      "Epoch: 50, Samples: 512/5760, Loss: 0.016271382570266724\n",
      "Epoch: 50, Samples: 544/5760, Loss: 0.02914273738861084\n",
      "Epoch: 50, Samples: 576/5760, Loss: 0.011698111891746521\n",
      "Epoch: 50, Samples: 608/5760, Loss: 0.013242453336715698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Samples: 640/5760, Loss: 0.023661881685256958\n",
      "Epoch: 50, Samples: 672/5760, Loss: 0.00804513692855835\n",
      "Epoch: 50, Samples: 704/5760, Loss: 0.019809186458587646\n",
      "Epoch: 50, Samples: 736/5760, Loss: 0.010083407163619995\n",
      "Epoch: 50, Samples: 768/5760, Loss: 0.02607119083404541\n",
      "Epoch: 50, Samples: 800/5760, Loss: 0.006010174751281738\n",
      "Epoch: 50, Samples: 832/5760, Loss: 0.009867817163467407\n",
      "Epoch: 50, Samples: 864/5760, Loss: 0.028020724654197693\n",
      "Epoch: 50, Samples: 896/5760, Loss: 0.010013192892074585\n",
      "Epoch: 50, Samples: 928/5760, Loss: 0.0074384212493896484\n",
      "Epoch: 50, Samples: 960/5760, Loss: 0.014528423547744751\n",
      "Epoch: 50, Samples: 992/5760, Loss: 0.015657037496566772\n",
      "Epoch: 50, Samples: 1024/5760, Loss: 0.024746954441070557\n",
      "Epoch: 50, Samples: 1056/5760, Loss: 0.010027468204498291\n",
      "Epoch: 50, Samples: 1088/5760, Loss: 0.008428573608398438\n",
      "Epoch: 50, Samples: 1120/5760, Loss: 0.022054940462112427\n",
      "Epoch: 50, Samples: 1152/5760, Loss: 0.012287914752960205\n",
      "Epoch: 50, Samples: 1184/5760, Loss: 0.05388273298740387\n",
      "Epoch: 50, Samples: 1216/5760, Loss: 0.01119464635848999\n",
      "Epoch: 50, Samples: 1248/5760, Loss: 0.016559302806854248\n",
      "Epoch: 50, Samples: 1280/5760, Loss: 0.013028129935264587\n",
      "Epoch: 50, Samples: 1312/5760, Loss: 0.013752162456512451\n",
      "Epoch: 50, Samples: 1344/5760, Loss: 0.022634431719779968\n",
      "Epoch: 50, Samples: 1376/5760, Loss: 0.008930414915084839\n",
      "Epoch: 50, Samples: 1408/5760, Loss: 0.005910277366638184\n",
      "Epoch: 50, Samples: 1440/5760, Loss: 0.013180345296859741\n",
      "Epoch: 50, Samples: 1472/5760, Loss: 0.020861908793449402\n",
      "Epoch: 50, Samples: 1504/5760, Loss: 0.006781488656997681\n",
      "Epoch: 50, Samples: 1536/5760, Loss: 0.009991198778152466\n",
      "Epoch: 50, Samples: 1568/5760, Loss: 0.0052973926067352295\n",
      "Epoch: 50, Samples: 1600/5760, Loss: 0.01892593502998352\n",
      "Epoch: 50, Samples: 1632/5760, Loss: 0.014177918434143066\n",
      "Epoch: 50, Samples: 1664/5760, Loss: 0.011043250560760498\n",
      "Epoch: 50, Samples: 1696/5760, Loss: 0.015353888273239136\n",
      "Epoch: 50, Samples: 1728/5760, Loss: 0.009375721216201782\n",
      "Epoch: 50, Samples: 1760/5760, Loss: 0.011391431093215942\n",
      "Epoch: 50, Samples: 1792/5760, Loss: 0.009798943996429443\n",
      "Epoch: 50, Samples: 1824/5760, Loss: 0.006929874420166016\n",
      "Epoch: 50, Samples: 1856/5760, Loss: 0.00786203145980835\n",
      "Epoch: 50, Samples: 1888/5760, Loss: 0.009534597396850586\n",
      "Epoch: 50, Samples: 1920/5760, Loss: 0.009533077478408813\n",
      "Epoch: 50, Samples: 1952/5760, Loss: 0.006151854991912842\n",
      "Epoch: 50, Samples: 1984/5760, Loss: 0.008279740810394287\n",
      "Epoch: 50, Samples: 2016/5760, Loss: 0.006915956735610962\n",
      "Epoch: 50, Samples: 2048/5760, Loss: 0.011106044054031372\n",
      "Epoch: 50, Samples: 2080/5760, Loss: 0.010120689868927002\n",
      "Epoch: 50, Samples: 2112/5760, Loss: 0.007658571004867554\n",
      "Epoch: 50, Samples: 2144/5760, Loss: 0.009634643793106079\n",
      "Epoch: 50, Samples: 2176/5760, Loss: 0.004652559757232666\n",
      "Epoch: 50, Samples: 2208/5760, Loss: 0.024690598249435425\n",
      "Epoch: 50, Samples: 2240/5760, Loss: 0.012691855430603027\n",
      "Epoch: 50, Samples: 2272/5760, Loss: 0.009074777364730835\n",
      "Epoch: 50, Samples: 2304/5760, Loss: 0.014134421944618225\n",
      "Epoch: 50, Samples: 2336/5760, Loss: 0.006657063961029053\n",
      "Epoch: 50, Samples: 2368/5760, Loss: 0.0060440897941589355\n",
      "Epoch: 50, Samples: 2400/5760, Loss: 0.012067079544067383\n",
      "Epoch: 50, Samples: 2432/5760, Loss: 0.019838809967041016\n",
      "Epoch: 50, Samples: 2464/5760, Loss: 0.008132725954055786\n",
      "Epoch: 50, Samples: 2496/5760, Loss: 0.0086783766746521\n",
      "Epoch: 50, Samples: 2528/5760, Loss: 0.009188145399093628\n",
      "Epoch: 50, Samples: 2560/5760, Loss: 0.00959765911102295\n",
      "Epoch: 50, Samples: 2592/5760, Loss: 0.005399167537689209\n",
      "Epoch: 50, Samples: 2624/5760, Loss: 0.013417452573776245\n",
      "Epoch: 50, Samples: 2656/5760, Loss: 0.009527057409286499\n",
      "Epoch: 50, Samples: 2688/5760, Loss: 0.007822930812835693\n",
      "Epoch: 50, Samples: 2720/5760, Loss: 0.0051594078540802\n",
      "Epoch: 50, Samples: 2752/5760, Loss: 0.011794626712799072\n",
      "Epoch: 50, Samples: 2784/5760, Loss: 0.004794180393218994\n",
      "Epoch: 50, Samples: 2816/5760, Loss: 0.006575137376785278\n",
      "Epoch: 50, Samples: 2848/5760, Loss: 0.010441124439239502\n",
      "Epoch: 50, Samples: 2880/5760, Loss: 0.0037679076194763184\n",
      "Epoch: 50, Samples: 2912/5760, Loss: 0.02164396643638611\n",
      "Epoch: 50, Samples: 2944/5760, Loss: 0.00510019063949585\n",
      "Epoch: 50, Samples: 2976/5760, Loss: 0.003947556018829346\n",
      "Epoch: 50, Samples: 3008/5760, Loss: 0.0056649744510650635\n",
      "Epoch: 50, Samples: 3040/5760, Loss: 0.008298754692077637\n",
      "Epoch: 50, Samples: 3072/5760, Loss: 0.015059292316436768\n",
      "Epoch: 50, Samples: 3104/5760, Loss: 0.010809600353240967\n",
      "Epoch: 50, Samples: 3136/5760, Loss: 0.008543610572814941\n",
      "Epoch: 50, Samples: 3168/5760, Loss: 0.00784870982170105\n",
      "Epoch: 50, Samples: 3200/5760, Loss: 0.007827192544937134\n",
      "Epoch: 50, Samples: 3232/5760, Loss: 0.010010749101638794\n",
      "Epoch: 50, Samples: 3264/5760, Loss: 0.006266266107559204\n",
      "Epoch: 50, Samples: 3296/5760, Loss: 0.020250588655471802\n",
      "Epoch: 50, Samples: 3328/5760, Loss: 0.009734272956848145\n",
      "Epoch: 50, Samples: 3360/5760, Loss: 0.008628755807876587\n",
      "Epoch: 50, Samples: 3392/5760, Loss: 0.0054014623165130615\n",
      "Epoch: 50, Samples: 3424/5760, Loss: 0.014288470149040222\n",
      "Epoch: 50, Samples: 3456/5760, Loss: 0.011143535375595093\n",
      "Epoch: 50, Samples: 3488/5760, Loss: 0.006648600101470947\n",
      "Epoch: 50, Samples: 3520/5760, Loss: 0.021334290504455566\n",
      "Epoch: 50, Samples: 3552/5760, Loss: 0.009928256273269653\n",
      "Epoch: 50, Samples: 3584/5760, Loss: 0.00930798053741455\n",
      "Epoch: 50, Samples: 3616/5760, Loss: 0.008422642946243286\n",
      "Epoch: 50, Samples: 3648/5760, Loss: 0.005502283573150635\n",
      "Epoch: 50, Samples: 3680/5760, Loss: 0.00845324993133545\n",
      "Epoch: 50, Samples: 3712/5760, Loss: 0.019820988178253174\n",
      "Epoch: 50, Samples: 3744/5760, Loss: 0.008224248886108398\n",
      "Epoch: 50, Samples: 3776/5760, Loss: 0.009986072778701782\n",
      "Epoch: 50, Samples: 3808/5760, Loss: 0.011867284774780273\n",
      "Epoch: 50, Samples: 3840/5760, Loss: 0.004073739051818848\n",
      "Epoch: 50, Samples: 3872/5760, Loss: 0.004717767238616943\n",
      "Epoch: 50, Samples: 3904/5760, Loss: 0.013417750597000122\n",
      "Epoch: 50, Samples: 3936/5760, Loss: 0.005453139543533325\n",
      "Epoch: 50, Samples: 3968/5760, Loss: 0.006914317607879639\n",
      "Epoch: 50, Samples: 4000/5760, Loss: 0.007382750511169434\n",
      "Epoch: 50, Samples: 4032/5760, Loss: 0.006807208061218262\n",
      "Epoch: 50, Samples: 4064/5760, Loss: 0.008248209953308105\n",
      "Epoch: 50, Samples: 4096/5760, Loss: 0.00869077444076538\n",
      "Epoch: 50, Samples: 4128/5760, Loss: 0.004653573036193848\n",
      "Epoch: 50, Samples: 4160/5760, Loss: 0.006310194730758667\n",
      "Epoch: 50, Samples: 4192/5760, Loss: 0.008634358644485474\n",
      "Epoch: 50, Samples: 4224/5760, Loss: 0.005682826042175293\n",
      "Epoch: 50, Samples: 4256/5760, Loss: 0.0067050158977508545\n",
      "Epoch: 50, Samples: 4288/5760, Loss: 0.010148704051971436\n",
      "Epoch: 50, Samples: 4320/5760, Loss: 0.00894191861152649\n",
      "Epoch: 50, Samples: 4352/5760, Loss: 0.006642639636993408\n",
      "Epoch: 50, Samples: 4384/5760, Loss: 0.005494952201843262\n",
      "Epoch: 50, Samples: 4416/5760, Loss: 0.00831836462020874\n",
      "Epoch: 50, Samples: 4448/5760, Loss: 0.013745278120040894\n",
      "Epoch: 50, Samples: 4480/5760, Loss: 0.010499447584152222\n",
      "Epoch: 50, Samples: 4512/5760, Loss: 0.009386181831359863\n",
      "Epoch: 50, Samples: 4544/5760, Loss: 0.00902053713798523\n",
      "Epoch: 50, Samples: 4576/5760, Loss: 0.006405919790267944\n",
      "Epoch: 50, Samples: 4608/5760, Loss: 0.01073870062828064\n",
      "Epoch: 50, Samples: 4640/5760, Loss: 0.0052603185176849365\n",
      "Epoch: 50, Samples: 4672/5760, Loss: 0.007733017206192017\n",
      "Epoch: 50, Samples: 4704/5760, Loss: 0.010647565126419067\n",
      "Epoch: 50, Samples: 4736/5760, Loss: 0.00780060887336731\n",
      "Epoch: 50, Samples: 4768/5760, Loss: 0.008283525705337524\n",
      "Epoch: 50, Samples: 4800/5760, Loss: 0.005993247032165527\n",
      "Epoch: 50, Samples: 4832/5760, Loss: 0.008470863103866577\n",
      "Epoch: 50, Samples: 4864/5760, Loss: 0.014945775270462036\n",
      "Epoch: 50, Samples: 4896/5760, Loss: 0.005594193935394287\n",
      "Epoch: 50, Samples: 4928/5760, Loss: 0.011932820081710815\n",
      "Epoch: 50, Samples: 4960/5760, Loss: 0.004297524690628052\n",
      "Epoch: 50, Samples: 4992/5760, Loss: 0.007784396409988403\n",
      "Epoch: 50, Samples: 5024/5760, Loss: 0.006430476903915405\n",
      "Epoch: 50, Samples: 5056/5760, Loss: 0.00923919677734375\n",
      "Epoch: 50, Samples: 5088/5760, Loss: 0.006877720355987549\n",
      "Epoch: 50, Samples: 5120/5760, Loss: 0.006969451904296875\n",
      "Epoch: 50, Samples: 5152/5760, Loss: 0.007946759462356567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Samples: 5184/5760, Loss: 0.009125560522079468\n",
      "Epoch: 50, Samples: 5216/5760, Loss: 0.007879525423049927\n",
      "Epoch: 50, Samples: 5248/5760, Loss: 0.011557787656784058\n",
      "Epoch: 50, Samples: 5280/5760, Loss: 0.004326492547988892\n",
      "Epoch: 50, Samples: 5312/5760, Loss: 0.007750481367111206\n",
      "Epoch: 50, Samples: 5344/5760, Loss: 0.005521029233932495\n",
      "Epoch: 50, Samples: 5376/5760, Loss: 0.008891135454177856\n",
      "Epoch: 50, Samples: 5408/5760, Loss: 0.022703945636749268\n",
      "Epoch: 50, Samples: 5440/5760, Loss: 0.005727142095565796\n",
      "Epoch: 50, Samples: 5472/5760, Loss: 0.007967561483383179\n",
      "Epoch: 50, Samples: 5504/5760, Loss: 0.012738943099975586\n",
      "Epoch: 50, Samples: 5536/5760, Loss: 0.012446612119674683\n",
      "Epoch: 50, Samples: 5568/5760, Loss: 0.0061445534229278564\n",
      "Epoch: 50, Samples: 5600/5760, Loss: 0.01186215877532959\n",
      "Epoch: 50, Samples: 5632/5760, Loss: 0.008114904165267944\n",
      "Epoch: 50, Samples: 5664/5760, Loss: 0.00636744499206543\n",
      "Epoch: 50, Samples: 5696/5760, Loss: 0.009582877159118652\n",
      "Epoch: 50, Samples: 5728/5760, Loss: 0.22269999980926514\n",
      "\n",
      "Epoch: 50\n",
      "Training set: Average loss: 0.0134\n",
      "Validation set: Average loss: 0.2701, Accuracy: 763/818 (93%)\n",
      "Training and validation complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class FlowerDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_paths, label_file, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels = np.load(label_file)\n",
    "        self.image_label_pairs = self._load_paths(image_paths)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def train_val_test_split(self, train_ratio, val_ratio):\n",
    "        dataset_length = len(self.image_label_pairs)\n",
    "        train_length = int(train_ratio * dataset_length)\n",
    "        val_length = int(val_ratio * dataset_length)\n",
    "        test_length = len(self) - train_length - val_length\n",
    "        splits = [train_length, val_length, test_length]\n",
    "        return random_split(self, splits)\n",
    "        \n",
    "    def _load_paths(self, file_path):\n",
    "        \"\"\"\n",
    "        params:  file_path, a path pointing to where the image paths are stored.\n",
    "        returns: dictionary with keys 'full_image_path', and values 'label'\n",
    "        \"\"\"\n",
    "        split_set = {}\n",
    "        with open(file_path) as f:\n",
    "            lines = f.readlines()\n",
    "            num_lines = len(lines)\n",
    "            assert(num_lines == len(self.labels))\n",
    "            for line_num in range(num_lines):\n",
    "                full_image_path = os.path.join(self.image_dir, lines[line_num].strip('\\n'))\n",
    "                split_set[full_image_path] = self.labels[line_num]\n",
    "        return pd.DataFrame.from_dict(split_set, orient='index')\n",
    "        \n",
    "    def _load_image(self, image_path):\n",
    "        img = Image.open(image_path)\n",
    "        img.load()\n",
    "        img = np.array(img)\n",
    "        if len(img.shape) == 2:\n",
    "            img = np.expand_dims(img, 2)\n",
    "            img = np.repeat(img, 3, 2)\n",
    "        return Image.fromarray(img)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_label_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # apply transforms\n",
    "        image_path = self.image_label_pairs.index[idx]\n",
    "        image = self._load_image(image_path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return {'image': image,\n",
    "                'label': label}\n",
    "    \n",
    "transform = transforms.Compose([transforms.CenterCrop(200), transforms.ToTensor()])\n",
    "dataset = FlowerDataset('data', 'image_paths.txt', 'labels.npy', transform=transform)\n",
    "train_set, val_set, test_set = dataset.train_val_test_split(0.7, 0.1)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        data, target = batch['image'].to(device), batch['label'].long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = CE(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        print('Epoch: {}, Samples: {}/{}, Loss: {}'.format(epoch, idx*batch_size,\n",
    "                                                           len(train_loader)*batch_size,\n",
    "                                                           loss.item()))\n",
    "    train_loss = torch.mean(torch.tensor(train_losses))\n",
    "    print('\\nEpoch: {}'.format(epoch))\n",
    "    print('Training set: Average loss: {:.4f}'.format(train_loss))\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "def validate(model, device, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(val_loader):\n",
    "            data, target = batch['image'].to(device), batch['label'].long().to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            # compute the batch loss\n",
    "            batch_loss = CE(output, target).item()\n",
    "            val_loss += batch_loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    # divide by the number of batches of batch size 32\n",
    "    # get the average validation over all bins\n",
    "    val_loss /= len(val_loader)\n",
    "    print('Validation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        val_loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "DATA_DIRECTORY = 'data/'\n",
    "use_cuda = 1\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(512, 102)\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "CE = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "    val_loss = validate(model, device, val_loader)\n",
    "\n",
    "    if (len(val_losses) > 0) and (val_loss < min(val_losses)):\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"Saving model (epoch {}) with lowest validation loss: {}\"\n",
    "              .format(epoch, val_loss))\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "print(\"Training and validation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "epoch_list = np.arange(1, num_epochs+1)\n",
    "plt.xticks(epoch_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(epoch_list, train_losses, label=\"Training loss\")\n",
    "plt.plot(epoch_list, val_losses, label=\"Validation loss\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 1522/1639 (93%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(test_loader):\n",
    "        data = batch['image'].to(device)\n",
    "        labels = batch['label'].long().to(device)\n",
    "        result = model(data)\n",
    "        pred = result.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    \n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(correct, len(test_loader.dataset), \n",
    "                                                       100. * correct / len(test_loader.dataset)))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
