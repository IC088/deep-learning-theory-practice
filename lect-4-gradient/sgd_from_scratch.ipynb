{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing SGD from scratch\n",
    "\n",
    "The equation for the gradient update in SGD is $w^{new}=w^{old}+\\eta (y_{i}-x_{i}'w^{old})x_{i}$, source: https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Extensions_and_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running closed-form solution...\n",
      "w: [ 0.48545013 -1.99831363] true w: [0.5, -2] diff: 0.0002145426024416703\n",
      "MSE: 0.6602361244034094\n",
      "Running SGD solution...\n",
      "max_epochs reached without convergence\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [ 0.46286258 -1.99021377] true w: [0.5, -2] diff: 0.0014749581787689586\n",
      "MSE: 0.6611316038490116\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def random_split(x, y, numtr):\n",
    "    inds = np.arange(y.size)\n",
    "    np.random.shuffle(inds)\n",
    "    xtr = x[inds[0:numtr], :]\n",
    "    ytr = y[inds[0:numtr]]\n",
    "    xv = x[inds[numtr:], :]\n",
    "    yv = y[inds[numtr:]]\n",
    "    return xtr, ytr, xv, yv\n",
    "\n",
    "def generate_1d_data(numtotal):\n",
    "    w1 = 0.5\n",
    "    w2 = -2\n",
    "    xtr, ytr = generate_2d_data(w1=w1, w2=w2, epsilon=0.8, num=int(numtotal)) #numtotal\n",
    "    xv, yv = generate_2d_data(w1=w1, w2=w2, epsilon=0.8, num=3000) #fix 3k test samples\n",
    "    return xtr, ytr, xv, yv, w1, w2\n",
    "\n",
    "def generate_1d_data2(numtotal):\n",
    "    w1 = 0.5\n",
    "    w2 = -2\n",
    "    xtr, ytr = generate_2d_data(w1=w1, w2=w2, epsilon=0.8, num=int(0.7 * numtotal)) #70% of numtotal\n",
    "    xv, yv = generate_2d_data(w1=w1, w2=w2, epsilon=0.8, num=int(0.3 * numtotal)) #30 % of numtotal\n",
    "    return xtr, ytr, xv, yv, w1, w2\n",
    "\n",
    "def generate_2d_data(w1, w2, epsilon, num):\n",
    "    x = np.random.normal(size=(num, 2))\n",
    "    y = x[:, 0] * w1 + x[:, 1] * w2 + epsilon * np.random.normal(size=(num))\n",
    "    return x, y\n",
    "\n",
    "def linreg_train(xtr, ytr, C):\n",
    "    mat = np.linalg.inv(np.dot(xtr.T, xtr) + C * np.eye(xtr.shape[1]))\n",
    "    w = np.dot(mat, np.dot(xtr.T, ytr))\n",
    "    return w\n",
    "\n",
    "def get_mse(ypred, ytrue):\n",
    "    error = np.mean((ypred - ytrue)**2)\n",
    "    return error\n",
    "\n",
    "def get_validation_loss(w, xv, yv):\n",
    "    ypred = np.dot(xv, w)\n",
    "    error = get_mse(ypred, yv) \n",
    "    return error\n",
    "\n",
    "def compute_gradient_single(w, x, y, C):\n",
    "    g0 = 2 * np.dot((np.dot(w[0], x[0]) - y), x[0])\n",
    "    g1 = 2 * np.dot((np.dot(w[1], x[1]) - y), x[1])\n",
    "    return np.array([g0, g1])\n",
    "\n",
    "def compute_gradient_minibatch(w, xb, yb, C):\n",
    "    num_samples = xb.shape[0]\n",
    "    grad = np.zeros(2)\n",
    "    for i in range(num_samples):\n",
    "        grad += 1. / float(num_samples) * compute_gradient_single(w, xb[i, :], yb[i], C)\n",
    "    return grad       \n",
    "\n",
    "def train_sgd(xtr, ytr, C, learning_rate, batch_size, max_epochs, threshold, xv, yv):\n",
    "    # randomly initialize w, close to zero but not equal to zero\n",
    "    w = np.random.normal(size=2)\n",
    "    prev_loss = get_validation_loss(w, xv, yv) \n",
    "    conv = False\n",
    "    losses = []\n",
    "    \n",
    "    for epoch_num in range(max_epochs):\n",
    "        inds = np.arange(xtr.shape[0])\n",
    "        # the order of samples should be randomized in every epoch                     \n",
    "        # get a random order of indices that are used to sample minibatches \n",
    "        np.random.shuffle(inds)\n",
    "        num_batches = int(xtr.shape[0] // batch_size)\n",
    "\n",
    "        for batch_num in range(num_batches):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = start_index + batch_size\n",
    "            batch_indices = inds[start_index : end_index]\n",
    "            batch_grad = compute_gradient_minibatch(w, xtr[batch_indices],  ytr[batch_indices], C)\n",
    "            w = w - learning_rate * batch_grad\n",
    "            \n",
    "        # get validation loss with your newly updated parameters\n",
    "        loss = get_validation_loss(w, xv, yv)\n",
    "        losses.append(loss)\n",
    "        # stop if loss change between old and current epoch is too small\n",
    "        # and update value of prev_loss\n",
    "        if (prev_loss - loss < threshold):\n",
    "            conv = True\n",
    "            \n",
    "    if not conv:\n",
    "        print('max_epochs reached without convergence')\n",
    "        \n",
    "    plot_loss(range(max_epochs), losses)\n",
    "    return w\n",
    "\n",
    "def plot_loss(iters, losses):\n",
    "    plt.plot(iters, losses)\n",
    "    plt.yscale('log')\n",
    "    plt.show()    \n",
    "\n",
    "def run1(xtr, ytr, xv, yv, w1, w2, C):\n",
    "    print(\"Running closed-form solution...\")\n",
    "    w = linreg_train(xtr, ytr, C=C) # 0.1\n",
    "    wtrue = np.asarray([w1, w2])\n",
    "    print('w:', w, 'true w:', [w1, w2], 'diff:', np.dot((w - wtrue).T, w - wtrue))\n",
    "    ypred = np.dot(xv, w)\n",
    "    e = get_mse(ypred, yv)\n",
    "    print('MSE:', e)\n",
    "    return e, np.dot((w - wtrue).T, w - wtrue)\n",
    "\n",
    "def run2(xtr, ytr, xv, yv, w1, w2, C):\n",
    "    print(\"Running SGD solution...\")\n",
    "    learning_rate = 0.01\n",
    "    batch_size = 100\n",
    "    max_epochs = 1000 \n",
    "    threshold = 0.0001\n",
    "    w = train_sgd(xtr, ytr, C, learning_rate, batch_size, max_epochs, threshold, xv, yv)\n",
    "    wtrue = np.asarray([w1, w2])\n",
    "    print('w:', w, 'true w:', [w1, w2], 'diff:', np.dot((w-wtrue).T, w-wtrue))\n",
    "    ypred = np.dot(xv, w)\n",
    "    e = get_mse(ypred, yv)\n",
    "    print('MSE:', e)\n",
    "    return e, np.dot((w - wtrue).T, w - wtrue)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    xtr, ytr, xv, yv, w1, w2 = generate_1d_data2(3000)    \n",
    "    run1(xtr, ytr, xv, yv, w1, w2, 1e-3)\n",
    "    run2(xtr, ytr, xv, yv, w1, w2, 1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
