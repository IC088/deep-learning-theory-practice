{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50.039 Theory and Practice of Deep Learning | Coding Homework 4 - Data Loading and Augmentation with CNNs\n",
    "## Joel Huang, 1002530\n",
    "\n",
    "\n",
    "# Problem 1\n",
    "\n",
    "The `ImageNetDataset` custom class is written in `dataloader.py`. It provides the following functionality:\n",
    "1. Load paths to images, and load labels from a .csv file\n",
    "2. Return the length of the dataset\n",
    "3. Provides access through `__getitem__`. This returns a transformed `dict` with the image and label.\n",
    "4. Support for grayscale images, which are repeated along the channel axis to get a 3-channel image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 1615/2500 (65%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                     std=[0.229, 0.224, 0.225])])\n",
    "dataset = dataloader.ImageNetDataset('data/imagespart', 'data.csv',\n",
    "                                     crop_size=224,\n",
    "                                     transform=transform)\n",
    "dataset_loader = DataLoader(dataset,\n",
    "                            batch_size=4,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = models.resnet18(pretrained=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(dataset_loader):\n",
    "        images, labels = batch['image'], batch['label']\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        output = model(images)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    \n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(correct, len(dataset_loader.dataset), \n",
    "                                                       100. * correct / len(dataset_loader.dataset)))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 987/2500 (39%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "dataset = dataloader.ImageNetDataset('data/imagespart', 'data.csv',\n",
    "                                     crop_size=224,\n",
    "                                     transform=transform)\n",
    "dataset_loader = DataLoader(dataset,\n",
    "                            batch_size=4,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = models.resnet18(pretrained=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(dataset_loader):\n",
    "        images, labels = batch['image'], batch['label']\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        output = model(images)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    \n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(correct, len(dataset_loader.dataset), \n",
    "                                                       100. * correct / len(dataset_loader.dataset)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "1. According to the PyTorch docs, `FiveCrop()` returns a 5-tuple of tensors. So we need to write lambda functions for the next transforms in the pipeline to deal with this shape.\n",
    "2. We can consider each of the 5 crops as separate images. So now the crops (5) and batch size (4) flatten to (20,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 1718/2500 (69%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Following the method at\n",
    "# https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.FiveCrop\n",
    "\n",
    "transform = transforms.Compose([transforms.FiveCrop(224),\n",
    "                lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]),\n",
    "                lambda norms: torch.stack([transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                std=[0.229, 0.224, 0.225])(norm) for norm in norms])])\n",
    "\n",
    "dataset = dataloader.ImageNetDataset('data/imagespart', 'data.csv',\n",
    "                                     crop_size=280,\n",
    "                                     transform=transform)\n",
    "dataset_loader = DataLoader(dataset,\n",
    "                            batch_size=4,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = models.resnet18(pretrained=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(dataset_loader):\n",
    "        batched_fives = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "                \n",
    "        batch_size, num_crops, c, h, w = batched_fives.size()\n",
    "        \n",
    "        # flatten over batch and five crops\n",
    "        stacked_fives = batched_fives.view(-1, c, h, w)\n",
    "        \n",
    "        result = model(stacked_fives)\n",
    "        result_avg = result.view(batch_size, num_crops, -1).mean(1) # avg over crops\n",
    "        pred = result_avg.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    \n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(correct, len(dataset_loader.dataset), \n",
    "                                                       100. * correct / len(dataset_loader.dataset)))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mirroring as an augmentation method\n",
    "Mirroring is a bad augmentation when the viewpoint of the object matters greatly. For example, OCR tasks, where individual characters need to be detected and classified. A 'p' might become a 'q' when mirrored!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "\n",
    "We need to compose the transforms including fivecrop.\n",
    "\n",
    "1. According to the PyTorch docs, `FiveCrop()` returns a 5-tuple of tensors. So we need to write lambda functions for the next transforms in the pipeline to deal with this shape.\n",
    "2. In this PyTorch ver (1.0.0) ResNet does not have an `AdaptiveAvgPool` layer. We allow ResNet to accept a 330x330 image by replacing the `model.avgpool`.\n",
    "\n",
    "## Variable input size for Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 1696/2500 (68%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.FiveCrop(330),\n",
    "                lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]),\n",
    "                lambda norms: torch.stack([transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                std=[0.229, 0.224, 0.225])(norm) for norm in norms])])\n",
    "\n",
    "dataset = dataloader.ImageNetDataset('data/imagespart', 'data.csv',\n",
    "                                     crop_size=330,\n",
    "                                     transform=transform)\n",
    "\n",
    "dataset_loader = DataLoader(dataset,\n",
    "                            batch_size=4,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = models.resnet18(pretrained=True).to(device)\n",
    "model.avgpool = torch.nn.AdaptiveAvgPool2d((1,1))\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(dataset_loader):\n",
    "        batched_fives = batch['image']\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        batch_size, num_crops, c, h, w = batched_fives.size()\n",
    "        \n",
    "        # flatten over batch and five crops\n",
    "        stacked_fives = batched_fives.view(-1, c, h, w).to(device)\n",
    "     \n",
    "        result = model(stacked_fives)\n",
    "        result_avg = result.view(batch_size, num_crops, -1).mean(1) # avg over crops\n",
    "        pred = result_avg.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    \n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(correct, len(dataset_loader.dataset), \n",
    "                                                       100. * correct / len(dataset_loader.dataset)))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable input size for Densenet121\n",
    "\n",
    "We need to modify the forward pass here to use `F.adaptive_avg_pool2d`. We subclass `DenseNet` for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "model_urls = {\n",
    "    'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n",
    "    'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n",
    "    'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth',\n",
    "    'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n",
    "}\n",
    "\n",
    "class DenseNetModified(models.DenseNet):\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "def densenet121(pretrained=False, **kwargs):\n",
    "    r\"\"\"Densenet-121 model from\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = DenseNetModified(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                     **kwargs)\n",
    "    if pretrained:\n",
    "        # '.'s are no longer allowed in module names, but pervious _DenseLayer\n",
    "        # has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'.\n",
    "        # They are also in the checkpoints in model_urls. This pattern is used\n",
    "        # to find such keys.\n",
    "        pattern = re.compile(\n",
    "            r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
    "        state_dict = model_zoo.load_url(model_urls['densenet121'])\n",
    "        for key in list(state_dict.keys()):\n",
    "            res = pattern.match(key)\n",
    "            if res:\n",
    "                new_key = res.group(1) + res.group(2)\n",
    "                state_dict[new_key] = state_dict[key]\n",
    "                del state_dict[key]\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 1907/2500 (76%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "transform = transforms.Compose([transforms.FiveCrop(330),\n",
    "                lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]),\n",
    "                lambda norms: torch.stack([transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                std=[0.229, 0.224, 0.225])(norm) for norm in norms])])\n",
    "\n",
    "dataset = dataloader.ImageNetDataset('data/imagespart', 'data.csv',\n",
    "                                     crop_size=330,\n",
    "                                     transform=transform)\n",
    "\n",
    "dataset_loader = DataLoader(dataset,\n",
    "                            batch_size=4,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = densenet121(pretrained=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(dataset_loader):\n",
    "        batched_fives = batch['image']\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        batch_size, num_crops, c, h, w = batched_fives.size()\n",
    "        \n",
    "        # flatten over batch and five crops\n",
    "        stacked_fives = batched_fives.view(-1, c, h, w).to(device)\n",
    "     \n",
    "        result = model(stacked_fives)\n",
    "        result_avg = result.view(batch_size, num_crops, -1).mean(1) # avg over crops\n",
    "        pred = result_avg.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    \n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(correct, len(dataset_loader.dataset), \n",
    "                                                       100. * correct / len(dataset_loader.dataset)))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
