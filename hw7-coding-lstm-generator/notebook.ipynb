{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "torch.cuda.manual_seed_all(17)\n",
    "random.seed(17)\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "valid_chars = string.ascii_letters + string.digits + \"\"\" .,:;\"'()[]!?+=/\"\"\"\n",
    "num_chars = len(valid_chars) + 2\n",
    "\n",
    "class Corpus(Dataset):\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.mapping = self._get_mapping()\n",
    "        self.text = self._load_text()\n",
    "        \n",
    "    def sentence_to_tensor(self, sentence):\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape (chars, onehot)\n",
    "        \"\"\"\n",
    "        sentence_tensor = torch.Tensor(len(sentence), len(self.mapping))\n",
    "        for position in range(len(sentence)):\n",
    "            character = sentence[position]\n",
    "            sentence_tensor[position] = self.one_hot(character)\n",
    "        return sentence_tensor\n",
    "    \n",
    "    def get_label_from_tensor(self, sequence):\n",
    "        \"\"\"\n",
    "        Returns a tensor shifted by one to the left,\n",
    "        with an EOS ('|') tag at the end.\n",
    "        \"\"\"\n",
    "        label_tensor = torch.Tensor(len(sequence))\n",
    "        for position in range(len(sequence) - 1):\n",
    "            character = torch.argmax(sequence[position + 1])\n",
    "            label_tensor[position] = character\n",
    "        label_tensor[-1] = self.mapping['|']\n",
    "        return label_tensor\n",
    "    \n",
    "    def _load_text(self):\n",
    "        with open(self.file) as f:\n",
    "            return f.read().strip().split('\\n')\n",
    "        \n",
    "    def _get_mapping(self):\n",
    "        mapping = {}\n",
    "        mapping['*'] = 0\n",
    "        for idx in range(1, len(valid_chars) + 1):\n",
    "            mapping[valid_chars[idx - 1]] = idx\n",
    "        mapping['|'] = len(valid_chars) + 1\n",
    "        return mapping\n",
    "    \n",
    "    def one_hot(self, character):\n",
    "        \"\"\"\n",
    "        Transforms a single character toa one-hot tensor.\n",
    "        \"\"\"\n",
    "        one_hot = torch.zeros(len(self.mapping))\n",
    "        index = torch.Tensor([self.mapping[character]])\n",
    "        one_hot = one_hot.scatter(0, index.long(), 1)\n",
    "        return one_hot\n",
    "    \n",
    "    def split(self, train_ratio, val_ratio, test_ratio):\n",
    "        if (train_ratio + val_ratio + test_ratio != 1.0):\n",
    "            raise Exception(\"Ratios should sum to one.\")\n",
    "        dataset_length = len(self)\n",
    "        train_length = int(train_ratio * dataset_length)\n",
    "        val_length = int(val_ratio * dataset_length)\n",
    "        test_length = len(self) - train_length - val_length\n",
    "        splits = [train_length, val_length, test_length]\n",
    "        return random_split(self, splits)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            sequence: tensor of shape (chars, onehot)\n",
    "            length: int length of sequence\n",
    "            label: shifted tensor of shape (chars, onehot)\n",
    "        \"\"\"\n",
    "        sequence = self.sentence_to_tensor(self.text[idx])\n",
    "        length = len(sequence)\n",
    "        label = self.get_label_from_tensor(sequence)\n",
    "        return sequence, length, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "class SpockNet(nn.Module):\n",
    "    def __init__(self, num_chars, hidden_size, num_layers, dropout=0.5):\n",
    "        super(SpockNet, self).__init__()\n",
    "        self.num_chars = num_chars\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(num_chars, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_chars)\n",
    "        \n",
    "    def forward(self, sentence, lengths, hc):\n",
    "        batch_size, sequence_length = sentence.size()[:-1]\n",
    "        sentence = pack_padded_sequence(sentence, lengths, batch_first=True)\n",
    "        lstm_out, hc = self.lstm(sentence, hc)\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True, padding_value=0)\n",
    "        lstm_out = lstm_out.contiguous()\n",
    "        lstm_out = lstm_out.view(-1, lstm_out.shape[2])\n",
    "        output = self.fc(lstm_out)\n",
    "        output = output.view(batch_size, sequence_length, self.num_chars)\n",
    "        return output, hc\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_().to(device),\n",
    "            weight.new(self.num_layers, batch_size, self.hidden_size).zero_().to(device))\n",
    "        return hidden\n",
    "\n",
    "class Batch:\n",
    "    def __init__(self, batch):\n",
    "        # batch: list of N tuples of (data (tensor), length (int), label (tensor))\n",
    "        batch.sort(reverse=True, key=lambda x: x[1])\n",
    "        data, _, label = zip(*batch)\n",
    "        self.lengths = torch.Tensor([len(d) for d in data])\n",
    "        self.data = pad_sequence(data, batch_first=True)\n",
    "        self.label = pad_sequence(label, batch_first=True)\n",
    "        \n",
    "def batch_function(batch):\n",
    "    return Batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import time\n",
    "\n",
    "corpus = Corpus('clean.txt')\n",
    "train, val, test = corpus.split(0.8, 0.2, 0)\n",
    "\n",
    "mapping = list(corpus.mapping.keys())\n",
    "\n",
    "batch_size = 16\n",
    "model = SpockNet(num_chars, 200, 2, dropout=0.1).to(device)\n",
    "train_loader = DataLoader(train, collate_fn=batch_function, batch_size=batch_size, num_workers=8)\n",
    "val_loader = DataLoader(val, collate_fn=batch_function, batch_size=batch_size, num_workers=8)\n",
    "\n",
    "start_letters = 'ABCDEFGHIJKLMNOPRSTUVWZ'\n",
    "max_sampling_length = 500\n",
    "temperature = 0.5\n",
    "\n",
    "def get_sample(model, hc):\n",
    "    model.eval()\n",
    "    mapping = list(corpus.mapping.keys())\n",
    "    letter = random.choice(start_letters)\n",
    "    prediction = letter\n",
    "    letter_tensor = corpus.one_hot(letter).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        hc = model.init_hidden(1)\n",
    "        while (letter != '|') and (len(prediction) < max_sampling_length):\n",
    "            output, hc = model(letter_tensor, torch.tensor([1]), hc)\n",
    "            distribution = torch.softmax(torch.div(torch.flatten(output), temperature), dim=0)\n",
    "            distribution = torch.distributions.multinomial.Multinomial(1, distribution)\n",
    "            sample = distribution.sample()\n",
    "            letter = mapping[torch.argmax(sample)]\n",
    "            if letter != '|':\n",
    "                prediction += letter\n",
    "            letter_tensor = corpus.one_hot(letter).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    return prediction\n",
    "\n",
    "def compute_accuracy(output, target):\n",
    "    \"\"\"\n",
    "    Compute accuracy assuming all [PAD] characters in\n",
    "    the target tensor have value 0.\n",
    "    \"\"\"\n",
    "    mask = target.nonzero().squeeze(1)\n",
    "    _, pred = torch.max(output, dim=1)\n",
    "    sample_count = len(mask)\n",
    "    true_positive = torch.sum(pred[mask] == target[mask].long())\n",
    "    acc = (true_positive.float() / sample_count)\n",
    "    return acc\n",
    "\n",
    "def run_train(epoch):\n",
    "    train_loss = []\n",
    "    train_accs = []\n",
    "    true_positive = 0\n",
    "    sample_count = 0\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        model.train()\n",
    "        start = time()\n",
    "        optimizer.zero_grad()\n",
    "        input = batch.data.to(device)\n",
    "        target = batch.label.to(device)\n",
    "        lengths = batch.lengths.to(device)\n",
    "        hc = model.init_hidden(len(input))\n",
    "        output, _ = model(input, lengths, hc)\n",
    "        output = output.to(device)\n",
    "        output = output.view(-1, num_chars)\n",
    "        target = target.view(-1)\n",
    "        loss = criterion(output, target.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc = compute_accuracy(output, target)\n",
    "        train_loss.append(loss.item())\n",
    "        train_accs.append(train_acc)\n",
    "        if idx % 200 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Epoch {} | Training batch {}/{} ({}%)\".format(epoch, idx + 1, len(train_loader),\n",
    "                    int((idx + 1) / len(train_loader) * 100)))\n",
    "            print(\"Training loss: {} | Training accuracy: {} | Time per batch: {}\".format(loss.item(),\n",
    "                    train_acc, time() - start))\n",
    "            prediction = get_sample(model, epoch)\n",
    "            print(prediction)\n",
    "\n",
    "    with open('experiment-{}/temperature_sample_epoch_{}'.format(experiment, epoch), 'a') as file:\n",
    "        for i in range(20):\n",
    "            prediction = get_sample(model, epoch)\n",
    "            file.write(prediction)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def run_validate(epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        true_positive = 0\n",
    "        sample_count = 0\n",
    "        start = time()\n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            input = batch.data.to(device)\n",
    "            target = batch.label.to(device)\n",
    "            lengths = batch.lengths.to(device)\n",
    "            hc = model.init_hidden(len(input))\n",
    "            output, _ = model(input, lengths, hc)\n",
    "            output = output.to(device)\n",
    "            output = output.view(-1, num_chars)\n",
    "            target = target.view(-1)\n",
    "            loss = criterion(output, target.long())\n",
    "            val_loss += loss\n",
    "            \n",
    "            mask = target.nonzero().squeeze(1)\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            sample_count += len(mask)\n",
    "            true_positive += torch.sum(pred[mask] == target[mask].long())\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            print(\"Epoch {} | Validating batch {}/{} ({}%)\".format(epoch, idx + 1, len(val_loader),\n",
    "                                                        int((idx + 1) / len(val_loader) * 100)))\n",
    "        clear_output(wait=True)\n",
    "        print(\"Epoch {} | Validating batch {}/{} ({}%)\".format(epoch, idx + 1, len(val_loader),\n",
    "                                                    int((idx + 1) / len(val_loader) * 100)))\n",
    "        val_acc = (true_positive.float() / sample_count)\n",
    "        val_loss /= len(val_loader)\n",
    "        print(\"Validation loss: {} | Validation accuracy: {} | Time for validation: {}\".format(val_loss.item(),\n",
    "                val_acc, time() - start))\n",
    "    return val_loss.item(), val_acc\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "\n",
    "num_epochs = 30\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "experiment = 'redoacc'\n",
    "import os\n",
    "if not os.path.exists('experiment-{}'.format(experiment)):\n",
    "    os.mkdir('experiment-{}'.format(experiment))\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    scheduler.step()\n",
    "    train_loss, train_acc = run_train(epoch)\n",
    "    val_loss, val_acc = run_validate(epoch)\n",
    "    training_losses.append(torch.mean(torch.Tensor(train_loss)))\n",
    "    training_accuracies.append(train_acc)\n",
    "    validation_losses.append(val_loss)\n",
    "    validation_accuracies.append(val_acc)\n",
    "    \n",
    "    torch.save(model.state_dict(), 'experiment-{}/model.pt'.format(experiment))\n",
    "    with open('experiment-{}/epochs.txt'.format(experiment), 'a') as f:\n",
    "        f.write(str(epoch) + ' ')\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    ta = np.array([x.item() for x in training_accuracies]) *100\n",
    "    va = np.array([x.item() for x in validation_accuracies]) *100\n",
    "    fig, ax = plt.subplots(figsize=(16,9))\n",
    "    ax2 = ax.twinx()\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.plot(training_losses, '--', linewidth=4, label=\"Training loss\")\n",
    "    ax.plot(validation_losses, 'r--', linewidth=4, label=\"Validation loss\")\n",
    "    ax2.plot(ta, linewidth=4, label=\"Training acc\")\n",
    "    ax2.plot(va, 'r', linewidth=4, label=\"Validation acc\")\n",
    "    ax.legend(loc='center', bbox_to_anchor=(0.8, 0.3))\n",
    "    ax2.legend(loc='center', bbox_to_anchor=(0.8, 0.8))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
